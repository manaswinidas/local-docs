<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title xmlns:d="http://docbook.org/ns/docbook">Chapter 11. Putting It All Together</title><link rel="stylesheet" type="text/css" href="Common_Content/css/epub.css"/><meta xmlns:d="http://docbook.org/ns/docbook" name="generator" content="publican v4.3.3"/><meta xmlns:d="http://docbook.org/ns/docbook" name="package" content=""/></head><body><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="chap-Performance_Tuning_Guide-Putting_It_All_Together">
      ⁠</a>Chapter 11. Putting It All Together</h1></div></div></div><div class="para">
		This book has covered many aspects of JBoss Enterprise Application Platform and opportunities for performance optimization. In presenting example performance improvements an OLTP application has been used as the test case. Before presenting the results of optimizing the performance of this application, here are details of the underlying infrastructure:
	</div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="sect-Performance_Tuning_Guide-Putting_It_All_Together-Test_Configuration">
      ⁠</a>11.1. Test Configuration</h1></div></div></div><div class="para">
			Server Configuration
		</div><div xmlns:d="http://docbook.org/ns/docbook" class="itemizedlist"><ul><li class="listitem"><div class="para">
					CPU: Intel Nehalem-based server with two, four-core Xeon E5520 processors, running at 2.27GHz, with hyper-threading enabled, giving 16 virtual cores
				</div></li><li class="listitem"><div class="para">
					Memory: 24GB of RAM
				</div></li><li class="listitem"><div class="para">
					Local storage: two 80GB solid state drives, in a RAID-0 configuration
				</div></li><li class="listitem"><div class="para">
					Operating system: Linux
				</div></li></ul></div><div class="para">
			A RAID 0 configuration was chosen for local storage to avoid I/O contention with the database
		</div><div class="para">
			Database
		</div><div class="para">
			Hosted on MySQL 5.5.x with the new asynchronous I/O capabilities and the transaction limit removed.
		</div><div class="para">
			Load Driver
		</div><div class="para">
			Laptop with four-core CPU and hyper-threading enabled, dedicated gigabit network connection between the laptop and the server.
		</div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="sect-Performance_Tuning_Guide-Putting_It_All_Together-Testing_Strategy">
      ⁠</a>11.2. Testing Strategy</h1></div></div></div><div class="para">
			The tests conducted with the number of users increased from 1 to 100 users, stepping up by 20 users at a time, with no think time between requests from the load driver. This allowed quick turnaround time per test run, and created results that were easier to present. The same loads were run using 1 second think times, but it scales to so many users that it makes it difficult to present the data. The results were very similar anyway, from a throughput improvement perspective. Before each test run the order transactions in the workload were deleted and the InnoDB buffer pool was checked for dirty pages before starting the next run. Each test was run five times to ensure reproducibility of the results. A maximum of 100 users was used because at was at that point which throughput plateaued.
		</div></div><div class="section"><div class="titlepage"><div><div><h1 class="title"><a id="sect-Performance_Tuning_Guide-Putting_It_All_Together-Test_Results">
      ⁠</a>11.3. Test Results</h1></div></div></div><div class="figure"><a id="idm139629684265808">
      ⁠</a><div class="figure-contents"><div class="mediaobject"><img src="images/img-throughput.png" width="444" alt=""/></div></div><p class="title"><strong>Figure 11.1. </strong></p></div><div class="para">
			The above graph compares the throughput of four configurations (transactions per second). Running the same workload, tests were run with the “all” configuration unmodified, expect for deploying the application and its data source, as the baseline with no optimizations. As discussed in the book, the “all” configuration is the basis for the production configuration. The next test was the “production” configuration, with nothing modified but the application and data source deployed, just like the “all” configuration. As you can see from the results, the changes made to the production configuration had a very positive effect on throughput. In fact, throughput is 39.13% higher at the peaks and that was achieved simply by deploying the application into a different configuration. The next result is using the “production” configuration again, but with the heap size set to match the fully optimized test run, which was 12GB. This test was run to demonstrate that making the heap size larger does not necessarily account for much of a performance gains since the throughput in this test was only 4.14% higher. An increase in heap size must be matched with changes in garbage collection and large page memory. Without those optimizations a larger heap may help, but not by as much as might be expected. The fully optimized result is the very best with throughput 77.82% higher.
		</div><div class="figure"><a id="idm139629681945296">
      ⁠</a><div class="figure-contents"><div class="mediaobject"><img src="images/img-response_times.png" width="444" alt=""/></div></div><p class="title"><strong>Figure 11.2. </strong></p></div><div class="para">
			Optimizations made a significant difference in response times. Not only is the fully optimized configuration the fastest by a wide margin, its slope is also less steep, showing the improved scalability of the fully optimized result. At the peak throughput level, the fully optimized result has a response time that is 45.45% lower than the baseline. At the 100 user level, the response time is 78.26% lower!
		</div></div></div></body></html>