<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 1.5.0">
<meta name="author" content="Sabby Anandan, Marius Bogoevici, Eric Bottard, Mark Fisher, Ilayaperumal Gopinathan, Gunnar Hillert, Mark Pollack, Patrick Peralta, Glenn Renfro, Thomas Risberg, Dave Syer, David Turanski, Janne Valkealahti">
<title>Spring Cloud Data Flow for Apache YARN</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic|Noto+Serif:400,400italic,700,700italic|Droid+Sans+Mono:400">
<style>
/* Asciidoctor default stylesheet | MIT License | http://asciidoctor.org */
/* Remove the comments around the @import statement below when using this as a custom stylesheet */
/*@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic|Noto+Serif:400,400italic,700,700italic|Droid+Sans+Mono:400";*/
article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section,summary{display:block}
audio,canvas,video{display:inline-block}
audio:not([controls]){display:none;height:0}
[hidden],template{display:none}
script{display:none!important}
html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}
body{margin:0}
a{background:transparent}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
abbr[title]{border-bottom:1px dotted}
b,strong{font-weight:bold}
dfn{font-style:italic}
hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}
input[type="search"]{-webkit-appearance:textfield;-moz-box-sizing:content-box;-webkit-box-sizing:content-box;box-sizing:content-box}
input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,*:before,*:after{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;font-weight:400;font-style:normal;line-height:1;position:relative;cursor:auto}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
#map_canvas img,#map_canvas embed,#map_canvas object,.map_canvas img,.map_canvas embed,.map_canvas object{max-width:none!important}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
.antialiased,body{-webkit-font-smoothing:antialiased}
img{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
p.lead,.paragraph.lead>p,#preamble>.sectionbody>.paragraph:first-of-type p{font-size:1.21875em;line-height:1.6}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0;direction:ltr}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:none}
p{font-family:inherit;font-weight:400;font-size:1em;line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #ddddd8;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em;height:0}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{font-size:1em;line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol,ul.no-bullet,ol.no-bullet{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0;font-size:1em}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.no-bullet{list-style:none}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
abbr,acronym{text-transform:uppercase;font-size:90%;color:rgba(0,0,0,.8);border-bottom:1px dotted #ddd;cursor:help}
abbr{text-transform:none}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote cite{display:block;font-size:.9375em;color:rgba(0,0,0,.6)}
blockquote cite:before{content:"\2014 \0020"}
blockquote cite a,blockquote cite a:visited{color:rgba(0,0,0,.6)}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media only screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}table{background:#fff;margin-bottom:1.25em;border:solid 1px #dedede}
table thead,table tfoot{background:#f7f8f7;font-weight:bold}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt,table tr:nth-of-type(even){background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{display:table-cell;line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.clearfix:before,.clearfix:after,.float-group:before,.float-group:after{content:" ";display:table}
.clearfix:after,.float-group:after{clear:both}
*:not(pre)>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background-color:#f7f7f8;-webkit-border-radius:4px;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre,pre>code{line-height:1.45;color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;text-rendering:optimizeSpeed}
.keyseq{color:rgba(51,51,51,.8)}
kbd{display:inline-block;color:rgba(0,0,0,.8);font-size:.75em;line-height:1.4;background-color:#f7f7f7;border:1px solid #ccc;-webkit-border-radius:3px;border-radius:3px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em white inset;box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em #fff inset;margin:-.15em .15em 0 .15em;padding:.2em .6em .2em .5em;vertical-align:middle;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menu{color:rgba(0,0,0,.8)}
b.button:before,b.button:after{position:relative;top:-1px;font-weight:400}
b.button:before{content:"[";padding:0 3px 0 2px}
b.button:after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin-left:auto;margin-right:auto;margin-top:0;margin-bottom:0;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header:before,#header:after,#content:before,#content:after,#footnotes:before,#footnotes:after,#footer:before,#footer:after{content:" ";display:table}
#header:after,#content:after,#footnotes:after,#footer:after{clear:both}
#content{margin-top:1.25em}
#content:before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #ddddd8}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #ddddd8;padding-bottom:8px}
#header .details{border-bottom:1px solid #ddddd8;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:-ms-flexbox;display:-webkit-flex;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span:before{content:"\00a0\2013\00a0"}
#header .details br+span.author:before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark:before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber:after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #ddddd8;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #efefed;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media only screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background-color:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #efefed;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #efefed;left:auto;right:0}}@media only screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}#content #toc{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:100%;background-color:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:rgba(255,255,255,.8);line-height:1.44}
.sect1{padding-bottom:.625em}
@media only screen and (min-width:768px){.sect1{padding-bottom:1.25em}}.sect1+.sect1{border-top:1px solid #efefed}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor:before,h2>a.anchor:before,h3>a.anchor:before,#toctitle>a.anchor:before,.sidebarblock>.content>.title>a.anchor:before,h4>a.anchor:before,h5>a.anchor:before,h6>a.anchor:before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock>caption.title{white-space:nowrap;overflow:visible;max-width:0}
.paragraph.lead>p,#preamble>.sectionbody>.paragraph:first-of-type p{color:rgba(0,0,0,.85)}
table.tableblock #preamble>.sectionbody>.paragraph:first-of-type p{font-size:inherit}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #ddddd8;color:rgba(0,0,0,.6)}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border-style:solid;border-width:1px;border-color:#e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;-webkit-border-radius:4px;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock pre:not(.highlight),.listingblock pre[class="highlight"],.listingblock pre[class^="highlight "],.listingblock pre.CodeRay,.listingblock pre.prettyprint{background:#f7f7f8}
.sidebarblock .literalblock pre,.sidebarblock .listingblock pre:not(.highlight),.sidebarblock .listingblock pre[class="highlight"],.sidebarblock .listingblock pre[class^="highlight "],.sidebarblock .listingblock pre.CodeRay,.sidebarblock .listingblock pre.prettyprint{background:#f2f1f1}
.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{-webkit-border-radius:4px;border-radius:4px;word-wrap:break-word;padding:1em;font-size:.8125em}
.literalblock pre.nowrap,.literalblock pre[class].nowrap,.listingblock pre.nowrap,.listingblock pre[class].nowrap{overflow-x:auto;white-space:pre;word-wrap:normal}
@media only screen and (min-width:768px){.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{font-size:.90625em}}@media only screen and (min-width:1280px){.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{font-size:1em}}.literalblock.output pre{color:#f7f7f8;background-color:rgba(0,0,0,.9)}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;-webkit-border-radius:4px;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.listingblock>.content{position:relative}
.listingblock code[data-lang]:before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:#999}
.listingblock:hover code[data-lang]:before{display:block}
.listingblock.terminal pre .command:before{content:attr(data-prompt);padding-right:.5em;color:#999}
.listingblock.terminal pre .command:not([data-prompt]):before{content:"$"}
table.pyhltable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.pyhltable td{vertical-align:top;padding-top:0;padding-bottom:0}
table.pyhltable td.code{padding-left:.75em;padding-right:0}
pre.pygments .lineno,table.pyhltable td:not(.code){color:#999;padding-left:0;padding-right:.5em;border-right:1px solid #ddddd8}
pre.pygments .lineno{display:inline-block;margin-right:.25em}
table.pyhltable .linenodiv{background:none!important;padding-right:0!important}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock blockquote p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote:before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.5em;margin-right:.5ex;text-align:right}
.quoteblock .quoteblock{margin-left:0;margin-right:0;padding:.5em 0;border-left:3px solid rgba(0,0,0,.6)}
.quoteblock .quoteblock blockquote{padding:0 0 0 .75em}
.quoteblock .quoteblock blockquote:before{display:none}
.verseblock{margin:0 1em 1.25em 1em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.05em;color:rgba(0,0,0,.6)}
.quoteblock.abstract{margin:0 0 1.25em 0;display:block}
.quoteblock.abstract blockquote,.quoteblock.abstract blockquote p{text-align:left;word-spacing:0}
.quoteblock.abstract blockquote:before,.quoteblock.abstract blockquote p:first-of-type:before{display:none}
table.tableblock{max-width:100%;border-collapse:separate}
table.tableblock td>.paragraph:last-child p>p:last-child,table.tableblock th>p:last-child,table.tableblock td>p:last-child{margin-bottom:0}
table.spread{width:100%}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all th.tableblock,table.grid-all td.tableblock{border-width:0 1px 1px 0}
table.grid-all tfoot>tr>th.tableblock,table.grid-all tfoot>tr>td.tableblock{border-width:1px 1px 0 0}
table.grid-cols th.tableblock,table.grid-cols td.tableblock{border-width:0 1px 0 0}
table.grid-all *>tr>.tableblock:last-child,table.grid-cols *>tr>.tableblock:last-child{border-right-width:0}
table.grid-rows th.tableblock,table.grid-rows td.tableblock{border-width:0 0 1px 0}
table.grid-all tbody>tr:last-child>th.tableblock,table.grid-all tbody>tr:last-child>td.tableblock,table.grid-all thead:last-child>tr>th.tableblock,table.grid-rows tbody>tr:last-child>th.tableblock,table.grid-rows tbody>tr:last-child>td.tableblock,table.grid-rows thead:last-child>tr>th.tableblock{border-bottom-width:0}
table.grid-rows tfoot>tr>th.tableblock,table.grid-rows tfoot>tr>td.tableblock{border-width:1px 0 0 0}
table.frame-all{border-width:1px}
table.frame-sides{border-width:0 1px}
table.frame-topbot{border-width:1px 0}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{display:table-cell;line-height:1.6;background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
td>div.verse{white-space:pre}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
ol>li p,ul>li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.unstyled,ol.unnumbered,ul.checklist,ul.none{list-style-type:none}
ul.unstyled,ol.unnumbered,ul.checklist{margin-left:.625em}
ul.checklist li>p:first-child>.fa-check-square-o:first-child,ul.checklist li>p:first-child>input[type="checkbox"]:first-child{margin-right:.25em}
ul.checklist li>p:first-child>input[type="checkbox"]:first-child{position:relative;top:1px}
ul.inline{margin:0 auto .625em auto;margin-left:-1.375em;margin-right:0;padding:0;list-style:none;overflow:hidden}
ul.inline>li{list-style:none;float:left;margin-left:1.375em;display:block}
ul.inline>li>*{display:block}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1{padding-right:.75em;font-weight:bold}
td.hdlist1,td.hdlist2{vertical-align:top}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist>table tr>td:first-of-type{padding:0 .75em;line-height:1}
.colist>table tr>td:last-of-type{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:solid 4px #fff;-webkit-box-shadow:0 0 0 1px #ddd;box-shadow:0 0 0 1px #ddd}
.imageblock.left,.imageblock[style*="float: left"]{margin:.25em .625em 1.25em 0}
.imageblock.right,.imageblock[style*="float: right"]{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none}
span.footnote,span.footnoteref{vertical-align:super;font-size:.875em}
span.footnote a,span.footnoteref a{text-decoration:none}
span.footnote a:active,span.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em 0;border-width:1px 0 0 0}
#footnotes .footnote{padding:0 .375em;line-height:1.3;font-size:.875em;margin-left:1.2em;text-indent:-1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
.gist .file-data>table{border:0;background:#fff;width:100%;margin-bottom:0}
.gist .file-data>table td.line-data{width:99%}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background-color:#00fafa}
.black{color:#000}
.black-background{background-color:#000}
.blue{color:#0000bf}
.blue-background{background-color:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background-color:#fa00fa}
.gray{color:#606060}
.gray-background{background-color:#7d7d7d}
.green{color:#006000}
.green-background{background-color:#007d00}
.lime{color:#00bf00}
.lime-background{background-color:#00fa00}
.maroon{color:#600000}
.maroon-background{background-color:#7d0000}
.navy{color:#000060}
.navy-background{background-color:#00007d}
.olive{color:#606000}
.olive-background{background-color:#7d7d00}
.purple{color:#600060}
.purple-background{background-color:#7d007d}
.red{color:#bf0000}
.red-background{background-color:#fa0000}
.silver{color:#909090}
.silver-background{background-color:#bcbcbc}
.teal{color:#006060}
.teal-background{background-color:#007d7d}
.white{color:#bfbfbf}
.white-background{background-color:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background-color:#fafa00}
span.icon>.fa{cursor:default}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note:before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip:before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning:before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution:before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important:before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background-color:rgba(0,0,0,.8);-webkit-border-radius:100px;border-radius:100px;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]:after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
h1,h2{letter-spacing:-.01em}
dt,th.tableblock,td.content{text-rendering:optimizeLegibility}
p,td.content{letter-spacing:-.01em}
p strong,td.content strong{letter-spacing:-.005em}
p,blockquote,dt,td.content{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background-color:#fffef7;border-color:#e0e0dc;-webkit-box-shadow:0 1px 4px #e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@media print{@page{margin:1.25cm .75cm}
*{-webkit-box-shadow:none!important;box-shadow:none!important;text-shadow:none!important}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare):after,a[href^="https:"]:not(.bare):after,a[href^="mailto:"]:not(.bare):after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]:after{content:" (" attr(title) ")"}
pre,blockquote,tr,img{page-break-inside:avoid}
thead{display:table-header-group}
img{max-width:100%!important}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #ddddd8!important;padding-bottom:0!important}
.sect1{padding-bottom:0!important}
.sect1+.sect1{border:0!important}
#header>h1:first-child{margin-top:1.25rem}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em 0}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span:before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]:before{display:block}
#footer{background:none!important;padding:0 .9375em}
#footer-text{color:rgba(0,0,0,.6)!important;font-size:.9em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css">
</head>
<body class="book toc2 toc-left">
<div id="header">
<h1>Spring Cloud Data Flow for Apache YARN</h1>
<div class="details">
<span id="author" class="author">Sabby Anandan, Marius Bogoevici, Eric Bottard, Mark Fisher, Ilayaperumal Gopinathan, Gunnar Hillert, Mark Pollack, Patrick Peralta, Glenn Renfro, Thomas Risberg, Dave Syer, David Turanski, Janne Valkealahti</span><br>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel0">
<li><a href="#preface">Preface</a>
<ul class="sectlevel1">
<li><a href="#dataflow-documentation-about">1. About the documentation</a></li>
<li><a href="#dataflow-documentation-getting-help">2. Getting help</a></li>
</ul>
</li>
<li><a href="#introduction">Introduction</a>
<ul class="sectlevel1">
<li><a href="#dataflow-yarn-intro">3. Introducing Spring Cloud Data Flow for Apache YARN project</a></li>
<li><a href="#spring-cloud-data-flow-overview">4. Spring Cloud Data Flow</a></li>
<li><a href="#spring-cloud-stream-overview">5. Spring Cloud Stream</a></li>
<li><a href="#spring-cloud-task-feature-overview">6. Spring Cloud Task</a></li>
</ul>
</li>
<li><a href="#architecture">Architecture</a>
<ul class="sectlevel1">
<li><a href="#arch-intro">7. Introduction</a></li>
<li><a href="#arch-microservice-style">8. Microservice Architectural Style</a>
<ul class="sectlevel2">
<li><a href="#arch-comparison">8.1. Comparison to other Platform architectures</a></li>
</ul>
</li>
<li><a href="#arch-streaming-apps">9. Streaming Applications</a>
<ul class="sectlevel2">
<li><a href="#arch-streaming-imperative-programming">9.1. Imperative Programming Model</a></li>
<li><a href="#arch-streaming-functional-programming">9.2. Functional Programming Model</a></li>
</ul>
</li>
<li><a href="#arch-streams">10. Streams</a>
<ul class="sectlevel2">
<li><a href="#arch-streams-topologies">10.1. Topologies</a></li>
<li><a href="#arch-streams-concurrency">10.2. Concurrency</a></li>
<li><a href="#arch-streams-partitioning">10.3. Partitioning</a></li>
<li><a href="#arch-streams-delivery">10.4. Message Delivery Guarantees</a></li>
</ul>
</li>
<li><a href="#arch-analytics">11. Analytics</a></li>
<li><a href="#arch-task">12. Task Applications</a></li>
<li><a href="#arch-data-flow-server">13. Data Flow Server</a>
<ul class="sectlevel2">
<li><a href="#arch-data-flow-server-endpoints">13.1. Endpoints</a></li>
<li><a href="#arch-data-flow-server-customization">13.2. Customization</a></li>
<li><a href="#arch-data-flow-server-security">13.3. Security</a></li>
</ul>
</li>
<li><a href="#arch-runtime">14. Runtime</a>
<ul class="sectlevel2">
<li><a href="#arch-runtime-fault-tolerance">14.1. Fault Tolerance</a></li>
<li><a href="#arch-runtime-resource-management">14.2. Resource Management</a></li>
<li><a href="#arch-runtime-scaling">14.3. Scaling at runtime</a></li>
<li><a href="#arch-application-versioning">14.4. Application Versioning</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_spring_cloud_data_flow_runtime">Spring Cloud Data Flow Runtime</a>
<ul class="sectlevel1">
<li><a href="#yarn-deploying-on-yarn">15. Deploying on YARN</a>
<ul class="sectlevel2">
<li><a href="#_prerequisites">15.1. Prerequisites</a></li>
<li><a href="#_download_and_extract_distribution">15.2. Download and Extract Distribution</a></li>
<li><a href="#_configure_settings">15.3. Configure Settings</a></li>
<li><a href="#_start_server">15.4. Start Server</a></li>
<li><a href="#_connect_shell">15.5. Connect Shell</a></li>
<li><a href="#_register_applications">15.6. Register Applications</a>
<ul class="sectlevel3">
<li><a href="#_sourcing_applications_from_hdfs">15.6.1. Sourcing Applications from HDFS</a></li>
</ul>
</li>
<li><a href="#_create_stream">15.7. Create Stream</a></li>
<li><a href="#_create_task">15.8. Create Task</a></li>
<li><a href="#_using_yarn_cli">15.9. Using YARN Cli</a>
<ul class="sectlevel3">
<li><a href="#_check_yarn_app_statuses">15.9.1. Check YARN App Statuses</a></li>
<li><a href="#_push_apps">15.9.2. Push Apps</a></li>
</ul>
</li>
<li><a href="#_using_metric_collectors">15.10. Using Metric Collectors</a></li>
</ul>
</li>
<li><a href="#yarn-deploying-on-ambari">16. Deploying on AMBARI</a>
<ul class="sectlevel2">
<li><a href="#_install_ambari_server">16.1. Install Ambari Server</a></li>
<li><a href="#_deploy_data_flow">16.2. Deploy Data Flow</a></li>
<li><a href="#_using_configuration">16.3. Using Configuration</a>
<ul class="sectlevel3">
<li><a href="#_change_datasource">16.3.1. Change Datasource</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#yarn-configure-settings">17. Configuring Runtime Settings and Environment</a>
<ul class="sectlevel2">
<li><a href="#_generic_app_settings">17.1. Generic App Settings</a></li>
<li><a href="#_configuring_application_resources">17.2. Configuring Application Resources</a></li>
<li><a href="#_configure_base_directory">17.3. Configure Base Directory</a></li>
<li><a href="#yarn-pre-populate">17.4. Pre-populate Applications</a></li>
<li><a href="#_configure_logging">17.5. Configure Logging</a></li>
<li><a href="#_configure_metrics">17.6. Configure Metrics</a></li>
<li><a href="#_global_yarn_memory_settings">17.7. Global YARN Memory Settings</a></li>
<li><a href="#_configure_kerberos">17.8. Configure Kerberos</a>
<ul class="sectlevel3">
<li><a href="#_working_with_kerberized_kafka">17.8.1. Working with Kerberized Kafka</a></li>
</ul>
</li>
<li><a href="#_configure_hdfs_ha">17.9. Configure Hdfs HA</a></li>
<li><a href="#_configure_database">17.10. Configure Database</a></li>
<li><a href="#_configure_network_discovery">17.11. Configure Network Discovery</a></li>
</ul>
</li>
<li><a href="#yarn-how-it-works">18. How YARN Deployment Works</a></li>
<li><a href="#yarn-troubleshooting">19. Troubleshooting</a></li>
<li><a href="#_using_sandboxes">20. Using Sandboxes</a>
<ul class="sectlevel2">
<li><a href="#_hortonworks_sandbox">20.1. Hortonworks Sandbox</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#streams">Streams</a>
<ul class="sectlevel1">
<li><a href="#spring-cloud-dataflow-stream-intro">21. Introduction</a></li>
<li><a href="#_stream_dsl">22. Stream DSL</a></li>
<li><a href="#spring-cloud-dataflow-register-apps">23. Register a Stream App</a>
<ul class="sectlevel2">
<li><a href="#spring-cloud-dataflow-stream-app-whitelisting">23.1. Whitelisting application properties</a></li>
<li><a href="#spring-cloud-dataflow-stream-app-metadata-artifact">23.2. Creating and using a dedicated metadata artifact</a>
<ul class="sectlevel3">
<li><a href="#_using_the_companion_artifact">23.2.1. Using the companion artifact</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#custom-applications">24. Creating custom applications</a></li>
<li><a href="#spring-cloud-dataflow-create-stream">25. Creating a Stream</a>
<ul class="sectlevel2">
<li><a href="#_application_properties">25.1. Application properties</a>
<ul class="sectlevel3">
<li><a href="#_passing_application_properties_when_creating_a_stream">25.1.1. Passing application properties when creating a stream</a></li>
</ul>
</li>
<li><a href="#_deployment_properties">25.2. Deployment properties</a>
<ul class="sectlevel3">
<li><a href="#_application_properties_versus_deployer_properties">25.2.1. Application properties versus Deployer properties</a></li>
<li><a href="#_passing_instance_count_as_deployment_property">25.2.2. Passing instance count as deployment property</a></li>
<li><a href="#_inline_vs_file_reference_properties">25.2.3. Inline vs file reference properties</a></li>
<li><a href="#_passing_application_properties_when_deploying_a_stream">25.2.4. Passing application properties when deploying a stream</a></li>
<li><a href="#passing_producer_consumer_properties">25.2.5. Passing Spring Cloud Stream properties for the application</a></li>
<li><a href="#_passing_per_binding_producer_consumer_properties">25.2.6. Passing per-binding producer consumer properties</a></li>
<li><a href="#passing_stream_partition_properties">25.2.7. Passing stream partition properties during stream deployment</a></li>
<li><a href="#passing_content_type_properties">25.2.8. Passing application content type properties</a></li>
<li><a href="#_overriding_application_properties_during_stream_deployment">25.2.9. Overriding application properties during stream deployment</a></li>
</ul>
</li>
<li><a href="#spring-cloud-dataflow-global-properties">25.3. Common application properties</a></li>
</ul>
</li>
<li><a href="#spring-cloud-dataflow-destroy-stream">26. Destroying a Stream</a></li>
<li><a href="#spring-cloud-dataflow-deploy-undeploy-stream">27. Deploying and Undeploying Streams</a></li>
<li><a href="#spring-cloud-dataflow-stream-app-types">28. Other Source and Sink Application Types</a></li>
<li><a href="#spring-cloud-dataflow-simple-stream">29. Simple Stream Processing</a></li>
<li><a href="#spring-cloud-dataflow-stream-partitions">30. Stateful Stream Processing</a></li>
<li><a href="#spring-cloud-dataflow-stream-tap-dsl">31. Tap a Stream</a></li>
<li><a href="#spring-cloud-dataflow-stream-app-labels">32. Using Labels in a Stream</a></li>
<li><a href="#spring-cloud-dataflow-stream-explicit-destination-names">33. Explicit Broker Destinations in a Stream</a></li>
<li><a href="#spring-cloud-dataflow-stream-advanced">34. Directed Graphs in a Stream</a></li>
<li><a href="#spring-cloud-dataflow-stream-multi-binder">35. Stream applications with multiple binder configurations</a></li>
</ul>
</li>
<li><a href="#spring-cloud-task">Tasks</a>
<ul class="sectlevel1">
<li><a href="#spring-cloud-dataflow-task-intro">36. Introducing Spring Cloud Task</a></li>
<li><a href="#_the_lifecycle_of_a_task">37. The Lifecycle of a task</a>
<ul class="sectlevel2">
<li><a href="#_creating_a_custom_task_application">37.1. Creating a custom Task Application</a></li>
<li><a href="#_registering_a_task_application">37.2. Registering a Task Application</a></li>
<li><a href="#_creating_a_task">37.3. Creating a Task</a></li>
<li><a href="#_launching_a_task">37.4. Launching a Task</a>
<ul class="sectlevel3">
<li><a href="#_common_application_properties">37.4.1. Common application properties</a></li>
</ul>
</li>
<li><a href="#_reviewing_task_executions">37.5. Reviewing Task Executions</a></li>
<li><a href="#_destroying_a_task">37.6. Destroying a Task</a></li>
</ul>
</li>
<li><a href="#spring-cloud-dataflow-task-repository">38. Task Repository</a>
<ul class="sectlevel2">
<li><a href="#_configuring_the_task_execution_repository">38.1. Configuring the Task Execution Repository</a>
<ul class="sectlevel3">
<li><a href="#_local">38.1.1. Local</a></li>
<li><a href="#_task_application_repository">38.1.2. Task Application Repository</a></li>
</ul>
</li>
<li><a href="#_datasource">38.2. Datasource</a></li>
</ul>
</li>
<li><a href="#spring-cloud-dataflow-task-events">39. Subscribing to Task/Batch Events</a></li>
<li><a href="#spring-cloud-dataflow-launch-tasks-from-stream">40. Launching Tasks from a Stream</a>
<ul class="sectlevel2">
<li><a href="#_triggertask">40.1. TriggerTask</a></li>
<li><a href="#_tasklaunchrequest_transform">40.2. TaskLaunchRequest-transform</a></li>
</ul>
</li>
<li><a href="#spring-cloud-dataflow-composed-tasks">41. Composed Tasks</a>
<ul class="sectlevel2">
<li><a href="#_configuring_the_composed_task_runner_in_spring_cloud_data_flow">41.1. Configuring the Composed Task Runner in Spring Cloud Data Flow</a>
<ul class="sectlevel3">
<li><a href="#_registering_the_composed_task_runner_application">41.1.1. Registering the Composed Task Runner application</a></li>
<li><a href="#_configuring_the_composed_task_runner_application">41.1.2. Configuring the Composed Task Runner application</a></li>
</ul>
</li>
<li><a href="#_creating_launching_and_destroying_a_composed_task">41.2. Creating, Launching, and Destroying a Composed Task</a>
<ul class="sectlevel3">
<li><a href="#_creating_a_composed_task">41.2.1. Creating a Composed Task</a>
<ul class="sectlevel4">
<li><a href="#_task_application_parameters">Task Application Parameters</a></li>
</ul>
</li>
<li><a href="#_launching_a_composed_task">41.2.2. Launching a Composed Task</a>
<ul class="sectlevel4">
<li><a href="#_exit_statuses">Exit Statuses</a></li>
</ul>
</li>
<li><a href="#_destroying_a_composed_task">41.2.3. Destroying a Composed Task</a></li>
<li><a href="#_stopping_a_composed_task">41.2.4. Stopping a Composed Task</a></li>
<li><a href="#_restarting_a_composed_task">41.2.5. Restarting a Composed Task</a></li>
</ul>
</li>
<li><a href="#_composed_task_dsl">41.3. Composed Task DSL</a>
<ul class="sectlevel3">
<li><a href="#_conditional_execution">41.3.1. Conditional Execution</a></li>
<li><a href="#_transitional_execution">41.3.2. Transitional Execution</a>
<ul class="sectlevel4">
<li><a href="#_basic_transition">Basic Transition</a></li>
<li><a href="#_transition_with_a_wildcard">Transition With a Wildcard</a></li>
<li><a href="#_transition_with_a_following_conditional_execution">Transition With a Following Conditional Execution</a></li>
</ul>
</li>
<li><a href="#_split_execution">41.3.3. Split Execution</a>
<ul class="sectlevel4">
<li><a href="#_split_containing_conditional_execution">Split Containing Conditional Execution</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#dashboard">Dashboard</a>
<ul class="sectlevel1">
<li><a href="#dashboard-introduction">42. Introduction</a></li>
<li><a href="#dashboard-apps">43. Apps</a>
<ul class="sectlevel2">
<li><a href="#_bulk_import_of_applications">43.1. Bulk Import of Applications</a></li>
</ul>
</li>
<li><a href="#dashboard-runtime">44. Runtime</a></li>
<li><a href="#dashboard-streams">45. Streams</a></li>
<li><a href="#dashboard-flo-streams-designer">46. Create Stream</a></li>
<li><a href="#dashboard-tasks">47. Tasks</a>
<ul class="sectlevel2">
<li><a href="#dashboard-tasks-apps">47.1. Apps</a>
<ul class="sectlevel3">
<li><a href="#_create_a_task_definition_from_a_selected_task_app">47.1.1. Create a Task Definition from a selected Task App</a></li>
<li><a href="#_view_task_app_details">47.1.2. View Task App Details</a></li>
</ul>
</li>
<li><a href="#dashboard-task-definition">47.2. Definitions</a>
<ul class="sectlevel3">
<li><a href="#_creating_task_definitions_using_the_bulk_define_interface">47.2.1. Creating Task Definitions using the bulk define interface</a></li>
<li><a href="#_creating_composed_task_definitions">47.2.2. Creating Composed Task Definitions</a></li>
<li><a href="#_launching_tasks">47.2.3. Launching Tasks</a></li>
</ul>
</li>
<li><a href="#dashboard-tasks-executions">47.3. Executions</a></li>
</ul>
</li>
<li><a href="#dashboard-jobs">48. Jobs</a>
<ul class="sectlevel2">
<li><a href="#dashboard-job-executions-list">48.1. List job executions</a>
<ul class="sectlevel3">
<li><a href="#dashboard-job-executions-details">48.1.1. Job execution details</a></li>
<li><a href="#dashboard-job-executions-steps">48.1.2. Step execution details</a></li>
<li><a href="#dashboard-job-executions-steps-progress">48.1.3. Step Execution Progress</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#dashboard-analytics">49. Analytics</a></li>
</ul>
</li>
<li><a href="#howto">&#8216;How-to&#8217; guides</a>
<ul class="sectlevel1">
<li><a href="#_configure_maven_properties">50. Configure Maven Properties</a></li>
<li><a href="#_logging">51. Logging</a>
<ul class="sectlevel2">
<li><a href="#_deployment_logs">51.1. Deployment Logs</a></li>
<li><a href="#_application_logs">51.2. Application Logs</a></li>
</ul>
</li>
<li><a href="#faqs">52. Frequently asked questions</a>
<ul class="sectlevel2">
<li><a href="#_advanced_spel_expressions">52.1. Advanced SpEL expressions</a></li>
<li><a href="#dataflow-jdbc-sink">52.2. How to use JDBC-sink?</a></li>
<li><a href="#dataflow-multiple-brokers">52.3. How to use multiple message-binders?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#appendix">Appendices</a>
<ul class="sectlevel1">
<li><a href="#migration-guide">Appendix A: Migrating from Spring XD to Spring Cloud Data Flow</a>
<ul class="sectlevel2">
<li><a href="#_terminology_changes">A.1. Terminology Changes</a></li>
<li><a href="#_modules_to_applications">A.2. Modules to Applications</a>
<ul class="sectlevel3">
<li><a href="#_custom_applications">A.2.1. Custom Applications</a></li>
<li><a href="#_application_registration">A.2.2. Application Registration</a></li>
<li><a href="#_application_properties_2">A.2.3. Application Properties</a></li>
</ul>
</li>
<li><a href="#_message_bus_to_binders">A.3. Message Bus to Binders</a>
<ul class="sectlevel3">
<li><a href="#_message_bus">A.3.1. Message Bus</a></li>
<li><a href="#_binders">A.3.2. Binders</a></li>
<li><a href="#_named_channels">A.3.3. Named Channels</a></li>
<li><a href="#_directed_graphs">A.3.4. Directed Graphs</a></li>
</ul>
</li>
<li><a href="#_batch_to_tasks">A.4. Batch to Tasks</a></li>
<li><a href="#_shell_dsl_commands">A.5. Shell/DSL Commands</a></li>
<li><a href="#_rest_api">A.6. REST-API</a></li>
<li><a href="#_ui_flo">A.7. UI / Flo</a></li>
<li><a href="#_architecture_components">A.8. Architecture Components</a>
<ul class="sectlevel3">
<li><a href="#_zookeeper">A.8.1. ZooKeeper</a></li>
<li><a href="#_rdbms">A.8.2. RDBMS</a></li>
<li><a href="#_redis">A.8.3. Redis</a></li>
<li><a href="#_cluster_topology">A.8.4. Cluster Topology</a></li>
</ul>
</li>
<li><a href="#_central_configuration">A.9. Central Configuration</a></li>
<li><a href="#_distribution">A.10. Distribution</a></li>
<li><a href="#_hadoop_distribution_compatibility">A.11. Hadoop Distribution Compatibility</a></li>
<li><a href="#_yarn_deployment">A.12. YARN Deployment</a></li>
<li><a href="#_use_case_comparison">A.13. Use Case Comparison</a>
<ul class="sectlevel3">
<li><a href="#_use_case_1">A.13.1. Use Case #1</a></li>
<li><a href="#_use_case_2">A.13.2. Use Case #2</a></li>
<li><a href="#_use_case_3">A.13.3. Use Case #3</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#building">Appendix B: Building</a>
<ul class="sectlevel2">
<li><a href="#_documentation">B.1. Documentation</a></li>
<li><a href="#_working_with_the_code">B.2. Working with the code</a>
<ul class="sectlevel3">
<li><a href="#_importing_into_eclipse_with_m2eclipse">B.2.1. Importing into eclipse with m2eclipse</a></li>
<li><a href="#_importing_into_eclipse_without_m2eclipse">B.2.2. Importing into eclipse without m2eclipse</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#contributing">Appendix C: Contributing</a>
<ul class="sectlevel2">
<li><a href="#_sign_the_contributor_license_agreement">C.1. Sign the Contributor License Agreement</a></li>
<li><a href="#_code_conventions_and_housekeeping">C.2. Code Conventions and Housekeeping</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<h1 id="preface" class="sect0">Preface</h1>
<div class="sect1">
<h2 id="dataflow-documentation-about">1. About the documentation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Spring Cloud Data Flow for Apache Yarn reference guide is available as <a href="http://docs.spring.io/spring-cloud-dataflow-server-yarn/docs/current/reference/html">html</a>,
<a href="http://docs.spring.io/spring-cloud-dataflow-server-yarn/docs/current/reference/pdf/spring-cloud-dataflow-reference.pdf">pdf</a>
and <a href="http://docs.spring.io/spring-cloud-dataflow-server-yarn/docs/current/reference/epub/spring-cloud-dataflow-reference.epub">epub</a> documents. The latest copy
is available at <a href="http://docs.spring.io/spring-cloud-dataflow-server-yarn/docs/current-SNAPSHOT/reference/html/" class="bare">docs.spring.io/spring-cloud-dataflow-server-yarn/docs/current-SNAPSHOT/reference/html/</a>.</p>
</div>
<div class="paragraph">
<p>Copies of this document may be made for your own use and for
distribution to others, provided that you do not charge any fee for such copies and
further provided that each copy contains this Copyright Notice, whether distributed in
print or electronically.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="dataflow-documentation-getting-help">2. Getting help</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Having trouble with Spring Cloud Data Flow, We&#8217;d like to help!</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Try the <a href="howto.html#howto">How-to&#8217;s</a>&#8201;&#8212;&#8201;they provide solutions to the most common
questions.</p>
</li>
<li>
<p>Ask a question - we monitor <a href="http://stackoverflow.com">stackoverflow.com</a> for questions
tagged with <a href="http://stackoverflow.com/tags/spring-cloud"><code>spring-cloud</code></a>.</p>
</li>
<li>
<p>Report bugs with Spring Cloud Dataflow for Apache YARN at <a href="https://github.com/spring-cloud/spring-cloud-dataflow-server-yarn/issues" class="bare">github.com/spring-cloud/spring-cloud-dataflow-server-yarn/issues</a>.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
All of Spring Cloud Data Flow is open source, including the documentation! If you find problems
with the docs; or if you just want to improve them, please <a href="http://github.com/spring-cloud/spring-cloud-dataflow-server-yarn/tree/master">get involved</a>.
</td>
</tr>
</table>
</div>
</div>
</div>
<h1 id="introduction" class="sect0">Introduction</h1>
<div class="sect1">
<h2 id="dataflow-yarn-intro">3. Introducing Spring Cloud Data Flow for Apache YARN project</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This project provides support for orchestrating long-running (<em>streaming</em>) and short-lived (<em>task/batch</em>) data microservices to Apache YARN.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-data-flow-overview">4. Spring Cloud Data Flow</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Spring Cloud Data Flow is a cloud-native orchestration service for composable data microservices on modern runtimes. With Spring Cloud Data Flow, developers can create and orchestrate data pipelines for common use cases such as data ingest, real-time analytics, and data import/export.</p>
</div>
<div class="paragraph">
<p>The Spring Cloud Data Flow architecture consists of a server that deploys <a href="http://docs.spring.io/spring-cloud-dataflow/docs/1.2.2.RELEASE/reference/htmlsingle/#streams">Streams</a> and <a href="http://docs.spring.io/spring-cloud-dataflow/docs/1.2.2.RELEASE/reference/htmlsingle/#spring-cloud-task-overview">Tasks</a>.  Streams are defined using a <a href="http://docs.spring.io/spring-cloud-dataflow/docs/1.2.2.RELEASE/reference/html/_dsl_syntax.html">DSL</a> or visually through the browser based designer UI.  Streams are based on the <a href="http://cloud.spring.io/spring-cloud-stream/">Spring Cloud Stream</a> programming model while Tasks are based on the <a href="http://cloud.spring.io/spring-cloud-task/">Spring Cloud Task</a> programming model. The sections below describe more information about creating your own custom Streams and Tasks</p>
</div>
<div class="paragraph">
<p>For more details about the core architecture components and the supported features, please review Spring Cloud Data Flow&#8217;s <a href="http://docs.spring.io/spring-cloud-dataflow/docs/1.2.2.RELEASE/reference/htmlsingle/">core reference guide</a>. There&#8217;re several <a href="https://github.com/spring-cloud/spring-cloud-dataflow-samples">samples</a> available for reference.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-stream-overview">5. Spring Cloud Stream</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Spring Cloud Stream is a framework for building message-driven microservice applications. Spring Cloud Stream builds upon Spring Boot to create standalone, production-grade Spring applications, and uses Spring Integration to provide connectivity to message brokers. It provides opinionated configuration of middleware from several vendors, introducing the concepts of persistent publish-subscribe semantics, consumer groups, and partitions.</p>
</div>
<div class="paragraph">
<p>For more details about the core framework components and the supported features, please review Spring Cloud Stream&#8217;s <a href="http://docs.spring.io/spring-cloud-stream/docs/current-SNAPSHOT/reference/htmlsingle/">reference guide</a>.</p>
</div>
<div class="paragraph">
<p>There&#8217;s a rich ecosystem of Spring Cloud Stream <a href="http://docs.spring.io/spring-cloud-stream-app-starters/docs/current-SNAPSHOT/reference/htmlsingle">Application-Starters</a> that can be used either as standalone data microservice applications or in Spring Cloud Data Flow. For convenience, we have generated RabbitMQ and Apache Kafka variants of these application-starters that are available for use from <a href="http://repo.spring.io/libs-snapshot/org/springframework/cloud/stream/app/">Maven Repo</a> and <a href="https://hub.docker.com/r/springcloudstream/">Docker Hub</a> as maven artifacts and docker images, respectively.</p>
</div>
<div class="paragraph">
<p>Do you have a requirement to develop custom applications? No problem. Refer to this guide to create <a href="http://docs.spring.io/spring-cloud-stream-app-starters/docs/current-SNAPSHOT/reference/htmlsingle/#_creating_custom_artifacts">custom stream applications</a>. There&#8217;re several <a href="https://github.com/spring-cloud/spring-cloud-stream-samples">samples</a> available for reference.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-task-feature-overview">6. Spring Cloud Task</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Spring Cloud Task makes it easy to create short-lived microservices. We provide capabilities that allow short-lived JVM processes to be executed on demand in a production environment.</p>
</div>
<div class="paragraph">
<p>For more details about the core framework components and the supported features, please review Spring Cloud Task&#8217;s <a href="http://docs.spring.io/spring-cloud-task/1.2.1.RELEASE/reference/htmlsingle/">reference guide</a>.</p>
</div>
<div class="paragraph">
<p>There&#8217;s a rich ecosystem of Spring Cloud Task <a href="http://docs.spring.io/spring-cloud-task-app-starters/docs/current-SNAPSHOT/reference/htmlsingle">Application-Starters</a> that can be used either as standalone data microservice applications or in Spring Cloud Data Flow. For convenience, the generated application-starters are available for use from <a href="http://repo.spring.io/libs-snapshot/org/springframework/cloud/task/app/">Maven Repo</a>. There are several <a href="https://github.com/spring-cloud/spring-cloud-task/tree/master/spring-cloud-task-samples">samples</a> available for reference.</p>
</div>
</div>
</div>
<h1 id="architecture" class="sect0">Architecture</h1>
<div class="sect1">
<h2 id="arch-intro">7. Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Spring Cloud Data Flow simplifies the development and deployment of applications focused on data processing use-cases.  The major concepts of the architecture are Applications, the Data Flow Server, and the target runtime.</p>
</div>
<div class="paragraph">
<p>Applications come in two flavors</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Long lived Stream applications where an unbounded amount of data is consumed or produced via messaging middleware.</p>
</li>
<li>
<p>Short lived Task applications that process a finite set of data and then terminate.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Depending on the runtime, applications can be packaged in two ways</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Spring Boot uber-jar that is hosted in a maven repository, file, http or any other Spring resource implementation.</p>
</li>
<li>
<p>Docker</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The runtime is the place where applications execute.  The target runtimes for applications are platforms that you may already be using for other application deployments.</p>
</div>
<div class="paragraph">
<p>The supported runtimes are</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Cloud Foundry</p>
</li>
<li>
<p>Apache YARN</p>
</li>
<li>
<p>Kubernetes</p>
</li>
<li>
<p>Apache Mesos</p>
</li>
<li>
<p>Local Server for development</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>There is a deployer Service Provider Interface (SPI) that enables you to extend Data Flow to deploy onto other runtimes, for example to support Docker Swarm. There are community implementations of Hashicorp&#8217;s Nomad and RedHat Openshift is available. We look forward to working with the community for further contributions!</p>
</div>
<div class="paragraph">
<p>The component that is responsible for deploying applications to a runtime is the Data Flow Server.  There is a Data Flow Server executable jar provided for each of the target runtimes.  The Data Flow server is responsible for interpreting</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A stream DSL that describes the logical flow of data through multiple applications.</p>
</li>
<li>
<p>A deployment manifest that describes the mapping of applications onto the runtime. For example, to set the initial number of instances, memory requirements, and data partitioning.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>As an example, the DSL to describe the flow of data from an http source to an Apache Cassandra sink would be written as “http | cassandra”.  These names in the DSL are registered with the Data Flow Server and map onto application artifacts that can be hosted in Maven or Docker repositories.  Many source, processor, and sink applications for common use-cases (e.g. jdbc, hdfs, http, router) are provided by the Spring Cloud Data Flow team.  The pipe symbol represents the communication between the two applications via messaging middleware. The two messaging middleware brokers that are supported are</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Apache Kafka</p>
</li>
<li>
<p>RabbitMQ</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In the case of Kafka, when deploying the stream, the Data Flow server is responsible to create the topics that correspond to each pipe symbol and configure each application to produce or consume from the topics so the desired flow of data is achieved.</p>
</div>
<div class="paragraph">
<p>The interaction of the main components is shown below</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-arch.png" alt="The Spring Cloud Data Flow High Level Architecture">
</div>
<div class="title">Figure 1. The Spring Cloud Data High Level Architecure</div>
</div>
<div class="paragraph">
<p>In this diagram a DSL description of a stream is POSTed to the Data Flow Server.  Based on the mapping of DSL application names to Maven and Docker artifacts, the http-source and cassandra-sink applications are deployed on the target runtime.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="arch-microservice-style">8. Microservice Architectural Style</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Data Flow Server deploys applications onto the target runtime that conform to the microservice architectural style.  For example, a stream represents a high level application that consists of multiple small microservice applications each running in their own process.  Each microservice application can be scaled up or down independent of the other and each has their own versioning lifecycle.</p>
</div>
<div class="paragraph">
<p>Both Streaming and Task based microservice applications build upon Spring Boot as the foundational library.
This gives all microservice applications functionality such as health checks, security, configurable logging, monitoring and management functionality, as well as executable JAR packaging.</p>
</div>
<div class="paragraph">
<p>It is important to emphasise that these microservice applications are ‘just apps’ that you can run by yourself using ‘java -jar’ and passing in appropriate configuration properties.  We provide many common microservice applications for common operations so you don’t have to start from scratch when addressing common use-cases which build upon the rich ecosystem of Spring Projects, e.g Spring Integration, Spring Data, Spring Hadoop and Spring Batch.  Creating your own microservice application is similar to creating other Spring Boot applications, you can start using the Spring Initialzr web site or the UI to create the basic scaffolding of either a Stream or Task based microservice.</p>
</div>
<div class="paragraph">
<p>In addition to passing in the appropriate configuration to the applications, the Data Flow server is responsible for preparing the target platform’s infrastructure so that the application can be deployed.  For example, in Cloud Foundry it would be binding specified services to the applications and executing the ‘cf push’ command for each application.  For Kubernetes it would be creating the replication controller, service, and load balancer.</p>
</div>
<div class="paragraph">
<p>The Data Flow Server helps simplify the deployment of multiple applications onto a target runtime, but one could also opt to deploy each of the microservice applications manually and not use Data Flow at all. This approach might be more appropriate to start out with for small scale deployments, gradually adopting the convenience and consistency of Data Flow as you develop more applications.
Manual deployment of Stream and Task based microservices is also a useful educational exercise that will help you better understand some of the automatic applications configuration and platform targeting steps that the Data Flow Server provides.</p>
</div>
<div class="sect2">
<h3 id="arch-comparison">8.1. Comparison to other Platform architectures</h3>
<div class="paragraph">
<p>Spring Cloud Data Flow’s architectural style is different than other Stream and Batch processing platforms.  For example in Apache Spark, Apache Flink, and Google Cloud Dataflow applications run on a dedicated compute engine cluster.  The nature of the compute engine gives these platforms a richer environment for performing complex calculations on the data as compared to Spring Cloud Data Flow, but it introduces complexity of another execution environment that is often not needed when creating data centric applications.  That doesn’t mean you cannot do real time data computations when using Spring Cloud Data Flow.  Refer to the analytics section which describes the integration of Redis to handle common counting based use-cases as well as the RxJava integration for functional API driven analytics use-cases, such as time-sliding-window and moving-average among others.</p>
</div>
<div class="paragraph">
<p>Similarly, Apache Storm, Hortonworks DataFlow and Spring Cloud Data Flow’s predecessor, Spring XD, use a dedicated application execution cluster, unique to each product, that determines where your code should execute on the cluster and perform health checks to ensure that long lived applications are restarted if they fail.  Often, framework specific interfaces are required to be used in order to correctly “plug in” to the cluster’s execution framework.</p>
</div>
<div class="paragraph">
<p>As we discovered during the evolution of Spring XD, the rise of multiple container frameworks in 2015 made creating our own runtime a duplication of efforts.  There is no reason to build your own resource management mechanics, when there are multiple runtime platforms that offer this functionality already.  Taking these considerations into account is what made us shift to the current architecture where we delegate the execution to popular runtimes, runtimes that you may already be using for other purposes.  This is an advantage in that it reduces the cognitive distance for creating and managing data centric applications as many of the same skills used for deploying other end-user/web applications are applicable.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="arch-streaming-apps">9. Streaming Applications</h2>
<div class="sectionbody">
<div class="paragraph">
<p>While Spring Boot provides the foundation for creating DevOps friendly microservice applications, other libraries in the Spring ecosystem help create Stream based microservice applications.  The most important of these is Spring Cloud Stream.</p>
</div>
<div class="paragraph">
<p>The essence of the Spring Cloud Stream programming model is to provide an easy way to describe multiple inputs and outputs of an application that communicate over messaging middleware.  These input and outputs map onto Kafka topics or Rabbit exchanges and queues.  Common application configuration for a Source that generates data, a Process that consumes and produces data and a Sink that consumes data is provided as part of the library.</p>
</div>
<div class="sect2">
<h3 id="arch-streaming-imperative-programming">9.1. Imperative Programming Model</h3>
<div class="paragraph">
<p>Spring Cloud Stream is most closely integrated with Spring Integration’s imperative "event at a time" programming model.  This means you write code that handles a single event callback.  For example,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">@EnableBinding(Sink.class)
public class LoggingSink {

    @StreamListener(Sink.INPUT)
    public void log(String message) {
        System.out.println(message);
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this case the String payload of a message coming on the input channel, is handed to the log method.  The <code>@EnableBinding</code> annotation is what is used to tie together the input channel to the external middleware.</p>
</div>
</div>
<div class="sect2">
<h3 id="arch-streaming-functional-programming">9.2. Functional Programming Model</h3>
<div class="paragraph">
<p>However, Spring Cloud Stream can support other programming styles. The use of reactive APIs where incoming and outgoing data is handled as continuous data flows and it defines how each individual message should be handled. You can also use operators that describe functional transformations from inbound to outbound data flows. The upcoming versions will support Apache Kafka’s KStream API in the programming model.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="arch-streams">10. Streams</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="arch-streams-topologies">10.1. Topologies</h3>
<div class="paragraph">
<p>The Stream DSL describes linear sequences of data flowing through the system.  For example, in the stream definition <code>http | transformer | cassandra</code>, each pipe symbol connects the application on the left to the one on the right.  Named channels can be used for routing and to fan out data to multiple messaging destinations.</p>
</div>
<div class="paragraph">
<p>Taps can be used to ‘listen in’ to the data that if flowing across any of the pipe symbols.  Taps can be used as sources for new streams with an in independent life cycle.</p>
</div>
</div>
<div class="sect2">
<h3 id="arch-streams-concurrency">10.2. Concurrency</h3>
<div class="paragraph">
<p>For an application that will consume events, Spring Cloud stream exposes a concurrency setting that controls the size of a thread pool used for dispatching incoming messages.  See the 1.2.2.RELEASE#_consumer_properties[Consumer properties] documentation for more information.</p>
</div>
</div>
<div class="sect2">
<h3 id="arch-streams-partitioning">10.3. Partitioning</h3>
<div class="paragraph">
<p>A common pattern in stream processing is to partition the data as it moves from one application to the next.  Partitioning is a critical concept in stateful processing, for either performance or consistency reasons, to ensure that all related data is processed together. For example, in a time-windowed average calculation example, it is important that all measurements from any given sensor are processed by the same application instance.  Alternatively, you may want to cache some data related to the incoming events so that it can be enriched without making a remote procedure call to retrieve the related data.</p>
</div>
<div class="paragraph">
<p>Spring Cloud Data Flow supports partitioning by configuring Spring Cloud Stream&#8217;s output and input bindings.  Spring Cloud Stream provides a common abstraction for implementing partitioned processing use cases in a uniform fashion across different types of middleware.  Partitioning can thus be used whether the broker itself is naturally partitioned (e.g., Kafka topics) or not (e.g., RabbitMQ).  The following image shows how data could be partitioned into two buckets, such that each instance of the average processor application consumes a unique set of data.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/stream-partitioning.png" alt="Stream Partitioning Architecture">
</div>
<div class="title">Figure 2. Spring Cloud Stream Partitioning</div>
</div>
<div class="paragraph">
<p>To use a simple partitioning strategy in Spring Cloud Data Flow, you only need set the instance count for each application in the stream and a <code>partitionKeyExpression</code> producer property when deploying the stream.  The <code>partitionKeyExpression</code> identifies what part of the message will be used as the key to partition data in the underlying middleware.  An <code>ingest</code> stream can be defined as <code>http | averageprocessor | cassandra</code>  (Note that the Cassandra sink isn&#8217;t shown in the diagram above).  Suppose the payload being sent to the http source was in JSON format and had a field called <code>sensorId</code>.  Deploying the stream with the shell command <code>stream deploy ingest --propertiesFile ingestStream.properties</code> where the contents of the file <code>ingestStream.properties</code> are</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">deployer.http.count=3
deployer.averageprocessor.count=2
app.http.producer.partitionKeyExpression=payload.sensorId</code></pre>
</div>
</div>
<div class="paragraph">
<p>will deploy the stream such that all the input and output destinations are configured for data to flow through the applications but also ensure that a unique set of data is always delivered to each averageprocessor instance.  In this case the default algorithm is to evaluate <code>payload.sensorId % partitionCount</code> where the <code>partitionCount</code> is the application count in the case of RabbitMQ and the partition count of the topic in the case of Kafka.</p>
</div>
<div class="paragraph">
<p>Please refer to <a href="#passing_stream_partition_properties">Passing stream partition properties during stream deployment</a> for additional strategies to partition streams during deployment and how they map onto the underlying 1.2.2.RELEASE#_partitioning[Spring Cloud Stream Partitioning properties].</p>
</div>
<div class="paragraph">
<p>Also note, that you can&#8217;t currently scale partitioned streams.  Read the section <a href="#arch-runtime-scaling">Scaling at runtime</a> for more information.</p>
</div>
</div>
<div class="sect2">
<h3 id="arch-streams-delivery">10.4. Message Delivery Guarantees</h3>
<div class="paragraph">
<p>Streams are composed of applications that use the Spring Cloud Stream library as the basis for communicating with the underlying messaging middleware product.  Spring Cloud Stream also provides an opinionated configuration of middleware from several vendors, in particular providing 1.2.2.RELEASE#_persistent_publish_subscribe_support[persistent publish-subscribe semantics].</p>
</div>
<div class="paragraph">
<p>The 1.2.2.RELEASE#_binders[Binder abstraction] in Spring Cloud Stream is what connects the application to the middleware.  There are several configuration properties of the binder that are portable across all binder implementations and some that are specific to the middleware.</p>
</div>
<div class="paragraph">
<p>For consumer applications there is a retry policy for exceptions generated during message handling.  The retry policy is configured using the 1.2.2.RELEASE#_consumer_properties[common consumer properties] <code>maxAttempts</code>, <code>backOffInitialInterval</code>, <code>backOffMaxInterval</code>, and <code>backOffMultiplier</code>.  The default values of these properties will retry the callback method invocation 3 times and wait one second for the first retry.  A backoff multiplier of 2 is used for the second and third attempts.</p>
</div>
<div class="paragraph">
<p>When the number of retry attempts has exceeded the <code>maxAttempts</code> value, the exception and the failed message will become the payload of a message and be sent to the application&#8217;s error channel.  By default, the default message handler for this error channel logs the message.  You can change the default behavior in your application by creating your own message handler that subscribes to the error channel.</p>
</div>
<div class="paragraph">
<p>Spring Cloud Stream also supports a configuration option for both Kafka and RabbitMQ binder implementations that will send the failed message and stack trace to a dead letter queue.  The dead letter queue is a destination and its nature depends on the messaging middleware (e.g in the case of Kafka it is a dedicated topic).  To enable this for RabbitMQ set the 1.2.2.RELEASE#_rabbitmq_consumer_properties[consumer properties] <code>republishtoDlq</code> and <code>autoBindDlq</code> and the 1.2.2.RELEASE#_rabbit_producer_properties[producer property] <code>autoBindDlq</code> to true when deploying the stream.  To always apply these producer and consumer properties when deploying streams, configure them as <a href="#spring-cloud-dataflow-global-properties">common application properties</a> when starting the Data Flow server.</p>
</div>
<div class="paragraph">
<p>Additional messaging delivery guarantees are those provided by the underlying messaging middleware that is chosen for the application for both producing and consuming applications.  Refer to the Kafka 1.2.2.RELEASE#_kafka_consumer_properties[Consumer] and 1.2.2.RELEASE#_kafka_producer_properties[Producer] and Rabbit 1.2.2.RELEASE#_rabbitmq_consumer_properties[Consumer] and 1.2.2.RELEASE#_rabbit_producer_properties[Producer] documentation for more details.  You will find extensive declarative support for all the native QOS options.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="arch-analytics">11. Analytics</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Spring Cloud Data Flow is aware of certain Sink applications that will write counter data to Redis and provides an REST endpoint to read counter data.  The types of counters supported are</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/spring-cloud-stream-app-starters/counter/tree/master/spring-cloud-starter-stream-sink-counter">Counter</a> - Counts the number of messages it receives, optionally storing counts in a separate store such as redis.</p>
</li>
<li>
<p><a href="https://github.com/spring-cloud-stream-app-starters/field-value-counter/tree/master/spring-cloud-starter-stream-sink-field-value-counter">Field Value Counter</a> - Counts occurrences of unique values for a named field in a message payload</p>
</li>
<li>
<p><a href="https://github.com/spring-cloud-stream-app-starters/aggregate-counter/tree/master/spring-cloud-starter-stream-sink-aggregate-counter">Aggregate Counter</a> - Stores total counts but also retains the total count values for each minute, hour day and month.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>It is important to note that the timestamp that is used in the aggregate counter can come from a field in the message itself so that out of order messages are properly accounted.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="arch-task">12. Task Applications</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Spring Cloud Task programming model provides:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Persistence of the Task’s lifecycle events and exit code status.</p>
</li>
<li>
<p>Lifecycle hooks to execute code before or after a task execution.</p>
</li>
<li>
<p>Emit task events to a stream (as a source) during the task lifecycle.</p>
</li>
<li>
<p>Integration with Spring Batch Jobs.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="arch-data-flow-server">13. Data Flow Server</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="arch-data-flow-server-endpoints">13.1. Endpoints</h3>
<div class="paragraph">
<p>The Data Flow Server uses an embedded servlet container and exposes REST endpoints for creating, deploying, undeploying, and destroying streams and tasks, querying runtime state, analytics, and the like. The Data Flow Server is implemented using Spring’s MVC framework and the <a href="https://github.com/spring-projects/spring-hateoas">Spring HATEOAS</a> library to create REST representations that follow the HATEOAS principle.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-server-arch.png" alt="The Spring Cloud Data Flow Server Architecture">
</div>
<div class="title">Figure 3. The Spring Cloud Data Flow Server</div>
</div>
</div>
<div class="sect2">
<h3 id="arch-data-flow-server-customization">13.2. Customization</h3>
<div class="paragraph">
<p>Each Data Flow Server executable jar targets a single runtime by delegating to the implementation of the deployer Service Provider Interface found on the classpath.</p>
</div>
<div class="paragraph">
<p>We provide a Data Flow Server executable jar that targets a single runtime.  The Data Flow server delegates to the implementation of the deployer Service Provider Interface found on the classpath.  In the current version, there are no endpoints specific to a target runtime, but may be available in future releases as a convenience to access runtime specific features</p>
</div>
<div class="paragraph">
<p>While we provide a server executable for each of the target runtimes you can also create your own customized server application using Spring Initialzr.   This let’s you add or remove functionality relative to the executable jar we provide.  For example, adding additional security implementations, custom endpoints, or removing Task or Analytics REST endpoints.  You can also enable or disable some features through the use of feature toggles.</p>
</div>
</div>
<div class="sect2">
<h3 id="arch-data-flow-server-security">13.3. Security</h3>
<div class="paragraph">
<p>The Data Flow Server executable jars support basic http, LDAP(S), File-based, and OAuth 2.0 authentication to access its endpoints. Refer to the <a href="#configuration-security">security section</a> for more information.</p>
</div>
<div class="paragraph">
<p>Authorization via groups is planned for a future release.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="arch-runtime">14. Runtime</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="arch-runtime-fault-tolerance">14.1. Fault Tolerance</h3>
<div class="paragraph">
<p>The target runtimes supported by Data Flow all have the ability to restart a long lived application should it fail.  Spring Cloud Data Flow sets up whatever health probe is required by the runtime environment when deploying the application.</p>
</div>
<div class="paragraph">
<p>The collective state of all applications that comprise the stream is used to determine the state of the stream.  If an application fails, the state of the stream will change from ‘deployed’ to ‘partial’.</p>
</div>
</div>
<div class="sect2">
<h3 id="arch-runtime-resource-management">14.2. Resource Management</h3>
<div class="paragraph">
<p>Each target runtime lets you control the amount of memory, disk and CPU that is allocated to each application.  These are passed as properties in the deployment manifest using key names that are unique to each runtime.  Refer to the each platforms server documentation for more information.</p>
</div>
</div>
<div class="sect2">
<h3 id="arch-runtime-scaling">14.3. Scaling at runtime</h3>
<div class="paragraph">
<p>When deploying a stream, you can set the instance count for each individual application that comprises the stream.
Once the stream is deployed, each target runtime lets you control the target number of instances for each individual application.
Using the APIs, UIs, or command line tools for each runtime, you can scale up or down the number of instances as required.
Future work will provide a portable command in the Data Flow Server to perform this operation.</p>
</div>
<div class="paragraph">
<p>Currently, this is not supported with the Kafka binder (based on the 0.8 simple consumer at the time of the release), as well as partitioned streams, for which the suggested workaround is redeploying the stream with an updated number of instances.
Both cases require a static consumer set up based on information about the total instance count and current instance index, a limitation intended to be addressed in future releases.
For example, Kafka 0.9 and higher provides good infrastructure for scaling applications dynamically and will be available as an alternative to the current Kafka 0.8 based binder in the near future.
One specific concern regarding scaling partitioned streams is the handling of local state, which is typically reshuffled as the number of instances is changed.
This is also intended to be addressed in the future versions, by providing first class support for local state management.</p>
</div>
</div>
<div class="sect2">
<h3 id="arch-application-versioning">14.4. Application Versioning</h3>
<div class="paragraph">
<p>Application versioning, that is upgrading or downgrading an application from one version to another, is not directly supported by Spring Cloud Data Flow.  You must rely on specific target runtime features to perform these operational tasks.</p>
</div>
<div class="paragraph">
<p>The roadmap for Spring Cloud Data Flow will deploy applications that are compatible with Spinnaker to manage the complete application lifecycle.  This also includes automated canary analysis backed by  application metrics.  Portable commands in the Data Flow server to trigger pipelines in Spinnaker are also planned.</p>
</div>
</div>
</div>
</div>
<h1 id="_spring_cloud_data_flow_runtime" class="sect0">Spring Cloud Data Flow Runtime</h1>
<div class="openblock partintro">
<div class="content">
Data flow runtime can be deployed and used with <em>YARN</em> in two different
ways, firstly using it directly with a <em>YARN</em> cluster and secondly
letting <em>Apache Ambari</em> deploy it into its cluster as a service.
Difference between these two deployment types is that <em>YARN</em> only
provides a raw runtime environment for containers where user is required
to setup all needed dependencies while <em>Apache Ambari</em> will try to
focus on easy deployment where minimum set of required services exist
in ambari managed cluster.
</div>
</div>
<div class="sect1">
<h2 id="yarn-deploying-on-yarn">15. Deploying on YARN</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The server application is run as a standalone application. All
applications used for streams and tasks will be deployed on the YARN
cluster that is targeted by the server.</p>
</div>
<div class="sect2">
<h3 id="_prerequisites">15.1. Prerequisites</h3>
<div class="paragraph">
<p>These requirements are not something yarn runtime needs but generally
what dataflow core needs.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Rabbit - If dataflow apps using rabbit bindings are used.</p>
</li>
<li>
<p>Kafka - If dataflow apps using kafka bindings are used.</p>
</li>
<li>
<p>DB - we currently use embedded H2 database, though any supported
DB can be configured.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_download_and_extract_distribution">15.2. Download and Extract Distribution</h3>
<div class="paragraph">
<p>Download the Spring Cloud Data Flow YARN distribution ZIP file which
includes the Server and the Shell apps:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">$ wget http://repo.spring.io/release/org/springframework/cloud/dist/spring-cloud-dataflow-server-yarn-dist/1.2.2.RELEASE/spring-cloud-dataflow-server-yarn-dist-1.2.2.RELEASE.zip</code></pre>
</div>
</div>
<div class="paragraph">
<p>Unzip the distribution ZIP file and change to the directory containing the deployment files.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">$ cd spring-cloud-dataflow-server-yarn-1.2.2.RELEASE</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_configure_settings">15.3. Configure Settings</h3>
<div class="paragraph">
<p>Generic runtime settings can changed in <code>config/servers.yml</code>.
Dedicated section <a href="#yarn-configure-settings">Configuring Runtime Settings and Environment</a> contains detailed
information about configuration.</p>
</div>
<div class="paragraph">
<p><code>servers.yml</code> file is a central place to share common configuration as
it is added to Boot based jvm processes via option
<code>-Dspring.config.location=servers.yml</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_start_server">15.4. Start Server</h3>
<div class="paragraph">
<p>If this is the first time deploying make sure the user that runs
the <em>Server</em> app has rights to create and write to <em>/dataflow</em>
directory in <code>hdfs</code>. If there is an existing deployment on <code>hdfs</code>
remove it using:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">$ hdfs dfs -rm -R /dataflow</code></pre>
</div>
</div>
<div class="paragraph">
<p>Start the Spring Cloud Data Flow Server app for YARN</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">$ ./bin/dataflow-server-yarn</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_connect_shell">15.5. Connect Shell</h3>
<div class="paragraph">
<p>start <code>spring-cloud-dataflow-shell</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">$ ./bin/dataflow-shell</code></pre>
</div>
</div>
<div class="paragraph">
<p>Shell in a distribution package contains extension commands for a
<code>hdfs</code> file system.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">dataflow:&gt;hadoop fs
hadoop fs cat              hadoop fs copyFromLocal    hadoop fs copyToLocal      hadoop fs expunge
hadoop fs ls               hadoop fs mkdir            hadoop fs mv               hadoop fs rm
dataflow:&gt;hadoop fs ls /
rwxrwxrwx root         supergroup 0 2016-07-25 06:54:15 /
rwxrwxrwx jvalkealahti supergroup 0 2016-07-25 06:58:38 /dataflow
rwxr-xr-x jvalkealahti supergroup 0 2016-07-25 07:31:32 /repo
rwxrwxrwx root         supergroup 0 2016-07-20 16:25:31 /tmp
rwxrwxrwx jvalkealahti supergroup 0 2015-10-29 10:59:24 /user</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You can configure server address automatically by placing it in
a configuration using key <code>dataflow.uri</code>.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_register_applications">15.6. Register Applications</h3>
<div class="paragraph">
<p>By default, the application registry will be empty. If you would like
to register all out-of-the-box stream applications built with the RabbitMQ
binder in bulk, you can with the following command. For more details,
review how to <a href="streams.html#spring-cloud-dataflow-register-apps">register applications</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">dataflow:&gt;app import --uri http://bit.ly/stream-applications-rabbit-maven</code></pre>
</div>
</div>
<div class="sect3">
<h4 id="_sourcing_applications_from_hdfs">15.6.1. Sourcing Applications from HDFS</h4>
<div class="paragraph">
<p>YARN integration also allows you to store registered applications
directly in HDFS instead of relying on <code>maven</code> or any other
resolution. Only thing to change during a registration is to use
<code>hdfs</code> address as shown below.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">dataflow:&gt;app register --name ftp --type sink --uri hdfs:/dataflow/artifacts/repo/ftp-sink-kafka-1.0.0.RC1.jar</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_create_stream">15.7. Create Stream</h3>
<div class="paragraph">
<p>Create a stream:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">dataflow:&gt;stream create --name foostream --definition "time|log" --deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p>List streams:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">dataflow:&gt;stream list
╔═══════════╤═════════════════╤════════╗
║Stream Name│Stream Definition│ Status ║
╠═══════════╪═════════════════╪════════╣
║foostream  │time|log         │deployed║
╚═══════════╧═════════════════╧════════╝</code></pre>
</div>
</div>
<div class="paragraph">
<p>After some time, destroy the stream:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">dataflow:&gt;stream destroy --name foostream</code></pre>
</div>
</div>
<div class="paragraph">
<p>The YARN application is pushed and started automatically during a stream
deployment process. Once all streams are destroyed the YARN application
will exit.</p>
</div>
</div>
<div class="sect2">
<h3 id="_create_task">15.8. Create Task</h3>
<div class="paragraph">
<p>Create and launch task:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">dataflow:&gt;task create --name footask --definition "timestamp"
Created new task 'footask'
dataflow:&gt;task launch --name footask
Launched task 'footask'</code></pre>
</div>
</div>
<div class="paragraph">
<p>Launch tasks from streams:</p>
</div>
<div class="paragraph">
<p><code>task-launcher-yarn-sink</code> itself bundles a <em>YARN Deployer</em> but doesn&#8217;t
push any apps into hdfs, thus pushed app needs to exist and match a
deployer version <code>task-launcher-yarn-sink</code> uses.</p>
</div>
<div class="paragraph">
<p>In below sample we use <code>tasklaunchrequest</code> processor to pass needed
properties into <code>task-launcher-yarn</code> sink. We explicitely defined
<code>appVersion</code> as <code>appv1</code> which you would have pushed into hdfs prior
running this stream. With this processor you also need to define a
<code>uri</code> for a task application itself.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">stream create --name launchertest --definition "http
--server.port=9000|tasklaunchrequest
--deployment-properties=spring.cloud.deployer.yarn.app.appVersion=appv1
--uri=hdfs:/dataflow/repo/timestamp-task.jar|task-launcher-yarn"
--deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p>To fire up a task just post a dummy message into <code>http</code> source.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">http post --target http://localhost:9000 --data empty</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Using <code>http</code> source in YARN difficult as you don&#8217;t immediately know on
which cluster node that source app is running.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_using_yarn_cli">15.9. Using YARN Cli</h3>
<div class="paragraph">
<p>Overall app status can be seen from <em>YARN Resource Manager UI</em> or
using <em>Spring YARN CLI</em> which gives more info about running containers
within an app itself.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">$ ./bin/dataflow-server-yarn-cli shell</code></pre>
</div>
</div>
<div class="sect3">
<h4 id="_check_yarn_app_statuses">15.9.1. Check YARN App Statuses</h4>
<div class="paragraph">
<p>When stream has been submitted YARN shows it as <code>ACCEPTED</code> before its
turned to <code>RUNNING</code> state.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">$ submitted
  APPLICATION ID                  USER          NAME                     QUEUE    TYPE      STARTTIME       FINISHTIME  STATE     FINALSTATUS  ORIGINAL TRACKING URL
  ------------------------------  ------------  -----------------------  -------  --------  --------------  ----------  --------  -----------  ---------------------
  application_1461658614481_0001  jvalkealahti  scdstream:app:foostream  default  DATAFLOW  26/04/16 16:27  N/A         ACCEPTED  UNDEFINED

$ submitted
  APPLICATION ID                  USER          NAME                     QUEUE    TYPE      STARTTIME       FINISHTIME  STATE    FINALSTATUS  ORIGINAL TRACKING URL
  ------------------------------  ------------  -----------------------  -------  --------  --------------  ----------  -------  -----------  -------------------------
  application_1461658614481_0001  jvalkealahti  scdstream:app:foostream  default  DATAFLOW  26/04/16 16:27  N/A         RUNNING  UNDEFINED    http://192.168.1.96:58580</code></pre>
</div>
</div>
<div class="paragraph">
<p>More info about internals for stream apps can be queried by
<code>clustersinfo</code> and <code>clusterinfo</code> commands:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">$ clustersinfo -a application_1461658614481_0001
  CLUSTER ID
  --------------
  foostream:log
  foostream:time

$ clusterinfo -a application_1461658614481_0001 -c foostream:time
  CLUSTER STATE  MEMBER COUNT
  -------------  ------------
  RUNNING        1</code></pre>
</div>
</div>
<div class="paragraph">
<p>After stream is undeployed YARN app should close itself automatically:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">$ submitted -v
  APPLICATION ID                  USER          NAME                     QUEUE    TYPE      STARTTIME       FINISHTIME      STATE     FINALSTATUS  ORIGINAL TRACKING URL
  ------------------------------  ------------  -----------------------  -------  --------  --------------  --------------  --------  -----------  ---------------------
  application_1461658614481_0001  jvalkealahti  scdstream:app:foostream  default  DATAFLOW  26/04/16 16:27  26/04/16 16:28  FINISHED  SUCCEEDED</code></pre>
</div>
</div>
<div class="paragraph">
<p>Launching a task will be shown in <code>RUNNING</code> state while app is
executing its batch jobs:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">$ submitted -v
  APPLICATION ID                  USER          NAME                     QUEUE    TYPE      STARTTIME       FINISHTIME      STATE     FINALSTATUS  ORIGINAL TRACKING URL
  ------------------------------  ------------  -----------------------  -------  --------  --------------  --------------  --------  -----------  -------------------------
  application_1461658614481_0002  jvalkealahti  scdtask:timestamp        default  DATAFLOW  26/04/16 16:29  N/A             RUNNING   UNDEFINED    http://192.168.1.96:39561
  application_1461658614481_0001  jvalkealahti  scdstream:app:foostream  default  DATAFLOW  26/04/16 16:27  26/04/16 16:28  FINISHED  SUCCEEDED

$ submitted -v
  APPLICATION ID                  USER          NAME                     QUEUE    TYPE      STARTTIME       FINISHTIME      STATE     FINALSTATUS  ORIGINAL TRACKING URL
  ------------------------------  ------------  -----------------------  -------  --------  --------------  --------------  --------  -----------  ---------------------
  application_1461658614481_0002  jvalkealahti  scdtask:timestamp        default  DATAFLOW  26/04/16 16:29  26/04/16 16:29  FINISHED  SUCCEEDED
  application_1461658614481_0001  jvalkealahti  scdstream:app:foostream  default  DATAFLOW  26/04/16 16:27  26/04/16 16:28  FINISHED  SUCCEEDED</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_push_apps">15.9.2. Push Apps</h4>
<div class="paragraph">
<p>Yarn applications needed for a dataflow can be pushed manually
into hdfs with a given version which default to <code>app</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">Spring YARN Cli (v2.4.0.RELEASE)
Hit TAB to complete. Type 'help' and hit RETURN for help, and 'exit' to quit.
$ push -t STREAM
New version installed
$ push -t TASK
New version installed
$ push -t TASK -v appv1
New version installed</code></pre>
</div>
</div>
<div class="paragraph">
<p>After above commands base directories for different app versions would
look like as shown below. Streams and tasks can then use different
versions which allows to use alternate configurations.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">/dataflow/apps/stream/app
/dataflow/apps/task/app
/dataflow/apps/task/appv1</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Push happens automatically when stream is deployer or task
launched.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_using_metric_collectors">15.10. Using Metric Collectors</h3>
<div class="paragraph">
<p>We package three different metrics collector implementations, one for
<em>RabbitMQ</em> and two for different <em>Kafka</em> versions. There can be
started using shell scripts,
<code>dataflow-server-metrics-collector-kafka-09</code>,
<code>dataflow-server-metrics-collector-kafka-10</code> and
<code>dataflow-server-metrics-collector-rabbit</code> respectively. These
applications are not using <code>servers.yml</code> file for config, instead
<code>collectors.yml</code> is used where custom settings can be placed.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>With <code>Kafka 0.10.1</code> and later, <code>kafka-10</code> should be used. With <code>Kafka
0.10.0</code> and earlier, <code>kafka-09</code> should be used.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="yarn-deploying-on-ambari">16. Deploying on AMBARI</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Ambari basically automates YARN installation instead of requiring user
to do it manually. Also a lot of other configuration steps are automated as
much as possible to easy overall installation process.</p>
</div>
<div class="paragraph">
<p>There is no difference on components deployed into ambari comparing of
a manual usage with a separate YARN cluster. With ambari we simply package
needed dataflow components into a rpm package so that it can be managed as
an ambari service. After that ambari really only manage a runtime
configuration of those components.</p>
</div>
<div class="sect2">
<h3 id="_install_ambari_server">16.1. Install Ambari Server</h3>
<div class="paragraph">
<p>Generally it is only needed to install <code>scdf-plugin-hdp</code> plugin into
ambari server which adds needed service definitions.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">[root@ambari-1 ~]# yum -y install ambari-server
[root@ambari-1 ~]# ambari-server setup -s
[root@ambari-1 ~]# wget -nv http://repo.spring.io/yum-release-local/scdf/1.2.2/scdf-release-1.2.2.repo -O /etc/yum.repos.d/scdf-release-1.2.2.repo
[root@ambari-1 ~]# yum -y install scdf-plugin-hdp
[root@ambari-1 ~]# ambari-server start</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Ambari plugin only works for redhat6/redhat7 and related centos based systems for now.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_deploy_data_flow">16.2. Deploy Data Flow</h3>
<div class="paragraph">
<p>When you create your cluster and choose a stack, make sure that
<code>redhat6</code> or/and <code>redhat7</code> sections contains repository named
<code>SCDF-1.2.2</code> and that it points to
<code><a href="http://repo.spring.io/yum-release-local/scdf/1.2.2" class="bare">repo.spring.io/yum-release-local/scdf/1.2.2</a></code>.</p>
</div>
<div class="paragraph">
<p><code>Ambari 2.4</code> contains major rewrites for stack definitions and how it
is possible to integrate with those from external contributions. Our
plugin will eventually integrate via extensions or management packs,
but for now you need to choose stack marked as a <em>Default Version
Definition</em> which contains correct yum repository. For example with
<code>HDP 2.5</code> you have two default choices, <em>HDP-2.5.0.0</em> and <em>HDP-2.5
(Default Version Definition)</em>. As mentioned you need to pick latter.
With older ambari versions you don&#8217;t have these new options.</p>
</div>
<div class="paragraph">
<p>From services choose <code>Spring Cloud Data Flow</code> and <code>Kafka</code>. <code>Hdfs</code>,
<code>Yarn</code> and <code>Zookeeper</code> are forced dependencies.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>With <code>Kafka</code> you can do "one-click" installation while using <code>Rabbit</code>
you need to provide appropriate connection settings as <code>Rabbit</code> is not
part of a Ambari managed service.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Then in <em>Customize Services</em> what is really left for user to do is to
customise settings if needed. Everything else is automatically
configured. Technically it also allows you to switch to use rabbit by
leaving Kafka out and defining rabbit settings there. But generally
use of Kafka is a good choice.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>We also install H2 DB as service so that it can be accessed from every
node.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_using_configuration">16.3. Using Configuration</h3>
<div class="paragraph">
<p><code>servers.yml</code> file is also used to store common configuration with
Ambari. Settings in <em>Advanced scdf-site</em> and <em>Custom scdf-site</em> are
used to dynamically create a this file which is then copied over to
hdfs when needed application files are deployd.</p>
</div>
<div class="paragraph">
<p>Every additional entry added via <em>Custom scdf-site</em> is added into
<code>servers.yml</code> as is and overrides everything else in it.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If ambari configuration is modified, you need to delete
<code>/dataflow/apps/stream/app</code> and <code>/dataflow/apps/task/app</code> directories
from hdfs for new settings to get applied. Files in above directories
will not get overridden including generated <code>servers.yml</code> config file.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="_change_datasource">16.3.1. Change Datasource</h4>
<div class="paragraph">
<p>Ambari managed service defaults to <code>H2</code> database. We currently support
using <code>MySQL</code>, <code>PostgreSQL</code> and <code>HSQLDB</code> as external datasources.
Custom datasource configuration can be applied via <em>Custom scdf-site</em>
as shown in below screenshot. After these settings are modified, all
related services needs to be restarted.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/ambari-custom-scdf-site-dbconfig.png" alt="Custom Datasource Config">
</div>
<div class="title">Figure 4. Custom Datasource Config</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Managed service <em>SCDF H2 Database</em> can be stopped and put in a
maintenance mode after custom datasource settings has been added.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="yarn-configure-settings">17. Configuring Runtime Settings and Environment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This section describes how settings related to running YARN
application can be modified.</p>
</div>
<div class="sect2">
<h3 id="_generic_app_settings">17.1. Generic App Settings</h3>
<div class="paragraph">
<p>All applications whether those are stream apps or task apps can be
centrally configured with <code>servers.yml</code> as that file is passed to apps
using <code>--spring.config.location='servers.yml'</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_configuring_application_resources">17.2. Configuring Application Resources</h3>
<div class="paragraph">
<p>Stream and task processes for application master and containers can be
further tuned by setting memory and cpu settings. Also java options
allow to define actual jvm options.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">spring:
  cloud:
    deployer:
      yarn:
        app:
          streamappmaster:
            memory: 512m
            virtualCores: 1
            javaOpts: "-Xms512m -Xmx512m"
          streamcontainer:
            priority: 5
            memory: 256m
            virtualCores: 1
            javaOpts: "-Xms64m -Xmx256m"
          taskappmaster:
            memory: 512m
            virtualCores: 1
            javaOpts: "-Xms512m -Xmx512m"
          taskcontainer:
            priority: 10
            memory: 256m
            virtualCores: 1
            javaOpts: "-Xms64m -Xmx256m"</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_configure_base_directory">17.3. Configure Base Directory</h3>
<div class="paragraph">
<p>Base directory where all needed files are kept defaults to <code>/dataflow</code>
and can be changed using <code>baseDir</code> property.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">spring:
  cloud:
    deployer:
      yarn:
        app:
          baseDir: /dataflow</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="yarn-pre-populate">17.4. Pre-populate Applications</h3>
<div class="paragraph">
<p>Spring Cloud Data Flow app registration is based on URI&#8217;s with various
different endpoints. As mentioned in section <a href="#yarn-how-it-works">How YARN Deployment Works</a> all
applications are first stored into hdfs before application container
is launched. Server can use <code>http</code>, <code>file</code>, <code>http</code> and <code>maven</code> based
uris as well direct <code>hdfs</code> uris.</p>
</div>
<div class="paragraph">
<p>It is possible to place these applications directly into HDFS and
register application based on that URI.</p>
</div>
</div>
<div class="sect2">
<h3 id="_configure_logging">17.5. Configure Logging</h3>
<div class="paragraph">
<p>Logging for all components is done centrally via <code>servers.yml</code> file
using normal Spring Boot properties.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">logging:
  level:
    org.apache.hadoop: INFO
    org.springframework.yarn: INFO</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_configure_metrics">17.6. Configure Metrics</h3>
<div class="paragraph">
<p>If metrics are enabled, needed settings are written into <code>servers.yml</code>
files used by applications. Also specific settings are written into
<code>collectors.yml</code> used by <em>SCDF Metrics Collector</em> service. You need to
choose a correct collector type, its service port and output channel
name.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/ambari-metrics-config.png" alt="Metrics Config">
</div>
<div class="title">Figure 5. Metrics Config</div>
</div>
</div>
<div class="sect2">
<h3 id="_global_yarn_memory_settings">17.7. Global YARN Memory Settings</h3>
<div class="paragraph">
<p>YARN Nodemanager is continously tracking how much memory is used by
individual YARN containers. If containers are using more memory than
what the configuration allows, containers are simply killed by a
Nodemanager. Application master controlling the app lifecycle is given
a little more freedom meaning that Nodemanager is not that aggressive
when making a desicion when a container should be killed.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>These are global cluster settings and cannot be changed during an
application deployment.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Lets take a quick look of memory related settings in YARN cluster and
in YARN applications. Below xml config is what a default vanilla
Apache
Hadoop uses for memory related settings. Other distributions may have
different defaults.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>yarn.nodemanager.pmem-check-enabled</strong></dt>
<dd>
<p>Enables a check for physical memory of a process. This check if
enabled is directly tracking amount of memory requested for a YARN
container.</p>
</dd>
<dt class="hdlist1"><strong>yarn.nodemanager.vmem-check-enabled</strong></dt>
<dd>
<p>Enables a check for virtual memory of a process. This setting is one
which is usually causing containers of a custom YARN applications to
get killed by a node manager. Usually the actual ratio between
physical and virtual memory is higher than a default <code>2.1</code> or bugs in
a OS is causing wrong calculation of a used virtual memory.</p>
</dd>
<dt class="hdlist1"><strong>yarn.nodemanager.vmem-pmem-ratio</strong></dt>
<dd>
<p>Defines a ratio of allowed virtual memory compared to physical memory.
This ratio simply defines how much virtual memory a process can use
but the actual tracked size is always calculated from a physical
memory limit.</p>
</dd>
<dt class="hdlist1"><strong>yarn.scheduler.minimum-allocation-mb</strong></dt>
<dd>
<p>Defines a minimum allocated memory for container.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>This setting also indirectly defines what is the actual physical
memory limit requested during a container allocation. Actual physical
memory limit is always going to be multiple of this setting rounded to
upper bound. For example if this setting is left to default <code>1024</code> and
container is requested with <code>512M</code>, <code>1024M</code> is going to be used.
However if requested size is <code>1100M</code>, actual size is set to <code>2048M</code>.</p>
</div>
</td>
</tr>
</table>
</div>
</dd>
<dt class="hdlist1"><strong>yarn.scheduler.maximum-allocation-mb</strong></dt>
<dd>
<p>Defines a maximum allocated memory for container.</p>
</dd>
<dt class="hdlist1"><strong>yarn.nodemanager.resource.memory-mb</strong></dt>
<dd>
<p>Defines how much memory a node controlled by a node manager is allowed
to allocate. This setting should be set to amount of which OS is able
give to YARN managed processes in a way which doesn&#8217;t cause OS to
swap, etc.</p>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="_configure_kerberos">17.8. Configure Kerberos</h3>
<div class="paragraph">
<p>Enabling kerberos is relatively easy when existing kerberized
cluster exists. Just like with every other hadoop related service,
use a specific user and a keytab.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">spring:
  hadoop:
    security:
      userPrincipal: scdf/_HOST@HORTONWORKS.COM
      userKeytab: /etc/security/keytabs/scdf.service.keytab
      authMethod: kerberos
      namenodePrincipal: nn/_HOST@HORTONWORKS.COM
      rmManagerPrincipal: rm/_HOST@HORTONWORKS.COM
      jobHistoryPrincipal: jhs/_HOST@HORTONWORKS.COM</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When using ambari, configuration and keytab generation are
fully automated.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="_working_with_kerberized_kafka">17.8.1. Working with Kerberized Kafka</h4>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Currently released kafka based apps doesn&#8217;t work with cluster
where zookeeper and kafka itself are configured to for kerberos
authentication. Workaround is to use rabbit based apps or
build stream apps based on new kafka binder having support
for kerberized kafka.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>After a kafka based stream app has a kerberos support, some settings
in ambari&#8217;s kafka configuration needs to be changed. Effectively
<code>listeners</code> and <code>security.inter.broker.protocol</code> needs to use
<em>SASL_PLAINTEXT</em>. Also binder needs to be able to create topics, thus
<code>scdf</code> user needs to be added to a kafka&#8217;s super users.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">listeners=SASL_PLAINTEXT://localhost:6667
security.inter.broker.protocol=SASL_PLAINTEXT
super.users=user:kafka;user:scdf</code></pre>
</div>
</div>
<div class="paragraph">
<p>Additional configs are needed for binder and sasl config.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">spring:
  cloud:
    stream:
      kafka:
        binder:
          configuration:
            security:
              protocol: SASL_PLAINTEXT
spring:
  cloud:
    deployer:
      yarn:
        app:
          streamcontainer:
            saslConfig: "-Djava.security.auth.login.config=/etc/scdf/conf/scdf_kafka_jaas.conf"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Where <code>scdf_kafka_jaas.conf</code> looks something like shown below.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">KafkaClient {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/etc/security/keytabs/scdf.service.keytab"
   storeKey=true
   useTicketCache=false
   serviceName="kafka"
   principal="scdf/sandbox.hortonworks.com@HORTONWORKS.COM";
};</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When ambari is kerberized via its wizard, everything else is
automatically configured except kafka settings for a <code>super.users</code>,
<code>listeners</code> and <code>security.inter.broker.protocol</code>.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_configure_hdfs_ha">17.9. Configure Hdfs HA</h3>
<div class="paragraph">
<p>Generic settings for dataflow components to work with
HA setup can be seen below where id is set to <code>mycluster</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">spring:
  hadoop:
    fsUri: hdfs://mycluster:8020
    config:
      dfs.ha.automatic-failover.enabled=True
      dfs.nameservices=mycluster
      dfs.client.failover.proxy.provider.mycluster=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
      dfs.ha.namenodes.mycluster=nn1,nn2
      dfs.namenode.rpc-address.mycluster.nn2=ambari-3.localdomain:8020
      dfs.namenode.rpc-address.mycluster.nn1=ambari-2.localdomain:8020</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When using ambari and Hdfs HA setup, configuration is fully automated.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_configure_database">17.10. Configure Database</h3>
<div class="paragraph">
<p>On default a dataflow server will start embedded H2 database
using in-memory storage and effectively using configuration.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">spring:
  datasource:
    url: jdbc:h2:tcp://localhost:19092/mem:dataflow
    username: sa
    password:
    driverClassName: org.h2.Driver</code></pre>
</div>
</div>
<div class="paragraph">
<p>Distribution package contains a bundled self-contained
H2 executable which can be used instead. This allows
to persist data throughout server restarts and is not
limited to single host.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">./bin/dataflow-server-yarn-h2 --dataflow.database.h2.directory=/var/run/scdf/data</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">spring:
  datasource:
    url: jdbc:h2:tcp://neo:19092/dataflow
    username: sa
    password:
    driverClassName: org.h2.Driver</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>With external H2 instance you cannot use <code>localhost</code>, instead
use a real hostname.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Port can be changed using property <code>dataflow.database.h2.port</code>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>This bundled H2 database is also used in ambari to have a default
out of a box functionality. Any database supported by a dataflow
itself can be used by changing <code>datasource</code> settings.</p>
</div>
</div>
<div class="sect2">
<h3 id="_configure_network_discovery">17.11. Configure Network Discovery</h3>
<div class="paragraph">
<p><em>YARN Deployer</em> has to be able to talk with <em>Application Master</em>
which then is responsible controlling containers running stream and
task applications. The way this work is that <em>Application Master</em>
tries to discover its own address which <em>YARN Deployer</em> is then able
to use. If <em>YARN</em> cluster nodes have multiple <em>NICs</em> or for some other
reason address is discovered wrongly, some settings can be changed to
alter default discovery logic.</p>
</div>
<div class="paragraph">
<p>Below is a generic settings what can be changed.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">spring
  yarn:
    hostdiscovery:
      pointToPoint: false
      loopback: false
      preferInterface: ['eth', 'en']
      matchIpv4: 192.168.0.0/24
      matchInterface: eth\\d*</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>pointToPoint</strong> - Skips all interfaces which are most likely i.e.
VPNs. Defaults to <em>false</em>.</p>
</li>
<li>
<p><strong>loopback</strong> - Don&#8217;t take loopback interface. Defaults to <em>false</em>.</p>
</li>
<li>
<p><strong>preferInterface</strong> - In case multiple interface names exist, setup
preference order for discovery. Format is interface name without
number qualifier so with <em>eth0</em>, use <em>eth</em>. There&#8217;s no defaults.</p>
</li>
<li>
<p><strong>matchIpv4</strong> - Interface can be matched using its existing ip address
which is given as <em>CIDR</em> format. There&#8217;s no defaults.</p>
</li>
<li>
<p><strong>matchInterface</strong> - Interface can also matched using a simple regex
pattern which gives even better control if complex interface combinations
exist in a cluster. There&#8217;s no defaults.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="yarn-how-it-works">18. How YARN Deployment Works</h2>
<div class="sectionbody">
<div class="paragraph">
<p>When YARN application is deployed into a YARN cluster it consists of
two parts, <em>Application Master</em> and <em>Containers</em>. Application master
is a control program responsible of handling applications lifecycle
and allocation of containers. Containers are then where a real heavy
lifting is done. In case of a stream there is always minimum of 3
containers, one for application master, one for sink and one for
source. When running tasks there is always one application master and
one container running a particular task.</p>
</div>
<div class="paragraph">
<p>Needed application files are pushed into hdfs automatically when
needed. After stream and task is used once hdfs directory structure
would like like shown above.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">/dataflow/apps
/dataflow/apps/stream
/dataflow/apps/stream/app
/dataflow/apps/stream/app/application.properties
/dataflow/apps/stream/app/servers.yml
/dataflow/apps/stream/app/spring-cloud-deployer-yarn-appdeployerappmaster-1.0.0.BUILD-SNAPSHOT.jar
/dataflow/apps/task
/dataflow/apps/task/app
/dataflow/apps/task/app/application.properties
/dataflow/apps/task/app/servers.yml
/dataflow/apps/task/app/spring-cloud-deployer-yarn-tasklauncherappmaster-1.0.0.BUILD-SNAPSHOT.jar</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><code>/dataflow/apps</code> can deleted in case application version is changed or
configuration related to <code>servers.yml</code> is modified. Once created these
files are not overridden.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Application artifacts are cached under <code>/dataflow/artifacts/cache</code>
directory.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">/dataflow/artifacts
/dataflow/artifacts/cache
/dataflow/artifacts/cache/hdfs-sink-rabbit-1.0.0.RC1.jar
/dataflow/artifacts/cache/time-source-rabbit-1.0.0.RC1.jar
/dataflow/artifacts/cache/timestamp-task-1.0.0.RC1.jar</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Artifact caching is happening on two levels, firstly on a local
disk where server is running, and secondly in a hdfs cache directory.
If working with snapshots or own development, it may be required to wipe
out <code>/dataflow/artifacts/cache</code> directory and do a server restart.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="yarn-troubleshooting">19. Troubleshooting</h2>
<div class="sectionbody">
<div class="paragraph">
<p>YARN is fantastic runtime environment for running various workflows
but when things don&#8217;t work excatly as it was planned, it may be a little
bit of a tedious process to find out what went wrong. This section
tries to provide instructions how to troubleshoot various issues
causing abnormal behaviour.</p>
</div>
<div class="paragraph">
<p>When something is about to get launched into yarn, a generic procedure
goes like this:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Client is requesting resources(cpu and memory) for an application master.</p>
</li>
<li>
<p>Application master is started as an jvm process controlling
lifecycle of a yarn application as whole.</p>
</li>
<li>
<p>Application master is requesting resources(cpu and memory) for its
containers where real work is executed.</p>
</li>
<li>
<p>Containers are executed as a jvm processes.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>There are various places where things can go wrong in this flow:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>YARN resource scheduler will not allocate resources for a container
possibly due to overallocation or misconfiguration.</p>
</li>
<li>
<p>YARN will kill container because it thinks that a container is
abusing requested amount of memory.</p>
</li>
<li>
<p>JVM process itself dies either by abnormal behaviour or OOM errors
caused by a wrong jvm options.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Log files are the most obvious place to look errors. YARN application
itself writes log files name <code>Appmaster.stdout</code>, <code>Appmaster.stderr</code>,
<code>Container.stdout</code> and <code>Container.stderr</code> under yarn&#8217;s application
logging directory. Also yarn&#8217;s own logs for <em>Resource Manager</em> and
especially for <em>Node Manager</em> contains additional information when
i.e. containers are getting killed by yarn itself.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using_sandboxes">20. Using Sandboxes</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Sandboxes are a single VM images to ease testing and demos without
going through a full multi-machine cluster setup. However these images
have a natural restrictions of resources which are a cornerstone of
YARN to be able to run applications on it. With same limitations and a
carefull configuration it is possible to install Spring Cloud Data
Flow on those sandboxes. In this section we try to provide some
instructions how this can be accomplished.</p>
</div>
<div class="sect2">
<h3 id="_hortonworks_sandbox">20.1. Hortonworks Sandbox</h3>
<div class="paragraph">
<p>Install plugin repository.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">$ wget -nv http://repo.spring.io/yum-release-local/scdf/1.2.2/scdf-release-1.2.2.repo -O /etc/yum.repos.d/scdf-release-1.2.2.repo</code></pre>
</div>
</div>
<div class="paragraph">
<p>Install plugin.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">$ ambari-server stop
$ yum -y install scdf-plugin-hdp
$ ambari-server start</code></pre>
</div>
</div>
<div class="paragraph">
<p>Add needed services together spring <em>Spring Cloud Data Flow</em>. Tune
server jvm options. Spring Cloud Data Flow &#8594; Configs &#8594; Advanced
scdf-server-env &#8594; scdf-server-env template:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">export JAVA_OPTS="-Xms512m -Xmx512m"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Tune jvm options for application masters and container. Spring Cloud
Data Flow &#8594; Configs &#8594; Custom scdf-site:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-text" data-lang="text">spring.cloud.deployer.yarn.app.streamappmaster.javaOpts=-Xms512m -Xmx512m
spring.cloud.deployer.yarn.app.streamcontainer.javaOpts=-Xms512m -Xmx512m
spring.cloud.deployer.yarn.app.taskappmaster.javaOpts=-Xms512m -Xmx512m
spring.cloud.deployer.yarn.app.taskcontainer.javaOpts=-Xms512m -Xmx512m</code></pre>
</div>
</div>
</div>
</div>
</div>
<h1 id="streams" class="sect0">Streams</h1>
<div class="openblock partintro">
<div class="content">
<div class="paragraph">
<p>In this section you will learn all about Streams and how to use them with Spring Cloud Data Flow.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-stream-intro">21. Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In Spring Cloud Data Flow, a basic stream defines the ingestion of event data from a <em>source</em> to a <em>sink</em> that passes through any number of <em>processors</em>. Streams are composed of <a href="http://cloud.spring.io/spring-cloud-stream/">Spring Cloud Stream</a> applications and the deployment of stream definitions is done via the Data Flow Server (REST API). The <a href="#getting-started">Getting Started</a> section shows you how to start the server and how to start and use the Spring Cloud Data Flow shell.</p>
</div>
<div class="paragraph">
<p>A high level DSL is used to create stream definitions. The DSL to define a stream that has an http source and a file sink (with no processors) is shown below</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>http | file</code></pre>
</div>
</div>
<div class="paragraph">
<p>The DSL mimics UNIX pipes and filters syntax. Default values for ports and filenames are used in this example but can be overridden using <code>--</code> options, such as</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>http --server.port=8091 | file --directory=/tmp/httpdata/</code></pre>
</div>
</div>
<div class="paragraph">
<p>To create these stream definitions you use the shell or make an HTTP POST request to the Spring Cloud Data Flow Server.  For more information on making HTTP request directly to the server, consult the <a href="#api-guide">REST API Guide</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_stream_dsl">22. Stream DSL</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In the example above, we connected a source to a sink using the pipe symbol <code>|</code>. You can also pass properties to the source and sink configurations. The property names will depend on the individual app implementations, but as an example, the <code>http</code> source app exposes a <code>server.port</code> setting and it allows you to change the data ingestion port from the default value. To create the stream using port 8000, we would use</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt; stream create --definition "http --server.port=8000 | log" --name myhttpstream</code></pre>
</div>
</div>
<div class="paragraph">
<p>The shell provides tab completion for application properties and also the shell command <code>app info &lt;appType&gt;:&lt;appName&gt;</code> provides additional documentation for all the supported properties.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Supported Stream &lt;appType&gt;'s are: source, processor, and sink
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-register-apps">23. Register a Stream App</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Register a Stream App with the App Registry using the Spring Cloud Data Flow Shell
<code>app register</code> command. You must provide a unique name, application type, and a URI that can be
resolved to the app artifact. For the type, specify "source", "processor", or "sink".
Here are a few examples:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;app register --name mysource --type source --uri maven://com.example:mysource:0.0.1-SNAPSHOT

dataflow:&gt;app register --name myprocessor --type processor --uri file:///Users/example/myprocessor-1.2.3.jar

dataflow:&gt;app register --name mysink --type sink --uri http://example.com/mysink-2.0.1.jar</code></pre>
</div>
</div>
<div class="paragraph">
<p>When providing a URI with the <code>maven</code> scheme, the format should conform to the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>maven://&lt;groupId&gt;:&lt;artifactId&gt;[:&lt;extension&gt;[:&lt;classifier&gt;]]:&lt;version&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example, if you would like to register the snapshot versions of the <code>http</code> and <code>log</code>
applications built with the RabbitMQ binder, you could do the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;app register --name http --type source --uri maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.1.BUILD-SNAPSHOT
dataflow:&gt;app register --name log --type sink --uri maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.2.1.BUILD-SNAPSHOT</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you would like to register multiple apps at one time, you can store them in a properties file
where the keys are formatted as <code>&lt;type&gt;.&lt;name&gt;</code> and the values are the URIs.</p>
</div>
<div class="paragraph">
<p>For example, if you would like to register the snapshot versions of the <code>http</code> and <code>log</code>
applications built with the RabbitMQ binder, you could have the following in a properties file [<em>eg: stream-apps.properties</em>]:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.2.1.BUILD-SNAPSHOT
sink.log=maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.2.1.BUILD-SNAPSHOT</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then to import the apps in bulk, use the <code>app import</code> command and provide the location of the properties file via <code>--uri</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;app import --uri file:///&lt;YOUR_FILE_LOCATION&gt;/stream-apps.properties</code></pre>
</div>
</div>
<div class="paragraph">
<p>For convenience, we have the static files with application-URIs (for both maven and docker) available
for all the out-of-the-box stream and task/batch app-starters. You can point to this file and import
all the application-URIs in bulk. Otherwise, as explained in previous paragraphs, you can register them individually or have your own custom property file with only the required application-URIs in it. It is recommended, however, to have a "focused" list of desired application-URIs in a custom property file.</p>
</div>
<div class="paragraph">
<p>List of available Stream Application Starters:</p>
</div>
<table class="tableblock frame-topbot grid-all spread">
<colgroup>
<col style="width: 33%;">
<col style="width: 33%;">
<col style="width: 33%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Artifact Type</th>
<th class="tableblock halign-left valign-top">Stable Release</th>
<th class="tableblock halign-left valign-top">SNAPSHOT Release</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">RabbitMQ + Maven</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven" class="bare">bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</a></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Bacon-BUILD-SNAPSHOT-stream-applications-rabbit-maven" class="bare">bit.ly/Bacon-BUILD-SNAPSHOT-stream-applications-rabbit-maven</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">RabbitMQ + Docker</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-docker" class="bare">bit.ly/Bacon-RELEASE-stream-applications-rabbit-docker</a></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">N/A</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Kafka 0.9 + Maven</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Bacon-RELEASE-stream-applications-kafka-09-maven" class="bare">bit.ly/Bacon-RELEASE-stream-applications-kafka-09-maven</a></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Bacon-BUILD-SNAPSHOT-stream-applications-kafka-09-maven" class="bare">bit.ly/Bacon-BUILD-SNAPSHOT-stream-applications-kafka-09-maven</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Kafka 0.9 + Docker</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Bacon-RELEASE-stream-applications-kafka-09-docker" class="bare">bit.ly/Bacon-RELEASE-stream-applications-kafka-09-docker</a></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">N/A</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Kafka 0.10 + Maven</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Bacon-RELEASE-stream-applications-kafka-10-maven" class="bare">bit.ly/Bacon-RELEASE-stream-applications-kafka-10-maven</a></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Bacon-BUILD-SNAPSHOT-stream-applications-kafka-10-maven" class="bare">bit.ly/Bacon-BUILD-SNAPSHOT-stream-applications-kafka-10-maven</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Kafka 0.10 + Docker</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Bacon-RELEASE-stream-applications-kafka-10-docker" class="bare">bit.ly/Bacon-RELEASE-stream-applications-kafka-10-docker</a></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">N/A</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>List of available Task Application Starters:</p>
</div>
<table class="tableblock frame-topbot grid-all spread">
<colgroup>
<col style="width: 33%;">
<col style="width: 33%;">
<col style="width: 33%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Artifact Type</th>
<th class="tableblock halign-left valign-top">Stable Release</th>
<th class="tableblock halign-left valign-top">SNAPSHOT Release</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Maven</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Belmont-GA-task-applications-maven" class="bare">bit.ly/Belmont-GA-task-applications-maven</a></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Belmont-BUILD-SNAPSHOT-task-applications-maven" class="bare">bit.ly/Belmont-BUILD-SNAPSHOT-task-applications-maven</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Docker</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Belmont-GA-task-applications-docker" class="bare">bit.ly/Belmont-GA-task-applications-docker</a></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">N/A</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>You can find more information about the available task starters in the <a href="http://cloud.spring.io/spring-cloud-task-app-starters/">Task App Starters Project Page</a> and
related reference documentation.  For more information about the available stream starters look at the <a href="http://cloud.spring.io/spring-cloud-stream-app-starters/">Stream App Starters Project Page</a>
and related reference documentation.</p>
</div>
<div class="paragraph">
<p>As an example, if you would like to register all out-of-the-box stream applications built with the RabbitMQ binder in bulk, you can with
the following command.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-rabbit-maven</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can also pass the <code>--local</code> option (which is <code>true</code> by default) to indicate whether the
properties file location should be resolved within the shell process itself. If the location should
be resolved from the Data Flow Server process, specify <code>--local false</code>.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When using either <code>app register</code> or <code>app import</code>, if an app is already registered with
the provided name and type, it will not be overridden by default. If you would like to override the
pre-existing app coordinates, then include the <code>--force</code> option.</p>
</div>
<div class="paragraph">
<p>Note however that once downloaded, applications may be cached locally on the Data Flow server, based on the resource
location. If the resource location doesn&#8217;t change (even though the actual resource <em>bytes</em> may be different), then it
won&#8217;t be re-downloaded. When using <code>maven://</code> resources on the other hand, using a constant location still may circumvent
caching (if using <code>-SNAPSHOT</code> versions).</p>
</div>
<div class="paragraph">
<p>Moreover, if a stream is already deployed and using some version of a registered app, then (forcibly) re-registering a
different app will have no effect until the stream is deployed anew.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
In some cases the Resource is resolved on the server side, whereas in others the
URI will be passed to a runtime container instance where it is resolved. Consult
the specific documentation of each Data Flow Server for more detail.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="spring-cloud-dataflow-stream-app-whitelisting">23.1. Whitelisting application properties</h3>
<div class="paragraph">
<p>Stream and Task applications are Spring Boot applications which are aware of many <a href="#spring-cloud-dataflow-global-properties">Common application properties</a>, e.g. <code>server.port</code> but also families of properties such as those with the prefix <code>spring.jmx</code> and <code>logging</code>.  When creating your own application it is desirable to whitelist properties so that the shell and the UI can display them first as primary properties when presenting options via TAB completion or in drop-down boxes.</p>
</div>
<div class="paragraph">
<p>To whitelist application properties create a file named <code>spring-configuration-metadata-whitelist.properties</code> in the <code>META-INF</code> resource directory.  There are two property keys that can be used inside this file. The first key is named <code>configuration-properties.classes</code>.  The value is a comma separated list of fully qualified <code>@ConfigurationProperty</code> class names.  The second key is <code>configuration-properties.names</code> whose value is a comma separated list of property names.  This can contain the full name of property, such as <code>server.port</code> or a partial name to whitelist a category of property names, e.g. <code>spring.jmx</code>.</p>
</div>
<div class="paragraph">
<p>The <a href="https://github.com/spring-cloud-stream-app-starters">Spring Cloud Stream application starters</a> are a good place to look for examples of usage.  Here is a simple example of the file sink&#8217;s <code>spring-configuration-metadata-whitelist.properties</code> file</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>configuration-properties.classes=org.springframework.cloud.stream.app.file.sink.FileSinkProperties</code></pre>
</div>
</div>
<div class="paragraph">
<p>If we also wanted to add <code>server.port</code> to be white listed, then it would look like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>configuration-properties.classes=org.springframework.cloud.stream.app.file.sink.FileSinkProperties
configuration-properties.names=server.port</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Make sure to add 'spring-boot-configuration-processor' as an optional dependency to generate configuration metadata file for the properties.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt;
    &lt;optional&gt;true&lt;/optional&gt;
&lt;/dependency&gt;</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="spring-cloud-dataflow-stream-app-metadata-artifact">23.2. Creating and using a dedicated metadata artifact</h3>
<div class="paragraph">
<p>You can go a step further in the process of describing the main properties that your stream or task app supports by
creating a so-called metadata companion artifact. This simple jar file contains only the Spring boot JSON file about
configuration properties metadata, as well as the whitelisting file described in the previous section.</p>
</div>
<div class="paragraph">
<p>Here is the contents of such an artifact, for the canonical <code>log</code> sink:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ jar tvf log-sink-rabbit-1.2.1.BUILD-SNAPSHOT-metadata.jar
373848 META-INF/spring-configuration-metadata.json
   174 META-INF/spring-configuration-metadata-whitelist.properties</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that the <code>spring-configuration-metadata.json</code> file is quite large. This is because it contains the concatenation of <em>all</em> the properties that
are available at runtime to the <code>log</code> sink (some of them come from <code>spring-boot-actuator.jar</code>, some of them come from
<code>spring-boot-autoconfigure.jar</code>, even some more from <code>spring-cloud-starter-stream-sink-log.jar</code>, <em>etc.</em>) Data Flow
always relies on all those properties, even when a companion artifact is not available, but here all have been merged
into a single file.</p>
</div>
<div class="paragraph">
<p>To help with that (as a matter of fact, you don&#8217;t want to try to craft this giant JSON file by hand), you can use the
following plugin in your build:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">&lt;plugin&gt;
 	&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
 	&lt;artifactId&gt;spring-cloud-app-starter-metadata-maven-plugin&lt;/artifactId&gt;
 	&lt;executions&gt;
 		&lt;execution&gt;
 			&lt;id&gt;aggregate-metadata&lt;/id&gt;
 			&lt;phase&gt;compile&lt;/phase&gt;
 			&lt;goals&gt;
 				&lt;goal&gt;aggregate-metadata&lt;/goal&gt;
 			&lt;/goals&gt;
 		&lt;/execution&gt;
 	&lt;/executions&gt;
 &lt;/plugin&gt;</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This plugin comes in <em>addition</em> to the <code>spring-boot-configuration-processor</code> that creates the individual JSON files.
Be sure to configure the two!
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The benefits of a companion artifact are manifold:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>being way lighter (usually a few kilobytes, as opposed to megabytes for the actual app), they are quicker to download,
allowing quicker feedback when using <em>e.g.</em> <code>app info</code> or the Dashboard UI</p>
</li>
<li>
<p>as a consequence of the above, they can be used in resource constrained environments (such as PaaS) when metadata is
the only piece of information needed</p>
</li>
<li>
<p>finally, for environments that don&#8217;t deal with boot uberjars directly (for example, Docker-based runtimes such as
Kubernetes or Mesos), this is the only way to provide metadata about the properties supported by the app.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Remember though, that this is entirely optional when dealing with uberjars. The uberjar itself <em>also</em> includes the
metadata in it already.</p>
</div>
<div class="sect3">
<h4 id="_using_the_companion_artifact">23.2.1. Using the companion artifact</h4>
<div class="paragraph">
<p>Once you have a companion artifact at hand, you need to make the system aware of it so that it can be used.</p>
</div>
<div class="paragraph">
<p>When registering a single app <em>via</em> <code>app register</code>, you can use the optional <code>--metadata-uri</code> option in the shell, like so:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;app register --name log --type sink
    --uri maven://org.springframework.cloud.stream.app:log-sink-kafka-10:1.2.1.BUILD-SNAPSHOT
    --metadata-uri=maven://org.springframework.cloud.stream.app:log-sink-kafka-10:jar:metadata:1.2.1.BUILD-SNAPSHOT</code></pre>
</div>
</div>
<div class="paragraph">
<p>When registering several files using the <code>app import</code> command, the file should contain a <code>&lt;type&gt;.&lt;name&gt;.metadata</code> line
in addition to each <code>&lt;type&gt;.&lt;name&gt;</code> line. This is optional (<em>i.e.</em> if some apps have it but some others don&#8217;t, that&#8217;s fine).</p>
</div>
<div class="paragraph">
<p>Here is an example for a Dockerized app, where the metadata artifact is being hosted in a Maven repository (but retrieving
it <em>via</em> <code>http://</code> or <code>file://</code> would be equally possible).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-properties" data-lang="properties">...
source.http=docker:springcloudstream/http-source-rabbit:latest
source.http.metadata=maven://org.springframework.cloud.stream.app:http-source-rabbit:jar:metadata:1.2.1.BUILD-SNAPSHOT
...</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="custom-applications">24. Creating custom applications</h2>
<div class="sectionbody">
<div class="paragraph">
<p>While there are out of the box source, processor, sink applications available, one can extend these applications or write a custom <a href="https://github.com/spring-cloud/spring-cloud-stream">Spring Cloud Stream</a> application.</p>
</div>
<div class="paragraph">
<p>The process of creating Spring Cloud Stream applications via Spring Initializr is detailed in the Spring Cloud Stream 1.2.2.RELEASE#_getting_started[documentation].
It is possible to include multiple binders to an application.
If doing so, refer the instructions in <a href="#passing_producer_consumer_properties">Passing Spring Cloud Stream properties for the application</a> on how to configure them.</p>
</div>
<div class="paragraph">
<p>For supporting property whitelisting, Spring Cloud Stream applications running in Spring Cloud Data Flow may include the Spring Boot <code>configuration-processor</code> as an optional dependency, as in the following example.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">&lt;dependencies&gt;
  &lt;!-- other dependencies --&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt;
    &lt;optional&gt;true&lt;/optional&gt;
  &lt;/dependency&gt;
&lt;/dependencies&gt;</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Make sure that the <code>spring-boot-maven-plugin</code> is included in the POM.
The plugin is necesary for creating the executable jar that will be registered with Spring Cloud Data Flow.
Spring Initialzr will include the plugin in the generated POM.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Once a custom application has been created, it can be registered as described in <a href="#spring-cloud-dataflow-register-apps">Register a Stream App</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-create-stream">25. Creating a Stream</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Spring Cloud Data Flow Server exposes a full RESTful API for managing the lifecycle of stream definitions, but the easiest way to use is it is via the Spring Cloud Data Flow shell. Start the shell as described in the <a href="#getting-started">Getting Started</a> section.</p>
</div>
<div class="paragraph">
<p>New streams are created by with the help of stream definitions. The definitions are built from a simple DSL. For example, let&#8217;s walk through what happens if we execute the following shell command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt; stream create --definition "time | log" --name ticktock</code></pre>
</div>
</div>
<div class="paragraph">
<p>This defines a stream named <code>ticktock</code> based off the DSL expression <code>time | log</code>.  The DSL uses the "pipe" symbol <code>|</code>, to connect a source to a sink.</p>
</div>
<div class="paragraph">
<p>Then to deploy the stream execute the following shell command (or alternatively add the <code>--deploy</code> flag when creating the stream so that this step is not needed):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt; stream deploy --name ticktock</code></pre>
</div>
</div>
<div class="paragraph">
<p>The Data Flow Server resolves <code>time</code> and <code>log</code> to maven coordinates and uses those to launch the <code>time</code> and <code>log</code> applications of the stream.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>2016-06-01 09:41:21.728  INFO 79016 --- [nio-9393-exec-6] o.s.c.d.spi.local.LocalAppDeployer       : deploying app ticktock.log instance 0
   Logs will be in /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/ticktock-1464788481708/ticktock.log
2016-06-01 09:41:21.914  INFO 79016 --- [nio-9393-exec-6] o.s.c.d.spi.local.LocalAppDeployer       : deploying app ticktock.time instance 0
   Logs will be in /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/ticktock-1464788481910/ticktock.time</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this example, the time source simply sends the current time as a message each second, and the log sink outputs it using the logging framework.
You can tail the <code>stdout</code> log (which has an "_&lt;instance&gt;" suffix). The log files are located within the directory displayed in the Data Flow Server&#8217;s log output, as shown above.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>$ tail -f /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/ticktock-1464788481708/ticktock.log/stdout_0.log
2016-06-01 09:45:11.250  INFO 79194 --- [  kafka-binder-] log.sink    : 06/01/16 09:45:11
2016-06-01 09:45:12.250  INFO 79194 --- [  kafka-binder-] log.sink    : 06/01/16 09:45:12
2016-06-01 09:45:13.251  INFO 79194 --- [  kafka-binder-] log.sink    : 06/01/16 09:45:13</code></pre>
</div>
</div>
<div class="sect2">
<h3 id="_application_properties">25.1. Application properties</h3>
<div class="paragraph">
<p>Application properties are the properties associated with each application in the stream. When the application is deployed, the application properties are applied to the application via
command line arguments or environment variables based on the underlying deployment implementation.</p>
</div>
<div class="sect3">
<h4 id="_passing_application_properties_when_creating_a_stream">25.1.1. Passing application properties when creating a stream</h4>
<div class="paragraph">
<p>The following stream</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt; stream create --definition "time | log" --name ticktock</code></pre>
</div>
</div>
<div class="paragraph">
<p>can have application properties defined at the time of stream creation.</p>
</div>
<div class="paragraph">
<p>The shell command <code>app info &lt;appType&gt;:&lt;appName&gt;</code> displays the white-listed application properties for the application.
For more info on the property white listing refer to <a href="#spring-cloud-dataflow-stream-app-whitelisting">Whitelisting application properties</a></p>
</div>
<div class="paragraph">
<p>Below are the white listed properties for the app <code>time</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt; app info source:time
╔══════════════════════════════╤══════════════════════════════╤══════════════════════════════╤══════════════════════════════╗
║         Option Name          │         Description          │           Default            │             Type             ║
╠══════════════════════════════╪══════════════════════════════╪══════════════════════════════╪══════════════════════════════╣
║trigger.time-unit             │The TimeUnit to apply to delay│&lt;none&gt;                        │java.util.concurrent.TimeUnit ║
║                              │values.                       │                              │                              ║
║trigger.fixed-delay           │Fixed delay for periodic      │1                             │java.lang.Integer             ║
║                              │triggers.                     │                              │                              ║
║trigger.cron                  │Cron expression value for the │&lt;none&gt;                        │java.lang.String              ║
║                              │Cron Trigger.                 │                              │                              ║
║trigger.initial-delay         │Initial delay for periodic    │0                             │java.lang.Integer             ║
║                              │triggers.                     │                              │                              ║
║trigger.max-messages          │Maximum messages per poll, -1 │1                             │java.lang.Long                ║
║                              │means infinity.               │                              │                              ║
║trigger.date-format           │Format for the date value.    │&lt;none&gt;                        │java.lang.String              ║
╚══════════════════════════════╧══════════════════════════════╧══════════════════════════════╧══════════════════════════════╝</code></pre>
</div>
</div>
<div class="paragraph">
<p>Below are the white listed properties for the app <code>log</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt; app info sink:log
╔══════════════════════════════╤══════════════════════════════╤══════════════════════════════╤══════════════════════════════╗
║         Option Name          │         Description          │           Default            │             Type             ║
╠══════════════════════════════╪══════════════════════════════╪══════════════════════════════╪══════════════════════════════╣
║log.name                      │The name of the logger to use.│&lt;none&gt;                        │java.lang.String              ║
║log.level                     │The level at which to log     │&lt;none&gt;                        │org.springframework.integratio║
║                              │messages.                     │                              │n.handler.LoggingHandler$Level║
║log.expression                │A SpEL expression (against the│payload                       │java.lang.String              ║
║                              │incoming message) to evaluate │                              │                              ║
║                              │as the logged message.        │                              │                              ║
╚══════════════════════════════╧══════════════════════════════╧══════════════════════════════╧══════════════════════════════╝</code></pre>
</div>
</div>
<div class="paragraph">
<p>The application properties for the <code>time</code> and <code>log</code> apps can be specified at the time of <code>stream</code> creation as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt; stream create --definition "time --fixed-delay=5 | log --level=WARN" --name ticktock</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that the properties <code>fixed-delay</code> and <code>level</code> defined above for the apps <code>time</code> and <code>log</code> are the 'short-form' property names provided by the shell completion.
These 'short-form' property names are applicable only for the white-listed properties and in all other cases, only <em>fully qualified</em> property names should be used.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_deployment_properties">25.2. Deployment properties</h3>
<div class="paragraph">
<p>When deploying the stream, properties that control the deployment of the apps into the target platform are known as <code>deployment</code> properties.
For instance, one can specify how many instances need to be deployed for the specific application defined in the stream using the deployment property called <code>count</code>.</p>
</div>
<div class="sect3">
<h4 id="_application_properties_versus_deployer_properties">25.2.1. Application properties versus Deployer properties</h4>
<div class="paragraph">
<p>Starting with version 1.2, the distinction between properties that are meant for the <em>deployed app</em> and properties that
govern <em>how</em> this app is deployed (thanks to some implementation of a
<a href="https://github.com/spring-cloud/spring-cloud-deployer/">spring cloud deployer</a>) is more explicit. The former should be
passed using the syntax <code>app.&lt;app-name&gt;.&lt;property-name&gt;=&lt;value&gt;</code> while the latter use the
<code>deployer.&lt;app-name&gt;.&lt;short-property-name&gt;=&lt;value&gt;</code></p>
</div>
<div class="paragraph">
<p>The following table recaps the difference in behavior between the two.</p>
</div>
<table class="tableblock frame-all grid-all spread">
<colgroup>
<col style="width: 33%;">
<col style="width: 33%;">
<col style="width: 33%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"></th>
<th class="tableblock halign-left valign-top">Application Properties</th>
<th class="tableblock halign-left valign-top">Deployer Properties</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Example Syntax</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>app.filter.expression=foo</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>deployer.filter.count=3</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>What the application "sees"</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>expression=foo</code> or <code>&lt;some-prefix&gt;.expression=foo</code> if <code>expression</code> is one of the whitelisted properties</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Nothing</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>What the deployer "sees"</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Nothing</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spring.cloud.deployer.count=3</code> The <code>spring.cloud.deployer</code> prefix is automatically and always prepended to the property name</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Typical usage</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Passing/Overriding application properties, passing Spring Cloud Stream binder or partitionning properties</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Setting the number of instances, memory, disk, etc.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect3">
<h4 id="_passing_instance_count_as_deployment_property">25.2.2. Passing instance count as deployment property</h4>
<div class="paragraph">
<p>If you would like to have multiple instances of an application in the stream, you
can include a deployer property with the deploy command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:> stream deploy --name ticktock --properties "deployer.time.count=3"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that <code>count</code> is the <strong>reserved</strong> property name used by the underlying deployer. Hence, if the application also has a custom property named <code>count</code>, it is <strong>not</strong> supported
 when specified in 'short-form' form during stream <em>deployment</em> as it could conflict with the <em>instance</em> count deployer property. Instead, the <code>count</code> as a custom application property can be
 specified in its <em>fully qualified</em> form (example: <code>app.foo.bar.count</code>) during stream <em>deployment</em> or it can be specified using 'short-form' or <em>fully qualified</em> form during the stream <em>creation</em>
 where it will be considered as an app property.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
See <a href="#spring-cloud-dataflow-stream-app-labels">Using Labels in a Stream</a>.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_inline_vs_file_reference_properties">25.2.3. Inline vs file reference properties</h4>
<div class="paragraph">
<p>When using the Spring Cloud Data Flow Shell, there are two ways to provide deployment
properties: either <strong>inline</strong> or via a <strong>file reference</strong>. Those two ways are exclusive
and documented below:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>Inline properties</strong></dt>
<dd>
<p>use the <code>--properties</code> shell option and list properties as a comma separated
list of key=value pairs, like so:</p>
</dd>
</dl>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">stream deploy foo
    --properties "deployer.transform.count=2,app.transform.producer.partitionKeyExpression=payload"</code></pre>
</div>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>Using a file reference</strong></dt>
<dd>
<p>use the <code>--propertiesFile</code> option and point it to a local <code>.properties</code>, <code>.yaml</code> or <code>.yml</code> file
(i.e. that lives in the filesystem of the machine running the shell). Being read
as a <code>.properties</code> file, normal rules apply (ISO 8859-1 encoding, <code>=</code>, <code>&lt;space&gt;</code> or
<code>:</code> delimiter, etc.) although we recommend using <code>=</code> as a key-value pair delimiter
for consistency:</p>
</dd>
</dl>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">stream deploy foo --propertiesFile myprops.properties</code></pre>
</div>
</div>
<div class="paragraph">
<p>where <code>myprops.properties</code> contains:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>deployer.transform.count=2
app.transform.producer.partitionKeyExpression=payload</code></pre>
</div>
</div>
<div class="paragraph">
<p>Both the above properties will be passed as deployment properties for the stream <code>foo</code> above.</p>
</div>
<div class="paragraph">
<p>In case of using YAML as the format for the deployment properties, use the <code>.yaml</code> or <code>.yml</code> file extention when deploying the stream,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">stream deploy foo --propertiesFile myprops.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>where <code>myprops.yaml</code> contains:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>deployer:
  transform:
    count: 2
app:
  transform:
    producer:
      partitionKeyExpression: payload</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_passing_application_properties_when_deploying_a_stream">25.2.4. Passing application properties when deploying a stream</h4>
<div class="paragraph">
<p>The application properties can also be specified when deploying a stream. When specified during deployment, these application properties can either be specified as
 'short-form' property names (applicable for white-listed properties) or <em>fully qualified</em> property names. The application properties should have the prefix "app.&lt;appName/label&gt;".</p>
</div>
<div class="paragraph">
<p>For example, the stream</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt; stream create --definition "time | log" --name ticktock</code></pre>
</div>
</div>
<div class="paragraph">
<p>can be deployed with application properties using the 'short-form' property names:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt;stream deploy ticktock --properties "app.time.fixed-delay=5,app.log.level=ERROR"</code></pre>
</div>
</div>
<div class="paragraph">
<p>When using the app label,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">stream create ticktock --definition "a: time | b: log"</code></pre>
</div>
</div>
<div class="paragraph">
<p>the application properties can be defined as:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">stream deploy ticktock --properties "app.a.fixed-delay=4,app.b.level=ERROR"</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="passing_producer_consumer_properties">25.2.5. Passing Spring Cloud Stream properties for the application</h4>
<div class="paragraph">
<p>Spring Cloud Data Flow sets the <code>required</code> Spring Cloud Stream properties for the applications inside the stream. Most importantly, the <code>spring.cloud.stream.bindings.&lt;input/output&gt;.destination</code> is set internally for the apps to bind.</p>
</div>
<div class="paragraph">
<p>If someone wants to override any of the Spring Cloud Stream properties, they can be set via deployment properties.</p>
</div>
<div class="paragraph">
<p>For example, for the below stream</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt; stream create --definition "http | transform --expression=payload.getValue('hello').toUpperCase() | log" --name ticktock</code></pre>
</div>
</div>
<div class="paragraph">
<p>if there are multiple binders available in the classpath for each of the applications and the binder is chosen for each deployment then the stream can be deployed with the specific Spring Cloud Stream properties as:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt;stream deploy ticktock --properties "app.time.spring.cloud.stream.bindings.output.binder=kafka,app.transform.spring.cloud.stream.bindings.input.binder=kafka,app.transform.spring.cloud.stream.bindings.output.binder=rabbit,app.log.spring.cloud.stream.bindings.input.binder=rabbit"</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Overriding the destination names is not recommended as Spring Cloud Data Flow takes care of setting this internally.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_passing_per_binding_producer_consumer_properties">25.2.6. Passing per-binding producer consumer properties</h4>
<div class="paragraph">
<p>A Spring Cloud Stream application can have producer and consumer properties set <code>per-binding</code> basis.
While Spring Cloud Data Flow supports specifying short-hand notation for per binding producer properties such as <code>partitionKeyExpression</code>, <code>partitionKeyExtractorClass</code> as described in <a href="#passing_stream_partition_properties">Passing stream partition properties during stream deployment</a>, all the supported Spring Cloud Stream producer/consumer properties can be set as Spring Cloud Stream properties for the app directly as well.</p>
</div>
<div class="paragraph">
<p>The consumer properties can be set for the <code>inbound</code> channel name with the prefix <code>app.[app/label name].spring.cloud.stream.bindings.&lt;channelName&gt;.consumer.</code> and the producer properties can be set for the <code>outbound</code> channel name with the prefix <code>app.[app/label name].spring.cloud.stream.bindings.&lt;channelName&gt;.producer.</code>.
For example, the stream</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt; stream create --definition "time | log" --name ticktock</code></pre>
</div>
</div>
<div class="paragraph">
<p>can be deployed with producer/consumer properties as:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt;stream deploy ticktock --properties "app.time.spring.cloud.stream.bindings.output.producer.requiredGroups=myGroup,app.time.spring.cloud.stream.bindings.output.producer.headerMode=raw,app.log.spring.cloud.stream.bindings.input.consumer.concurrency=3,app.log.spring.cloud.stream.bindings.input.consumer.maxAttempts=5"</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>binder</code> specific producer/consumer properties can also be specified in a similar way.</p>
</div>
<div class="paragraph">
<p>For instance</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt;stream deploy ticktock --properties "app.time.spring.cloud.stream.rabbit.bindings.output.producer.autoBindDlq=true,app.log.spring.cloud.stream.rabbit.bindings.input.consumer.transacted=true"</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="passing_stream_partition_properties">25.2.7. Passing stream partition properties during stream deployment</h4>
<div class="paragraph">
<p>A common pattern in stream processing is to partition the data as it is streamed.
This entails deploying multiple instances of a message consuming app and using
content-based routing so that messages with a given key (as determined at runtime)
are always routed to the same app instance. You can pass the partition properties during
stream deployment to declaratively configure a partitioning strategy to route each
message to a specific consumer instance.</p>
</div>
<div class="paragraph">
<p>See below for examples of deploying partitioned streams:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>app.[app/label name].producer.partitionKeyExtractorClass</strong></dt>
<dd>
<p>The class name of a PartitionKeyExtractorStrategy (default <code>null</code>)</p>
</dd>
<dt class="hdlist1"><strong>app.[app/label name].producer.partitionKeyExpression</strong></dt>
<dd>
<p>A SpEL expression, evaluated against the message, to determine the partition key;
only applies if <code>partitionKeyExtractorClass</code> is null. If both are null, the app
is not partitioned (default <code>null</code>)</p>
</dd>
<dt class="hdlist1"><strong>app.[app/label name].producer.partitionSelectorClass</strong></dt>
<dd>
<p>The class name of a PartitionSelectorStrategy (default <code>null</code>)</p>
</dd>
<dt class="hdlist1"><strong>app.[app/label name].producer.partitionSelectorExpression</strong></dt>
<dd>
<p>A SpEL expression, evaluated against the partition key, to determine the partition
index to which the message will be routed. The final partition index will be the
return value (an integer) modulo <code>[nextModule].count</code>. If both the class and
expression are null, the underlying binder&#8217;s default PartitionSelectorStrategy
will be applied to the key (default <code>null</code>)</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>In summary, an app is partitioned if its count is &gt; 1 and the previous app has a
<code>partitionKeyExtractorClass</code> or <code>partitionKeyExpression</code> (class takes precedence).
When a partition key is extracted, the partitioned app instance is determined by
invoking the <code>partitionSelectorClass</code>, if present, or the <code>partitionSelectorExpression % partitionCount</code>,
where <code>partitionCount</code> is application count in the case of RabbitMQ, and the underlying
partition count of the topic in the case of Kafka.</p>
</div>
<div class="paragraph">
<p>If neither a <code>partitionSelectorClass</code> nor a <code>partitionSelectorExpression</code> is
present the result is <code>key.hashCode() % partitionCount</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="passing_content_type_properties">25.2.8. Passing application content type properties</h4>
<div class="paragraph">
<p>In a stream definition you can specify that the input or the output of an application need to be converted to a different type.
You can use the <code>inputType</code> and <code>outputType</code> properties to specify the content type for the incoming data and outgoing data, respectively.</p>
</div>
<div class="paragraph">
<p>For example, consider the following stream:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;stream create tuple --definition "http | filter --inputType=application/x-spring-tuple
 --expression=payload.hasFieldName('hello') | transform --expression=payload.getValue('hello').toUpperCase()
 | log" --deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>http</code> app is expected to send the data in JSON and the <code>filter</code> app receives the JSON data
and processes it as a Spring Tuple.
In order to do so, we use the <code>inputType</code> property on the filter app to convert the data into the expected Spring Tuple format.
The <code>transform</code> application processes the Tuple data and sends the processed data to the downstream <code>log</code> application.</p>
</div>
<div class="paragraph">
<p>When sending some data to the <code>http</code> application:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;http post --data {"hello":"world","foo":"bar"} --contentType application/json --target http://localhost:&lt;http-port&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>At the log application you see the content as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>INFO 18745 --- [transform.tuple-1] log.sink                                 : WORLD</code></pre>
</div>
</div>
<div class="paragraph">
<p>Depending on how applications are chained, the content type conversion can be specified either as via the <code>--outputType</code> in the upstream app or as an <code>--inputType</code> in the downstream app.
For instance, in the above stream, instead of specifying the <code>--inputType</code> on the 'transform' application to convert, the option <code>--outputType=application/x-spring-tuple</code> can also be specified on the 'http' application.</p>
</div>
<div class="paragraph">
<p>For the complete list of message conversion and message converters, please refer to Spring Cloud Stream 1.2.2.RELEASE#contenttypemanagement[documentation].</p>
</div>
</div>
<div class="sect3">
<h4 id="_overriding_application_properties_during_stream_deployment">25.2.9. Overriding application properties during stream deployment</h4>
<div class="paragraph">
<p>Application properties that are defined during deployment override the same properties defined during the stream creation.</p>
</div>
<div class="paragraph">
<p>For example, the following stream has application properties defined during stream creation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt; stream create --definition "time --fixed-delay=5 | log --level=WARN" --name ticktock</code></pre>
</div>
</div>
<div class="paragraph">
<p>To override these application properties, one can specify the new property values during deployment:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt;stream deploy ticktock --properties "app.time.fixed-delay=4,app.log.level=ERROR"</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="spring-cloud-dataflow-global-properties">25.3. Common application properties</h3>
<div class="paragraph">
<p>In addition to configuration via DSL, Spring Cloud Data Flow provides a mechanism for setting common properties to all
the streaming applications that are launched by it.
This can be done by adding properties prefixed with <code>spring.cloud.dataflow.applicationProperties.stream</code> when starting
the server.
When doing so, the server will pass all the properties, without the prefix, to the instances it launches.</p>
</div>
<div class="paragraph">
<p>For example, all the launched applications can be configured to use a specific Kafka broker by launching the
Data Flow server with the following options:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>--spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.kafka.binder.brokers=192.168.1.100:9092
--spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.kafka.binder.zkNodes=192.168.1.100:2181</code></pre>
</div>
</div>
<div class="paragraph">
<p>This will cause the properties <code>spring.cloud.stream.kafka.binder.brokers</code> and <code>spring.cloud.stream.kafka.binder.zkNodes</code>
to be passed to all the launched applications.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Properties configured using this mechanism have lower precedence than stream deployment properties.
They will be overridden if a property with the same key is specified at stream deployment time (e.g.
<code>app.http.spring.cloud.stream.kafka.binder.brokers</code> will override the common property).
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-destroy-stream">26. Destroying a Stream</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You can delete a stream by issuing the <code>stream destroy</code> command from the shell:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt; stream destroy --name ticktock</code></pre>
</div>
</div>
<div class="paragraph">
<p>If the stream was deployed, it will be undeployed before the stream definition is deleted.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-deploy-undeploy-stream">27. Deploying and Undeploying Streams</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Often you will want to stop a stream, but retain the name and definition for future use. In that case you can <code>undeploy</code> the stream by name and issue the <code>deploy</code> command at a later time to restart it.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt; stream undeploy --name ticktock
dataflow:&gt; stream deploy --name ticktock</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-stream-app-types">28. Other Source and Sink Application Types</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Let&#8217;s try something a bit more complicated and swap out the <code>time</code> source for something else. Another supported source type is <code>http</code>, which accepts data for ingestion over HTTP POSTs. Note that the <code>http</code> source accepts data on a different port from the Data Flow Server (default 8080). By default the port is randomly assigned.</p>
</div>
<div class="paragraph">
<p>To create a stream using an <code>http</code> source, but still using the same <code>log</code> sink, we would change the original command above to</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt; stream create --definition "http | log" --name myhttpstream --deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p>which will produce the following output from the server</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>2016-06-01 09:47:58.920  INFO 79016 --- [io-9393-exec-10] o.s.c.d.spi.local.LocalAppDeployer       : deploying app myhttpstream.log instance 0
   Logs will be in /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/myhttpstream-1464788878747/myhttpstream.log
2016-06-01 09:48:06.396  INFO 79016 --- [io-9393-exec-10] o.s.c.d.spi.local.LocalAppDeployer       : deploying app myhttpstream.http instance 0
   Logs will be in /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/myhttpstream-1464788886383/myhttpstream.http</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that we don&#8217;t see any other output this time until we actually post some data (using a shell command). In order to see the randomly assigned port on which the http source is listening, execute:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt; runtime apps</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should see that the corresponding http source has a <code>url</code> property containing the host and port information on which it is listening. You are now ready to post to that url, e.g.:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt; http post --target http://localhost:1234 --data "hello"
dataflow:&gt; http post --target http://localhost:1234 --data "goodbye"</code></pre>
</div>
</div>
<div class="paragraph">
<p>and the stream will then funnel the data from the http source to the output log implemented by the log sink</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>2016-06-01 09:50:22.121  INFO 79654 --- [  kafka-binder-] log.sink    : hello
2016-06-01 09:50:26.810  INFO 79654 --- [  kafka-binder-] log.sink    : goodbye</code></pre>
</div>
</div>
<div class="paragraph">
<p>Of course, we could also change the sink implementation. You could pipe the output to a file (<code>file</code>), to hadoop (<code>hdfs</code>) or to any of the other sink apps which are available. You can also define your own apps.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-simple-stream">29. Simple Stream Processing</h2>
<div class="sectionbody">
<div class="paragraph">
<p>As an example of a simple processing step, we can transform the payload of the HTTP posted data to upper case using the stream definitions</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>http | transform --expression=payload.toUpperCase() | log</code></pre>
</div>
</div>
<div class="paragraph">
<p>To create this stream enter the following command in the shell</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt; stream create --definition "http | transform --expression=payload.toUpperCase() | log" --name mystream --deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p>Posting some data (using a shell command)</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt; http post --target http://localhost:1234 --data "hello"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Will result in an uppercased 'HELLO' in the log</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>2016-06-01 09:54:37.749  INFO 80083 --- [  kafka-binder-] log.sink    : HELLO</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-stream-partitions">30. Stateful Stream Processing</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To demonstrate the data partitioning functionality, let&#8217;s deploy the following stream with Kafka as the binder.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;stream create --name words --definition "http --server.port=9900 | splitter --expression=payload.split(' ') | log"
Created new stream 'words'

dataflow:&gt;stream deploy words --properties "app.splitter.producer.partitionKeyExpression=payload,deployer.log.count=2"
Deployed stream 'words'

dataflow:&gt;http post --target http://localhost:9900 --data "How much wood would a woodchuck chuck if a woodchuck could chuck wood"
&gt; POST (text/plain;Charset=UTF-8) http://localhost:9900 How much wood would a woodchuck chuck if a woodchuck could chuck wood
&gt; 202 ACCEPTED</code></pre>
</div>
</div>
<div class="paragraph">
<p>You&#8217;ll see the following in the server logs.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>2016-06-05 18:33:24.982  INFO 58039 --- [nio-9393-exec-9] o.s.c.d.spi.local.LocalAppDeployer       : deploying app words.log instance 0
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-dataflow-694182453710731989/words-1465176804970/words.log
2016-06-05 18:33:24.988  INFO 58039 --- [nio-9393-exec-9] o.s.c.d.spi.local.LocalAppDeployer       : deploying app words.log instance 1
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-dataflow-694182453710731989/words-1465176804970/words.log</code></pre>
</div>
</div>
<div class="paragraph">
<p>Review the <code>words.log instance 0</code> logs:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>2016-06-05 18:35:47.047  INFO 58638 --- [  kafka-binder-] log.sink                                 : How
2016-06-05 18:35:47.066  INFO 58638 --- [  kafka-binder-] log.sink                                 : chuck
2016-06-05 18:35:47.066  INFO 58638 --- [  kafka-binder-] log.sink                                 : chuck</code></pre>
</div>
</div>
<div class="paragraph">
<p>Review the <code>words.log instance 1</code> logs:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>2016-06-05 18:35:47.047  INFO 58639 --- [  kafka-binder-] log.sink                                 : much
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : wood
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : would
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : a
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : woodchuck
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : if
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : a
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : woodchuck
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : could
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : wood</code></pre>
</div>
</div>
<div class="paragraph">
<p>This shows that payload splits that contain the same word are routed to the same application instance.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-stream-tap-dsl">31. Tap a Stream</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Taps can be created at various producer endpoints in a stream. For a stream like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>stream create --definition "http | step1: transform --expression=payload.toUpperCase() | step2: transform --expression=payload+'!' | log" --name mainstream --deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p>taps can be created at the output of <code>http</code>, <code>step1</code> and <code>step2</code>.</p>
</div>
<div class="paragraph">
<p>To create a stream that acts as a 'tap' on another stream requires to specify the <code>source destination name</code> for the tap stream. The syntax for source destination name is:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>`:&lt;streamName&gt;.&lt;label/appName&gt;`</code></pre>
</div>
</div>
<div class="paragraph">
<p>To create a tap at the output of <code>http</code> in the stream above, the source destination name is <code>mainstream.http</code>
To create a tap at the output of the first transform app in the stream above, the source destination name is <code>mainstream.step1</code></p>
</div>
<div class="paragraph">
<p>The tap stream DSL looks like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>stream create --definition ":mainstream.http &gt; counter" --name tap_at_http --deploy

stream create --definition ":mainstream.step1 &gt; jdbc" --name tap_at_step1_transformer --deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note the colon (:) prefix before the destination names. The colon allows the parser to recognize this as a destination name instead of an app name.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-stream-app-labels">32. Using Labels in a Stream</h2>
<div class="sectionbody">
<div class="paragraph">
<p>When a stream is comprised of multiple apps with the same name, they must be qualified with labels:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>stream create --definition "http | firstLabel: transform --expression=payload.toUpperCase() | secondLabel: transform --expression=payload+'!' | log" --name myStreamWithLabels --deploy</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-stream-explicit-destination-names">33. Explicit Broker Destinations in a Stream</h2>
<div class="sectionbody">
<div class="paragraph">
<p>One can connect to a specific destination name located in the broker (Rabbit, Kafka etc.,) either at the <code>source</code> or at the <code>sink</code> position.</p>
</div>
<div class="paragraph">
<p>The following stream has the destination name at the <code>source</code> position:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>stream create --definition ":myDestination &gt; log" --name ingest_from_broker --deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p>This stream receives messages from the destination <code>myDestination</code> located at the broker and connects it to the <code>log</code> app.</p>
</div>
<div class="paragraph">
<p>The following stream has the destination name at the <code>sink</code> position:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>stream create --definition "http &gt; :myDestination" --name ingest_to_broker --deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p>This stream sends the messages from the <code>http</code> app to the destination <code>myDestination</code> located at the broker.</p>
</div>
<div class="paragraph">
<p>From the above streams, notice that the <code>http</code> and <code>log</code> apps are interacting with each other via the broker (through the destination <code>myDestination</code>) rather than having a pipe directly between <code>http</code> and <code>log</code> within a single stream.</p>
</div>
<div class="paragraph">
<p>It is also possible to connect two different destinations (<code>source</code> and <code>sink</code> positions) at the broker in a stream.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>stream create --definition ":destination1 &gt; :destination2" --name bridge_destinations --deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the above stream, both the destinations (<code>destination1</code> and <code>destination2</code>) are located in the broker. The messages flow from the source destination to the sink destination via a <code>bridge</code> app that connects them.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-stream-advanced">34. Directed Graphs in a Stream</h2>
<div class="sectionbody">
<div class="paragraph">
<p>If directed graphs are needed instead of the simple linear streams described above, two features are relevant.</p>
</div>
<div class="paragraph">
<p>First, named destinations may be used as a way to combine the output from multiple streams or for multiple consumers to share the output from a single stream.
This can be done using the DSL syntax <code>http &gt; :mydestination</code> or <code>:mydestination &gt; log</code>.</p>
</div>
<div class="paragraph">
<p>Second, you may need to determine the output channel of a stream based on some information that is only known at runtime.
In that case, a router may be used in the sink position of a stream definition. For more information, refer to the Router Sink starter&#8217;s
<a href="https://github.com/spring-cloud-stream-app-starters/router/tree/master/spring-cloud-starter-stream-sink-router">README</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-stream-multi-binder">35. Stream applications with multiple binder configurations</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In some cases, a stream can have its applications bound to multiple spring cloud stream binders when they are required to connect to different messaging
middleware configurations. In those cases, it is important to make sure the applications are configured appropriately with their binder
configurations. For example, let&#8217;s consider the following stream:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>http | transform --expression=payload.toUpperCase() | log</code></pre>
</div>
</div>
<div class="paragraph">
<p>and in this stream, each application connects to messaging middleware in the following way:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>Http source sends events to RabbitMQ (rabbit1)
Transform processor receives events from RabbitMQ (rabbit1) and sends the processed events into Kafka (kafka1)
Log sink receives events from Kafka (kafka1)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Here, <code>rabbit1</code> and <code>kafka1</code> are the binder names given in the spring cloud stream application properties.
Based on this setup, the applications will have the following binder(s) in their classpath with the appropriate configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>Http - Rabbit binder
Transform - Both Kafka and Rabbit binders
Log - Kafka binder</code></pre>
</div>
</div>
<div class="paragraph">
<p>The spring-cloud-stream <code>binder</code> configuration properties can be set within the applications themselves.
If not, they can be passed via <code>deployment</code> properties when the stream is deployed.</p>
</div>
<div class="paragraph">
<p>For example,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;stream create --definition "http | transform --expression=payload.toUpperCase() | log" --name mystream</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;stream deploy mystream --properties "app.http.spring.cloud.stream.bindings.output.binder=rabbit1,app.transform.spring.cloud.stream.bindings.input.binder=rabbit1,
app.transform.spring.cloud.stream.bindings.output.binder=kafka1,app.log.spring.cloud.stream.bindings.input.binder=kafka1"</code></pre>
</div>
</div>
<div class="paragraph">
<p>One can override any of the binder configuration properties by specifying them via deployment properties.</p>
</div>
</div>
</div>
<h1 id="spring-cloud-task" class="sect0">Tasks</h1>
<div class="openblock partintro">
<div class="content">
<div class="paragraph">
<p>This section goes into more detail about how you can work with
<a href="http://cloud.spring.io/spring-cloud-task/">Spring Cloud Task</a>. It covers topics such as
creating and running task applications.</p>
</div>
<div class="paragraph">
<p>If you&#8217;re just starting out with Spring Cloud Data Flow, you should probably read the
<em><a href="#getting-started">Getting Started</a></em> guide before diving into
this section.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-task-intro">36. Introducing Spring Cloud Task</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A task executes a process on demand.  In this case a task is a
<a href="http://projects.spring.io/spring-boot/">Spring Boot</a> application that is annotated with
<code>@EnableTask</code>.  Hence a user launches a task that performs a certain process, and once
complete the task ends. An example of a task would be a boot application that exports
data from a JDBC repository to an HDFS instance.  Tasks record the start time and the end
time as well as the boot exit code in a relational database. The task implementation is
based on the <a href="http://cloud.spring.io/spring-cloud-task/">Spring Cloud Task</a> project.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_lifecycle_of_a_task">37. The Lifecycle of a task</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Before we dive deeper into the details of creating Tasks, we need to understand the
typical lifecycle for tasks in the context of Spring Cloud Data Flow:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Register a Task App</p>
</li>
<li>
<p>Create a Task Definition</p>
</li>
<li>
<p>Launch a Task</p>
</li>
<li>
<p>Task Execution</p>
</li>
<li>
<p>Destroy a Task Definition</p>
</li>
</ol>
</div>
<div class="sect2">
<h3 id="_creating_a_custom_task_application">37.1. Creating a custom Task Application</h3>
<div class="paragraph">
<p>While Spring Cloud Task does provide a number of out of the box applications (via the
<a href="https://github.com/spring-cloud-task-app-starters">spring-cloud-task-app-starters</a>),
most task applications will be custom developed.  In order to create a custom task application:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a new project via <a href="http://start.spring.io">Spring Initializer</a> via either the
website or your IDE making sure to select the following starters:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p><code>Cloud Task</code> - This dependency is the <code>spring-cloud-starter-task</code>.</p>
</li>
<li>
<p><code>JDBC</code> - This is the dependency for the <code>spring-jdbc</code> starter.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Within your new project, create a new class that will serve as your main class:</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>@EnableTask
@SpringBootApplication
public class MyTask {

    public static void main(String[] args) {
		SpringApplication.run(MyTask.class, args);
	}
}</code></pre>
</div>
</div>
<div class="olist arabic">
<ol class="arabic" start="3">
<li>
<p>With this, you&#8217;ll need one or more <code>CommandLineRunner</code> or <code>ApplicationRunner</code> within
your application.  You can either implement your own or use the ones provided by Spring
Boot (there is one for running batch jobs for example).</p>
</li>
<li>
<p>Packaging your application up via Spring Boot into an über jar is done via the standard
Boot conventions.</p>
</li>
<li>
<p>The packaged application can be registered and deployed as noted below.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_registering_a_task_application">37.2. Registering a Task Application</h3>
<div class="paragraph">
<p>Register a Task App with the App Registry using the Spring Cloud Data Flow Shell
<code>app register</code> command. You must provide a unique name and a URI that can be
resolved to the app artifact. For the type, specify "task". Here are a few examples:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;app register --name task1 --type task --uri maven://com.example:mytask:1.0.2

dataflow:&gt;app register --name task2 --type task --uri file:///Users/example/mytask-1.0.2.jar

dataflow:&gt;app register --name task3 --type task --uri http://example.com/mytask-1.0.2.jar</code></pre>
</div>
</div>
<div class="paragraph">
<p>When providing a URI with the <code>maven</code> scheme, the format should conform to the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>maven://&lt;groupId&gt;:&lt;artifactId&gt;[:&lt;extension&gt;[:&lt;classifier&gt;]]:&lt;version&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you would like to register multiple apps at one time, you can store them in a properties file
where the keys are formatted as <code>&lt;type&gt;.&lt;name&gt;</code> and the values are the URIs. For example, this
would be a valid properties file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>task.foo=file:///tmp/foo.jar
task.bar=file:///tmp/bar.jar</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then use the <code>app import</code> command and provide the location of the properties file via <code>--uri</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>app import --uri file:///tmp/task-apps.properties</code></pre>
</div>
</div>
<div class="paragraph">
<p>For convenience, we have the static files with application-URIs (for both maven and docker) available for all the out-of-the-box
Task app-starters. You can point to this file and import all the application-URIs in bulk. Otherwise, as explained in
previous paragraphs, you can register them individually or have your own custom property file with only the required application-URIs
in it. It is recommended, however, to have a "focused" list of desired application-URIs in a custom property file.</p>
</div>
<div class="paragraph">
<p>List of available static property files:</p>
</div>
<table class="tableblock frame-topbot grid-all spread">
<colgroup>
<col style="width: 33%;">
<col style="width: 33%;">
<col style="width: 33%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Artifact Type</th>
<th class="tableblock halign-left valign-top">Stable Release</th>
<th class="tableblock halign-left valign-top">SNAPSHOT Release</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Maven</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Belmont-GA-task-applications-maven">http://bit.ly/Belmont-GA-task-applications-maven</a></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Belmont-BUILD-SNAPSHOT-task-applications-maven">http://bit.ly/Belmont-BUILD-SNAPSHOT-task-applications-maven</a></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Docker</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Belmont-GA-task-applications-docker">http://bit.ly/Belmont-GA-task-applications-docker</a></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a href="http://bit.ly/Belmont-BUILD-SNAPSHOT-task-applications-docker">http://bit.ly/Belmont-BUILD-SNAPSHOT-task-applications-docker</a></p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>For example, if you would like to register all out-of-the-box task applications in bulk, you can with
the following command.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;app import --uri http://bit.ly/Belmont-GA-task-applications-maven</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can also pass the <code>--local</code> option (which is TRUE by default) to indicate whether the
properties file location should be resolved within the shell process itself. If the location should
be resolved from the Data Flow Server process, specify <code>--local false</code>.</p>
</div>
<div class="paragraph">
<p>When using either <code>app register</code> or <code>app import</code>, if a task app is already registered with
the provided name, it will not be overridden by default. If you would like to override the
pre-existing task app, then include the <code>--force</code> option.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
In some cases the Resource is resolved on the server side, whereas in others the
URI will be passed to a runtime container instance where it is resolved. Consult
the specific documentation of each Data Flow Server for more detail.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_creating_a_task">37.3. Creating a Task</h3>
<div class="paragraph">
<p>Create a Task Definition from a Task App by providing a definition name as well as
properties that apply to the task execution.  Creating a task definition can be done via
the restful API or the shell.  To create a task definition using the shell, use the
<code>task create</code> command to create the task definition.  For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;task create mytask --definition "timestamp --format=\"yyyy\""
 Created new task 'mytask'</code></pre>
</div>
</div>
<div class="paragraph">
<p>A listing of the current task definitions can be obtained via the restful API or the
shell.  To get the task definition list using the shell, use the <code>task list</code> command.</p>
</div>
</div>
<div class="sect2">
<h3 id="_launching_a_task">37.4. Launching a Task</h3>
<div class="paragraph">
<p>An adhoc task can be launched via the restful API or via the shell.  To launch an ad-hoc
task via the shell use the <code>task launch</code> command.  For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;task launch mytask
 Launched task 'mytask'</code></pre>
</div>
</div>
<div class="paragraph">
<p>When a task is launched, any properties that need to be passed as the command line arguments
to the task application can be set when launching the task as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;task launch mytask --arguments "--server.port=8080,--foo=bar"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Additional properties meant for a <code>TaskLauncher</code> itself can be passed
in using a <code>--properties</code> option. Format of this option is a comma
delimited string of properties prefixed with <code>app.&lt;task definition
name&gt;.&lt;property&gt;</code>. Properties are passed
to <code>TaskLauncher</code> as application properties and it is up to an
implementation to choose how those are passed into an actual task
application. If the property is prefixed with <code>deployer</code> instead of <code>app</code> it is
passed to <code>TaskLauncher</code> as a deployment property and its meaning may
be <code>TaskLauncher</code> implementation specific.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;task launch mytask --properties "deployer.timestamp.foo1=bar1,app.timestamp.foo2=bar2"</code></pre>
</div>
</div>
<div class="sect3">
<h4 id="_common_application_properties">37.4.1. Common application properties</h4>
<div class="paragraph">
<p>In addition to configuration via DSL, Spring Cloud Data Flow provides a mechanism for setting common properties to all
the task applications that are launched by it.
This can be done by adding properties prefixed with <code>spring.cloud.dataflow.applicationProperties.task</code> when starting the server.
When doing so, the server will pass all the properties, without the prefix, to the instances it launches.</p>
</div>
<div class="paragraph">
<p>For example, all the launched applications can be configured to use the properties <code>foo</code> and <code>fizz</code> by launching the Data Flow server
with the following options:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>--spring.cloud.dataflow.applicationProperties.task.foo=bar
--spring.cloud.dataflow.applicationProperties.task.fizz=bar2</code></pre>
</div>
</div>
<div class="paragraph">
<p>This will cause the properties <code>foo=bar</code> and <code>fizz=bar2</code> to be passed to all the launched applications.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Properties configured using this mechanism have lower precedence than task deployment properties.
They will be overridden if a property with the same key is specified at task launch time (e.g. <code>app.trigger.fizz</code>
will override the common property).
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_reviewing_task_executions">37.5. Reviewing Task Executions</h3>
<div class="paragraph">
<p>Once the task is launched the state of the task is stored in a relational DB.  The state
includes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Task Name</p>
</li>
<li>
<p>Start Time</p>
</li>
<li>
<p>End Time</p>
</li>
<li>
<p>Exit Code</p>
</li>
<li>
<p>Exit Message</p>
</li>
<li>
<p>Last Updated Time</p>
</li>
<li>
<p>Parameters</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>A user can check the status of their task executions via the restful API or by the shell.
To display the latest task executions via the shell use the <code>task execution list</code> command.</p>
</div>
<div class="paragraph">
<p>To get a list of task executions for just one task definition, add <code>--name</code> and
the task definition name, for example <code>task execution list --name foo</code>.  To retrieve full
details for a task execution use the <code>task display</code> command with the id of the task execution,
for example <code>task display --id 549</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_destroying_a_task">37.6. Destroying a Task</h3>
<div class="paragraph">
<p>Destroying a Task Definition will remove the definition from the definition repository.
This can be done via the restful API or via the shell.  To destroy a task via the shell
use the <code>task destroy</code> command. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;task destroy mytask
 Destroyed task 'mytask'</code></pre>
</div>
</div>
<div class="paragraph">
<p>The task execution information for previously launched tasks for the definition will
remain in the task repository.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This will not stop any currently executing tasks for this definition, instead it just
removes the task definition from the database.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-task-repository">38. Task Repository</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Out of the box Spring Cloud Data Flow offers an embedded instance of the H2 database.
The H2 is good for development purposes but is not recommended for production use.</p>
</div>
<div class="sect2">
<h3 id="_configuring_the_task_execution_repository">38.1. Configuring the Task Execution Repository</h3>
<div class="paragraph">
<p>To add a driver for the database that will store the Task Execution information, a
dependency for the driver will need to be added to a maven pom file and the
Spring Cloud Data Flow will need to be rebuilt.  Since Spring Cloud Data Flow is comprised of an SPI for
each environment it supports, please review the SPI&#8217;s documentation on which POM should be
updated to add the dependency and how to build.  This document will cover how to setup the
dependency for local SPI.</p>
</div>
<div class="sect3">
<h4 id="_local">38.1.1. Local</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Open the spring-cloud-dataflow-server-local/pom.xml in your IDE.</p>
</li>
<li>
<p>In the <code>dependencies</code> section add the dependency for the database driver required.  In
the sample below postgresql has been chosen.</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;dependencies&gt;
...
    &lt;dependency&gt;
        &lt;groupId&gt;org.postgresql&lt;/groupId&gt;
        &lt;artifactId&gt;postgresql&lt;/artifactId&gt;
    &lt;/dependency&gt;
...
&lt;/dependencies&gt;</code></pre>
</div>
</div>
<div class="olist arabic">
<ol class="arabic" start="3">
<li>
<p>Save the changed pom.xml</p>
</li>
<li>
<p>Build the application as described here: <a href="appendix-building.html#building">Building Spring Cloud Data Flow</a></p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_task_application_repository">38.1.2. Task Application Repository</h4>
<div class="paragraph">
<p>When launching a task application be sure that the database driver that is being
used by Spring Cloud Data Flow is also a dependency on the task application. For
example if your Spring Cloud Data Flow is set to use Postgresql, be sure that
the task application <em>also</em> has Postgresql as a dependency.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
When executing tasks externally (i.e. command line) and you wish for
Spring Cloud Data Flow to show the TaskExecutions in its UI, be sure that
common datasource settings are shared among the both. By default
Spring Cloud Task will use a local H2 instance and the execution will
not be recorded to the database used by Spring Cloud Data Flow.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_datasource">38.2. Datasource</h3>
<div class="paragraph">
<p>To configure the datasource Add the following properties to the dataflow-server.yml or via
environment variables:</p>
</div>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>spring.datasource.url</p>
</li>
<li>
<p>spring.datasource.username</p>
</li>
<li>
<p>spring.datasource.password</p>
</li>
<li>
<p>spring.datasource.driver-class-name</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>For example adding postgres would look something like this:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Environment variables:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>export spring_datasource_url=jdbc:postgresql://localhost:5432/mydb
export spring_datasource_username=myuser
export spring_datasource_password=mypass
export spring_datasource_driver-class-name="org.postgresql.Driver"</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>dataflow-server.yml</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>spring:
  datasource:
    url: jdbc:postgresql://localhost:5432/mydb
    username: myuser
    password: mypass
    driver-class-name:org.postgresql.Driver</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-task-events">39. Subscribing to Task/Batch Events</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You can also tap into various task/batch events when the task is launched.
If the task is enabled to generate task and/or batch events (with the additional dependencies <code>spring-cloud-task-stream</code> and <code>spring-cloud-stream-binder-kafka</code>, in the case of Kafka as the binder), those events are published during the task lifecycle.
By default, the destination names for those published events on the broker (rabbit, kafka etc.,) are the event names themselves (for instance: <code>task-events</code>, <code>job-execution-events</code> etc.,).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;task create myTask --definition “myBatchJob"
dataflow:&gt;task launch myTask
dataflow:&gt;stream create task-event-subscriber1 --definition ":task-events &gt; log" --deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can control the destination name for those events by specifying explicit names when launching the task such as:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;task launch myTask --properties "spring.cloud.stream.bindings.task-events.destination=myTaskEvents"
dataflow:&gt;stream create task-event-subscriber2 --definition ":myTaskEvents &gt; log" --deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p>The default Task/Batch event and destination names on the broker are enumerated below:</p>
</div>
<table class="tableblock frame-all grid-all spread">
<caption class="title">Table 1. Task/Batch Event Destinations</caption>
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Event</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Destination</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Task events</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>task-events</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Job Execution events</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>job-execution-events</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Step Execution events</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>step-execution-events</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Item Read events</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>item-read-events</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Item Process events</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>item-process-events</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Item Write events</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>item-write-events</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Skip events</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>skip-events</code></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-launch-tasks-from-stream">40. Launching Tasks from a Stream</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You can launch a task from a stream by using one of the available <code>task-launcher</code> sinks. Currently the platforms supported
via the <code>task-launcher</code> sinks are
<a href="https://github.com/spring-cloud-stream-app-starters/tasklauncher-local">local</a>,
<a href="https://github.com/spring-cloud-stream-app-starters/tasklauncher-cloudfoundry">Cloud Foundry</a>, and
<a href="https://github.com/spring-cloud-stream-app-starters/tasklauncher-yarn">Yarn</a>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<code>task-launcher-local</code> is meant for development purposes only.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>A <code>task-launcher</code> sink expects a message containing a <a href="https://github.com/spring-cloud/spring-cloud-task/blob/master/spring-cloud-task-stream/src/main/java/org/springframework/cloud/task/launcher/TaskLaunchRequest.java">TaskLaunchRequest</a> object in its payload. From the <code>TaskLaunchRequest</code> object the <code>task-launcher</code> will obtain the URI of the artifact to be launched as well as the environment properties, command line arguments, deployment properties and application name to be used by the task.</p>
</div>
<div class="paragraph">
<p>The <a href="https://github.com/spring-cloud-stream-app-starters/tasklauncher-local/blob/v1.2.0.RELEASE/spring-cloud-starter-stream-sink-task-launcher-local/README.adoc">task-launcher-local</a> can be added to the available sinks by executing the app register command as follows (for the Rabbit Binder):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>app register --name task-launcher-local --type sink --uri maven://org.springframework.cloud.stream.app:task-launcher-local-sink-rabbit:jar:1.2.0.RELEASE</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the case of a maven based task that is to be launched, the <code>task-launcher</code> application is responsible for downloading the artifact.  You <strong>must</strong> configure the <code>task-launcher</code> with the appropriate configuration of <a href="https://github.com/spring-cloud/spring-cloud-deployer/blob/master/spring-cloud-deployer-resource-maven/src/main/java/org/springframework/cloud/deployer/resource/maven/MavenProperties.java">Maven Properties</a> such as <code>--maven.remote-repositories.repo1.url=http://repo.spring.io/libs-milestone"</code> to resolve artifacts, in this case against a milestone repo.  Note that this repo can be different than the one used to register the <code>task-launcher</code> application itself.</p>
</div>
<div class="sect2">
<h3 id="_triggertask">40.1. TriggerTask</h3>
<div class="paragraph">
<p>One way to launch a task using the <code>task-launcher</code> is to use the <a href="https://github.com/spring-cloud-stream-app-starters/triggertask/blob/v1.2.0.RELEASE/spring-cloud-starter-stream-source-triggertask/README.adoc">triggertask</a> source. The <code>triggertask</code> source
will emit a message with a <code>TaskLaunchRequest</code> object containing the required launch information.
The <code>triggertask</code> can be added to the available sources by executing the app register command as follows (for the Rabbit Binder):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>app register --type source --name triggertask --uri maven://org.springframework.cloud.stream.app:triggertask-source-rabbit:1.2.0.RELEASE</code></pre>
</div>
</div>
<div class="paragraph">
<p>An example of this would be to launch the timestamp task once every 60 seconds, the stream to implement this would look like:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>stream create foo --definition "triggertask --triggertask.uri=maven://org.springframework.cloud.task.app:timestamp-task:jar:1.2.0.RELEASE --trigger.fixed-delay=60 --triggertask.environment-properties=spring.datasource.url=jdbc:h2:tcp://localhost:19092/mem:dataflow,spring.datasource.username=sa | task-launcher-local --maven.remote-repositories.repo1.url=http://repo.spring.io/libs-release" --deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you execute <code>runtime apps</code> you can find the log file for the task launcher sink. Tailing that file you can find the log file for the launched tasks. The setting of <code>triggertask.environment-properties</code> is so that all the task executions can be collected in the same H2 database used in the local version of the Data Flow Server.  You can then see the list of task executions using the shell command <code>task execution list</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;task execution list
╔════════════════════╤══╤════════════════════════════╤════════════════════════════╤═════════╗
║     Task Name      │ID│         Start Time         │          End Time          │Exit Code║
╠════════════════════╪══╪════════════════════════════╪════════════════════════════╪═════════╣
║timestamp-task_26176│4 │Tue May 02 12:13:49 EDT 2017│Tue May 02 12:13:49 EDT 2017│0        ║
║timestamp-task_32996│3 │Tue May 02 12:12:49 EDT 2017│Tue May 02 12:12:49 EDT 2017│0        ║
║timestamp-task_58971│2 │Tue May 02 12:11:50 EDT 2017│Tue May 02 12:11:50 EDT 2017│0        ║
║timestamp-task_13467│1 │Tue May 02 12:10:50 EDT 2017│Tue May 02 12:10:50 EDT 2017│0        ║
╚════════════════════╧══╧════════════════════════════╧════════════════════════════╧═════════╝</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_tasklaunchrequest_transform">40.2. TaskLaunchRequest-transform</h3>
<div class="paragraph">
<p>Another option to start a task using the <code>task-launcher</code> would be to create a stream using the
<a href="https://github.com/spring-cloud-stream-app-starters/tasklaunchrequest-transform">Tasklaunchrequest-transform</a> processor to translate a message payload to a <code>TaskLaunchRequest</code>.</p>
</div>
<div class="paragraph">
<p>The <code>tasklaunchrequest-transform</code> can be added to the available processors by executing the app register command as follows (for the Rabbit Binder):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>app register --type processor --name tasklaunchrequest-transform --uri maven://org.springframework.cloud.stream.app:tasklaunchrequest-transform-processor-rabbit:1.2.0.RELEASE</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>stream create task-stream --definition "http --port=9000 | tasklaunchrequest-transform --uri=maven://org.springframework.cloud.task.app:timestamp-task:jar:1.2.0.RELEASE | task-launcher-local --maven.remote-repositories.repo1.url=http://repo.spring.io/libs-release"</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="spring-cloud-dataflow-composed-tasks">41. Composed Tasks</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Spring Cloud Data Flow allows a user to create a directed graph where each node
of the graph is a task application.  This is done by using the DSL for composed
tasks.  A composed task can be created via the RESTful API, the Spring Cloud
Data Flow Shell, or the Spring Cloud Data Flow UI.</p>
</div>
<div class="sect2">
<h3 id="_configuring_the_composed_task_runner_in_spring_cloud_data_flow">41.1. Configuring the Composed Task Runner in Spring Cloud Data Flow</h3>
<div class="paragraph">
<p>Composed tasks are executed via a task application called the <a href="https://github.com/spring-cloud-task-app-starters/composed-task-runner">Composed Task Runner</a>.</p>
</div>
<div class="sect3">
<h4 id="_registering_the_composed_task_runner_application">41.1.1. Registering the Composed Task Runner application</h4>
<div class="paragraph">
<p>Out of the box the Composed Task Runner application is not registered with Spring Cloud Data Flow. So, to launch composed tasks we must first register the Composed
Task Runner as an application with Spring Cloud Data Flow as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>app register --name composed-task-runner --type task --uri maven://org.springframework.cloud.task.app:composedtaskrunner-task:&lt;DESIRED_VERSION&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can also configure Spring Cloud Data Flow to use a different task definition
name for the composed task runner.  This can be done by setting the
<code>spring.cloud.dataflow.task.composedTaskRunnerName</code> property to the name
of your choice.  You can then register the composed task runner application with
the name you set using that property.</p>
</div>
</div>
<div class="sect3">
<h4 id="_configuring_the_composed_task_runner_application">41.1.2. Configuring the Composed Task Runner application</h4>
<div class="paragraph">
<p>The Composed Task Runner application has a <code>dataflow.server.uri</code> property that is used for validation and for launching child tasks. This defaults
to <code><a href="http://localhost:9393" class="bare">localhost:9393</a></code>. If you run a distributed Spring Cloud Data Flow server, like you would do if you deploy the server on Cloud Foundry,
YARN or Kubernetes, then you need to provide the URI that can be used to access the server. You can either provide this <code>dataflow.server.uri</code>
property for the Composed Task Runner application when launching a composed task, or you can provide a <code>spring.cloud.dataflow.server.uri</code> property
for the Spring Cloud Data Flow server when it is started. For the latter case the <code>dataflow.server.uri</code> Composed Task Runner application property
will be automatically set when a composed task is launched.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_creating_launching_and_destroying_a_composed_task">41.2. Creating, Launching, and Destroying a Composed Task</h3>
<div class="sect3">
<h4 id="_creating_a_composed_task">41.2.1. Creating a Composed Task</h4>
<div class="paragraph">
<p>The DSL for the composed tasks is used when creating a task definition via the
task create command. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt; app register --name timestamp --type task --uri maven://org.springframework.cloud.task.app:timestamp-task:&lt;DESIRED_VERSION&gt;
dataflow:&gt; app register --name mytaskapp --type task --uri file:///home/tasks/mytask.jar
dataflow:&gt; task create my-composed-task --definition "mytaskapp &amp;&amp; timestamp"
dataflow:&gt; task launch my-composed-task</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the example above we assume that the applications to be used by our composed
task have not been registered yet.  So the first two steps we register two task
applications.  We then create our composed task definition by using the task
create command.  The composed task DSL in the example above will, when launched,
execute mytaskapp and then execute the timestamp application.</p>
</div>
<div class="paragraph">
<p>But before we launch the my-composed-task definition,  we can view what
Spring Cloud Data Flow generated for us.  This can be done by executing the
task list command.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;task list
╔══════════════════════════╤═══════════════════════════════════════════════════════════════
║        Task Name         │                      Task Definition
╠══════════════════════════╪═══════════════════════════════════════════════════════════════
║my-composed-task          │mytaskapp &amp;&amp; timestamp
║my-composed-task-mytaskapp│mytaskapp
║my-composed-task-timestamp│timestamp</code></pre>
</div>
</div>
<div class="paragraph">
<p>Spring Cloud Data Flow created three task definitions, one for each of the
applications that comprises our composed task (<code>my-composed-task-mytaskapp</code> and
<code>my-composed-task-timestamp</code>) as well as the composed task (<code>my-composed-task</code>)
definition.  We also see that each of the generated
names for the child tasks is comprised of the name of the composed task and
the name of the application separated by a dash <code>-</code>.  i.e. <em>my-composed-task</em> <code>-</code>
<em>mytaskapp</em>.</p>
</div>
<div class="sect4">
<h5 id="_task_application_parameters">Task Application Parameters</h5>
<div class="paragraph">
<p>The task applications that comprise the composed task definition can also
contain parameters.  For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt; task create my-composed-task --definition "mytaskapp --displayMessage=hello &amp;&amp; timestamp --format=YYYY"</code></pre>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_launching_a_composed_task">41.2.2. Launching a Composed Task</h4>
<div class="paragraph">
<p>Launching a composed task is done the same way as launching a stand-alone task.
i.e.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>task launch my-composed-task</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once the task is launched and assuming all the tasks complete successfully you will
see three task executions when executing a <code>task execution list</code>.  For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;task execution list
╔══════════════════════════╤═══╤════════════════════════════╤════════════════════════════╤═════════╗
║        Task Name         │ID │         Start Time         │          End Time          │Exit Code║
╠══════════════════════════╪═══╪════════════════════════════╪════════════════════════════╪═════════╣
║my-composed-task-timestamp│713│Wed Apr 12 16:43:07 EDT 2017│Wed Apr 12 16:43:07 EDT 2017│0        ║
║my-composed-task-mytaskapp│712│Wed Apr 12 16:42:57 EDT 2017│Wed Apr 12 16:42:57 EDT 2017│0        ║
║my-composed-task          │711│Wed Apr 12 16:42:55 EDT 2017│Wed Apr 12 16:43:15 EDT 2017│0        ║
╚══════════════════════════╧═══╧════════════════════════════╧════════════════════════════╧═════════╝</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the example above we see that my-compose-task launched and it also launched
the other tasks in sequential order and all of them executed successfully with
"Exit Code" as <code>0</code>.</p>
</div>
<div class="sect4">
<h5 id="_exit_statuses">Exit Statuses</h5>
<div class="paragraph">
<p>The following list shows how the Exit Status will be set for each step (task)
contained in the composed task following each step execution.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If the <code>TaskExecution</code> has an <code>ExitMessage</code> that will be used as the <code>ExitStatus</code></p>
</li>
<li>
<p>If no <code>ExitMessage</code> is present and the <code>ExitCode</code> is set to zero then the <code>ExitStatus</code>
for the step will be <code>COMPLETED</code>.</p>
</li>
<li>
<p>If no <code>ExitMessage</code> is present and the <code>ExitCode</code> is set to any non zero number
then the <code>ExitStatus</code> for the step will be <code>FAILED</code>.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_destroying_a_composed_task">41.2.3. Destroying a Composed Task</h4>
<div class="paragraph">
<p>The same command used to destroy a stand-alone task is the same as destroying a
composed task.  The only difference is that destroying a composed task will
also destroy the child tasks associated with it.   For example</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>dataflow:&gt;task list
╔══════════════════════════╤═══════════════════════════════════════════════════════════════
║        Task Name         │                      Task Definition
╠══════════════════════════╪═══════════════════════════════════════════════════════════════
║my-composed-task          │mytaskapp &amp;&amp; timestamp
║my-composed-task-mytaskapp│mytaskapp
║my-composed-task-timestamp│timestamp

...
dataflow:&gt;task destroy my-composed-task
dataflow:&gt;task list
╔══════════════════════════╤═══════════════════════════════════════════════════════════════
║        Task Name         │                      Task Definition
╠══════════════════════════╪═══════════════════════════════════════════════════════════════
╚══════════════════════════╧═══════════════════════════════════════════════════════════════</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_stopping_a_composed_task">41.2.4. Stopping a Composed Task</h4>
<div class="paragraph">
<p>In cases where a composed task execution needs to be stopped.  This can be done
via the:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>RESTful API</p>
</li>
<li>
<p>Spring Cloud Data Flow Dashboard by selecting the Job&#8217;s tab and then
clicking the stop button by the job execution that needs to be stopped.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The composed task run will be stopped
when the currently running child task completes.  The step associated with the
child task that was running at the time that the composed task was stopped will
be marked as <code>STOPPED</code> as well as the composed task job execution.</p>
</div>
</div>
<div class="sect3">
<h4 id="_restarting_a_composed_task">41.2.5. Restarting a Composed Task</h4>
<div class="paragraph">
<p>In cases where a composed task fails during execution and the status of the
composed task is <code>FAILED</code> then the task can be restarted.  This can be done
via the:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>RESTful API</p>
</li>
<li>
<p>Shell by launching the task using the same parameters</p>
</li>
<li>
<p>Spring Cloud Data Flow Dashboard by selecting the Job&#8217;s tab and then
clicking the restart button by the job execution that needs to be restarted.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Restarting a Composed Task job that has been stopped (via the
Spring Cloud Data Flow Dashboard or RESTful API), will relaunch  the
<code>STOPPED</code> child task, and then launch the remaining (unlaunched) child tasks
in the specified order.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_composed_task_dsl">41.3. Composed Task DSL</h3>
<div class="sect3">
<h4 id="_conditional_execution">41.3.1. Conditional Execution</h4>
<div class="paragraph">
<p>Conditional execution is expressed using a double ampersand symbol <code>&amp;&amp;</code>.
This allows each task in the sequence to be launched only if the previous task
successfully completed. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>task create my-composed-task --definition "foo &amp;&amp; bar"</code></pre>
</div>
</div>
<div class="paragraph">
<p>When the composed task my-composed-task is launched, it will launch the
task <code>foo</code> and if it completes successfully, then the task <code>bar</code> will be
launched. If the <code>foo</code> task fails, then the task <code>bar</code> will not launch.</p>
</div>
<div class="paragraph">
<p>You can also use the Spring Cloud Data Flow Dashboard to create your conditional
execution. By using the designer to drag and drop applications
that are required, and connecting them together to create your directed graph.
For example:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-ctr-conditional-execution.png" alt="Composed Task Conditional Execution">
</div>
<div class="title">Figure 6. Conditional Execution</div>
</div>
<div class="paragraph">
<p>The diagram above is a screen capture of the directed graph as it being created
using the Spring Cloud Data Flow Dashboard.  We see that are 4 components
in the diagram that comprise a conditional execution:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Start icon - All directed graphs start from this symbol.  There will
only be one.</p>
</li>
<li>
<p>Task icon - Represents each task in the directed graph.</p>
</li>
<li>
<p>End icon - Represents the termination of a directed graph.</p>
</li>
<li>
<p>Solid line arrow - Represents the flow conditional execution flow
between:</p>
<div class="ulist">
<ul>
<li>
<p>Two applications</p>
</li>
<li>
<p>The start control node and an application</p>
</li>
<li>
<p>An application and the end control node</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You can view a diagram of your directed graph by clicking the detail
button next to the composed task definition on the definitions tab.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_transitional_execution">41.3.2. Transitional Execution</h4>
<div class="paragraph">
<p>The DSL supports fine grained control over the transitions taken during the
execution of the directed graph. Transitions are specified by providing a
condition for equality based on the exit status of the previous task.
A task transition is represented by the following symbol <code>-&gt;</code>.</p>
</div>
<div class="sect4">
<h5 id="_basic_transition">Basic Transition</h5>
<div class="paragraph">
<p>A basic transition would look like the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>task create my-transition-composed-task --definition "foo 'FAILED' -&gt; bar 'COMPLETED' -&gt; baz"</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the example above <code>foo</code> would launch and if it had an exit status of <code>FAILED</code>,
then the <code>bar</code> task would launch. If the exit status of <code>foo</code> was <code>COMPLETED</code>
then <code>baz</code> would launch. All other statuses returned by <code>foo</code> will have no effect
and task would terminate normally.</p>
</div>
<div class="paragraph">
<p>Using the Spring Cloud Data Flow Dashboard to create  the same "basic
transition" would look like:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-ctr-transition-basic.png" alt="Composed Task Basic Transition">
</div>
<div class="title">Figure 7. Basic Transition</div>
</div>
<div class="paragraph">
<p>The diagram above is a screen capture of the directed graph as it being created
using the Spring Cloud Data Flow Dashboard.  Notice that there are 2 different
types of connectors:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Dashed line - Is the line used to represent transitions from the application
to one of the possible destination applications.</p>
</li>
<li>
<p>Solid line - Used to connect applications in a conditional execution or a
connection between the application and a control node (end, start).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>When creating a transition, link the application to each of possible
destination using the connector.  Once complete go to each connection and
select it by clicking it.  A bolt icon should appear, click that icon and
enter the exit status required for that connector.  The solid line for that
connector will turn to a dashed line.</p>
</div>
</div>
<div class="sect4">
<h5 id="_transition_with_a_wildcard">Transition With a Wildcard</h5>
<div class="paragraph">
<p>Wildcards are supported for transitions by the DSL for example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>task create my-transition-composed-task --definition "foo 'FAILED' -&gt; bar '*' -&gt; baz"</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the example above <code>foo</code> would launch and if it had an exit status of <code>FAILED</code>,
then the <code>bar</code> task would launch. Any exit status of <code>foo</code> other than <code>FAILED</code>
then <code>baz</code> would launch.</p>
</div>
<div class="paragraph">
<p>Using the Spring Cloud Data Flow Dashboard to create the same
"transition with wildcard" would look like:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-ctr-transition-basic-wildcard.png" alt="Composed Task Basic Transition with Wildcard">
</div>
<div class="title">Figure 8. Basic Transition With Wildcard</div>
</div>
</div>
<div class="sect4">
<h5 id="_transition_with_a_following_conditional_execution">Transition With a Following Conditional Execution</h5>
<div class="paragraph">
<p>A transition can be followed by a conditional execution so long as the wildcard
is not used. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>task create my-transition-conditional-execution-task --definition "foo 'FAILED' -&gt; bar 'UNKNOWN' -&gt; baz &amp;&amp; qux &amp;&amp; quux"</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the example above <code>foo</code> would launch and if it had an exit status of <code>FAILED</code>,
then the <code>bar</code> task would launch.  If <code>foo</code> had an exit status of <code>UNKNOWN</code> then
<code>baz</code> would launch.  Any exit status of <code>foo</code> other than <code>FAILED</code> or <code>UNKNOWN</code>
then <code>qux</code> would launch and upon successful completion <code>quux</code> would launch.</p>
</div>
<div class="paragraph">
<p>Using the Spring Cloud Data Flow Dashboard to create the same
"transition with conditional execution" would look like:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-ctr-transition-conditional-execution.png" alt="Composed Task Transition with Conditional Execution">
</div>
<div class="title">Figure 9. Transition With Conditional Execution</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
In this diagram we see the dashed line (transition) connecting the <code>foo</code> application
to the target applications, but a solid line connecting the conditional executions
between <code>foo</code>, <code>qux</code>, and  <code>quux</code>.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_split_execution">41.3.3. Split Execution</h4>
<div class="paragraph">
<p>Splits allow for multiple tasks within a composed task to be run in parallel.
It is denoted by using angle brackets &lt;&gt; to group tasks and flows that are to
be run in parallel. These tasks and flows are separated by the double pipe <code>||</code>
. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>task create my-split-task --definition "&lt;foo || bar || baz&gt;"</code></pre>
</div>
</div>
<div class="paragraph">
<p>The example above will launch tasks <code>foo</code>, <code>bar</code> and <code>baz</code> in parallel.</p>
</div>
<div class="paragraph">
<p>Using the Spring Cloud Data Flow Dashboard to create the same
"split execution" would look like:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-ctr-split.png" alt="Composed Task Split">
</div>
<div class="title">Figure 10. Split</div>
</div>
<div class="paragraph">
<p>With the task DSL a user may also execute multiple split groups
in succession. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>task create my-split-task --definition "&lt;foo || bar || baz&gt; &amp;&amp; &lt;qux || quux&gt;"</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the example above tasks <code>foo</code>, <code>bar</code> and <code>baz</code> will be launched in parallel,
once they all complete then tasks <code>qux</code>, <code>quux</code> will be launched in parallel.
Once they complete the composed task will end.   However if <code>foo</code>, <code>bar</code>, or
<code>baz</code> fails then, the split containing <code>qux</code> and <code>quux</code> will not launch.</p>
</div>
<div class="paragraph">
<p>Using the Spring Cloud Data Flow Dashboard to create the same
"split with multiple groups" would look like:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-ctr-multiple-splits.png" alt="Composed Task Split">
</div>
<div class="title">Figure 11. Split as a part of a conditional execution</div>
</div>
<div class="paragraph">
<p>Notice that there is a <code>SYNC</code> control node that is by the designer when
connecting two consecutive splits.</p>
</div>
<div class="sect4">
<h5 id="_split_containing_conditional_execution">Split Containing Conditional Execution</h5>
<div class="paragraph">
<p>A split can also have a conditional execution within the angle brackets.  For
example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>task create my-split-task --definition "&lt;foo &amp;&amp; bar || baz&gt;"</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the example above we see that <code>foo</code> and <code>baz</code> will be launched in parallel,
however <code>bar</code> will not launch until <code>foo</code> completes successfully.</p>
</div>
<div class="paragraph">
<p>Using the Spring Cloud Data Flow Dashboard to create the same
"split containing conditional execution" would look like:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-ctr-split-contains-conditional.png" alt="Composed Task Split With Conditional Execution">
</div>
<div class="title">Figure 12. Split with conditional execution</div>
</div>
</div>
</div>
</div>
</div>
</div>
<h1 id="dashboard" class="sect0">Dashboard</h1>
<div class="openblock partintro">
<div class="content">
<div class="paragraph">
<p>This section describe how to use the Dashboard of Spring Cloud Data Flow.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="dashboard-introduction">42. Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Spring Cloud Data Flow provides a browser-based GUI and it currently includes 6 tabs:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Apps</strong> Lists all available applications and provides the control to register/unregister them</p>
</li>
<li>
<p><strong>Runtime</strong> Provides the Data Flow cluster view with the list of all running applications</p>
</li>
<li>
<p><strong>Streams</strong> List, create, deploy, and destroy Stream Definitions</p>
</li>
<li>
<p><strong>Tasks</strong> List, create, launch and destroy Task Definitions</p>
</li>
<li>
<p><strong>Jobs</strong> Perform Batch Job related functions</p>
</li>
<li>
<p><strong>Analytics</strong> Create data visualizations for the various analytics applications</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Upon starting Spring Cloud Data Flow, the Dashboard is available at:</p>
</div>
<div class="paragraph">
<p><code>http://&lt;host&gt;:&lt;port&gt;/dashboard</code></p>
</div>
<div class="paragraph">
<p>For example: <a href="http://localhost:9393/dashboard">http://localhost:9393/dashboard</a></p>
</div>
<div class="paragraph">
<p>If you have enabled https, then it will be located at <code>https://localhost:9393/dashboard</code>.
If you have enabled security, a login form is available at <code>http://localhost:9393/dashboard/#/login</code>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The default Dashboard server port is <code>9393</code>
</td>
</tr>
</table>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-dashboard-about.png" alt="The Spring Cloud Data Flow Dashboard">
</div>
<div class="title">Figure 13. The Spring Cloud Data Flow Dashboard</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="dashboard-apps">43. Apps</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <em>Apps</em> section of the Dashboard lists all the available applications and
provides the control to register/unregister them (if applicable). It is possible
to import a number of applications at once using the <strong>Bulk Import Applications</strong> action.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-available-apps-list.png" alt="List of available applications">
</div>
<div class="title">Figure 14. List of Available Applications</div>
</div>
<div class="sect2">
<h3 id="_bulk_import_of_applications">43.1. Bulk Import of Applications</h3>
<div class="paragraph">
<p>The bulk import applications page provides numerous options for defining and importing a set of
applications in one go. For bulk import the application definitions are expected to be
expressed in a properties style:</p>
</div>
<div class="paragraph">
<p><code>&lt;type&gt;.&lt;name&gt; = &lt;coordinates&gt;</code></p>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="paragraph">
<p><code>task.timestamp=maven://org.springframework.cloud.task.app:timestamp-task:1.2.0.RELEASE</code></p>
</div>
<div class="paragraph">
<p><code>processor.transform=maven://org.springframework.cloud.stream.app:transform-processor-rabbit:1.2.0.RELEASE</code></p>
</div>
<div class="paragraph">
<p>At the top of the bulk import page an <em>Uri</em> can be specified that points to a properties file stored elsewhere, it
should contain properties formatted as above. Alternatively, using the textbox labeled <em>Apps as Properties</em>
it is possible to directly list each property string. Finally, if the properties are stored in a local file
the <em>Select Properties File</em> option will open a local file browser to select the file. After setting your definitions
via one of these routes, click <strong>Import</strong>.</p>
</div>
<div class="paragraph">
<p>At the bottom of the page there are quick links to the property files for common groups of stream apps and
task apps. If those meet your needs, simply select your appropriate variant (rabbit, kafka, docker, etc) and
click the <strong>Import</strong> action on those lines to immediately import all those applications.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-bulk-import-applications.png" alt="Bulk Import Applications">
</div>
<div class="title">Figure 15. Bulk Import Applications</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="dashboard-runtime">44. Runtime</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <em>Runtime</em> section of the Dashboard application shows the Spring Cloud Data Flow
cluster view with the list of all running applications. For each runtime app the
state of the deployment and the number of deployed instances is shown.
A list of the used deployment properties is available by clicking on the
app id.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-runtime.png" alt="List of running applications">
</div>
<div class="title">Figure 16. List of Running Applications</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="dashboard-streams">45. Streams</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <em>Streams</em> section of the Dashboard provides the <em>Definitions</em> tab that provides
a listing of Stream definitions. There you have the option to <strong>deploy</strong> or <strong>undeploy</strong>
those stream definitions. Additionally you can remove the definition by clicking on <strong>destroy</strong>.
Each row includes an arrow on the left, which can be clicked to see a visual representation
of the definition. Hovering over the boxes in the visual representation will show more details
about the apps including any options passed to them. In this screenshot the timer stream has
been expanded to show the visual representation:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-streams-list-definitions.png" alt="List of Stream Definitions">
</div>
<div class="title">Figure 17. List of Stream Definitions</div>
</div>
<div class="paragraph">
<p>If the <strong>details</strong> button is clicked the view will change to show a visual representation of that
stream and also any related streams.  In the above example, if clicking <strong>details</strong> for the timer stream, the
view will change to the one shown below which clearly shows the relationship between the three streams (two of
them are tapping into the timer stream).</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-stream-details.png" alt="Stream Details Page">
</div>
<div class="title">Figure 18. Stream Details Page</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="dashboard-flo-streams-designer">46. Create Stream</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <em>Create Stream</em> section of the Dashboard includes the <a href="https://github.com/spring-projects/spring-flo">Spring Flo</a> designer tab that provides the canvas application, offering a interactive graphical interface for creating data pipelines.</p>
</div>
<div class="paragraph">
<p>In this tab, you can:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Create, manage, and visualize stream pipelines using DSL, a graphical canvas, or both</p>
</li>
<li>
<p>Write pipelines via DSL with content-assist and auto-complete</p>
</li>
<li>
<p>Use auto-adjustment and grid-layout capabilities in the GUI for simpler and interactive organization of pipelines</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Watch this <a href="https://www.youtube.com/watch?v=78CgV46OstI">screencast</a> that highlights some of the "Flo for Spring Cloud Data Flow" capabilities. Spring Flo <a href="https://github.com/spring-projects/spring-flo/wiki">wiki</a> includes more detailed content on core Flo capabilities.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-flo-create-stream.png" alt="Flo for Spring Cloud Data Flo">
</div>
<div class="title">Figure 19. Flo for Spring Cloud Data Flow</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="dashboard-tasks">47. Tasks</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <em>Tasks</em> section of the Dashboard currently has three tabs:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Apps</p>
</li>
<li>
<p>Definitions</p>
</li>
<li>
<p>Executions</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="dashboard-tasks-apps">47.1. Apps</h3>
<div class="paragraph">
<p><em>Apps</em> encapsulate a unit of work into a reusable component. Within the Data Flow
runtime environment <em>Apps</em> allow users to create definitions for <em>Streams</em> as
well as <em>Tasks</em>. Consequently, the <em>Apps</em> tab within the <em>Tasks</em> section
allows users to create <em>Task</em> definitions.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You will also use this tab to create Batch Jobs.
</td>
</tr>
</table>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-task-apps-list.png" alt="List of Task Apps">
</div>
<div class="title">Figure 20. List of Task Apps</div>
</div>
<div class="paragraph">
<p>On this screen you can perform the following actions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>View details such as the task app options.</p>
</li>
<li>
<p>Create a Task Definition from the respective App.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_create_a_task_definition_from_a_selected_task_app">47.1.1. Create a Task Definition from a selected Task App</h4>
<div class="paragraph">
<p>On this screen you can create a new Task Definition. As a minimum you must provide a
name for the new definition. You will also have the option
to specify various properties that are used during the deployment of the app.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Each parameter is only included if the <em>Include</em> checkbox is selected.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_view_task_app_details">47.1.2. View Task App Details</h4>
<div class="paragraph">
<p>On this page you can view the details of a selected task app,
including the list of available options (properties) for that app.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="dashboard-task-definition">47.2. Definitions</h3>
<div class="paragraph">
<p>This page lists the Data Flow Task definitions and provides actions to <strong>launch</strong>
or <strong>destroy</strong> those tasks. It also provides a shortcut operation to define
one or more tasks using simple textual input, indicated by
the <strong>bulk define tasks</strong> button.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-task-definitions-list.png" alt="List of Task Definitions">
</div>
<div class="title">Figure 21. List of Task Definitions</div>
</div>
<div class="sect3">
<h4 id="_creating_task_definitions_using_the_bulk_define_interface">47.2.1. Creating Task Definitions using the bulk define interface</h4>
<div class="paragraph">
<p>After pressing <strong>bulk define tasks</strong>, the following screen will
be shown.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-bulk-define-tasks.png" alt="Bulk Define Tasks">
</div>
<div class="title">Figure 22. Bulk Define Tasks</div>
</div>
<div class="paragraph">
<p>It includes a textbox where one or more definitions can be entered
and then various actions performed on those definitions.
The required input text format for task definitions is very basic, each line should be
of the form:</p>
</div>
<div class="paragraph">
<p><code>&lt;task-definition-name&gt; = &lt;task-application&gt; &lt;options&gt;</code></p>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="paragraph">
<p><code>demo-timestamp = timestamp --format=hhmmss</code></p>
</div>
<div class="paragraph">
<p>After entering any data a validator will run asynchronously to
verify both the syntax and that the application name entered is
a valid application and it supports the options
specified. If validation fails the editor will show the errors with more
information via tooltips.</p>
</div>
<div class="paragraph">
<p>To make it easier to enter definitions into the text area, content assist is
supported. Pressing <strong>Ctrl+Space</strong> will invoke content assist to suggest
simple task names (based on the line on which it is invoked), task applications
and task application options. Press ESCape to close the content assist
window without taking a selection.</p>
</div>
<div class="paragraph">
<p>If the validator should not verify the applications or the options
(for example if specifying non-whitelisted options to the
applications) then turn off that part of validation by toggling the checkbox
off on the <strong>Verify Apps</strong> button - the validator will then only perform
syntax checking.  When correctly validated, the <strong>create</strong> button will
be clickable and on pressing it the UI will proceed to create each task definition. If there
are any errors during creation then after creation finishes the
editor will show any lines of input, as it cannot be used in task definitions.
These can then be fixed up and creation repeated.  There is an
<strong>import file</strong> button to open a file browser on the
local file system if the definitions are in a file and it is easier
to import than copy/paste.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Bulk loading of composed task definitions is not currently supported.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_creating_composed_task_definitions">47.2.2. Creating Composed Task Definitions</h4>
<div class="paragraph">
<p>The dashboard includes the Create Composed Task tab that provides the canvas
application, offering a interactive graphical interface for creating
composed tasks.</p>
</div>
<div class="paragraph">
<p>In this tab, you can:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Create and visualize composed tasks using DSL, a graphical canvas, or both</p>
</li>
<li>
<p>Use auto-adjustment and grid-layout capabilities in the GUI for simpler and
interactive organization of the composed task</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-ctr-flo-tab.png" alt="Composed Task Designer">
</div>
<div class="title">Figure 23. Composed Task Designer</div>
</div>
</div>
<div class="sect3">
<h4 id="_launching_tasks">47.2.3. Launching Tasks</h4>
<div class="paragraph">
<p>Once the task definition is created, they can be launched through the Dashboard
as well. Navigate to the <strong>Definitions</strong> tab. Select the Task you want to launch by
pressing <code>Launch</code>.</p>
</div>
<div class="paragraph">
<p>On the following screen, you can define one or more Task parameters by entering:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Parameter Key</p>
</li>
<li>
<p>Parameter Value</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Task parameters are not typed.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="dashboard-tasks-executions">47.3. Executions</h3>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-task-executions-list.png" alt="List of Task Executions">
</div>
<div class="title">Figure 24. List of Task Executions</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="dashboard-jobs">48. Jobs</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <em>Jobs</em> section of the Dashboard allows you to inspect <strong>Batch Jobs</strong>. The main
section of the screen provides a list of Job Executions. <strong>Batch Jobs</strong>
are <strong>Tasks</strong> that were executing one or more <strong>Batch Job</strong>. As such each
Job Execution has a back reference to the <strong>Task Execution Id</strong> (Task Id).</p>
</div>
<div class="paragraph">
<p>In case of a failed job, you can also restart the task. When dealing with long-running
Batch Jobs, you can also request to stop it.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-job-executions-list.png" alt="List of Job Executions">
</div>
<div class="title">Figure 25. List of Job Executions</div>
</div>
<div class="sect2">
<h3 id="dashboard-job-executions-list">48.1. List job executions</h3>
<div class="paragraph">
<p>This page lists the Batch Job Executions and provides the option to <strong>restart</strong> or <strong>stop</strong> a specific job execution, provided the operation is available.
Furthermore, you have the option to view the Job execution details.</p>
</div>
<div class="paragraph">
<p>The list of Job Executions also shows the state of the underlying Job Definition.
Thus, if the underlying definition has been deleted, <em>deleted</em> will be shown.</p>
</div>
<div class="sect3">
<h4 id="dashboard-job-executions-details">48.1.1. Job execution details</h4>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-jobs-job-execution-details.png" alt="Job Execution Details">
</div>
<div class="title">Figure 26. Job Execution Details</div>
</div>
<div class="paragraph">
<p>The Job Execution Details screen also contains a list of the executed steps. You can
further drill into the <em>Step Execution Details</em> by clicking onto the magnifying glass.</p>
</div>
</div>
<div class="sect3">
<h4 id="dashboard-job-executions-steps">48.1.2. Step execution details</h4>
<div class="paragraph">
<p>On the top of the page, you will see progress indicator the respective step, with
the option to refresh the indicator. Furthermore, a link is provided to view the
<em>step execution history</em>.</p>
</div>
<div class="paragraph">
<p>The Step Execution details screen provides a complete list of all Step Execution
Context key/value pairs.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
In case of exceptions, the <em>Exit Description</em> field will contain
additional error information. Please be aware, though, that this field can only
have a maximum of <strong>2500 characters</strong>. Therefore, in case of long exception
stacktraces, trimming of error messages may occur. In that case, please refer to
the server log files for further details.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="dashboard-job-executions-steps-progress">48.1.3. Step Execution Progress</h4>
<div class="paragraph">
<p>On this screen, you can see a progress bar indicator in regards to the execution
of the current step. Under the <strong>Step Execution History</strong>, you can also view various
metrics associated with the selected step such as <strong>duration</strong>, <strong>read counts</strong>, <strong>write
counts</strong> etc.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.2.2.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-step-execution-history.png" alt="Step Execution History">
</div>
<div class="title">Figure 27. Step Execution History</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="dashboard-analytics">49. Analytics</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <em>Analytics</em> section of the Dashboard provided data visualization capabilities
for the various analytics applications available in <em>Spring Cloud Data Flow</em>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Counters</p>
</li>
<li>
<p>Field-Value Counters</p>
</li>
<li>
<p>Aggregate Counters</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For example, if you create a stream with a <a href="https://github.com/spring-cloud-stream-app-starters/counter/tree/master/spring-cloud-starter-stream-sink-counter">Counter</a> application, you can now easily create the corresponding
graph from within the <strong>Dashboard</strong> tab:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Under <code>Metric Type</code>, select <code>Counters</code> from the select box</p>
</li>
<li>
<p>Under <code>Stream</code>, select <code>tweetcount</code></p>
</li>
<li>
<p>Under <code>Visualization</code>, select the desired chart option, <code>Bar Chart</code></p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Using the icons to the right, you can add additional charts to the Dashboard,
re-arange the order of created dashboards or remove data visualizations.</p>
</div>
</div>
</div>
<h1 id="howto" class="sect0">&#8216;How-to&#8217; guides</h1>
<div class="openblock partintro">
<div class="content">
<div class="paragraph">
<p>This section provides answers to some common &#8216;how do I do that&#8230;&#8203;&#8217; type of questions
that often arise when using Spring Cloud Data Flow.</p>
</div>
<div class="paragraph">
<p>If you are having a specific problem that we don&#8217;t cover here, you might want to check out
<a href="http://stackoverflow.com/tags/spring-cloud-dataflow">stackoverflow.com</a> to see if someone has
already provided an answer; this is also a great place to ask new questions (please use
the <code>spring-cloud-dataflow</code> tag).</p>
</div>
<div class="paragraph">
<p>We&#8217;re also more than happy to extend this section; If you want to add a &#8216;how-to&#8217; you
can send us a <a href="http://github.com/spring-cloud/spring-cloud-dataflow-server-yarn/tree/master">pull request</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_configure_maven_properties">50. Configure Maven Properties</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You can set the maven properties such as local maven repository location, remote maven repositories and their authentication credentials including
the proxy server properties via commandline properties when starting the Dataflow server or using the <code>SPRING_APPLICATION_JSON</code> environment property
for the Dataflow server.</p>
</div>
<div class="paragraph">
<p>The remote maven repositories need to be configured explicitly if the apps are resolved using maven repository except for <code>local</code> Data Flow server. The other
 Data Flow server implementations (that use maven resources for app artifacts resolution) have no default value for remote repositories.
 The <code>local</code> server has <code><a href="https://repo.spring.io/libs-snapshot" class="bare">repo.spring.io/libs-snapshot</a></code> as the default remote repository.</p>
</div>
<div class="paragraph">
<p>To pass the properties as commandline options:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ java -jar &lt;dataflow-server&gt;.jar --maven.localRepository=mylocal
--maven.remote-repositories.repo1.url=https://repo1
--maven.remote-repositories.repo1.auth.username=repo1user
--maven.remote-repositories.repo1.auth.password=repo1pass
--maven.remote-repositories.repo2.url=https://repo2 --maven.proxy.host=proxyhost
--maven.proxy.port=9018 --maven.proxy.auth.username=proxyuser
--maven.proxy.auth.password=proxypass</code></pre>
</div>
</div>
<div class="paragraph">
<p>or, using the <code>SPRING_APPLICATION_JSON</code> environment property:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">export SPRING_APPLICATION_JSON='{ "maven": { "local-repository": "local","remote-repositories": { "repo1": { "url": "https://repo1", "auth": { "username": "repo1user", "password": "repo1pass" } },
"repo2": { "url": "https://repo2" } }, "proxy": { "host": "proxyhost", "port": 9018, "auth": { "username": "proxyuser", "password": "proxypass" } } } }'</code></pre>
</div>
</div>
<div class="paragraph">
<p>Formatted JSON:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">SPRING_APPLICATION_JSON='{
  "maven": {
    "local-repository": "local",
    "remote-repositories": {
      "repo1": {
        "url": "https://repo1",
        "auth": {
          "username": "repo1user",
          "password": "repo1pass"
        }
      },
      "repo2": {
        "url": "https://repo2"
      }
    },
    "proxy": {
      "host": "proxyhost",
      "port": 9018,
      "auth": {
        "username": "proxyuser",
        "password": "proxypass"
      }
    }
  }
}'</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Depending on Spring Cloud Data Flow server implementation, you may have to pass the
environment properties using the platform specific environment-setting capabilities. For instance,
in Cloud Foundry, you&#8217;d be passing them as <code>cf set-env SPRING_APPLICATION_JSON</code>.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_logging">51. Logging</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Spring Cloud Data Flow is built upon several Spring projects, but ultimately the dataflow-server is a
Spring Boot app, so the logging techniques that apply to any <a href="http://docs.spring.io/spring-boot/docs/current/reference/html/howto-logging.html#howto-logging">Spring Boot</a>
application are applicable here as well.</p>
</div>
<div class="paragraph">
<p>While troubleshooting, following are the two primary areas where enabling the DEBUG logs could be
useful.</p>
</div>
<div class="sect2">
<h3 id="_deployment_logs">51.1. Deployment Logs</h3>
<div class="paragraph">
<p>Spring Cloud Data Flow builds upon <a href="https://github.com/spring-cloud/spring-cloud-deployer">Spring Cloud Deployer</a> SPI
and the platform specific dataflow-server uses the respective <a href="https://github.com/spring-cloud?utf8=%E2%9C%93&amp;q=spring-cloud-deployer">SPI implementations</a>.
Specifically, if we were to troubleshoot deployment specific issues; such as the network errors, it&#8217;d
be useful to enable the DEBUG logs at the underlying deployer and the libraries used by it.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>For instance, if you&#8217;d like to enable DEBUG logs for the <a href="https://github.com/spring-cloud/spring-cloud-deployer-local">local-deployer</a>,
you&#8217;d be starting the server with following.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ java -jar &lt;dataflow-server&gt;.jar --logging.level.org.springframework.cloud.deployer.spi.local=DEBUG</code></pre>
</div>
</div>
<div class="paragraph">
<p>(<em>where, <code>org.springframework.cloud.deployer.spi.local</code> is the global package for everything local-deployer
related</em>)</p>
</div>
</li>
<li>
<p>For instance, if you&#8217;d like to enable DEBUG logs for the <a href="https://github.com/spring-cloud/spring-cloud-deployer-cloudfoundry">cloudfoundry-deployer</a>,
you&#8217;d be setting the following environment variable and upon restaging the dataflow-server, we will
see more logs around request, response and the elaborate stack traces (<em>upon failures</em>). The cloudfoundry-deployer
uses <a href="https://github.com/cloudfoundry/cf-java-client">cf-java-client</a>, so we will have to enable DEBUG
logs for this library.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ cf set-env dataflow-server JAVA_OPTS '-Dlogging.level.cloudfoundry-client=DEBUG'
$ cf restage dataflow-server</code></pre>
</div>
</div>
<div class="paragraph">
<p>(<em>where, <code>cloudfoundry-client</code> is the global package for everything <code>cf-java-client</code> related</em>)</p>
</div>
</li>
<li>
<p>If there&#8217;s a need to review Reactor logs, which is used by the <code>cf-java-client</code>, then the following
would be helpful.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ cf set-env dataflow-server JAVA_OPTS '-Dlogging.level.cloudfoundry-client=DEBUG -Dlogging.level.reactor.ipc.netty=DEBUG'
$ cf restage dataflow-server</code></pre>
</div>
</div>
<div class="paragraph">
<p>(<em>where, <code>reactor.ipc.netty</code> is the global package for everything <code>reactor-netty</code> related</em>)</p>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Similar to the <code>local-deployer</code> and <code>cloudfoundry-deployer</code> options as discussed above, there
are equivalent settings available for Apache YARN, Apache Mesos and Kubernetes variants, too. Check out the
respective <a href="https://github.com/spring-cloud?utf8=%E2%9C%93&amp;q=spring-cloud-deployer">SPI implementations</a> to
find out more details about the packages to configure for logging.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_application_logs">51.2. Application Logs</h3>
<div class="paragraph">
<p>The streaming applications in Spring Cloud Data Flow are Spring Boot applications and they can be
independently setup with logging configurations.</p>
</div>
<div class="paragraph">
<p>For instance, if you&#8217;d have to troubleshoot the <code>header</code> and <code>payload</code> specifics that are being passed
around source, processor and sink channels, you&#8217;d be deploying the stream with the following
options.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt;stream create foo --definition "http --logging.level.org.springframework.integration=DEBUG | transform --logging.level.org.springframework.integration=DEBUG | log --logging.level.org.springframework.integration=DEBUG" --deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p>(<em>where, <code>org.springframework.integration</code> is the global package for everything Spring Integration related,
which is responsible for messaging channels</em>)</p>
</div>
<div class="paragraph">
<p>These properties can also be specified via <code>deployment</code> properties when deploying the stream.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt;stream deploy foo --properties "app.*.logging.level.org.springframework.integration=DEBUG"</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="faqs">52. Frequently asked questions</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this section, we will review the frequently discussed questions in Spring Cloud Data Flow.</p>
</div>
<div class="sect2">
<h3 id="_advanced_spel_expressions">52.1. Advanced SpEL expressions</h3>
<div class="paragraph">
<p>One of the powerful features of SpEL expressions is <a href="http://docs.spring.io/spring/docs/current/spring-framework-reference/html/expressions.html#expressions-ref-functions">functions</a>.
Spring Integration provides <code>jsonPath()</code> and <code>xpath()</code> out-of-the-box <a href="http://docs.spring.io/spring-integration/reference/html/spel.html#spel-functions">SpEL-functions</a>, if appropriate libraries are in the classpath.
All the provided Spring Cloud Stream application starters are supplied with the <code>json-path</code> and <code>spring-integration-xml</code> jars, thus we can use those SpEL-functions in Spring Cloud Data Flow streams whenever expressions are possible.
For example we can transform JSON-aware <code>payload</code> from the HTTP request using some <code>jsonPath()</code> expression:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt;stream create jsonPathTransform --definition "http | transform --expression=#jsonPath(payload,'$.price') | log" --deploy
...
dataflow:&gt; http post --target http://localhost:8080 --data {"symbol":"SCDF","price":72.04}
dataflow:&gt; http post --target http://localhost:8080 --data {"symbol":"SCDF","price":72.06}
dataflow:&gt; http post --target http://localhost:8080 --data {"symbol":"SCDF","price":72.08}</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this sample we apply jsonPath for the incoming payload to extract just only the <code>price</code> field value.
Similar syntax can be used with <code>splitter</code> or <code>filter</code> <code>expression</code> options.
Actually any available SpEL-based option has access to the built-in SpEL-functions.
For example we can extract some value from JSON data to calculate the <code>partitionKey</code> before sending output to the Binder:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt;stream deploy foo --properties "deployer.transform.count=2,app.transform.producer.partitionKeyExpression=#jsonPath(payload,'$.symbol')"</code></pre>
</div>
</div>
<div class="paragraph">
<p>The same syntax can be applied for <code>xpath()</code> SpEL-function when you deal with XML data.
Any other custom SpEL-function can also be used, but for this purpose you should build a library with the <code>@Configuration</code> class containing an appropriate <code>SpelFunctionFactoryBean</code> <code>@Bean</code> definition.
The target Spring Cloud Stream application starter should be re-packaged to supply such a custom extension via built-in Spring Boot <code>@ComponentScan</code> mechanism or auto-configuration hook.</p>
</div>
</div>
<div class="sect2">
<h3 id="dataflow-jdbc-sink">52.2. How to use JDBC-sink?</h3>
<div class="paragraph">
<p>The JDBC-sink can be used to insert message payload data into a relational database table. By default,
it inserts the entire payload into a table named after the <code>jdbc.table-name</code> property, and if it is not set,
by default the application expects to use a table with the name <code>messages</code>. To alter this behavior, the
JDBC sink accepts <a href="http://docs.spring.io/spring-cloud-stream-app-starters/docs/current/reference/html/spring-cloud-stream-modules-sinks.html#spring-cloud-stream-modules-jdbc-sink">several options</a> that you can pass using the --foo=bar notation in the stream, or change globally.
The JDBC sink has a <code>jdbc.initialize</code> property that if set to <code>true</code> will result in the sink creating a table based on the specified configuration when the it starts up. If that initialize property is <code>false</code>, which is the default, you will have to make sure that the table to use is already available.</p>
</div>
<div class="paragraph">
<p>A stream definition using <code>jdbc</code> sink relying on all defaults with MySQL as the backing database looks
like the following. In this example, the system time is persisted in MySQL for every second.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt;stream create --name mydata --definition "time | jdbc --spring.datasource.url=jdbc:mysql://localhost:3306/test --spring.datasource.username=root --spring.datasource.password=root --spring.datasource.driver-class-name=org.mariadb.jdbc.Driver" --deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p>For this to work, you&#8217;d have to have the following table in the MySQL database.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">CREATE TABLE test.messages
(
  payload varchar(255)
);</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">mysql&gt; desc test.messages;
+---------+--------------+------+-----+---------+-------+
| Field   | Type         | Null | Key | Default | Extra |
+---------+--------------+------+-----+---------+-------+
| payload | varchar(255) | YES  |     | NULL    |       |
+---------+--------------+------+-----+---------+-------+
1 row in set (0.00 sec)</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">mysql&gt; select * from test.messages;
+-------------------+
| payload           |
+-------------------+
| 04/25/17 09:10:04 |
| 04/25/17 09:10:06 |
| 04/25/17 09:10:07 |
| 04/25/17 09:10:08 |
| 04/25/17 09:10:09 |
.............
.............
.............</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="dataflow-multiple-brokers">52.3. How to use multiple message-binders?</h3>
<div class="paragraph">
<p>For situations where the data is consumed and processed between two different message brokers, Spring
Cloud Data Flow provides easy to override global configurations, out-of-the-box <a href="https://github.com/spring-cloud-stream-app-starters/bridge"><code>bridge-processor</code></a>,
and DSL primitives to build these type of topologies.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s assume we have data queueing up in RabbitMQ <em>(e.g., queue = <code>fooRabbit</code>)</em> and the requirement
is to consume all the payloads and publish them to Apache Kafka <em>(e.g., topic = <code>barKafka</code>)</em>, as the
destination for downstream processing.</p>
</div>
<div class="paragraph">
<p>Follow the global application of <a href="streams.html#spring-cloud-dataflow-global-properties">configurations</a>
to define multiple binder configurations.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-properties" data-lang="properties"># Apache Kafka Global Configurations (i.e., identified by "kafka1")
spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.binders.kafka1.type=kafka
spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.binders.kafka1.environment.spring.cloud.stream.kafka.binder.brokers=localhost:9092
spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.binders.kafka1.environment.spring.cloud.stream.kafka.binder.zkNodes=localhost:2181

# RabbitMQ Global Configurations (i.e., identified by "rabbit1")
spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.binders.rabbit1.type=rabbit
spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.binders.rabbit1.environment.spring.rabbitmq.host=localhost
spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.binders.rabbit1.environment.spring.rabbitmq.port=5672</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
In this example, both the message brokers are running locally and reachable at <code>localhost</code>
with respective ports.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>These properties can be supplied in a ".properties" file that is accessible to the server directly or via
<code>config-server</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">java -jar spring-cloud-dataflow-server-local/target/spring-cloud-dataflow-server-local-1.1.4.RELEASE.jar --spring.config.location=&lt;PATH-TO-FILE&gt;/foo.properties</code></pre>
</div>
</div>
<div class="paragraph">
<p>Spring Cloud Data Flow internally uses <code>bridge-processor</code> to directly connect different named channel
destinations. Since we are publishing and subscribing from two different messaging systems, you&#8217;d have
to build the <code>bridge-processor</code> with both RabbitMQ and Apache Kafka binders in the classpath. To do that,
head over to <a href="http://start-scs.cfapps.io/" class="bare">start-scs.cfapps.io/</a> and select <code>Bridge Processor</code>, <code>Kafka binder starter</code>, and
<code>Rabbit binder starter</code> as the dependencies and follow the patching procedure described in the
<a href="http://docs.spring.io/spring-cloud-stream-app-starters/docs/Bacon.RELEASE/reference/html/_introduction.html#customizing-binder">reference guide</a>.
Specifically, for the <code>bridge-processor</code>, you&#8217;d have to import the <code>BridgeProcessorConfiguration</code>
provided by the starter.</p>
</div>
<div class="paragraph">
<p>Once you have the necessary adjustments, you can build the application. Let&#8217;s register the name of the
application as <code>multiBinderBridge</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt;app register --type processor --name multiBinderBridge --uri file:///&lt;PATH-TO-FILE&gt;/multipleBinderBridge-0.0.1-SNAPSHOT.jar</code></pre>
</div>
</div>
<div class="paragraph">
<p>It is time to create a stream definition with the newly registered processor application.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">dataflow:&gt;stream create fooRabbitToBarKafka --definition ":fooRabbit &gt; multiBinderBridge --spring.cloud.stream.bindings.input.binder=rabbit1 --spring.cloud.stream.bindings.output.binder=kafka1 &gt; :barKafka" --deploy</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Since we are to consume messages from RabbitMQ <em>(i.e., identified by <code>rabbit1</code>)</em> and then
publish the payload to Apache Kafka <em>(i.e., identified by <code>kafka1</code>)</em>, we are supplying them as <code>input</code>
and <code>output</code> channel settings respectively.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The queue <code>fooRabbit</code> in RabbitMQ is where the stream is consuming events from and the topic
<code>barKafka</code> in Apache Kafka is where the data is finally landing.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<h1 id="appendix" class="sect0">Appendices</h1>
<div class="sect1">
<h2 id="migration-guide">Appendix A: Migrating from Spring XD to Spring Cloud Data Flow</h2>
<div class="sectionbody">

<div class="sect2">
<h3 id="_terminology_changes">A.1. Terminology Changes</h3>
<table class="tableblock frame-topbot grid-all spread">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Old</th>
<th class="tableblock halign-left valign-top">New</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">XD-Admin</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Server (<em>implementations</em>: local, cloud foundry, apache yarn, kubernetes, and apache mesos)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">XD-Container</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">N/A</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Modules</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Applications</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Admin UI</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Dashboard</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Message Bus</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Binders</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Batch / Job</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Task</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="_modules_to_applications">A.2. Modules to Applications</h3>
<div class="paragraph">
<p>If you have custom Spring XD modules, you’d have to refactor them to use Spring Cloud
Stream and Spring Cloud Task annotations, with updated dependencies and built as normal
Spring Boot "applications".</p>
</div>
<div class="sect3">
<h4 id="_custom_applications">A.2.1. Custom Applications</h4>
<div class="ulist">
<ul>
<li>
<p>Spring XD&#8217;s stream and batch modules are refactored into <a href="https://github.com/spring-cloud-stream-app-starters">Spring Cloud Stream</a> and <a href="https://github.com/spring-cloud-task-app-starters">Spring
Cloud Task</a> application-starters, respectively. These applications can be used as the reference while refactoring Spring XD modules</p>
</li>
<li>
<p>There are also some samples for <a href="https://github.com/spring-cloud/spring-cloud-stream-samples">Spring Cloud Stream</a> and <a href="https://github.com/spring-cloud/spring-cloud-task/tree/master/spring-cloud-task-samples">Spring Cloud Task</a> applications for reference</p>
</li>
<li>
<p>If you’d like to create a brand new custom application, use the getting started guide for <a href="http://docs.spring.io/spring-cloud-stream/docs/current/reference/htmlsingle/#_getting_started">Spring Cloud Stream</a> and <a href="http://docs.spring.io/spring-cloud-task/docs/current/reference/htmlsingle/#getting-started">Spring Cloud Task</a> applications and as well as  review the development <a href="http://docs.spring.io/spring-cloud-stream-app-starters/docs/current/reference/htmlsingle/#_creating_your_own_applications">guide</a></p>
</li>
<li>
<p>Alternatively, if you’d like to patch any of the out-of-the-box stream applications, you can
follow the procedure <a href="http://docs.spring.io/spring-cloud-stream-app-starters/docs/current/reference/htmlsingle/#_patching_pre_built_applications">here</a></p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_application_registration">A.2.2. Application Registration</h4>
<div class="ulist">
<ul>
<li>
<p>Custom Stream/Task application requires being installed to a maven repository for Local, YARN, and
CF implementations or as docker images, when deploying to Kubernetes and Mesos. Other than maven and
docker resolution, you can also resolve application artifacts from <code>http</code>, <code>file</code>, or as <code>hdfs</code>
coordinates</p>
</li>
<li>
<p>Unlike Spring XD, you do not have to upload the application bits while registering custom applications anymore; instead, you’re expected to <a href="#spring-cloud-dataflow-register-apps">register</a> the application coordinates that are hosted in the maven repository or by other means as discussed in the previous bullet</p>
</li>
<li>
<p>By default, none of the out-of-the-box applications are preloaded already. It is intentionally designed to
provide the flexibility to register app(s), as you find appropriate for the given use-case requirement</p>
</li>
<li>
<p>Depending on the binder choice, you can manually add the appropriate binder dependency to build
applications specific to that binder-type. Alternatively, you can follow the Spring Initialzr <a href="https://github.com/spring-cloud/spring-cloud-stream-app-starters/blob/master/spring-cloud-stream-app-starters-docs/src/main/asciidoc/overview.adoc#using-the-starters-to-create-custom-components">procedure</a>
to create an application with binder embedded in it</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_application_properties_2">A.2.3. Application Properties</h4>
<div class="ulist">
<ul>
<li>
<p>counter-sink:</p>
<div class="ulist">
<ul>
<li>
<p>The peripheral <code>redis</code> is not required in Spring Cloud Data Flow. If you intend to use the <code>counter-sink</code>, then <code>redis</code> becomes required, and you’re expected to have your own running <code>redis</code> cluster</p>
</li>
</ul>
</div>
</li>
<li>
<p>field-value-counter-sink:</p>
<div class="ulist">
<ul>
<li>
<p>The peripheral <code>redis</code> is not required in Spring Cloud Data Flow. If you intend to use the <code>field-value-counter-sink</code>, then <code>redis</code> becomes required, and you’re expected to have your own running <code>redis</code> cluster</p>
</li>
</ul>
</div>
</li>
<li>
<p>aggregate-counter-sink:</p>
<div class="ulist">
<ul>
<li>
<p>The peripheral <code>redis</code> is not required in Spring Cloud Data Flow. If you intend to use the <code>aggregate-counter-sink</code>, then <code>redis</code> becomes required, and you’re expected to have your own running <code>redis</code> cluster</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_message_bus_to_binders">A.3. Message Bus to Binders</h3>
<div class="paragraph">
<p>Terminology wise, in Spring Cloud Data Flow, the message bus implementation is commonly referred to
as binders.</p>
</div>
<div class="sect3">
<h4 id="_message_bus">A.3.1. Message Bus</h4>
<div class="paragraph">
<p>Similar to Spring XD, there’s an abstraction available to extend the binder interface. By default,
we take the opinionated view of <a href="https://github.com/spring-cloud/spring-cloud-stream-binder-kafka">Apache Kafka</a> and <a href="https://github.com/spring-cloud/spring-cloud-stream-binder-rabbit">RabbitMQ</a> as the
production-ready binders and are available as GA releases.</p>
</div>
</div>
<div class="sect3">
<h4 id="_binders">A.3.2. Binders</h4>
<div class="paragraph">
<p>Selecting a binder is as simple as providing the right binder dependency in the classpath. If you’re
to choose Kafka as the binder, you’d register stream applications that are pre-built with Kafka binder
in it. If you were to create a custom application with Kafka binder, you&#8217;d add the following
dependency in the classpath.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
    &lt;artifactId&gt;spring-cloud-stream-binder-kafka&lt;/artifactId&gt;
    &lt;version&gt;1.0.2.RELEASE&lt;/version&gt;
&lt;/dependency&gt;</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Spring Cloud Stream supports <a href="https://github.com/spring-cloud/spring-cloud-stream-binder-kafka">Apache Kafka</a>, <a href="https://github.com/spring-cloud/spring-cloud-stream-binder-rabbit">RabbitMQ</a> and experimental
<a href="https://github.com/spring-cloud/spring-cloud-stream-binder-google-pubsub">Google PubSub</a> and
<a href="https://github.com/spring-cloud/spring-cloud-stream-binder-solace">Solace JMS</a>.  All binder implementations
are maintained and managed in their individual repositories</p>
</li>
<li>
<p>Every Stream/Task application can be built with a binder implementation of your choice.
All the out-of-the-box applications are pre-built for both Kafka and Rabbit and they’re
readily available for use as maven artifacts [<a href="http://repo.spring.io/libs-milestone/org/springframework/cloud/stream/app/">Spring Cloud Stream</a> / <a href="http://repo.spring.io/libs-milestone/org/springframework/cloud/task/app/">Spring Cloud Task</a> or docker images [<a href="https://hub.docker.com/r/springcloudstream/">Spring Cloud Stream</a> / <a href="https://hub.docker.com/r/springcloudtask/">Spring Cloud Task</a>
Changing the binder requires selecting the right binder <a href="http://docs.spring.io/spring-cloud-stream/docs/current/reference/htmlsingle/#_binders">dependency</a>. Alternatively, you can download the pre-built application from this version of <a href="http://start-scs.cfapps.io/">Spring Initializr</a> with the desired “binder-starter” dependency</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_named_channels">A.3.3. Named Channels</h4>
<div class="paragraph">
<p>Fundamentally, all the messaging channels are backed by pub/sub semantics. Unlike Spring XD, the
messaging channels are backed only by <code>topics</code> or <code>topic-exchange</code> and there’s no representation of
<code>queues</code> in the new architecture.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>${xd.module.index}</code> is not supported anymore; instead, you can directly interact with named
destinations</p>
</li>
<li>
<p><code>stream.index</code> changes to <code>:&lt;stream-name&gt;.&lt;label/app-name&gt;</code></p>
<div class="ulist">
<ul>
<li>
<p><em>for instance:</em> <code>ticktock.0</code> changes to <code>:ticktock.time</code></p>
</li>
</ul>
</div>
</li>
<li>
<p>“topic/queue” prefixes are not required to interact with named-channels</p>
<div class="ulist">
<ul>
<li>
<p><em>for instance:</em> <code>topic:foo</code> changes to <code>:foo</code></p>
</li>
<li>
<p><em>for instance:</em> <code>stream create stream1 --definition ":foo &gt; log"</code></p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_directed_graphs">A.3.4. Directed Graphs</h4>
<div class="paragraph">
<p>If you’re building non-linear streams, you could take advantage of named destinations to build
directed graphs.</p>
</div>
<div class="paragraph">
<p><em>for instance, in Spring XD:</em></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">stream create f --definition "queue:foo &gt; transform --expression=payload+'-foo' | log" --deploy
stream create b --definition "queue:bar &gt; transform --expression=payload+'-bar' | log" --deploy
stream create r --definition "http | router --expression=payload.contains('a')?'queue:foo':'queue:bar'" --deploy</code></pre>
</div>
</div>
<div class="paragraph">
<p><em>for instance, in Spring Cloud Data Flow:</em></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">stream create f --definition ":foo &gt; transform --expression=payload+'-foo' | log" --deploy
stream create b --definition ":bar &gt; transform --expression=payload+'-bar' | log" --deploy
stream create r --definition "http | router --expression=payload.contains('a')?'foo':'bar'" --deploy</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_batch_to_tasks">A.4. Batch to Tasks</h3>
<div class="paragraph">
<p>A Task by definition, is any application that does not run forever, including Spring Batch jobs, and they
end/stop at some point. Task applications can be majorly used for on-demand use-cases such as database migration, machine learning, scheduled operations etc. Using <a href="http://cloud.spring.io/spring-cloud-task/">Spring Cloud Task</a>, users can build Spring Batch jobs as microservice applications.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Spring Batch <a href="http://docs.spring.io/spring-xd/docs/current-SNAPSHOT/reference/html/#jobs">jobs</a>
from Spring XD are being refactored to Spring Boot applications a.k.a Spring Cloud Task
<a href="https://github.com/spring-cloud-task-app-starters">applications</a></p>
</li>
<li>
<p>Unlike Spring XD, these “Tasks” don&#8217;t require explicit deployment; instead, a task is ready to be
launched directly once the definition is declared</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_shell_dsl_commands">A.5. Shell/DSL Commands</h3>
<table class="tableblock frame-topbot grid-all spread">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Old Command</th>
<th class="tableblock halign-left valign-top">New Command</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">module upload</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">app register / app import</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">module list</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">app list</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">module info</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">app info</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">admin config server</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">dataflow config server</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">job create</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">task create</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">job launch</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">task launch</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">job list</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">task list</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">job status</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">task status</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">job display</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">task display</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">job destroy</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">task destroy</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">job execution list</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">task execution list</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">runtime modules</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">runtime apps</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="_rest_api">A.6. REST-API</h3>
<table class="tableblock frame-topbot grid-all" style="width: 70%;">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Old API</th>
<th class="tableblock halign-left valign-top">New API</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">/modules</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">/apps</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">/runtime/modules</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">/runtime/apps</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">/runtime/modules/{moduleId}</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">/runtime/apps/{appId}</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">/jobs/definitions</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">/task/definitions</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">/jobs/deployments</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">/task/deployments</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="_ui_flo">A.7. UI / Flo</h3>
<div class="paragraph">
<p>The Admin-UI is now renamed as Dashboard. The URI for accessing the Dashboard is changed from
<a href="http://localhost:9393/admin-ui" class="bare">localhost:9393/admin-ui</a> to <a href="http://localhost:9393/dashboard" class="bare">localhost:9393/dashboard</a></p>
</div>
<div class="ulist">
<ul>
<li>
<p><em>(New)</em> Apps: Lists all the registered applications that are available for use. This view includes informational details such as the URI and the properties supported by each application. You can also register/unregister applications from this view</p>
</li>
<li>
<p>Runtime: Container changes to Runtime. The notion of <code>xd-container</code> is gone, replaced by out-of-the-box applications running as autonomous Spring Boot applications. The Runtime tab displays the applications
running in the runtime platforms (<em>implementations:</em> cloud foundry, apache yarn, apache mesos, or
kubernetes). You can click on each application to review relevant details about the application such
as where it is running with, and what resources etc.</p>
</li>
<li>
<p><a href="https://github.com/spring-projects/spring-flo">Spring Flo</a> is now an OSS product. Flo for
Spring Cloud Data Flow’s “Create Stream”, the designer-tab comes pre-built in the Dashboard</p>
</li>
<li>
<p><em>(New)</em> Tasks:</p>
<div class="ulist">
<ul>
<li>
<p>The sub-tab “Modules” is renamed to “Apps”</p>
</li>
<li>
<p>The sub-tab “Definitions” lists all the Task definitions, including Spring Batch jobs that are
orchestrated as Tasks</p>
</li>
<li>
<p>The sub-tab “Executions” lists all the Task execution details similar to Spring XD&#8217;s Job executions</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_architecture_components">A.8. Architecture Components</h3>
<div class="paragraph">
<p>Spring Cloud Data Flow comes with a significantly simplified architecture. In fact, when compared with
Spring XD, there are less peripherals that are necessary to operationalize Spring Cloud Data Flow.</p>
</div>
<div class="sect3">
<h4 id="_zookeeper">A.8.1. ZooKeeper</h4>
<div class="paragraph">
<p>ZooKeeper is not used in the new architecture.</p>
</div>
</div>
<div class="sect3">
<h4 id="_rdbms">A.8.2. RDBMS</h4>
<div class="paragraph">
<p>Spring Cloud Data Flow uses an RDBMS instead of Redis for stream/task definitions, application
registration, and for job repositories.The default configuration uses an embedded H2 instance, but Oracle, DB2, SqlServer, MySQL/MariaDB, PostgreSQL, H2, and HSQLDB databases are supported. To use Oracle, DB2 and
SqlServer you will need to create your own Data Flow Server using <a href="https://start.spring.io/">Spring Initializr</a> and add the appropriate JDBC driver dependency.</p>
</div>
</div>
<div class="sect3">
<h4 id="_redis">A.8.3. Redis</h4>
<div class="paragraph">
<p>Running a Redis cluster is only required for analytics functionality. Specifically, when the <code>counter-sink</code>,
<code>field-value-counter-sink</code>, or <code>aggregate-counter-sink</code> applications are used, it is expected to also
have a running instance of Redis cluster.</p>
</div>
</div>
<div class="sect3">
<h4 id="_cluster_topology">A.8.4. Cluster Topology</h4>
<div class="paragraph">
<p>Spring XD’s <code>xd-admin</code> and <code>xd-container</code> server components are replaced by stream and task
applications themselves running as autonomous Spring Boot applications. The applications run natively
on various platforms including Cloud Foundry, Apache YARN, Apache Mesos, or Kubernetes. You can develop,
test, deploy, scale +/-, and interact with (Spring Boot) applications individually, and they can
evolve in isolation.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_central_configuration">A.9. Central Configuration</h3>
<div class="paragraph">
<p>To support centralized and consistent management of an application’s configuration properties,
<a href="https://cloud.spring.io/spring-cloud-config/">Spring Cloud Config</a> client libraries have been
included into the Spring Cloud Data Flow server as well as the Spring Cloud Stream applications provided
by the Spring Cloud Stream App Starters. You can also <a href="streams.html#spring-cloud-dataflow-global-properties">pass common application properties</a>
to all streams when the Data Flow Server starts.</p>
</div>
</div>
<div class="sect2">
<h3 id="_distribution">A.10. Distribution</h3>
<div class="paragraph">
<p>Spring Cloud Data Flow is a Spring Boot application. Depending on the platform of your choice, you
can download the respective release uber-jar and deploy/push it to the runtime platform
(cloud foundry, apache yarn, kubernetes, or apache mesos). For example, if you’re running Spring
Cloud Data Flow on Cloud Foundry, you’d download the Cloud Foundry server implementation and do a
<code>cf push</code> as explained in the <a href="http://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current-SNAPSHOT/reference/htmlsingle/#getting-started">reference guide</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_hadoop_distribution_compatibility">A.11. Hadoop Distribution Compatibility</h3>
<div class="paragraph">
<p>The <code>hdfs-sink</code> application builds upon Spring Hadoop 2.4.0 release, so this application is compatible
with following Hadoop distributions.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Cloudera - cdh5</p>
</li>
<li>
<p>Pivotal Hadoop - phd30</p>
</li>
<li>
<p>Hortonworks Hadoop - hdp24</p>
</li>
<li>
<p>Hortonworks Hadoop - hdp23</p>
</li>
<li>
<p>Vanilla Hadoop - hadoop26</p>
</li>
<li>
<p>Vanilla Hadoop - 2.7.x (default)</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_yarn_deployment">A.12. YARN Deployment</h3>
<div class="paragraph">
<p>Spring Cloud Data Flow can be deployed and used with Apche YARN in two different ways.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Deploy the server <a href="http://docs.spring.io/spring-cloud-dataflow-server-yarn/docs/current-SNAPSHOT/reference/htmlsingle/#yarn-deploying-on-yarn">directly</a> in a YARN cluster</p>
</li>
<li>
<p>Leverage Apache Ambari <a href="http://docs.spring.io/spring-cloud-dataflow-server-yarn/docs/current-SNAPSHOT/reference/htmlsingle/#yarn-deploying-on-ambari">plugin to provision</a> Spring Cloud Data Flow as
a service</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_use_case_comparison">A.13. Use Case Comparison</h3>
<div class="paragraph">
<p>Let&#8217;s review some use-cases to compare and contrast the differences between Spring XD and Spring
Cloud Data Flow.</p>
</div>
<div class="sect3">
<h4 id="_use_case_1">A.13.1. Use Case #1</h4>
<div class="paragraph">
<p>(<em>It is assumed both XD and SCDF distributions are already downloaded</em>)</p>
</div>
<div class="paragraph">
<p>Description: Simple <code>ticktock</code> example using local/singlenode.</p>
</div>
<table class="tableblock frame-topbot grid-all spread">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Spring XD</th>
<th class="tableblock halign-left valign-top">Spring Cloud Data Flow</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Start <code>xd-singlenode</code> server from CLI
</p><p class="tableblock"><code>→ xd-singlenode</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Start a binder of your choice
</p><p class="tableblock">Start <code>local-server</code> implementation of SCDF from the CLI
</p><p class="tableblock"><code>→ java -jar spring-cloud-dataflow-server-local-1.0.0.BUILD-SNAPSHOT.jar</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Start <code>xd-shell</code> server from the CLI
</p><p class="tableblock"><code>→ xd-shell</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Start <code>dataflow-shell</code> server from the CLI
</p><p class="tableblock"><code>→ java -jar spring-cloud-dataflow-shell-1.0.0.BUILD-SNAPSHOT.jar</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Create <code>ticktock</code> stream
</p><p class="tableblock"><code>xd:&gt;stream create ticktock --definition “time | log” --deploy</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Create <code>ticktock</code> stream
</p><p class="tableblock"><code>dataflow:&gt;stream create ticktock --definition “time | log” --deploy</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Review <code>ticktock</code> results in the <code>xd-singlenode</code> server console</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Review <code>ticktock</code> results by tailing the <code>ticktock.log/stdout_log</code> application logs</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect3">
<h4 id="_use_case_2">A.13.2. Use Case #2</h4>
<div class="paragraph">
<p>(<em>It is assumed both XD and SCDF distributions are already downloaded</em>)</p>
</div>
<div class="paragraph">
<p>Description: Stream with custom module/application.</p>
</div>
<table class="tableblock frame-topbot grid-all spread">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Spring XD</th>
<th class="tableblock halign-left valign-top">Spring Cloud Data Flow</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Start <code>xd-singlenode</code> server from CLI
</p><p class="tableblock"><code>→ xd-singlenode</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Start a binder of your choice
</p><p class="tableblock">Start <code>local-server</code> implementation of SCDF from the CLI
</p><p class="tableblock"><code>→ java -jar spring-cloud-dataflow-server-local-1.0.0.BUILD-SNAPSHOT.jar</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Start <code>xd-shell</code> server from the CLI
</p><p class="tableblock"><code>→ xd-shell</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Start <code>dataflow-shell</code> server from the CLI
</p><p class="tableblock"><code>→ java -jar spring-cloud-dataflow-shell-1.0.0.BUILD-SNAPSHOT.jar</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Register custom “processor” module to transform payload to a desired format
</p><p class="tableblock"><code>xd:&gt;module upload --name toupper --type processor --file &lt;CUSTOM_JAR_FILE_LOCATION&gt;</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Register custom “processor” application to transform payload to a desired format
</p><p class="tableblock"><code>dataflow:&gt;app register --name toupper --type processor --uri &lt;MAVEN_URI_COORDINATES&gt;</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Create a stream with custom module
</p><p class="tableblock"><code>xd:&gt;stream create testupper --definition “http | toupper | log” --deploy</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Create a stream with custom application
</p><p class="tableblock"><code>dataflow:&gt;stream create testupper --definition “http | toupper | log” --deploy</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Review results in the <code>xd-singlenode</code> server console</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Review results by tailing the <code>testupper.log/stdout_log</code> application logs</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect3">
<h4 id="_use_case_3">A.13.3. Use Case #3</h4>
<div class="paragraph">
<p>(<em>It is assumed both XD and SCDF distributions are already downloaded</em>)</p>
</div>
<div class="paragraph">
<p>Description: Simple batch-job.</p>
</div>
<table class="tableblock frame-topbot grid-all spread">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Spring XD</th>
<th class="tableblock halign-left valign-top">Spring Cloud Data Flow</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Start <code>xd-singlenode</code> server from CLI
</p><p class="tableblock"><code>→ xd-singlenode</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Start <code>local-server</code> implementation of SCDF from the CLI
</p><p class="tableblock"><code>→ java -jar spring-cloud-dataflow-server-local-1.0.0.BUILD-SNAPSHOT.jar</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Start <code>xd-shell</code> server from the CLI
</p><p class="tableblock"><code>→ xd-shell</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Start <code>dataflow-shell</code> server from the CLI
</p><p class="tableblock"><code>→ java -jar spring-cloud-dataflow-shell-1.0.0.BUILD-SNAPSHOT.jar</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Register custom “batch-job” module
</p><p class="tableblock"><code>xd:&gt;module upload --name simple-batch --type job --file &lt;CUSTOM_JAR_FILE_LOCATION&gt;</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Register
custom “batch-job” as task application
</p><p class="tableblock"><code>dataflow:&gt;app register --name simple-batch --type task --uri &lt;MAVEN_URI_COORDINATES&gt;</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Create a job with custom batch-job module
</p><p class="tableblock"><code>xd:&gt;job create batchtest --definition “simple-batch”</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Create a task with custom batch-job application
</p><p class="tableblock"><code>dataflow:&gt;task create batchtest --definition “simple-batch”</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Deploy job
</p><p class="tableblock"><code>xd:&gt;job deploy batchtest</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">NA</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Launch job
</p><p class="tableblock"><code>xd:&gt;job launch batchtest</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Launch task
</p><p class="tableblock"><code>dataflow:&gt;task launch batchtest</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Review results in the <code>xd-singlenode</code> server console as well as Jobs tab in UI
(executions sub-tab should include all step details)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Review results by tailing the <code>batchtest/stdout_log</code> application logs as well as Task tab in UI (executions sub-tab should include all step details)</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="building">Appendix B: Building</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To build the source you will need to install JDK 1.8.</p>
</div>
<div class="paragraph">
<p>The build uses the Maven wrapper so you don&#8217;t have to install a specific
version of Maven.  To enable the tests for Redis you should run the server
before bulding.  See below for more information on how to run Redis.</p>
</div>
<div class="paragraph">
<p>The main build command is</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ ./mvnw clean install</pre>
</div>
</div>
<div class="paragraph">
<p>You can also add '-DskipTests' if you like, to avoid running the tests.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You can also install Maven (&gt;=3.3.3) yourself and run the <code>mvn</code> command
in place of <code>./mvnw</code> in the examples below. If you do that you also
might need to add <code>-P spring</code> if your local Maven settings do not
contain repository declarations for spring pre-release artifacts.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Be aware that you might need to increase the amount of memory
available to Maven by setting a <code>MAVEN_OPTS</code> environment variable with
a value like <code>-Xmx512m -XX:MaxPermSize=128m</code>. We try to cover this in
the <code>.mvn</code> configuration, so if you find you have to do it to make a
build succeed, please raise a ticket to get the settings added to
source control.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The projects that require middleware generally include a
<code>docker-compose.yml</code>, so consider using
<a href="https://docs.docker.com/compose/">Docker Compose</a> to run the middeware servers
in Docker containers. See the README in the
<a href="https://github.com/spring-cloud-samples/scripts">scripts demo
repository</a> for specific instructions about the common cases of mongo,
rabbit and redis.</p>
</div>
<div class="sect2">
<h3 id="_documentation">B.1. Documentation</h3>
<div class="paragraph">
<p>There is a "full" profile that will generate documentation. You can build just the documentation by executing</p>
</div>
<div class="paragraph">
<p><code>$ ./mvnw clean package -DskipTests -P full -pl spring-cloud-dataflow-server-yarn-docs -am</code></p>
</div>
</div>
<div class="sect2">
<h3 id="_working_with_the_code">B.2. Working with the code</h3>
<div class="paragraph">
<p>If you don&#8217;t have an IDE preference we would recommend that you use
<a href="https://spring.io/tools">Spring Tools Suite</a> or
<a href="http://www.eclipse.org">Eclipse</a> when working with the code. We use the
<a href="http://www.eclipse.org/m2e/">m2eclipe</a> eclipse plugin for maven support. Other IDEs and tools
should also work without issue.</p>
</div>
<div class="sect3">
<h4 id="_importing_into_eclipse_with_m2eclipse">B.2.1. Importing into eclipse with m2eclipse</h4>
<div class="paragraph">
<p>We recommend the <a href="http://www.eclipse.org/m2e/">m2eclipe</a> eclipse plugin when working with
eclipse. If you don&#8217;t already have m2eclipse installed it is available from the "eclipse
marketplace".</p>
</div>
<div class="paragraph">
<p>Unfortunately m2e does not yet support Maven 3.3, so once the projects
are imported into Eclipse you will also need to tell m2eclipse to use
the <code>.settings.xml</code> file for the projects.  If you do not do this you
may see many different errors related to the POMs in the
projects.  Open your Eclipse preferences, expand the Maven
preferences, and select User Settings.  In the User Settings field
click Browse and navigate to the Spring Cloud project you imported
selecting the <code>.settings.xml</code> file in that project.  Click Apply and
then OK to save the preference changes.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Alternatively you can copy the repository settings from <a href="https://github.com/spring-cloud/spring-cloud-build/blob/master/.settings.xml"><code>.settings.xml</code></a> into your own <code>~/.m2/settings.xml</code>.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_importing_into_eclipse_without_m2eclipse">B.2.2. Importing into eclipse without m2eclipse</h4>
<div class="paragraph">
<p>If you prefer not to use m2eclipse you can generate eclipse project metadata using the
following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ ./mvnw eclipse:eclipse</pre>
</div>
</div>
<div class="paragraph">
<p>The generated eclipse projects can be imported by selecting <code>import existing projects</code>
from the <code>file</code> menu.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="contributing">Appendix C: Contributing</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Spring Cloud is released under the non-restrictive Apache 2.0 license,
and follows a very standard Github development process, using Github
tracker for issues and merging pull requests into master. If you want
to contribute even something trivial please do not hesitate, but
follow the guidelines below.</p>
</div>
<div class="sect2">
<h3 id="_sign_the_contributor_license_agreement">C.1. Sign the Contributor License Agreement</h3>
<div class="paragraph">
<p>Before we accept a non-trivial patch or pull request we will need you to sign the
<a href="https://cla.pivotal.io">contributor&#8217;s agreement</a>.
Signing the contributor&#8217;s agreement does not grant anyone commit rights to the main
repository, but it does mean that we can accept your contributions, and you will get an
author credit if we do.  Active contributors might be asked to join the core team, and
given the ability to merge pull requests.</p>
</div>
</div>
<div class="sect2">
<h3 id="_code_conventions_and_housekeeping">C.2. Code Conventions and Housekeeping</h3>
<div class="paragraph">
<p>None of these is essential for a pull request, but they will all help.  They can also be
added after the original pull request but before a merge.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Use the Spring Framework code format conventions. If you use Eclipse
you can import formatter settings using the
<code>eclipse-code-formatter.xml</code> file from the
<a href="https://github.com/spring-cloud/spring-cloud-build/blob/master/spring-cloud-dependencies-parent/eclipse-code-formatter.xml">Spring
Cloud Build</a> project. If using IntelliJ, you can use the
<a href="http://plugins.jetbrains.com/plugin/6546">Eclipse Code Formatter
Plugin</a> to import the same file.</p>
</li>
<li>
<p>Make sure all new <code>.java</code> files to have a simple Javadoc class comment with at least an
<code>@author</code> tag identifying you, and preferably at least a paragraph on what the class is
for.</p>
</li>
<li>
<p>Add the ASF license header comment to all new <code>.java</code> files (copy from existing files
in the project)</p>
</li>
<li>
<p>Add yourself as an <code>@author</code> to the .java files that you modify substantially (more
than cosmetic changes).</p>
</li>
<li>
<p>Add some Javadocs and, if you change the namespace, some XSD doc elements.</p>
</li>
<li>
<p>A few unit tests would help a lot as well&#8201;&#8212;&#8201;someone has to do it.</p>
</li>
<li>
<p>If no-one else is using your branch, please rebase it against the current master (or
other target branch in the main project).</p>
</li>
<li>
<p>When writing a commit message please follow <a href="http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html">these conventions</a>,
if you are fixing an existing issue please add <code>Fixes gh-XXXX</code> at the end of the commit
message (where XXXX is the issue number).</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2017-10-10 11:57:18 +00:00
</div>
</div>
</body>
</html>