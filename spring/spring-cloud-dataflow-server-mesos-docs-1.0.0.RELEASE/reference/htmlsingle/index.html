<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>Spring Cloud Data Flow Server for Apache Mesos</title><link rel="stylesheet" type="text/css" href="css/manual-singlepage.css"><meta name="generator" content="DocBook XSL Stylesheets V1.78.1"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div lang="en" class="book"><div class="titlepage"><div><div><h1 class="title"><a name="d0e3"></a>Spring Cloud Data Flow Server for Apache Mesos</h1></div><div><span xmlns:d="http://docbook.org/ns/docbook" class="author"><span class="firstname">Sabby Anandan, Artem Bilan, Marius Bogoevici, Eric Bottard, Mark Fisher, Ilayaperumal Gopinathan, Gunnar Hillert, Mark Pollack, Patrick Peralta, Glenn Renfro, Gary Russell, Thomas Risberg, David Turanski, Janne Valkealahti</span></span></div><div><p class="releaseinfo">1.0.0.RELEASE</p></div><div><p class="copyright">Copyright &copy; 2013-2016 Pivotal Software, Inc.</p></div><div><div class="legalnotice"><a name="d0e26" href="#d0e26"></a><p>
		Copies of this document may be made for your own use and for distribution to
		others, provided that you do not charge any fee for such copies and further
		provided that each copy contains this Copyright Notice, whether distributed in
		print or electronically.
	</p></div></div></div><hr></div><div class="toc"><p><b>Table of Contents</b></p><dl class="toc"><dt><span class="part"><a href="#introduction">I. Introduction</a></span></dt><dd><dl><dt><span class="chapter"><a href="#dataflow-mesos-intro">1. Introducing Spring Cloud Data Flow Server for Apache Mesos</a></span></dt><dt><span class="chapter"><a href="#spring-cloud-data-flow-overview">2. Spring Cloud Data Flow</a></span></dt><dt><span class="chapter"><a href="#spring-cloud-stream-overview">3. Spring Cloud Stream</a></span></dt><dt><span class="chapter"><a href="#spring-cloud-task-overview">4. Spring Cloud Task</a></span></dt></dl></dd><dt><span class="part"><a href="#architecture">II. Architecture</a></span></dt><dd><dl><dt><span class="chapter"><a href="#arch-intro">5. Introduction</a></span></dt><dt><span class="chapter"><a href="#arch-microservice-style">6. Microservice Architectural Style</a></span></dt><dd><dl><dt><span class="section"><a href="#arch-comparison">6.1. Comparison to other Platform architectures</a></span></dt></dl></dd><dt><span class="chapter"><a href="#arch-streaming-apps">7. Streaming Applications</a></span></dt><dd><dl><dt><span class="section"><a href="#arch-streaming-imperative-programming">7.1. Imperative Programming Model</a></span></dt><dt><span class="section"><a href="#arch-streaming-functional-programming">7.2. Functional Programming Model</a></span></dt></dl></dd><dt><span class="chapter"><a href="#arch-streams">8. Streams</a></span></dt><dd><dl><dt><span class="section"><a href="#arch-streams-topologies">8.1. Topologies</a></span></dt><dt><span class="section"><a href="#arch-streams-concurrency">8.2. Concurrency</a></span></dt><dt><span class="section"><a href="#arch-streams-partitioning">8.3. Partitioning</a></span></dt><dt><span class="section"><a href="#arch-streams-delivery">8.4. Message Delivery Guarantees</a></span></dt></dl></dd><dt><span class="chapter"><a href="#arch-analytics">9. Analytics</a></span></dt><dt><span class="chapter"><a href="#arch-task">10. Task Applications</a></span></dt><dt><span class="chapter"><a href="#arch-data-flow-server">11. Data Flow Server</a></span></dt><dd><dl><dt><span class="section"><a href="#arch-data-flow-server-endpoints">11.1. Endpoints</a></span></dt><dt><span class="section"><a href="#arch-data-flow-server-customization">11.2. Customization</a></span></dt><dt><span class="section"><a href="#arch-data-flow-server-security">11.3. Security</a></span></dt></dl></dd><dt><span class="chapter"><a href="#arch-runtime">12. Runtime</a></span></dt><dd><dl><dt><span class="section"><a href="#arch-runtime-fault-tolerance">12.1. Fault Tolerance</a></span></dt><dt><span class="section"><a href="#arch-runtime-resource-management">12.2. Resource Management</a></span></dt><dt><span class="section"><a href="#arch-runtime-scaling">12.3. Scaling at runtime</a></span></dt><dt><span class="section"><a href="#arch-application-versioning">12.4. Application Versioning</a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#_getting_started">III. Getting Started</a></span></dt><dd><dl><dt><span class="chapter"><a href="#_deploying_streams_and_tasks_on_apache_mesos_and_marathon_chronos">13. Deploying Streams and Tasks on Apache Mesos and Marathon/Chronos</a></span></dt></dl></dd><dt><span class="part"><a href="#streams">IV. Streams</a></span></dt><dd><dl><dt><span class="chapter"><a href="#spring-cloud-dataflow-stream-intro">14. Introduction</a></span></dt><dt><span class="chapter"><a href="#_stream_dsl">15. Stream DSL</a></span></dt><dt><span class="chapter"><a href="#spring-cloud-dataflow-register-apps">16. Register a Stream App</a></span></dt><dd><dl><dt><span class="section"><a href="#spring-cloud-dataflow-stream-app-whitelisting">16.1. Whitelisting application properties</a></span></dt></dl></dd><dt><span class="chapter"><a href="#spring-cloud-dataflow-create-stream">17. Creating a Stream</a></span></dt><dd><dl><dt><span class="section"><a href="#_application_properties">17.1. Application properties</a></span></dt><dd><dl><dt><span class="section"><a href="#_passing_application_properties_when_creating_a_stream">17.1.1. Passing application properties when creating a stream</a></span></dt><dt><span class="section"><a href="#_passing_application_properties_when_deploying_a_stream">17.1.2. Passing application properties when deploying a stream</a></span></dt><dt><span class="section"><a href="#passing_stream_partition_properties">17.1.3. Passing stream partition properties during stream deployment</a></span></dt><dt><span class="section"><a href="#_overriding_application_properties_during_stream_deployment">17.1.4. Overriding application properties during stream deployment</a></span></dt></dl></dd><dt><span class="section"><a href="#_deployment_properties">17.2. Deployment properties</a></span></dt><dd><dl><dt><span class="section"><a href="#_passing_instance_count_as_deployment_property">17.2.1. Passing instance count as deployment property</a></span></dt><dt><span class="section"><a href="#_inline_vs_file_reference_properties">17.2.2. Inline vs file reference properties</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#spring-cloud-dataflow-destroy-stream">18. Destroying a Stream</a></span></dt><dt><span class="chapter"><a href="#spring-cloud-dataflow-deploy-undeploy-stream">19. Deploying and Undeploying Streams</a></span></dt><dt><span class="chapter"><a href="#spring-cloud-dataflow-stream-app-types">20. Other Source and Sink Application Types</a></span></dt><dt><span class="chapter"><a href="#spring-cloud-dataflow-simple-stream">21. Simple Stream Processing</a></span></dt><dt><span class="chapter"><a href="#spring-cloud-dataflow-stream-partitions">22. Stateful Stream Processing</a></span></dt><dt><span class="chapter"><a href="#spring-cloud-dataflow-stream-tap-dsl">23. Tap a Stream</a></span></dt><dt><span class="chapter"><a href="#spring-cloud-dataflow-stream-app-labels">24. Using Labels in a Stream</a></span></dt><dt><span class="chapter"><a href="#spring-cloud-dataflow-stream-explicit-destination-names">25. Explicit Broker Destinations in a Stream</a></span></dt><dt><span class="chapter"><a href="#spring-cloud-dataflow-stream-advanced">26. Directed Graphs in a Stream</a></span></dt><dd><dl><dt><span class="section"><a href="#spring-cloud-dataflow-global-properties">26.1. Common application properties</a></span></dt></dl></dd><dt><span class="chapter"><a href="#spring-cloud-dataflow-stream-multi-binder">27. Stream applications with multiple binder configurations</a></span></dt></dl></dd><dt><span class="part"><a href="#spring-cloud-task">V. Tasks</a></span></dt><dd><dl><dt><span class="chapter"><a href="#spring-cloud-dataflow-task-intro">28. Introducing Spring Cloud Task</a></span></dt><dt><span class="chapter"><a href="#_the_lifecycle_of_a_task">29. The Lifecycle of a task</a></span></dt><dd><dl><dt><span class="section"><a href="#_registering_a_task_application">29.1. Registering a Task Application</a></span></dt><dt><span class="section"><a href="#_creating_a_task">29.2. Creating a Task</a></span></dt><dt><span class="section"><a href="#_launching_a_task">29.3. Launching a Task</a></span></dt><dt><span class="section"><a href="#_reviewing_task_executions">29.4. Reviewing Task Executions</a></span></dt><dt><span class="section"><a href="#_destroying_a_task">29.5. Destroying a Task</a></span></dt></dl></dd><dt><span class="chapter"><a href="#spring-cloud-dataflow-task-repository">30. Task Repository</a></span></dt><dd><dl><dt><span class="section"><a href="#_configuring_the_task_execution_repository">30.1. Configuring the Task Execution Repository</a></span></dt><dd><dl><dt><span class="section"><a href="#_local">30.1.1. Local</a></span></dt></dl></dd><dt><span class="section"><a href="#_datasource">30.2. Datasource</a></span></dt></dl></dd><dt><span class="chapter"><a href="#spring-cloud-dataflow-task-events">31. Subscribing to Task/Batch Events</a></span></dt><dt><span class="chapter"><a href="#spring-cloud-dataflow-launch-tasks-from-stream">32. Launching Tasks from a Stream</a></span></dt><dd><dl><dt><span class="section"><a href="#_triggertask">32.1. TriggerTask</a></span></dt><dt><span class="section"><a href="#_translator">32.2. Translator</a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#dashboard">VI. Dashboard</a></span></dt><dd><dl><dt><span class="chapter"><a href="#dashboard-introduction">33. Introduction</a></span></dt><dt><span class="chapter"><a href="#dashboard-apps">34. Apps</a></span></dt><dt><span class="chapter"><a href="#dashboard-runtime">35. Runtime</a></span></dt><dt><span class="chapter"><a href="#dashboard-streams">36. Streams</a></span></dt><dt><span class="chapter"><a href="#dashboard-flo-streams-designer">37. Create Stream</a></span></dt><dt><span class="chapter"><a href="#dashboard-tasks">38. Tasks</a></span></dt><dd><dl><dt><span class="section"><a href="#dashboard-tasks-apps">38.1. Apps</a></span></dt><dd><dl><dt><span class="section"><a href="#_create_a_task_definition_from_a_selected_task_app">38.1.1. Create a Task Definition from a selected Task App</a></span></dt><dt><span class="section"><a href="#_view_task_app_details">38.1.2. View Task App Details</a></span></dt></dl></dd><dt><span class="section"><a href="#dashboard-task-definition">38.2. Definitions</a></span></dt><dd><dl><dt><span class="section"><a href="#_launching_tasks">38.2.1. Launching Tasks</a></span></dt></dl></dd><dt><span class="section"><a href="#dashboard-tasks-executions">38.3. Executions</a></span></dt></dl></dd><dt><span class="chapter"><a href="#dashboard-jobs">39. Jobs</a></span></dt><dd><dl><dt><span class="section"><a href="#dashboard-job-executions-list">39.1. List job executions</a></span></dt><dd><dl><dt><span class="section"><a href="#dashboard-job-executions-details">39.1.1. Job execution details</a></span></dt><dt><span class="section"><a href="#dashboard-job-executions-steps">39.1.2. Step execution details</a></span></dt><dt><span class="section"><a href="#dashboard-job-executions-steps-progress">39.1.3. Step Execution Progress</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#dashboard-analytics">40. Analytics</a></span></dt></dl></dd><dt><span class="part"><a href="#appendix">VII. Appendices</a></span></dt><dd><dl><dt><span class="appendix"><a href="#migration-guide">A. Migrating from Spring XD to Spring Cloud Data Flow</a></span></dt><dd><dl><dt><span class="section"><a href="#_terminology_changes">A.1. Terminology Changes</a></span></dt><dt><span class="section"><a href="#_modules_to_applications">A.2. Modules to Applications</a></span></dt><dd><dl><dt><span class="section"><a href="#_custom_applications">A.2.1. Custom Applications</a></span></dt><dt><span class="section"><a href="#_application_registration">A.2.2. Application Registration</a></span></dt><dt><span class="section"><a href="#_application_properties_2">A.2.3. Application Properties</a></span></dt></dl></dd><dt><span class="section"><a href="#_message_bus_to_binders">A.3. Message Bus to Binders</a></span></dt><dd><dl><dt><span class="section"><a href="#_message_bus">A.3.1. Message Bus</a></span></dt><dt><span class="section"><a href="#_binders">A.3.2. Binders</a></span></dt><dt><span class="section"><a href="#_named_channels">A.3.3. Named Channels</a></span></dt><dt><span class="section"><a href="#_directed_graphs">A.3.4. Directed Graphs</a></span></dt></dl></dd><dt><span class="section"><a href="#_batch_to_tasks">A.4. Batch to Tasks</a></span></dt><dt><span class="section"><a href="#_shell_dsl_commands">A.5. Shell/DSL Commands</a></span></dt><dt><span class="section"><a href="#_rest_api">A.6. REST-API</a></span></dt><dt><span class="section"><a href="#_ui_flo">A.7. UI / Flo</a></span></dt><dt><span class="section"><a href="#_architecture_components">A.8. Architecture Components</a></span></dt><dd><dl><dt><span class="section"><a href="#_zookeeper">A.8.1. ZooKeeper</a></span></dt><dt><span class="section"><a href="#_rdbms">A.8.2. RDBMS</a></span></dt><dt><span class="section"><a href="#_redis">A.8.3. Redis</a></span></dt><dt><span class="section"><a href="#_cluster_topology">A.8.4. Cluster Topology</a></span></dt></dl></dd><dt><span class="section"><a href="#_central_configuration">A.9. Central Configuration</a></span></dt><dt><span class="section"><a href="#_distribution">A.10. Distribution</a></span></dt><dt><span class="section"><a href="#_hadoop_distribution_compatibility">A.11. Hadoop Distribution Compatibility</a></span></dt><dt><span class="section"><a href="#_yarn_deployment">A.12. YARN Deployment</a></span></dt><dt><span class="section"><a href="#_use_case_comparison">A.13. Use Case Comparison</a></span></dt><dd><dl><dt><span class="section"><a href="#_use_case_1">A.13.1. Use Case #1</a></span></dt><dt><span class="section"><a href="#_use_case_2">A.13.2. Use Case #2</a></span></dt><dt><span class="section"><a href="#_use_case_3">A.13.3. Use Case #3</a></span></dt></dl></dd></dl></dd><dt><span class="appendix"><a href="#test-cluster">B. Test Cluster</a></span></dt><dd><dl><dt><span class="section"><a href="#_clone_and_download_files">B.1. Clone and Download files</a></span></dt><dt><span class="section"><a href="#_start_vagrant_vms_for_the_mesos_cluster">B.2. Start Vagrant VMs for the Mesos cluster</a></span></dt><dt><span class="section"><a href="#_install_chronos_as_a_service_running_in_marathon">B.3. Install Chronos as a Service running in Marathon</a></span></dt></dl></dd><dt><span class="appendix"><a href="#building">C. Building</a></span></dt><dd><dl><dt><span class="section"><a href="#_documentation">C.1. Documentation</a></span></dt><dt><span class="section"><a href="#_working_with_the_code">C.2. Working with the code</a></span></dt><dd><dl><dt><span class="section"><a href="#_importing_into_eclipse_with_m2eclipse">C.2.1. Importing into eclipse with m2eclipse</a></span></dt><dt><span class="section"><a href="#_importing_into_eclipse_without_m2eclipse">C.2.2. Importing into eclipse without m2eclipse</a></span></dt></dl></dd></dl></dd><dt><span class="appendix"><a href="#contributing">D. Contributing</a></span></dt><dd><dl><dt><span class="section"><a href="#_sign_the_contributor_license_agreement">D.1. Sign the Contributor License Agreement</a></span></dt><dt><span class="section"><a href="#_code_conventions_and_housekeeping">D.2. Code Conventions and Housekeeping</a></span></dt></dl></dd></dl></dd></dl></div><div class="part"><div class="titlepage"><div><div><h1 class="title"><a name="introduction" href="#introduction"></a>Part&nbsp;I.&nbsp;Introduction</h1></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="dataflow-mesos-intro" href="#dataflow-mesos-intro"></a>1.&nbsp;Introducing Spring Cloud Data Flow Server for Apache Mesos</h2></div></div></div><p>This project provides support for orchestrating long-running (<span class="emphasis"><em>streaming</em></span>) and short-lived (<span class="emphasis"><em>task/batch</em></span>) data microservices on Apache Mesos with Marathon and Chronos.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-data-flow-overview" href="#spring-cloud-data-flow-overview"></a>2.&nbsp;Spring Cloud Data Flow</h2></div></div></div><p>Spring Cloud Data Flow is a cloud-native orchestration service for composable data microservices on modern runtimes. With Spring Cloud Data Flow, developers can create and orchestrate data pipelines for common use cases such as data ingest, real-time analytics, and data import/export.</p><p>The Spring Cloud Data Flow architecture consists of a server that deploys <a class="link" href="http://docs.spring.io/spring-cloud-dataflow/docs/1.0.1.RELEASE/reference/htmlsingle/#streams" target="_top">Streams</a> and <a class="link" href="http://docs.spring.io/spring-cloud-dataflow/docs/1.0.1.RELEASE/reference/htmlsingle/#spring-cloud-task-overview" target="_top">Tasks</a>.  Streams are defined using a <a class="link" href="http://docs.spring.io/spring-cloud-dataflow/docs/1.0.1.RELEASE/reference/html/_dsl_syntax.html" target="_top">DSL</a> or visually through the browser based designer UI.  Streams are based on the <a class="link" href="http://cloud.spring.io/spring-cloud-stream/" target="_top">Spring Cloud Stream</a> programming model while Tasks are based on the <a class="link" href="http://cloud.spring.io/spring-cloud-task/" target="_top">Spring Cloud Task</a> programming model. The sections below describe more information about creating your own custom Streams and Tasks</p><p>For more details about the core architecture components and the supported features, please review Spring Cloud Data Flow&#8217;s <a class="link" href="http://docs.spring.io/spring-cloud-dataflow/docs/1.0.1.RELEASE/reference/htmlsingle/" target="_top">core reference guide</a>. There&#8217;re several <a class="link" href="https://github.com/spring-cloud/spring-cloud-dataflow-samples" target="_top">samples</a> available for reference.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-stream-overview" href="#spring-cloud-stream-overview"></a>3.&nbsp;Spring Cloud Stream</h2></div></div></div><p>Spring Cloud Stream is a framework for building message-driven microservice applications. Spring Cloud Stream builds upon Spring Boot to create standalone, production-grade Spring applications, and uses Spring Integration to provide connectivity to message brokers. It provides opinionated configuration of middleware from several vendors, introducing the concepts of persistent publish-subscribe semantics, consumer groups, and partitions.</p><p>For more details about the core framework components and the supported features, please review Spring Cloud Stream&#8217;s <a class="link" href="http://docs.spring.io/spring-cloud-stream/docs/1.0.2.RELEASE/reference/htmlsingle/" target="_top">reference guide</a>.</p><p>There&#8217;s a rich ecosystem of Spring Cloud Stream <a class="link" href="http://docs.spring.io/spring-cloud-stream-app-starters/docs/1.0.2.RELEASE/reference/htmlsingle" target="_top">Application-Starters</a> that can be used either as standalone data microservice applications or in Spring Cloud Data Flow. For convenience, we have generated RabbitMQ and Apache Kafka variants of these application-starters that are available for use from <a class="link" href="http://repo.spring.io/libs-snapshot/org/springframework/cloud/stream/app/" target="_top">Maven Repo</a> and <a class="link" href="https://hub.docker.com/r/springcloudstream/" target="_top">Docker Hub</a> as maven artifacts and docker images, respectively.</p><p>Do you have a requirement to develop custom applications? No problem. Refer to this guide to create <a class="link" href="http://docs.spring.io/spring-cloud-stream-app-starters/docs/1.0.2.RELEASE/reference/htmlsingle/#_creating_custom_artifacts" target="_top">custom stream applications</a>. There&#8217;re several <a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-samples" target="_top">samples</a> available for reference.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-task-overview" href="#spring-cloud-task-overview"></a>4.&nbsp;Spring Cloud Task</h2></div></div></div><p>Spring Cloud Task makes it easy to create short-lived microservices. We provide capabilities that allow short-lived JVM processes to be executed on demand in a production environment.</p><p>For more details about the core framework components and the supported features, please review Spring Cloud Task&#8217;s <a class="link" href="http://docs.spring.io/spring-cloud-task/1.0.2.RELEASE/reference/htmlsingle/" target="_top">reference guide</a>.</p><p>There&#8217;s a rich ecosystem of Spring Cloud Task <a class="link" href="http://docs.spring.io/spring-cloud-task-app-starters/docs/1.0.1.RELEASE/reference/htmlsingle" target="_top">Application-Starters</a> that can be used either as standalone data microservice applications or in Spring Cloud Data Flow. For convenience, the generated application-starters are available for use from <a class="link" href="http://repo.spring.io/libs-snapshot/org/springframework/cloud/task/app/" target="_top">Maven Repo</a>. There are several <a class="link" href="https://github.com/spring-cloud/spring-cloud-task/tree/master/spring-cloud-task-samples" target="_top">samples</a> available for reference.</p></div></div><div class="part"><div class="titlepage"><div><div><h1 class="title"><a name="architecture" href="#architecture"></a>Part&nbsp;II.&nbsp;Architecture</h1></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="arch-intro" href="#arch-intro"></a>5.&nbsp;Introduction</h2></div></div></div><p>Spring Cloud Data Flow simplifies the development and deployment of applications focused on data processing use-cases.  The major concepts of the architecture are Applications, the Data Flow Server, and the target runtime.</p><p>Applications come in two flavors</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Long lived Stream applications where an unbounded amount of data is consumed or produced via messaging middleware.</li><li class="listitem">Short lived Task applications that process a finite set of data and then terminate.</li></ul></div><p>Depending on the runtime, applications can be packaged in two ways</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Spring Boot uber-jar that is hosted in a maven repository, file, http or any other Spring resource implementation.</li><li class="listitem">Docker</li></ul></div><p>The runtime is the place where applications execute.  The target runtimes for applications are platforms that you may already be using for other application deployments.</p><p>The supported runtimes are</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Cloud Foundry</li><li class="listitem">Apache YARN</li><li class="listitem">Kubernetes</li><li class="listitem">Apache Mesos</li><li class="listitem">Local Server for development</li></ul></div><p>There is a deployer Service Provider Interface (SPI) that enables you to extend Data Flow to deploy onto other runtimes, for example to support Hashicorp&#8217;s Nomad or Docker Swarm. Contributions are welcome!</p><p>The component that is responsible for deploying applications to a runtime is the Data Flow Server.  There is a Data Flow Server executable jar provided for each of the target runtimes.  The Data Flow server is responsible for interpreting</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">A stream DSL that describes the logical flow of data through multiple applications.</li><li class="listitem">A deployment manifest that describes the mapping of applications onto the runtime. For example, to set the initial number of instances, memory requirements, and data partitioning.</li></ul></div><p>As an example, the DSL to describe the flow of data from an http source to an Apache Cassandra sink would be written as &#8220;http | cassandra&#8221;.  These names in the DSL are registered with the Data Flow Server and map onto application artifacts that can be hosted in Maven or Docker repositories.  Many source, processor, and sink applications for common use-cases (e.g. jdbc, hdfs, http, router) are provided by the Spring Cloud Data Flow team.  The pipe symbol represents the communication between the two applications via messaging middleware. The two messaging middleware brokers that are supported are</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Apache Kafka</li><li class="listitem">RabbitMQ</li></ul></div><p>In the case of Kafka, when deploying the stream, the Data Flow server is responsible to create the topics that correspond to each pipe symbol and configure each application to produce or consume from the topics so the desired flow of data is achieved.</p><p>The interaction of the main components is shown below</p><div class="figure"><a name="d0e203" href="#d0e203"></a><p class="title"><b>Figure&nbsp;5.1.&nbsp;The Spring Cloud Data High Level Architecure</b></p><div class="figure-contents"><div class="mediaobject"><img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.0.1.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-arch.png" alt="The Spring Cloud Data Flow High Level Architecture"></div></div></div><br class="figure-break"><p>In this diagram a DSL description of a stream is POSTed to the Data Flow Server.  Based on the mapping of DSL application names to Maven and Docker artifacts, the http source and cassandra sink application are deployed on the target runtime.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="arch-microservice-style" href="#arch-microservice-style"></a>6.&nbsp;Microservice Architectural Style</h2></div></div></div><p>The Data Flow Server deploys applications onto the target runtime that conform to the microservice architectural style.  For example, a stream represents a high level application that consists of multiple small microservice applications each running in their own process.  Each microservice application can be scaled up or down independent of the other and each has their own versioning lifecycle.</p><p>Both Streaming and Task based microservice applications build upon Spring Boot as the foundational library.
This gives all microservice applications functionality such as health checks, security, configurable logging, monitoring and management functionality, as well as executable JAR packaging.</p><p>It is important to emphasise that these microservice applications are &#8216;just apps&#8217; that you can run by yourself using &#8216;java -jar&#8217; and passing in appropriate configuration properties.  We provide many common microservice applications for common operations so you don&#8217;t have to start from scratch when addressing common use-cases which build upon the rich ecosystem of Spring Projects, e.g Spring Integration, Spring Data, Spring Hadoop and Spring Batch.  Creating your own microservice application is similar to creating other Spring Boot applications, you can start using the Spring Initialzr web site or the UI to create the basic scaffolding of either a Stream or Task based microservice.</p><p>In addition to passing in the appropriate configuration to the applications, the Data Flow server is responsible for preparing the target platform&#8217;s infrastructure so that the application can be deployed.  For example, in Cloud Foundry it would be binding specified services to the applications and executing the &#8216;cf push&#8217; command for each application.  For Kubernetes it would be creating the replication controller, service, and load balancer.</p><p>The Data Flow Server helps simplify the deployment of multiple applications onto a target runtime, but one could also opt to deploy each of the microservice applications manually and not use Data Flow at all. This approach might be more appropriate to start out with for small scale deployments, gradually adopting the convenience and consistency of Data Flow as you develop more applications.
Manual deployment of Stream and Task based microservices is also a useful educational exercise that will help you better understand some of the automatic applications configuration and platform targeting steps that the Data Flow Server provides.</p><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch-comparison" href="#arch-comparison"></a>6.1&nbsp;Comparison to other Platform architectures</h2></div></div></div><p>Spring Cloud Data Flow&#8217;s architectural style is different than other Stream and Batch processing platforms.  For example in Apache Spark, Apache Flink, and Google Cloud Dataflow applications run on a dedicated compute engine cluster.  The nature of the compute engine gives these platforms a richer environment for performing complex calculations on the data as compared to Spring Cloud Data Flow, but it introduces complexity of another execution environment that is often not needed when creating data centric applications.  That doesn&#8217;t mean you cannot do real time data computations when using Spring Cloud Data Flow.  Refer to the analytics section which describes the integration of Redis to handle common counting based use-cases as well as the RxJava integration for functional API driven analytics use-cases, such as time-sliding-window and moving-average among others.</p><p>Similarly, Apache Storm, Hortonworks DataFlow and Spring Cloud Data Flow&#8217;s predecessor, Spring XD, use a dedicated application execution cluster, unique to each product, that determines where your code should execute on the cluster and perform health checks to ensure that long lived applications are restarted if they fail.  Often, framework specific interfaces are required to be used in order to correctly &#8220;plug in&#8221; to the cluster&#8217;s execution framework.</p><p>As we discovered during the evolution of Spring XD, the rise of multiple container frameworks in 2015 made creating our own runtime a duplication of efforts.  There is no reason to build your own resource management mechanics, when there&#8217;s multiple runtime platforms that offer this functionality already.  Taking these considerations into account is what made us shift to the current architecture where we delegate the execution to popular runtimes, runtimes that you may already be using for other purposes.  This is an advantage in that it reduces the cognitive distance for creating and managing data centric applications as many of the same skills used for deploying other end-user/web applications are applicable.</p></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="arch-streaming-apps" href="#arch-streaming-apps"></a>7.&nbsp;Streaming Applications</h2></div></div></div><p>While Spring Boot provides the foundation for creating DevOps friendly microservice applications, other libraries in the Spring ecosystem help create Stream based microservice applications.  The most important of these is Spring Cloud Stream.</p><p>The essence of the Spring Cloud Stream programming model is to provide an easy way to describe multiple inputs and outputs of an application that communicate over messaging middleware.  These input and outputs map onto Kafka topics or Rabbit exchanges and queues.  Common application configuration for a Source that generates data, a Process that consumes and produces data and a Sink that consumes data is provided as part of the library.</p><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch-streaming-imperative-programming" href="#arch-streaming-imperative-programming"></a>7.1&nbsp;Imperative Programming Model</h2></div></div></div><p>Spring Cloud Stream is most closely integrated with Spring Integration&#8217;s imperative "event at a time" programming model.  This means you write code that handles a single event callback.  For example,</p><pre class="programlisting"><em><span class="hl-annotation" style="color: gray">@EnableBinding(Sink.class)</span></em>
<span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">public</span> <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">class</span> LoggingSink {

    <em><span class="hl-annotation" style="color: gray">@StreamListener(Sink.INPUT)</span></em>
    <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">public</span> <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">void</span> log(String message) {
        System.out.println(message);
    }
}</pre><p>In this case the String payload of a message coming on the input channel, is handed to the log method.  The <code class="literal">@EnableBinding</code> annotation is what is used to tie together the input channel to the external middleware.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch-streaming-functional-programming" href="#arch-streaming-functional-programming"></a>7.2&nbsp;Functional Programming Model</h2></div></div></div><p>However, Spring Cloud Stream can support other programming styles.  There is initial support for functional style programming via <a class="link" href="http://docs.spring.io/spring-cloud-stream/docs/1.0.2.RELEASE/reference/htmlsingle/index.html#_rxjava_support" target="_top">RxJava Observable APIs</a> and upcoming versions will support callback methods with Project Reactor&#8217;s Flux API and Apache Kafka&#8217;s KStream API.</p></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="arch-streams" href="#arch-streams"></a>8.&nbsp;Streams</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch-streams-topologies" href="#arch-streams-topologies"></a>8.1&nbsp;Topologies</h2></div></div></div><p>The Stream DSL describes linear sequences of data flowing through the system.  For example, in the stream definition <code class="literal">http | transformer | cassandra</code>, each pipe symbol connects the application on the left to the one on the right.  Named channels can be used for routing and to fan out data to multiple messaging destinations.</p><p>Taps can be used to &#8216;listen in&#8217; to the data that if flowing across any of the pipe symbols.  Taps can be used as sources for new streams with an in independent life cycle.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch-streams-concurrency" href="#arch-streams-concurrency"></a>8.2&nbsp;Concurrency</h2></div></div></div><p>For an application that will consume events, Spring Cloud stream exposes a concurrency setting that controls the size of a thread pool used for dispatching incoming messages.  See the <a class="link" href="http://docs.spring.io/spring-cloud-stream/docs/1.0.2.RELEASE/reference/htmlsingle/index.html#_consumer_properties" target="_top">Consumer properties</a> documentation for more information.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch-streams-partitioning" href="#arch-streams-partitioning"></a>8.3&nbsp;Partitioning</h2></div></div></div><p>A common pattern in stream processing is to partition the data as it moves from one application to the next.  Partitioning is a critical concept in stateful processing, for either performance or consistency reasons, to ensure that all related data is processed together. For example, in a time-windowed average calculation example, it is important that all measurements from any given sensor are processed by the same application instance.  Alternatively, you may want to cache some data related to the incoming events so that it can be enriched without making a remote procedure call to retrieve the related data.</p><p>Spring Cloud Data Flow supports partitioning by configuring Spring Cloud Stream&#8217;s output and input bindings.  Spring Cloud Stream provides a common abstraction for implementing partitioned processing use cases in a uniform fashion across different types of middleware.  Partitioning can thus be used whether the broker itself is naturally partitioned (e.g., Kafka topics) or not (e.g., RabbitMQ).  The following image shows how data could be partitioned into two buckets, such that each instance of the average processor application consumes a unique set of data.</p><div class="figure"><a name="d0e291" href="#d0e291"></a><p class="title"><b>Figure&nbsp;8.1.&nbsp;Spring Cloud Stream Partitioning</b></p><div class="figure-contents"><div class="mediaobject"><img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.0.1.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/stream-partitioning.png" alt="Stream Partitioning Architecture"></div></div></div><br class="figure-break"><p>To use a simple partitioning strategy in Spring Cloud Data Flow, you only need set the instance count for each application in the stream and a <code class="literal">partitionKeyExpression</code> producer property when deploying the stream.  The <code class="literal">partitionKeyExpression</code> identifies what part of the message will be used as the key to partition data in the underlying middleware.  An <code class="literal">ingest</code> stream can be defined as <code class="literal">http | averageprocessor | cassandra</code>  (Note that the Cassandra sink isn&#8217;t shown in the diagram above).  Suppose the payload being sent to the http source was in JSON format and had a field called <code class="literal">sensorId</code>.  Deploying the stream with the shell command <code class="literal">stream deploy ingest --propertiesFile ingestStream.properties</code> where the contents of the file <code class="literal">ingestStream.properties</code> are</p><pre class="programlisting">app.http.count=<span class="hl-number">3</span>
app.averageprocessor.count=<span class="hl-number">2</span>
app.http.producer.partitionKeyExpression=payload.sensorId</pre><p>will deploy the stream such that all the input and output destinations are configured for data to flow through the applications but also ensure that a unique set of data is always delivered to each averageprocessor instance.  In this case the default algorithm is to evaluate <code class="literal">payload.sensorId % partitionCount</code> where the <code class="literal">partitionCount</code> is the application count in the case of RabbitMQ and the partition count of the topic in the case of Kafka.</p><p>Please refer to <a class="xref" href="#passing_stream_partition_properties" title="17.1.3&nbsp;Passing stream partition properties during stream deployment">Section&nbsp;17.1.3, &#8220;Passing stream partition properties during stream deployment&#8221;</a> for additional strategies to partition streams during deployment and how they map onto the underlying <a class="link" href="http://docs.spring.io/spring-cloud-stream/docs/1.0.2.RELEASE/reference/htmlsingle/index.html#_partitioning" target="_top">Spring Cloud Stream Partitioning properties</a>.</p><p>Also note, that you can&#8217;t currently scale partitioned streams.  Read the section <a class="xref" href="#arch-runtime-scaling" title="12.3&nbsp;Scaling at runtime">Section&nbsp;12.3, &#8220;Scaling at runtime&#8221;</a> for more information.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch-streams-delivery" href="#arch-streams-delivery"></a>8.4&nbsp;Message Delivery Guarantees</h2></div></div></div><p>For consumer applications, there is a retry policy for exceptions generated during message handling.  The default is to retry the callback method invocation 3 times and wait one second for the first retry.  A backoff multiplier of 2 is used for the second and third attempts.  All of these retry properties are configurable.</p><p>If there is still an exception on the last retry attempt, and dead letter queues are enabled, the message and exception message are published to the dead letter queue.  The dead letter queue is a destination and its nature depends on the messaging middleware (e.g in the case of Kafka it is a dedicated topic).  If dead letter functionality is not enabled, the message and exception is sent to the error channel, which by default logs the message and exception.</p><p>Additional messaging delivery guarantees are those provided by the underlying messaging middleware that is chosen for the application for both producing and consuming applications.  Refer to the Kafka <a class="link" href="http://docs.spring.io/spring-cloud-stream/docs/1.0.2.RELEASE/reference/htmlsingle/index.html#_kafka_consumer_properties" target="_top">Consumer</a> and <a class="link" href="http://docs.spring.io/spring-cloud-stream/docs/1.0.2.RELEASE/reference/htmlsingle/index.html#_kafka_producer_properties" target="_top">Producer</a> and Rabbit <a class="link" href="http://docs.spring.io/spring-cloud-stream/docs/1.0.3.BUILD-SNAPSHOT/reference/htmlsingle/index.html#_rabbitmq_consumer_properties" target="_top">Consumer</a> and <a class="link" href="http://docs.spring.io/spring-cloud-stream/docs/1.0.3.BUILD-SNAPSHOT/reference/htmlsingle/index.html#_rabbit_producer_properties" target="_top">Producer</a> documentation for more details.  You will find there to be extensive declarative support for all the native QOS options.</p></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="arch-analytics" href="#arch-analytics"></a>9.&nbsp;Analytics</h2></div></div></div><p>Spring Cloud Data Flow is aware of certain Sink applications that will write counter data to Redis and provides an REST endpoint to read counter data.  The types of counters supported are</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-app-starters/tree/master/metrics/spring-cloud-starter-stream-sink-counter" target="_top">Counter</a> - Counts the number of messages it receives, optionally storing counts in a separate store such as redis.</li><li class="listitem"><a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-app-starters/tree/master/metrics/spring-cloud-starter-stream-sink-field-value-counter" target="_top">Field Value Counter</a> - Counts occurrences of unique values for a named field in a message payload</li><li class="listitem"><a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-app-starters/tree/master/metrics/spring-cloud-starter-stream-sink-aggregate-counter" target="_top">Aggregate Counter</a> - Stores total counts but also retains the total count values for each minute, hour day and month.</li></ul></div><p>It is important to note that the timestamp that is used in the aggregate counter can come from a field in the message itself so that out of order messages are properly accounted.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="arch-task" href="#arch-task"></a>10.&nbsp;Task Applications</h2></div></div></div><p>The Spring Cloud Task programming model provides:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Persistence of the Task&#8217;s lifecycle events and exit code status.</li><li class="listitem">Lifecycle hooks to execute code before or after a task execution.</li><li class="listitem">Emit task events to a stream (as a source) during the task lifecycle.</li><li class="listitem">Integration with Spring Batch Jobs.</li></ul></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="arch-data-flow-server" href="#arch-data-flow-server"></a>11.&nbsp;Data Flow Server</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch-data-flow-server-endpoints" href="#arch-data-flow-server-endpoints"></a>11.1&nbsp;Endpoints</h2></div></div></div><p>The Data Flow Server uses an embedded servlet container and exposes REST endpoints for creating, deploying, undeploying, and destroying streams and tasks, querying runtime state, analytics, and the like. The Data Flow Server is implemented using Spring&#8217;s MVC framework and the <a class="link" href="https://github.com/SpringSource/spring-hateoas" target="_top">Spring HATEOAS</a> library to create REST representations that follow the HATEOAS principle.</p><div class="figure"><a name="d0e417" href="#d0e417"></a><p class="title"><b>Figure&nbsp;11.1.&nbsp;The Spring Cloud Data Flow Server</b></p><div class="figure-contents"><div class="mediaobject"><img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.0.1.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-server-arch.png" alt="The Spring Cloud Data Flow Server Architecture"></div></div></div><br class="figure-break"></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch-data-flow-server-customization" href="#arch-data-flow-server-customization"></a>11.2&nbsp;Customization</h2></div></div></div><p>Each Data Flow Server executable jar targets a single runtime by delegating to the implementation of the deployer Service Provider Interface found on the classpath.</p><p>We provide a Data Flow Server executable jar that targets a single runtime.  The Data Flow server delegates to the implementation of the deployer Service Provider Interface found on the classpath.  In the current version, there are no endpoints specific to a target runtime, but may be available in future releases as a convenience to access runtime specific features</p><p>While we provide a server executable for each of the target runtimes you can also create your own customized server application using Spring Initialzr.   This let&#8217;s you add or remove functionality relative to the executable jar we provide.  For example, adding additional security implementations, custom endpoints, or removing Task or Analytics REST endpoints.  You can also enable or disable some features through the use of feature toggles.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch-data-flow-server-security" href="#arch-data-flow-server-security"></a>11.3&nbsp;Security</h2></div></div></div><p>The Data Flow Server executable jars support basic http and OAuth 2.0 authentication to access it endpoints.  Refer to the security section for more information.</p><p>Authorization via groups is planned for a future release.</p></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="arch-runtime" href="#arch-runtime"></a>12.&nbsp;Runtime</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch-runtime-fault-tolerance" href="#arch-runtime-fault-tolerance"></a>12.1&nbsp;Fault Tolerance</h2></div></div></div><p>The target runtimes supported by Data Flow all have the ability to restart a long lived application should it fail.  Spring Cloud Data Flow sets up whatever health probe is required by the runtime environment when deploying the application.</p><p>The collective state of all applications that comprise the stream is used to determine the state of the stream.  If an application fails, the state of the stream will change from &#8216;deployed&#8217; to &#8216;partial&#8217;.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch-runtime-resource-management" href="#arch-runtime-resource-management"></a>12.2&nbsp;Resource Management</h2></div></div></div><p>Each target runtime lets you control the amount of memory, disk and CPU that is allocated to each application.  These are passed as properties in the deployment manifest using key names that are unique to each runtime.  Refer to the each platforms server documentation for more information.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch-runtime-scaling" href="#arch-runtime-scaling"></a>12.3&nbsp;Scaling at runtime</h2></div></div></div><p>When deploying a stream, you can set the instance count for each individual application that comprises the stream.
Once the stream is deployed, each target runtime lets you control the target number of instances for each individual application.
Using the APIs, UIs, or command line tools for each runtime, you can scale up or down the number of instances as required.
Future work will provide a portable command in the Data Flow Server to perform this operation.</p><p>Currently, this is not supported with the Kafka binder (based on the 0.8 simple consumer at the time of the release), as well as partitioned streams, for which the suggested workaround is redeploying the stream with an updated number of instances.
Both cases require a static consumer set up based on information about the total instance count and current instance index, a limitation intended to be addressed in future releases.
For example, Kafka 0.9 and higher provides good infrastructure for scaling applications dynamically and will be available as an alternative to the current Kafka 0.8 based binder in the near future.
One specific concern regarding scaling partitioned streams is the handling of local state, which is typically reshuffled as the number of instances is changed.
This is also intended to be addressed in the future versions, by providing first class support for local state management.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch-application-versioning" href="#arch-application-versioning"></a>12.4&nbsp;Application Versioning</h2></div></div></div><p>Application versioning, that is upgrading or downgrading an application from one version to another, is not directly supported by Spring Cloud Data Flow.  You must rely on specific target runtime features to perform these operational tasks.</p><p>The roadmap for Spring Cloud Data Flow will deploy applications that are compatible with Spinnaker to manage the complete application lifecycle.  This also includes automated canary analysis backed by  application metrics.  Portable commands in the Data Flow server to trigger pipelines in Spinnaker are also planned.</p></div></div></div><div class="part"><div class="titlepage"><div><div><h1 class="title"><a name="_getting_started" href="#_getting_started"></a>Part&nbsp;III.&nbsp;Getting Started</h1></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="_deploying_streams_and_tasks_on_apache_mesos_and_marathon_chronos" href="#_deploying_streams_and_tasks_on_apache_mesos_and_marathon_chronos"></a>13.&nbsp;Deploying Streams and Tasks on Apache Mesos and Marathon/Chronos</h2></div></div></div><p>In this getting started the Data Flow Server is running as a Docker container in Marathon and so are all the dependent services like a relational database for stream and task repositories, a message bus for stream apps and the key value store for analytics.</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">Deploy a Mesos and Marathon cluster.</p><p class="simpara">The <a class="link" href="https://open.mesosphere.com/getting-started/tools/" target="_top">Mesosphere getting started guide</a> provides a number of options for you to deploy a cluster. There is also a number of options listed on Mesosphere&#8217;s <a class="link" href="https://dcos.io/install/" target="_top">Install DC/OS</a> page. In <a class="xref" href="#test-cluster" title="Appendix&nbsp;B.&nbsp;Test Cluster">Appendix&nbsp;B, <i>Test Cluster</i></a> we describe how we configured a local test cluster using the DC/OS Vagrant project.</p><p class="simpara">The rest of this getting started guide assumes that you have a working Mesos and Marathon cluster and know the Marathon endpoint URL.</p><p class="simpara">We are using the Marathon endpoint URL of <a class="link" href="http://m1.dcos/service/marathon" target="_top">http://m1.dcos/service/marathon</a> for this document.</p></li><li class="listitem"><p class="simpara">Create a MySQL service on the Mesos cluster.</p><p class="simpara">The <code class="literal">mysql</code> service will be used for storing stream and task definitions in the stream and task repositories.  There is a sample <a class="link" href="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow-server-mesos/v1.0.0.RELEASE/src/etc/marathon/mysql.json" target="_top">application JSON file for MySQL</a> in the <code class="literal">spring-cloud-dataflow-server-mesos</code> repository that you can use as a starting point.  The service discovery mechanism is currently disabled so you need to look up the host and port to use for the connection.  Depending on how large your cluster is, you may want to tweak the CPU and/or memory values.</p><p class="simpara">Using the above JSON file and an Mesos and Marathon cluster installed you can deploy a Rabbit MQ application instance by issuing the following command</p><pre class="screen">curl -X POST http://m1.dcos/service/marathon/v2/apps -d @mysql.json -H "Content-type: application/json"</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Note the <code class="literal">@</code> symbol to reference a file input for the <code class="literal">curl</code> command.</p></td></tr></table></div></li><li class="listitem"><p class="simpara">Create a Rabbit MQ service on the Mesos cluster.</p><p class="simpara">The <code class="literal">rabbitmq</code> service will be used for messaging between applications in the stream.  There is a sample <a class="link" href="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow-server-mesos/v1.0.0.RELEASE/src/etc/marathon/rabbitmq.json" target="_top">application JSON file for Rabbit MQ</a> in the <code class="literal">spring-cloud-dataflow-server-mesos</code> repository that you can use as a starting point.  The service discovery mechanism is currently disabled so you need to look up the host and port to use for the connection.  Depending on how large your cluster is, you may want to tweak the CPU and/or memory values.</p><p class="simpara">Using the above JSON file and an Mesos and Marathon cluster installed you can deploy a Rabbit MQ service instance by issuing the following command</p><pre class="screen">curl -X POST http://m1.dcos/service/marathon/v2/apps -d @rabbitmq.json -H "Content-type: application/json"</pre></li><li class="listitem"><p class="simpara">Create a Redis service on the Mesos cluster.</p><p class="simpara">The <code class="literal">redis</code> service will be used for counters as part of the analytics.  There is a sample <a class="link" href="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow-server-mesos/v1.0.0.RELEASE/src/etc/marathon/redis.json" target="_top">application JSON file for Redis</a> in the <code class="literal">spring-cloud-dataflow-server-mesos</code> repository that you can use as a starting point.  The service discovery mechanism is currently disabled so you need to look up the host and port to use for the connection.  Depending on how large your cluster is, you may want to tweak the CPU and/or memory values.</p><p class="simpara">Using the above JSON file and an Mesos and Marathon cluster installed you can deploy a Redis service instance by issuing the following command</p><pre class="screen">curl -X POST http://m1.dcos/service/marathon/v2/apps -d @redis.json -H "Content-type: application/json"</pre><p class="simpara">Using the Marathon and Mesos UIs you can verify that <code class="literal">mysql</code>, <code class="literal">rabbitmq</code> and <code class="literal">redis</code> services are running on the cluster.</p></li><li class="listitem"><p class="simpara">Install Chronos on the Mesos cluster.</p><p class="simpara">The <a class="link" href="https://mesos.github.io/chronos/" target="_top">Chronos</a> service will be used for running task. If you haven&#8217;t already installed Chronos, this is the time to do it. You can install Chronos using the DC/OS UI (under the Universe section) or from the <code class="literal">dcos</code> command line:</p><pre class="screen">dcos package install chronos</pre></li><li class="listitem"><p class="simpara">Download the Marathon application JSON for the Spring Cloud Data Flow Server.</p><p class="simpara">Use the following command to download the Marathon application JSON file used to deploy Spring CLoud Data Flow Server for Mesos.</p><pre class="screen">$ wget https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow-server-mesos/v1.0.0.RELEASE/src/etc/marathon/scdf-server.json</pre><p class="simpara">We will need to modify the Docker image tag, API endpoints and host and port settings based on the current deployment environment. We hope to eliminiate most of this in future releases and instead rely on service discovery mechanisms. For now we do have to make the modifications manually. The downloaded file should look like this:</p><pre class="screen">{
  "id": "/spring-cloud-data-flow",
  "cpus": 0.5,
  "mem": 512.0,
  "instances": 1,
  "container": {
    "type": "DOCKER",
    "docker": {
      "image": "springcloud/spring-cloud-dataflow-server-mesos",
      "network": "BRIDGE",
      "portMappings": [
        { "containerPort": 9393 }
      ]
    }
  },
  "env": {
    "MESOS_MARATHON_URI": "http://m1.dcos/service/marathon",
    "MESOS_CHRONOS_URI": "http://m1.dcos/service/chronos",
    "JDBC_URL": "jdbc:mysql://192.168.65.111:5769/test",
    "JDBC_DRIVER": "org.mariadb.jdbc.Driver",
    "JDBC_USERNAME": "spring",
    "JDBC_PASSWORD": "secret",
    "RABBITMQ_HOST": "192.168.65.121",
    "RABBITMQ_PORT": "5261",
    "REDIS_HOST": "192.168.65.111",
    "REDIS_PORT": "19902",
    "SPRING_APPLICATION_JSON": "{\"spring.cloud.deployer.mesos.marathon.apiEndpoint\":\"${MESOS_MARATHON_URI}\",\"spring.cloud.deployer.mesos.chronos.apiEndpoint\":\"${MESOS_CHRONOS_URI}\",\"spring.datasource.url\":\"${JDBC_URL}\",\"spring.datasource.driverClassName\":\"${JDBC_DRIVER}\",\"spring.datasource.username\":\"${JDBC_USERNAME}\",\"spring.datasource.password\":\"${JDBC_PASSWORD}\",\"spring.datasource.testOnBorrow\":true,\"spring.datasource.validationQuery\":\"SELECT 1\",\"spring.redis.host\":\"${REDIS_HOST}\",\"spring.redis.port\":\"${REDIS_PORT}\",\"spring.cloud.deployer.mesos.marathon.environmentVariables\":\"SPRING_RABBITMQ_HOST=${RABBITMQ_HOST},SPRING_RABBITMQ_PORT=${RABBITMQ_PORT}\"}"
  },
  "healthChecks": [
    {
      "path": "/management/health",
      "portIndex": 0,
      "protocol": "HTTP",
      "ignoreHttp1xx": false,
      "gracePeriodSeconds": 120,
      "intervalSeconds": 60,
      "timeoutSeconds": 20,
      "maxConsecutiveFailures": 0
    }
  ]
}</pre><p class="simpara">First we need to modify the Docker image to use the tag <code class="literal">1.0.0.RELEASE</code>. It should be:</p><pre class="screen">    "image": "springcloud/spring-cloud-dataflow-server-mesos:1.0.0.RELEASE",</pre><p class="simpara">In the <code class="literal">env</code> section there are several environment variables that we need to adjust. We need to provide the following properties for accessing Marathon and Chronos APIs:</p><pre class="screen">    "MESOS_MARATHON_URI": "http://m1.dcos/service/marathon",
    "MESOS_CHRONOS_URI": "http://m1.dcos/service/chronos",</pre><p class="simpara">Here we did set them to the defaults for a local Vagrant DC/OS installation.</p><p class="simpara">&nbsp;</p><p class="simpara">We also need to provide the database configuration properties. Look up the database host and port from the Marathon UI. For the <code class="literal">mysql</code> service that we just installed, they were <code class="literal">192.168.65.111:5769</code>.</p><pre class="screen">    "JDBC_URL": "jdbc:mysql://192.168.65.111:5769/test",
    "JDBC_DRIVER": "org.mariadb.jdbc.Driver",
    "JDBC_USERNAME": "spring",
    "JDBC_PASSWORD": "secret",</pre><p class="simpara">Next, we need to provide the message bus configuration properties. Look up the host and port for the <code class="literal">rabbitmq</code> service. In our case they were <code class="literal">192.168.65.121:5261</code>.</p><pre class="screen">    "RABBITMQ_HOST": "192.168.65.121",
    "RABBITMQ_PORT": "5261",</pre><p class="simpara">Finally we need to do the same for the key value store properties. Look up the host and port for the <code class="literal">redis</code> service. In our case they were <code class="literal">192.168.65.111:19902</code>.</p><pre class="screen">    "REDIS_HOST": "192.168.65.111",
    "REDIS_PORT": "19902",</pre><p class="simpara">You can add properties to the <code class="literal">SPRING_APPLICATION_JSON</code> property as well. You might want to set default values for memory and cpu resource request.  For example <code class="literal">\"spring.cloud.deployer.mesos.marathon.memory\"=\"768\"</code> will by default allocate additional memory for the application vs. the default value of 512.  You can see all the available options in the <a class="link" href="https://raw.githubusercontent.com/spring-cloud/spring-cloud-deployer-mesos/v1.0.4.RELEASE/src/main/java/org/springframework/cloud/deployer/spi/mesos/marathon/MarathonAppDeployerProperties.java" target="_top">MarathonAppDeployerProperties.java</a> file.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>DC/OS in secured mode requires an Authorization header with a token when accessing the Marathon and Chronos REST end-points. To accommodate this you need to provide this token when deploying the Spring Cloud Data Flow server to a DC/OS secured cluster. See below for instructions.</p></td></tr></table></div><p class="simpara">If you are using a secured DC/OS cluster then you will need to add the authorization token to the above configuration. First, log in using <code class="literal">dcos auth login</code> command and enter the authentication token you get after authenticating with DC/OS web interface with the provided link. Next, run the <code class="literal">dcos config show core.dcos_acs_token</code> command to display the authorization token we need for our configuration. Copy and paste this token in the following environment variable that you add to the above application JSON:</p><pre class="screen">    "DCOS_TOKEN": "&lt;paste the token here&gt;",</pre><p class="simpara">We also need to add the <code class="literal">spring.cloud.deployer.mesos.dcos.authorizationToken</code> property to the <code class="literal">SPRING_APPLICATION_JSON</code> entry. Insert the following as another property entry:</p><pre class="screen">,\"spring.cloud.deployer.mesos.dcos.authorizationToken\":\"${DCOS_TOKEN}\"</pre></li><li class="listitem"><p class="simpara">Now, deploy the Spring Cloud Data Flow Server for Mesos and Marathon/Chronos using the above modified application JSON.</p><pre class="screen">curl -X POST http://m1.dcos/service/marathon/v2/apps -d @scdf-server.json -H "Content-type: application/json"</pre><p class="simpara">Verify that the <code class="literal">spring-cloud-data-flow</code> application is running before proceeding.</p></li><li class="listitem"><p class="simpara">Download and run the Spring Cloud Data Flow shell.</p><pre class="screen">$ wget http://repo.spring.io/release/org/springframework/cloud/spring-cloud-dataflow-shell/1.0.1.RELEASE/spring-cloud-dataflow-shell-1.0.1.RELEASE.jar

$ java -jar spring-cloud-dataflow-shell-1.0.1.RELEASE.jar</pre><p class="simpara">Lookup the host and port for the <code class="literal">spring-cloud-data-flow</code> application in the Marathon UI. Use those values to configure the server URI for the shell:</p><pre class="screen">dataflow:&gt;dataflow config server --uri http://192.168.65.111:20043</pre></li><li class="listitem"><p class="simpara">By default, the application registry will be empty. If you would like to register all out-of-the-box stream applications built with the RabbitMQ binder in bulk, you can with the following command. For more details, review how to <a class="link" href="http://docs.spring.io/spring-cloud-dataflow/docs/1.0.1.RELEASE/reference/html/spring-cloud-dataflow-register-apps.html" target="_top">register applications</a>.</p><pre class="screen">dataflow:&gt;app import --uri http://bit.ly/stream-applications-rabbit-docker</pre></li><li class="listitem"><p class="simpara">Deploy a simple stream in the shell</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>If you need to specify any of the app specific configuration properties then you must use "long-form" of them including the app specific prefix like <code class="literal">--jdbc.tableName=TEST_DATA</code>. This is due to the server not being able to access the metadata for the Docker based starter apps. You will also not see the configuration properties listed when using the <code class="literal">app info</code> command or in the Dashboard GUI.</p></td></tr></table></div><pre class="screen">dataflow:&gt;stream create --name ticktock --definition "time | log" --deploy</pre><p class="simpara">In the Mesos UI you can then look at the logs for the log sink. Look for a Mesos task with the name <code class="literal">log-0.log.ticktock</code>.</p><pre class="screen">2016-04-26 18:13:03.001  INFO 1 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)
2016-04-26 18:13:03.004  INFO 1 --- [           main] o.s.c.s.a.l.s.r.LogSinkRabbitApplication : Started LogSinkRabbitApplication in 7.766 seconds (JVM running for 8.24)
2016-04-26 18:13:54.443  INFO 1 --- [nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring FrameworkServlet 'dispatcherServlet'
2016-04-26 18:13:54.445  INFO 1 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization started
2016-04-26 18:13:54.459  INFO 1 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization completed in 14 ms
2016-04-26 18:14:09.088  INFO 1 --- [time.ticktock-1] log.sink                                 : 04/26/16 18:14:09
2016-04-26 18:14:10.077  INFO 1 --- [time.ticktock-1] log.sink                                 : 04/26/16 18:14:10
2016-04-26 18:14:11.080  INFO 1 --- [time.ticktock-1] log.sink                                 : 04/26/16 18:14:11
2016-04-26 18:14:12.083  INFO 1 --- [time.ticktock-1] log.sink                                 : 04/26/16 18:14:12
2016-04-26 18:14:13.090  INFO 1 --- [time.ticktock-1] log.sink                                 : 04/26/16 18:14:13
2016-04-26 18:14:14.091  INFO 1 --- [time.ticktock-1] log.sink                                 : 04/26/16 18:14:14
2016-04-26 18:14:15.093  INFO 1 --- [time.ticktock-1] log.sink                                 : 04/26/16 18:14:15
2016-04-26 18:14:16.095  INFO 1 --- [time.ticktock-1] log.sink                                 : 04/26/16 18:14:16</pre></li><li class="listitem"><p class="simpara">Destroy the stream</p><pre class="screen">dataflow:&gt;stream destroy --name ticktock</pre></li><li class="listitem"><p class="simpara">Register a task application using the shell</p><pre class="screen">dataflow:&gt;app register --name timestamp --type task --uri docker:springcloudtask/timestamp-task:latest</pre></li><li class="listitem"><p class="simpara">Create and launch the task using the shell</p><pre class="screen">dataflow:&gt;task create testtask --definition "timestamp"
dataflow:&gt;task launch testtask</pre><p class="simpara">In the Mesos UI you can then look at the logs for the <code class="literal">testtask</code> task. Look for a Mesos task with  the name <code class="literal">ChronosTask:testtask</code>.</p><pre class="screen">Starting task ct:1472062219364:0:testtask:

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v1.3.5.RELEASE)

2016-08-24 18:10:45.957  INFO 1 --- [           main] o.s.c.t.a.t.TimestampTaskApplication     : Starting TimestampTaskApplication v1.0.2.BUILD-SNAPSHOT on a2.dcos with PID 1 (/maven/timestamp-task.jar started by root in /)
2016-08-24 18:10:45.960  INFO 1 --- [           main] o.s.c.t.a.t.TimestampTaskApplication     : No active profile set, falling back to default profiles: default
2016-08-24 18:10:46.003  INFO 1 --- [           main] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@788c6159: startup date [Wed Aug 24 18:10:46 GMT 2016]; root of context hierarchy
2016-08-24 18:10:47.051  INFO 1 --- [           main] o.s.jdbc.datasource.init.ScriptUtils     : Executing SQL script from class path resource [org/springframework/cloud/task/schema-mysql.sql]
2016-08-24 18:10:47.062  INFO 1 --- [           main] o.s.jdbc.datasource.init.ScriptUtils     : Executed SQL script from class path resource [org/springframework/cloud/task/schema-mysql.sql] in 11 ms.
2016-08-24 18:10:47.207  INFO 1 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup
2016-08-24 18:10:47.211  INFO 1 --- [           main] o.s.c.support.DefaultLifecycleProcessor  : Starting beans in phase 0
2016-08-24 18:10:47.238  INFO 1 --- [           main] TimestampTaskConfiguration$TimestampTask : 2016-08-24 18:10:47.238
2016-08-24 18:10:47.249  INFO 1 --- [           main] s.c.a.AnnotationConfigApplicationContext : Closing org.springframework.context.annotation.AnnotationConfigApplicationContext@788c6159: startup date [Wed Aug 24 18:10:46 GMT 2016]; root of context hierarchy
2016-08-24 18:10:47.250  INFO 1 --- [           main] o.s.c.support.DefaultLifecycleProcessor  : Stopping beans in phase 0
2016-08-24 18:10:47.252  INFO 1 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Unregistering JMX-exposed beans on shutdown
2016-08-24 18:10:47.261  INFO 1 --- [           main] o.s.c.t.a.t.TimestampTaskApplication     : Started TimestampTaskApplication in 1.62 seconds (JVM running for 2.018)```</pre></li><li class="listitem"><p class="simpara">Destroy the task</p><pre class="screen">dataflow:&gt;task destroy --name testtask</pre></li></ol></div></div></div><div class="part"><div class="titlepage"><div><div><h1 class="title"><a name="streams" href="#streams"></a>Part&nbsp;IV.&nbsp;Streams</h1></div></div></div><div class="partintro"><div></div><p>In this section you will learn all about Streams and how to use them with Spring Cloud Data Flow.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-stream-intro" href="#spring-cloud-dataflow-stream-intro"></a>14.&nbsp;Introduction</h2></div></div></div><p>In Spring Cloud Data Flow, a basic stream defines the ingestion of event driven data from a <span class="emphasis"><em>source</em></span> to a <span class="emphasis"><em>sink</em></span> that passes through any number of <span class="emphasis"><em>processors</em></span>. Streams are composed of spring-cloud-stream applications and the deployment of stream definitions is done via the Data Flow Server (REST API). The <a class="link" href="#">Getting Started</a> section shows you how to start these servers and how to start and use the Spring Cloud Data Flow shell.</p><p>A high level DSL is used to create stream definitions. The DSL to define a stream that has an http source and a file sink (with no processors) is shown below</p><pre class="screen">http | file</pre><p>The DSL mimics a UNIX pipes and filters syntax. Default values for ports and filenames are used in this example but can be overridden using <code class="literal">--</code> options, such as</p><pre class="screen">http --server.port=8091 | file --directory=/tmp/httpdata/</pre><p>To create these stream definitions you use the shell or make an HTTP POST request to the Spring Cloud Data Flow Server. More details can be found in the sections below.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="_stream_dsl" href="#_stream_dsl"></a>15.&nbsp;Stream DSL</h2></div></div></div><p>In the examples above, we connected a source to a sink using the pipe symbol <code class="literal">|</code>. You can also pass properties to the source and sink configurations. The property names will depend on the individual app implementations, but as an example, the <code class="literal">http</code> source app exposes a <code class="literal">server.port</code> setting which allows you to change the data ingestion port from the default value. To create the stream using port 8000, we would use</p><pre class="screen">dataflow:&gt; stream create --definition "http --server.port=8000 | log" --name myhttpstream</pre><p>The shell provides tab completion for application properties and also the shell command <code class="literal">app info</code> provides some additional documentation.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-register-apps" href="#spring-cloud-dataflow-register-apps"></a>16.&nbsp;Register a Stream App</h2></div></div></div><p>Register a Stream App with the App Registry using the Spring Cloud Data Flow Shell
<code class="literal">app register</code> command. You must provide a unique name, application type, and a URI that can be
resolved to the app artifact. For the type, specify "source", "processor", or "sink".
Here are a few examples:</p><pre class="screen">dataflow:&gt;app register --name mysource --type source --uri maven://com.example:mysource:0.0.1-SNAPSHOT

dataflow:&gt;app register --name myprocessor --type processor --uri file:///Users/example/myprocessor-1.2.3.jar

dataflow:&gt;app register --name mysink --type sink --uri http://example.com/mysink-2.0.1.jar</pre><p>When providing a URI with the <code class="literal">maven</code> scheme, the format should conform to the following:</p><pre class="screen">maven://&lt;groupId&gt;:&lt;artifactId&gt;[:&lt;extension&gt;[:&lt;classifier&gt;]]:&lt;version&gt;</pre><p>For example, if you would like to register the snapshot versions of the <code class="literal">http</code> and <code class="literal">log</code>
applications built with the RabbitMQ binder, you could do the following:</p><pre class="screen">dataflow:&gt;app register --name http --type source --uri maven://org.springframework.cloud.stream.app:http-source-rabbit:1.0.0.BUILD-SNAPSHOT
dataflow:&gt;app register --name log --type sink --uri maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.0.0.BUILD-SNAPSHOT</pre><p>If you would like to register multiple apps at one time, you can store them in a properties file
where the keys are formatted as <code class="literal">&lt;type&gt;.&lt;name&gt;</code> and the values are the URIs.</p><p>For example, if you would like to register the snapshot versions of the <code class="literal">http</code> and <code class="literal">log</code>
applications built with the RabbitMQ binder, you could have the following in a properties file [<span class="emphasis"><em>eg: stream-apps.properties</em></span>]:</p><pre class="screen">source.http=maven://org.springframework.cloud.stream.app:http-source-rabbit:1.0.0.BUILD-SNAPSHOT
sink.log=maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.0.0.BUILD-SNAPSHOT</pre><p>Then to import the apps in bulk, use the <code class="literal">app import</code> command and provide the location of the properties file via <code class="literal">--uri</code>:</p><pre class="screen">dataflow:&gt;app import --uri file:///&lt;YOUR_FILE_LOCATION&gt;/stream-apps.properties</pre><p>For convenience, we have the static files with application-URIs (for both maven and docker) available for all the out-of-the-box
Stream app-starters. You can point to this file and import all the application-URIs in bulk. Otherwise, as explained in
previous paragraphs, you can register them individually or have your own custom property file with only the required application-URIs
in it. It is recommended, however, to have a "focused" list of desired application-URIs in a custom property file.</p><p>List of available static property files:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Maven based Stream Applications with RabbitMQ Binder: <a class="link" href="http://bit.ly/stream-applications-rabbit-maven" target="_top">bit.ly/stream-applications-rabbit-maven</a></li><li class="listitem">Maven based Stream Applications with Kafka Binder: <a class="link" href="http://bit.ly/stream-applications-kafka-maven" target="_top">bit.ly/stream-applications-kafka-maven</a></li><li class="listitem">Docker based Stream Applications with RabbitMQ Binder: <a class="link" href="http://bit.ly/stream-applications-rabbit-docker" target="_top">bit.ly/stream-applications-rabbit-docker</a></li><li class="listitem">Docker based Stream Applications with Kafka Binder: <a class="link" href="http://bit.ly/stream-applications-kafka-docker" target="_top">bit.ly/stream-applications-kafka-docker</a></li></ul></div><p>For example, if you would like to register all out-of-the-box stream applications built with the RabbitMQ binder in bulk, you can with
the following command.</p><pre class="screen">dataflow:&gt;app import --uri http://bit.ly/stream-applications-rabbit-maven</pre><p>You can also pass the <code class="literal">--local</code> option (which is TRUE by default) to indicate whether the
properties file location should be resolved within the shell process itself. If the location should
be resolved from the Data Flow Server process, specify <code class="literal">--local false</code>.</p><p>When using either <code class="literal">app register</code> or <code class="literal">app import</code>, if a stream app is already registered with
the provided name and type, it will not be overridden by default. If you would like to override the
pre-existing stream app, then include the <code class="literal">--force</code> option.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>In some cases the Resource is resolved on the server side, whereas in others the
URI will be passed to a runtime container instance where it is resolved. Consult
the specific documentation of each Data Flow Server for more detail.</p></td></tr></table></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-dataflow-stream-app-whitelisting" href="#spring-cloud-dataflow-stream-app-whitelisting"></a>16.1&nbsp;Whitelisting application properties</h2></div></div></div><p>Stream applications are Spring Boot applications which are aware of many <a class="link" href="http://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#common-application-properties" target="_top">common application properties</a>, e.g. <code class="literal">server.port</code> but also families of properties such as those with the prefix <code class="literal">spring.jmx</code> and <code class="literal">logging</code>.  When creating your own application it is desirable to whitelist properties so that the shell and the UI can display them first as primary properties when presenting options via TAB completion or in drop-down boxes.</p><p>To whitelist application properties create a file named <code class="literal">spring-configuration-metadata-whitelist.properties</code> in the <code class="literal">META-INF</code> resource directory.  There are two property keys that can be used inside this file. The first key is named <code class="literal">configuration-properties.classes</code>.  The value is a comma separated list of fully qualified <code class="literal">@ConfigurationProperty</code> class names.  The second key is <code class="literal">configuration-properties.names</code> whose value is a comma separated list of property names.  This can contain the full name of property, such as <code class="literal">server.port</code> or a partial name to whitelist a category of property names, e.g. <code class="literal">spring.jmx</code>.</p><p>The <a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-app-starters" target="_top">Spring Cloud Stream application starters</a> are a good place to look for examples of usage.  Here is a simple example of the file source&#8217;s <code class="literal">spring-configuration-metadata-whitelist.properties</code> file</p><pre class="screen">configuration.classes=org.springframework.cloud.stream.app.file.sink.FileSinkProperties</pre><p>If for some reason we also wanted to add <code class="literal">file.prefix</code> to this file, it would look like</p><pre class="screen">configuration.classes=org.springframework.cloud.stream.app.file.sink.FileSinkProperties
configuration-properties.names=server.port</pre><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="images/important.png"></td><th align="left">Important</th></tr><tr><td align="left" valign="top"><p>As of Spring Cloud Data Flow <code class="literal">1.0.0.RELEASE</code> the whitelisting of application properties
is only explicitly supported for Spring Boot <code class="literal">1.3.x</code> based application. Milestone releases
of the upcoming Spring Boot <code class="literal">1.4.0</code> release are not explicitly supported, yet.</p><p>The <code class="literal">spring-boot-maven-plugin</code> used in <code class="literal">1.4.x</code> has a different approach in handling
the nested archives inside the <code class="literal">jar</code>. As a result you will notice that the application properties
are not listed using <code class="literal">app info</code> command at all. As a temporary workaround, you can override the managed
version of your app&#8217;s <code class="literal">spring-boot-maven-plugin</code>
explicitly and revert to a version of the latest 1.3.x release:</p><p>For example, if your app&#8217;s <code class="literal">pom.xml</code> specifies to use Spring Boot <code class="literal">1.4.0.M3</code>:</p><pre class="programlisting"><span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;parent&gt;</span>
  <span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;artifactId&gt;</span>spring-boot-starter-parent<span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;/artifactId&gt;</span>
  <span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;groupId&gt;</span>org.springframework.boot<span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;/groupId&gt;</span>
  <span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;version&gt;</span>1.4.0.M3<span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;/version&gt;</span>
  <span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;relativePath&gt;</span><span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;/relativePath&gt;</span>
<span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;/parent&gt;</span></pre><p>Then you can override the managed version of the <code class="literal">spring-boot-maven-plugin</code> with:</p><pre class="programlisting"><span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;plugin&gt;</span>
  <span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;groupId&gt;</span>org.springframework.boot<span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;/groupId&gt;</span>
  <span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;artifactId&gt;</span>spring-boot-maven-plugin<span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;/artifactId&gt;</span>
  <span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;version&gt;</span>1.3.5.RELEASE<span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;/version&gt;</span>  <a name="CO1-1" href="#CO1-1"></a><span><img src="images/callouts/1.png" alt="1" border="0"></span>
<span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;/plugin&gt;</span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO1-1"><span><img src="images/callouts/1.png" alt="1" border="0"></span></a> </p></td><td valign="top" align="left"><p>Overriding the managed version <code class="literal">1.4.0.M3</code>.</p></td></tr></table></div><p>Also, if you have your own <code class="literal">dataflow</code> server built using <code class="literal">@EnableDataflowServer</code> and using Spring Boot <code class="literal">1.4.x</code> in that,
you would need to explicitly override the <code class="literal">spring-boot-maven-plugin</code> with any of <code class="literal">1.3.x</code> releases.</p></td></tr></table></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-create-stream" href="#spring-cloud-dataflow-create-stream"></a>17.&nbsp;Creating a Stream</h2></div></div></div><p>The Spring Cloud Data Flow Server exposes a full RESTful API for managing the lifecycle of stream definitions, but the easiest way to use is it is via the Spring Cloud Data Flow shell. Start the shell as described in the <a class="link" href="Getting-Started.xml#getting-started" target="_top">Getting Started</a> section.</p><p>New streams are created by posting stream definitions. The definitions are built from a simple DSL. For example, let&#8217;s walk through what happens if we execute the following shell command:</p><pre class="screen">dataflow:&gt; stream create --definition "time | log" --name ticktock</pre><p>This defines a stream named <code class="literal">ticktock</code> based off the DSL expression <code class="literal">time | log</code>.  The DSL uses the "pipe" symbol <code class="literal">|</code>, to connect a source to a sink.</p><p>Then to deploy the stream execute the following shell command (or alternatively add the <code class="literal">--deploy</code> flag when creating the stream so that this step is not needed):</p><pre class="screen">dataflow:&gt; stream deploy --name ticktock</pre><p>The Data Flow Server resolves <code class="literal">time</code> and <code class="literal">log</code> to maven coordinates and uses those to launch the <code class="literal">time</code> and <code class="literal">log</code> applications of the stream.</p><pre class="screen">2016-06-01 09:41:21.728  INFO 79016 --- [nio-9393-exec-6] o.s.c.d.spi.local.LocalAppDeployer       : deploying app ticktock.log instance 0
   Logs will be in /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/ticktock-1464788481708/ticktock.log
2016-06-01 09:41:21.914  INFO 79016 --- [nio-9393-exec-6] o.s.c.d.spi.local.LocalAppDeployer       : deploying app ticktock.time instance 0
   Logs will be in /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/ticktock-1464788481910/ticktock.time</pre><p>In this example, the time source simply sends the current time as a message each second, and the log sink outputs it using the logging framework.
You can tail the <code class="literal">stdout</code> log (which has an "_&lt;instance&gt;" suffix). The log files are located within the directory displayed in the Data Flow Server&#8217;s log output, as shown above.</p><pre class="screen">$ tail -f /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/ticktock-1464788481708/ticktock.log/stdout_0.log
2016-06-01 09:45:11.250  INFO 79194 --- [  kafka-binder-] log.sink    : 06/01/16 09:45:11
2016-06-01 09:45:12.250  INFO 79194 --- [  kafka-binder-] log.sink    : 06/01/16 09:45:12
2016-06-01 09:45:13.251  INFO 79194 --- [  kafka-binder-] log.sink    : 06/01/16 09:45:13</pre><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_application_properties" href="#_application_properties"></a>17.1&nbsp;Application properties</h2></div></div></div><p>Application properties are the properties associated with each application in the stream. When the application is deployed, the application properties are applied to the application via
command line arguments or environment variables based on the underlying deployment implementation.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_passing_application_properties_when_creating_a_stream" href="#_passing_application_properties_when_creating_a_stream"></a>17.1.1&nbsp;Passing application properties when creating a stream</h3></div></div></div><p>The following stream</p><pre class="programlisting">dataflow:&gt; stream create --definition <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"time | log"</span> --name ticktock</pre><p>can have application properties defined at the time of stream creation.</p><p>The shell command <code class="literal">app info</code> displays the white-listed application properties for the application.
For more info on the property white listing refer to <a class="xref" href="#spring-cloud-dataflow-stream-app-whitelisting" title="16.1&nbsp;Whitelisting application properties">Section&nbsp;16.1, &#8220;Whitelisting application properties&#8221;</a></p><p>Below are the white listed properties for the app <code class="literal">time</code>:</p><pre class="programlisting">dataflow:&gt; app info source:time
&#9556;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9572;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9572;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9572;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9559;
&#9553;         Option Name          &#9474;         Description          &#9474;           Default            &#9474;             Type             &#9553;
&#9568;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9578;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9578;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9578;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9571;
&#9553;trigger.time-unit             &#9474;The TimeUnit to apply to delay&#9474;&lt;none&gt;                        &#9474;java.util.concurrent.TimeUnit &#9553;
&#9553;                              &#9474;values.                       &#9474;                              &#9474;                              &#9553;
&#9553;trigger.fixed-delay           &#9474;Fixed delay <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">for</span> periodic      &#9474;<span class="hl-number">1</span>                             &#9474;java.lang.Integer             &#9553;
&#9553;                              &#9474;triggers.                     &#9474;                              &#9474;                              &#9553;
&#9553;trigger.cron                  &#9474;Cron expression value <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">for</span> the &#9474;&lt;none&gt;                        &#9474;java.lang.String              &#9553;
&#9553;                              &#9474;Cron Trigger.                 &#9474;                              &#9474;                              &#9553;
&#9553;trigger.initial-delay         &#9474;Initial delay <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">for</span> periodic    &#9474;<span class="hl-number">0</span>                             &#9474;java.lang.Integer             &#9553;
&#9553;                              &#9474;triggers.                     &#9474;                              &#9474;                              &#9553;
&#9553;trigger.max-messages          &#9474;Maximum messages per poll, -<span class="hl-number">1</span> &#9474;<span class="hl-number">1</span>                             &#9474;java.lang.Long                &#9553;
&#9553;                              &#9474;means infinity.               &#9474;                              &#9474;                              &#9553;
&#9553;trigger.date-format           &#9474;Format <span xmlns:d="http://docbook.org/ns/docbook" class="hl-keyword">for</span> the date value.    &#9474;&lt;none&gt;                        &#9474;java.lang.String              &#9553;
&#9562;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9575;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9575;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9575;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9565;</pre><p>Below are the white listed properties for the app <code class="literal">log</code>:</p><pre class="programlisting">dataflow:&gt; app info sink:log
&#9556;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9572;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9572;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9572;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9559;
&#9553;         Option Name          &#9474;         Description          &#9474;           Default            &#9474;             Type             &#9553;
&#9568;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9578;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9578;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9578;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9571;
&#9553;log.name                      &#9474;The name of the logger to use.&#9474;&lt;none&gt;                        &#9474;java.lang.String              &#9553;
&#9553;log.level                     &#9474;The level at which to log     &#9474;&lt;none&gt;                        &#9474;org.springframework.integratio&#9553;
&#9553;                              &#9474;messages.                     &#9474;                              &#9474;n.handler.LoggingHandler$Level&#9553;
&#9553;log.expression                &#9474;A SpEL expression (against the&#9474;payload                       &#9474;java.lang.String              &#9553;
&#9553;                              &#9474;incoming message) to evaluate &#9474;                              &#9474;                              &#9553;
&#9553;                              &#9474;as the logged message.        &#9474;                              &#9474;                              &#9553;
&#9562;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9575;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9575;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9575;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9552;&#9565;</pre><p>The application properties for the <code class="literal">time</code> and <code class="literal">log</code> apps can be specified at the time of <code class="literal">stream</code> creation as follows:</p><pre class="programlisting">dataflow:&gt; stream create --definition <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"time --fixed-delay=5 | log --level=WARN"</span> --name ticktock</pre><p>Note that the properties <code class="literal">fixed-delay</code> and <code class="literal">level</code> defined above for the apps <code class="literal">time</code> and <code class="literal">log</code> are the 'short-form' property names provided by the shell completion.
These 'short-form' property names are applicable only for the white-listed properties and in all other cases, only <span class="emphasis"><em>fully qualified</em></span> property names should be used.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_passing_application_properties_when_deploying_a_stream" href="#_passing_application_properties_when_deploying_a_stream"></a>17.1.2&nbsp;Passing application properties when deploying a stream</h3></div></div></div><p>The application properties can also be specified when deploying a stream. When specified during deployment, these application properties can either be specified as
 'short-form' property names (applicable for white-listed properties) or <span class="emphasis"><em>fully qualified</em></span> property names. The application properties should have the prefix "app.&lt;appName/label&gt;".</p><p>For example, the stream</p><pre class="programlisting">dataflow:&gt; stream create --definition <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"time | log"</span> --name ticktock</pre><p>can be deployed with application properties using the 'short-form' property names:</p><pre class="programlisting">dataflow:&gt;stream deploy ticktock --properties <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"app.time.fixed-delay=5,app.log.level=ERROR"</span></pre><p>When using the app label,</p><pre class="programlisting">stream create ticktock --definition <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"a: time | b: log"</span></pre><p>the application properties can be defined as:</p><pre class="programlisting">stream deploy ticktock --properties <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"app.a.fixed-delay=4,app.b.level=ERROR"</span></pre></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="passing_stream_partition_properties" href="#passing_stream_partition_properties"></a>17.1.3&nbsp;Passing stream partition properties during stream deployment</h3></div></div></div><p>A common pattern in stream processing is to partition the data as it is streamed.
This entails deploying multiple instances of a message consuming app and using
content-based routing so that messages with a given key (as determined at runtime)
are always routed to the same app instance. You can pass the partition properties during
stream deployment to declaratively configure a partitioning strategy to route each
message to a specific consumer instance.</p><p>See below for examples of deploying partitioned streams:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><span class="strong"><strong>app.[app/label name].producer.partitionKeyExtractorClass</strong></span></span></dt><dd>The class name of a PartitionKeyExtractorStrategy (default <code class="literal">null</code>)</dd><dt><span class="term"><span class="strong"><strong>app.[app/label name].producer.partitionKeyExpression</strong></span></span></dt><dd>A SpEL expression, evaluated against the message, to determine the partition key;
only applies if <code class="literal">partitionKeyExtractorClass</code> is null. If both are null, the app
is not partitioned (default <code class="literal">null</code>)</dd><dt><span class="term"><span class="strong"><strong>app.[app/label name].producer.partitionSelectorClass</strong></span></span></dt><dd>The class name of a PartitionSelectorStrategy (default <code class="literal">null</code>)</dd><dt><span class="term"><span class="strong"><strong>app.[app/label name].producer.partitionSelectorExpression</strong></span></span></dt><dd>A SpEL expression, evaluated against the partition key, to determine the partition
index to which the message will be routed. The final partition index will be the
return value (an integer) modulo <code class="literal">[nextModule].count</code>. If both the class and
expression are null, the underlying binder&#8217;s default PartitionSelectorStrategy
will be applied to the key (default <code class="literal">null</code>)</dd></dl></div><p>In summary, an app is partitioned if its count is &gt; 1 and the previous app has a
<code class="literal">partitionKeyExtractorClass</code> or <code class="literal">partitionKeyExpression</code> (class takes precedence).
When a partition key is extracted, the partitioned app instance is determined by
invoking the <code class="literal">partitionSelectorClass</code>, if present, or the <code class="literal">partitionSelectorExpression % partitionCount</code>,
where <code class="literal">partitionCount</code> is application count in the case of RabbitMQ, and the underlying
partition count of the topic in the case of Kafka.</p><p>If neither a <code class="literal">partitionSelectorClass</code> nor a <code class="literal">partitionSelectorExpression</code> is
present the result is <code class="literal">key.hashCode() % partitionCount</code>.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_overriding_application_properties_during_stream_deployment" href="#_overriding_application_properties_during_stream_deployment"></a>17.1.4&nbsp;Overriding application properties during stream deployment</h3></div></div></div><p>Application properties that are defined during deployment override the same properties defined during the stream creation.</p><p>For example, the following stream has application properties defined during stream creation:</p><pre class="programlisting">dataflow:&gt; stream create --definition <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"time --fixed-delay=5 | log --level=WARN"</span> --name ticktock</pre><p>To override these application properties, one can specify the new property values during deployment:</p><pre class="programlisting">dataflow:&gt;stream deploy ticktock --properties <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"app.time.fixed-delay=4,app.log.level=ERROR"</span></pre></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_deployment_properties" href="#_deployment_properties"></a>17.2&nbsp;Deployment properties</h2></div></div></div><p>When deploying the stream, properties that control the deployment of the apps into the target platform are known as <code class="literal">deployment</code> properties.
For instance, one can specify how many instances need to be deployed for the specific application defined in the stream using the deployment property called <code class="literal">count</code>.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_passing_instance_count_as_deployment_property" href="#_passing_instance_count_as_deployment_property"></a>17.2.1&nbsp;Passing instance count as deployment property</h3></div></div></div><p>If you would like to have multiple instances of an application in the stream, you
can include a property with the deploy command:</p><pre class="programlisting">dataflow:&gt; stream deploy --name ticktock --properties <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"app.time.count=3"</span></pre><p>Note that <code class="literal">count</code> is the <span class="strong"><strong>reserved</strong></span> property name used by the underlying deployer. Hence, if the application also has a custom property named <code class="literal">count</code>, it is <span class="strong"><strong>not</strong></span> supported
 when specified in 'short-form' form during stream <span class="emphasis"><em>deployment</em></span> as it could conflict with the <span class="emphasis"><em>instance</em></span> count deployer property. Instead, the <code class="literal">count</code> as a custom application property can be
 specified in its <span class="emphasis"><em>fully qualified</em></span> form (example: <code class="literal">app.foo.bar.count</code>) during stream <span class="emphasis"><em>deployment</em></span> or it can be specified using 'short-form' or <span class="emphasis"><em>fully qualified</em></span> form during the stream <span class="emphasis"><em>creation</em></span>
 where it will be considered as an app property.</p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="images/important.png"></td><th align="left">Important</th></tr><tr><td align="left" valign="top"><p>See <a class="xref" href="#spring-cloud-dataflow-stream-app-labels" title="24.&nbsp;Using Labels in a Stream">Chapter&nbsp;24, <i>Using Labels in a Stream</i></a>.</p></td></tr></table></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_inline_vs_file_reference_properties" href="#_inline_vs_file_reference_properties"></a>17.2.2&nbsp;Inline vs file reference properties</h3></div></div></div><p>When using the Spring Cloud Dataflow Shell, there are two ways to provide deployment
properties: either <span class="strong"><strong>inline</strong></span> or via a <span class="strong"><strong>file reference</strong></span>. Those two ways are exclusive
and documented below:</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><span class="strong"><strong>Inline properties</strong></span></span></dt><dd>use the <code class="literal">--properties</code> shell option and list properties as a comma separated
list of key=value pairs, like so:</dd></dl></div><pre class="programlisting">stream deploy foo
    --properties <span xmlns:d="http://docbook.org/ns/docbook" class="hl-string">"app.transform.count=2,app.transform.producer.partitionKeyExpression=payload"</span></pre><div class="variablelist"><dl class="variablelist"><dt><span class="term"><span class="strong"><strong>Using a file reference</strong></span></span></dt><dd>use the <code class="literal">--propertiesFile</code> option and point it to a local Java <code class="literal">.properties</code> file
(i.e. that lives in the filesystem of the machine running the shell). Being read
as a <code class="literal">.properties</code> file, normal rules apply (ISO 8859-1 encoding, <code class="literal">=</code>, <code class="literal">&lt;space&gt;</code> or
<code class="literal">:</code> delimiter, etc.) although we recommend using <code class="literal">=</code> as a key-value pair delimiter
for consistency:</dd></dl></div><pre class="programlisting">stream deploy foo --propertiesFile myprops.properties</pre><p>where <code class="literal">myprops.properties</code> contains:</p><pre class="screen">app.transform.count=2
app.transform.producer.partitionKeyExpression=payload</pre><p>Both the above properties will be passed as deployment properties for the stream <code class="literal">foo</code> above.</p></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-destroy-stream" href="#spring-cloud-dataflow-destroy-stream"></a>18.&nbsp;Destroying a Stream</h2></div></div></div><p>You can delete a stream by issuing the <code class="literal">stream destroy</code> command from the shell:</p><pre class="screen">dataflow:&gt; stream destroy --name ticktock</pre><p>If the stream was deployed, it will be undeployed before the stream definition is deleted.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-deploy-undeploy-stream" href="#spring-cloud-dataflow-deploy-undeploy-stream"></a>19.&nbsp;Deploying and Undeploying Streams</h2></div></div></div><p>Often you will want to stop a stream, but retain the name and definition for future use. In that case you can <code class="literal">undeploy</code> the stream by name and issue the <code class="literal">deploy</code> command at a later time to restart it.</p><pre class="screen">dataflow:&gt; stream undeploy --name ticktock
dataflow:&gt; stream deploy --name ticktock</pre></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-stream-app-types" href="#spring-cloud-dataflow-stream-app-types"></a>20.&nbsp;Other Source and Sink Application Types</h2></div></div></div><p>Let&#8217;s try something a bit more complicated and swap out the <code class="literal">time</code> source for something else. Another supported source type is <code class="literal">http</code>, which accepts data for ingestion over HTTP POSTs. Note that the <code class="literal">http</code> source accepts data on a different port from the Data Flow Server (default 8080). By default the port is randomly assigned.</p><p>To create a stream using an <code class="literal">http</code> source, but still using the same <code class="literal">log</code> sink, we would change the original command above to</p><pre class="screen">dataflow:&gt; stream create --definition "http | log" --name myhttpstream --deploy</pre><p>which will produce the following output from the server</p><pre class="screen">2016-06-01 09:47:58.920  INFO 79016 --- [io-9393-exec-10] o.s.c.d.spi.local.LocalAppDeployer       : deploying app myhttpstream.log instance 0
   Logs will be in /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/myhttpstream-1464788878747/myhttpstream.log
2016-06-01 09:48:06.396  INFO 79016 --- [io-9393-exec-10] o.s.c.d.spi.local.LocalAppDeployer       : deploying app myhttpstream.http instance 0
   Logs will be in /var/folders/wn/8jxm_tbd1vj28c8vj37n900m0000gn/T/spring-cloud-dataflow-912434582726479179/myhttpstream-1464788886383/myhttpstream.http</pre><p>Note that we don&#8217;t see any other output this time until we actually post some data (using a shell command). In order to see the randomly assigned port on which the http source is listening, execute:</p><pre class="screen">dataflow:&gt; runtime apps</pre><p>You should see that the corresponding http source has a <code class="literal">url</code> property containing the host and port information on which it is listening. You are now ready to post to that url, e.g.:</p><pre class="screen">dataflow:&gt; http post --target http://localhost:1234 --data "hello"
dataflow:&gt; http post --target http://localhost:1234 --data "goodbye"</pre><p>and the stream will then funnel the data from the http source to the output log implemented by the log sink</p><pre class="screen">2016-06-01 09:50:22.121  INFO 79654 --- [  kafka-binder-] log.sink    : hello
2016-06-01 09:50:26.810  INFO 79654 --- [  kafka-binder-] log.sink    : goodbye</pre><p>Of course, we could also change the sink implementation. You could pipe the output to a file (<code class="literal">file</code>), to hadoop (<code class="literal">hdfs</code>) or to any of the other sink apps which are available. You can also define your own apps.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-simple-stream" href="#spring-cloud-dataflow-simple-stream"></a>21.&nbsp;Simple Stream Processing</h2></div></div></div><p>As an example of a simple processing step, we can transform the payload of the HTTP posted data to upper case using the stream definitions</p><pre class="screen">http | transform --expression=payload.toUpperCase() | log</pre><p>To create this stream enter the following command in the shell</p><pre class="screen">dataflow:&gt; stream create --definition "http | transform --expression=payload.toUpperCase() | log" --name mystream --deploy</pre><p>Posting some data (using a shell command)</p><pre class="screen">dataflow:&gt; http post --target http://localhost:1234 --data "hello"</pre><p>Will result in an uppercased 'HELLO' in the log</p><pre class="screen">2016-06-01 09:54:37.749  INFO 80083 --- [  kafka-binder-] log.sink    : HELLO</pre></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-stream-partitions" href="#spring-cloud-dataflow-stream-partitions"></a>22.&nbsp;Stateful Stream Processing</h2></div></div></div><p>To demonstrate the data partitioning functionality, let&#8217;s deploy the following stream with Kafka as the binder.</p><pre class="screen">dataflow:&gt;stream create --name words --definition "http --server.port=9900 | splitter --expression=payload.split(' ') | log"
Created new stream 'words'

dataflow:&gt;stream deploy words --properties "app.splitter.producer.partitionKeyExpression=payload,app.log.count=2"
Deployed stream 'words'

dataflow:&gt;http post --target http://localhost:9900 --data "How much wood would a woodchuck chuck if a woodchuck could chuck wood"
&gt; POST (text/plain;Charset=UTF-8) http://localhost:9900 How much wood would a woodchuck chuck if a woodchuck could chuck wood
&gt; 202 ACCEPTED</pre><p>You&#8217;ll see the following in the server logs.</p><pre class="screen">2016-06-05 18:33:24.982  INFO 58039 --- [nio-9393-exec-9] o.s.c.d.spi.local.LocalAppDeployer       : deploying app words.log instance 0
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-dataflow-694182453710731989/words-1465176804970/words.log
2016-06-05 18:33:24.988  INFO 58039 --- [nio-9393-exec-9] o.s.c.d.spi.local.LocalAppDeployer       : deploying app words.log instance 1
   Logs will be in /var/folders/c3/ctx7_rns6x30tq7rb76wzqwr0000gp/T/spring-cloud-dataflow-694182453710731989/words-1465176804970/words.log</pre><p>Review the <code class="literal">words.log instance 0</code> logs:</p><pre class="screen">2016-06-05 18:35:47.047  INFO 58638 --- [  kafka-binder-] log.sink                                 : How
2016-06-05 18:35:47.066  INFO 58638 --- [  kafka-binder-] log.sink                                 : chuck
2016-06-05 18:35:47.066  INFO 58638 --- [  kafka-binder-] log.sink                                 : chuck</pre><p>Review the <code class="literal">words.log instance 1</code> logs:</p><pre class="screen">2016-06-05 18:35:47.047  INFO 58639 --- [  kafka-binder-] log.sink                                 : much
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : wood
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : would
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : a
2016-06-05 18:35:47.066  INFO 58639 --- [  kafka-binder-] log.sink                                 : woodchuck
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : if
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : a
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : woodchuck
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : could
2016-06-05 18:35:47.067  INFO 58639 --- [  kafka-binder-] log.sink                                 : wood</pre><p>This shows that payload splits that contain the same word are routed to the same application instance.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-stream-tap-dsl" href="#spring-cloud-dataflow-stream-tap-dsl"></a>23.&nbsp;Tap a Stream</h2></div></div></div><p>Taps can be created at various producer endpoints in a stream. For a stream like this:</p><pre class="screen">stream create --definition "http | step1: transform --expression=payload.toUpperCase() | step2: transform --expression=payload+'!' | log" --name mainstream --deploy</pre><p>taps can be created at the output of <code class="literal">http</code>, <code class="literal">step1</code> and <code class="literal">step2</code>.</p><p>To create a stream that acts as a 'tap' on another stream requires to specify the <code class="literal">source destination name</code> for the tap stream. The syntax for source destination name is:</p><pre class="screen">`:&lt;stream-name&gt;.&lt;label/app-name&gt;`</pre><p>To create a tap at the output of <code class="literal">http</code> in the stream above, the source destination name is <code class="literal">mainstream.http</code>
To create a tap at the output of the first transform app in the stream above, the source destination name is <code class="literal">mainstream.step1</code></p><p>The tap stream DSL looks like this:</p><pre class="screen">stream create --definition ":mainstream.http &gt; counter" --name tap_at_http --deploy

stream create --definition ":mainstream.step1 &gt; jdbc" --name tap_at_step1_transformer --deploy</pre><p>Note the colon (:) prefix before the destination names. The colon allows the parser to recognize this as a destination name instead of an app name.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-stream-app-labels" href="#spring-cloud-dataflow-stream-app-labels"></a>24.&nbsp;Using Labels in a Stream</h2></div></div></div><p>When a stream is comprised of multiple apps with the same name, they must be qualified with labels:</p><pre class="screen">stream create --definition "http | firstLabel: transform --expression=payload.toUpperCase() | secondLabel: transform --expression=payload+'!' | log" --name myStreamWithLabels --deploy</pre></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-stream-explicit-destination-names" href="#spring-cloud-dataflow-stream-explicit-destination-names"></a>25.&nbsp;Explicit Broker Destinations in a Stream</h2></div></div></div><p>One can connect to a specific destination name located in the broker (Rabbit, Kafka etc.,) either at the <code class="literal">source</code> or at the <code class="literal">sink</code> position.</p><p>The following stream has the destination name at the <code class="literal">source</code> position:</p><pre class="screen">stream create --definition ":myDestination &gt; log" --name ingest_from_broker --deploy</pre><p>This stream receives messages from the destination <code class="literal">myDestination</code> located at the broker and connects it to the <code class="literal">log</code> app.</p><p>The following stream has the destination name at the <code class="literal">sink</code> position:</p><pre class="screen">stream create --definition "http &gt; :myDestination" --name ingest_to_broker --deploy</pre><p>This stream sends the messages from the <code class="literal">http</code> app to the destination <code class="literal">myDestination</code> located at the broker.</p><p>From the above streams, notice that the <code class="literal">http</code> and <code class="literal">log</code> apps are interacting with each other via the broker (through the destination <code class="literal">myDestination</code>) rather than having a pipe directly between <code class="literal">http</code> and <code class="literal">log</code> within a single stream.</p><p>It is also possible to connect two different destinations (<code class="literal">source</code> and <code class="literal">sink</code> positions) at the broker in a stream.</p><pre class="screen">stream create --definition ":destination1 &gt; :destination2" --name bridge_destinations --deploy</pre><p>In the above stream, both the destinations (<code class="literal">destination1</code> and <code class="literal">destination2</code>) are located in the broker. The messages flow from the source destination to the sink destination via a <code class="literal">bridge</code> app that connects them.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-stream-advanced" href="#spring-cloud-dataflow-stream-advanced"></a>26.&nbsp;Directed Graphs in a Stream</h2></div></div></div><p>If directed graphs are needed instead of the simple linear streams described above, two features are relevant.</p><p>First, named destinations may be used as a way to combine the output from multiple streams or for multiple consumers to share the output from a single stream.
This can be done using the DSL syntax <code class="literal">http &gt; :mydestination</code> or <code class="literal">:mydestination &gt; log</code>.</p><p>Second, you may need to determine the output channel of a stream based on some information that is only known at runtime.
In that case, a router may be used in the sink position of a stream definition. For more information, refer to the Router Sink starter&#8217;s
<a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-app-starters/tree/master/router/spring-cloud-starter-stream-sink-router" target="_top">README</a>.</p><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="spring-cloud-dataflow-global-properties" href="#spring-cloud-dataflow-global-properties"></a>26.1&nbsp;Common application properties</h2></div></div></div><p>In addition to configuration via DSL, Spring Cloud Data Flow provides a mechanism for setting common properties to all the streaming applications that are launched by it.
This can be done by adding properties prefixed with <code class="literal">spring.cloud.dataflow.applicationProperties.stream</code> when starting the server.
When doing so, the server will pass all the properties, without the prefix, to the instances it launches.</p><p>For example, all the launched applications can be configured to use a specific Kafka broker by launching the configuration server with the following options:</p><pre class="screen">--spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.kafka.binder.brokers=192.168.1.100:9092
--spring.cloud.dataflow.applicationProperties.stream.spring.cloud.stream.kafka.binder.zkNodes=192.168.1.100:2181</pre><p>This will cause the properties <code class="literal">spring.cloud.stream.kafka.binder.brokers</code> and <code class="literal">spring.cloud.stream.kafka.binder.zkNodes</code> to be passed to all the launched applications.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Properties configured using this mechanism have lower precedence than stream deployment properties.
They will be overridden if a property with the same key is specified at stream deployment time (e.g. <code class="literal">app.http.spring.cloud.stream.kafka.binder.brokers</code> will override the common property).</p></td></tr></table></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-stream-multi-binder" href="#spring-cloud-dataflow-stream-multi-binder"></a>27.&nbsp;Stream applications with multiple binder configurations</h2></div></div></div><pre class="literallayout"> In some cases, a stream can have its applications bound to multiple spring cloud stream binders when they are required to connect to different messaging
middleware configurations. In those cases, it is important to make sure the applications are configured appropriately with their binder
configurations. For example, let's consider the following stream:</pre><pre class="screen">http | transform --expression=payload.toUpperCase() | log</pre><p>and in this stream, each application connects to messaging middleware in the following way:</p><pre class="screen">Http source sends events to RabbitMQ (rabbit1)
Transform processor receives events from RabbitMQ (rabbit1) and sends the processed events into Kafka (kafka1)
Log sink receives events from Kafka (kafka1)</pre><p>Here, <code class="literal">rabbit1</code> and <code class="literal">kafka1</code> are the binder names given in the spring cloud stream application properties.
Based on this setup, the applications will have the following binder(s) in their classpath with the appropriate configuration:</p><pre class="screen">Http - Rabbit binder
Transform - Both Kafka and Rabbit binders
Log - Kafka binder</pre><p>The spring-cloud-stream <code class="literal">binder</code> configuration properties can be set within the applications themselves.
If not, they can be passed via <code class="literal">deployment</code> properties when the stream is deployed.</p><p>For example,</p><pre class="screen">dataflow:&gt;stream create --definition "http | transform --expression=payload.toUpperCase() | log" --name mystream</pre><pre class="screen">dataflow:&gt;stream deploy mystream --properties "app.http.spring.cloud.stream.bindings.output.binder=rabbit1,app.transform.spring.cloud.stream.bindings.input.binder=rabbit1,
app.transform.spring.cloud.stream.bindings.output.binder=kafka1,app.log.spring.cloud.stream.bindings.input.binder=kafka1"</pre><p>One can override any of the binder configuration properties by specifying them via deployment properties.</p></div></div><div class="part"><div class="titlepage"><div><div><h1 class="title"><a name="spring-cloud-task" href="#spring-cloud-task"></a>Part&nbsp;V.&nbsp;Tasks</h1></div></div></div><div class="partintro"><div></div><p>This section goes into more detail about how you can work with
<a class="link" href="http://cloud.spring.io/spring-cloud-task/" target="_top">Spring Cloud Tasks</a>. It covers topics such as
creating and running task applications.</p><p>If you&#8217;re just starting out with Spring Cloud Data Flow, you should probably read the
<span class="emphasis"><em><a class="link" href="#">Getting Started</a></em></span> guide before diving into
this section.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-task-intro" href="#spring-cloud-dataflow-task-intro"></a>28.&nbsp;Introducing Spring Cloud Task</h2></div></div></div><p>A task executes a process on demand.  In this case a task is a
<a class="link" href="http://projects.spring.io/spring-boot/" target="_top">Spring Boot</a> application that is annotated with
<code class="literal">@EnableTask</code>.  Hence a user launches a task that performs a certain process, and once
complete the task ends. An example of a task would be a boot application that exports
data from a JDBC repository to an HDFS instance.  Tasks record the start time and the end
time as well as the boot exit code in a relational database. The task implementation is
based on the <a class="link" href="http://cloud.spring.io/spring-cloud-task/" target="_top">Spring Cloud Task</a> project.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="_the_lifecycle_of_a_task" href="#_the_lifecycle_of_a_task"></a>29.&nbsp;The Lifecycle of a task</h2></div></div></div><p>Before we dive deeper into the details of creating Tasks, we need to understand the
typical lifecycle for tasks in the context of Spring Cloud Data Flow:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">Register a Task App</li><li class="listitem">Create a Task Definition</li><li class="listitem">Launch a Task</li><li class="listitem">Task Execution</li><li class="listitem">Destroy a Task Definition</li></ol></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_registering_a_task_application" href="#_registering_a_task_application"></a>29.1&nbsp;Registering a Task Application</h2></div></div></div><p>Register a Task App with the App Registry using the Spring Cloud Data Flow Shell
<code class="literal">app register</code> command. You must provide a unique name and a URI that can be
resolved to the app artifact. For the type, specify "task". Here are a few examples:</p><pre class="screen">dataflow:&gt;app register --name task1 --type task --uri maven://com.example:mytask:1.0.2

dataflow:&gt;app register --name task2 --type task --uri file:///Users/example/mytask-1.0.2.jar

dataflow:&gt;app register --name task3 --type task --uri http://example.com/mytask-1.0.2.jar</pre><p>When providing a URI with the <code class="literal">maven</code> scheme, the format should conform to the following:</p><pre class="screen">maven://&lt;groupId&gt;:&lt;artifactId&gt;[:&lt;extension&gt;[:&lt;classifier&gt;]]:&lt;version&gt;</pre><p>If you would like to register multiple apps at one time, you can store them in a properties file
where the keys are formatted as <code class="literal">&lt;type&gt;.&lt;name&gt;</code> and the values are the URIs. For example, this
would be a valid properties file:</p><pre class="screen">task.foo=file:///tmp/foo.jar
task.bar=file:///tmp/bar.jar</pre><p>Then use the <code class="literal">app import</code> command and provide the location of the properties file via <code class="literal">--uri</code>:</p><pre class="screen">app import --uri file:///tmp/task-apps.properties</pre><p>For convenience, we have the static files with application-URIs (for both maven and docker) available for all the out-of-the-box
Task app-starters. You can point to this file and import all the application-URIs in bulk. Otherwise, as explained in
previous paragraphs, you can register them individually or have your own custom property file with only the required application-URIs
in it. It is recommended, however, to have a "focused" list of desired application-URIs in a custom property file.</p><p>List of available static property files:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Maven based Task Applications: <a class="link" href="http://bit.ly/task-applications-maven" target="_top">bit.ly/task-applications-maven</a></li><li class="listitem">Docker based Task Applications: <a class="link" href="http://bit.ly/task-applications-docker" target="_top">bit.ly/task-applications-docker</a></li></ul></div><p>For example, if you would like to register all out-of-the-box task applications in bulk, you can with
the following command.</p><pre class="screen">dataflow:&gt;app import --uri http://bit.ly/task-applications-maven</pre><p>You can also pass the <code class="literal">--local</code> option (which is TRUE by default) to indicate whether the
properties file location should be resolved within the shell process itself. If the location should
be resolved from the Data Flow Server process, specify <code class="literal">--local false</code>.</p><p>When using either <code class="literal">app register</code> or <code class="literal">app import</code>, if a task app is already registered with
the provided name, it will not be overridden by default. If you would like to override the
pre-existing task app, then include the <code class="literal">--force</code> option.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>In some cases the Resource is resolved on the server side, whereas in others the
URI will be passed to a runtime container instance where it is resolved. Consult
the specific documentation of each Data Flow Server for more detail.</p></td></tr></table></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_creating_a_task" href="#_creating_a_task"></a>29.2&nbsp;Creating a Task</h2></div></div></div><p>Create a Task Definition from a Task App by providing a definition name as well as
properties that apply to the task execution.  Creating a task definition can be done via
the restful API or the shell.  To create a task definition using the shell, use the
<code class="literal">task create</code> command to create the task definition.  For example:</p><pre class="screen">dataflow:&gt;task create mytask --definition "timestamp --format=\"yyyy\""
 Created new task 'mytask'</pre><p>A listing of the current task definitions can be obtained via the restful API or the
shell.  To get the task definition list using the shell, use the <code class="literal">task list</code> command.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_launching_a_task" href="#_launching_a_task"></a>29.3&nbsp;Launching a Task</h2></div></div></div><p>An adhoc task can be launched via the restful API or via the shell.  To launch an ad-hoc
task via the shell use the <code class="literal">task launch</code> command.  For Example:</p><pre class="screen">dataflow:&gt;task launch mytask
 Launched task 'mytask'</pre></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_reviewing_task_executions" href="#_reviewing_task_executions"></a>29.4&nbsp;Reviewing Task Executions</h2></div></div></div><p>Once the task is launched the state of the task is stored in a relational DB.  The state
includes:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Task Name</li><li class="listitem">Start Time</li><li class="listitem">End Time</li><li class="listitem">Exit Code</li><li class="listitem">Exit Message</li><li class="listitem">Last Updated Time</li><li class="listitem">Parameters</li></ul></div><p>A user can check the status of their task executions via the restful API or by the shell.
To display the latest task executions via the shell use the <code class="literal">task execution list</code> command.</p><p>To get a list of task executions for just one task definition, add <code class="literal">--name</code> and
the task definition name, for example <code class="literal">task execution list --name foo</code>.  To retrieve full
details for a task execution use the <code class="literal">task display</code> command with the id of the task execution
, for example <code class="literal">task display --id 549</code>.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_destroying_a_task" href="#_destroying_a_task"></a>29.5&nbsp;Destroying a Task</h2></div></div></div><p>Destroying a Task Definition will remove the definition from the definition repository.
This can be done via the restful API or via the shell.  To destroy a task via the shell
use the <code class="literal">task destroy</code> command. For Example:</p><pre class="screen">dataflow:&gt;task destroy mytask
 Destroyed task 'mytask'</pre><p>The task execution information for previously launched tasks for the definition will
remain in the task repository.</p><p><span class="strong"><strong>Note:</strong></span> This will not stop any currently executing tasks for this definition, this just
removes the definition.</p></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-task-repository" href="#spring-cloud-dataflow-task-repository"></a>30.&nbsp;Task Repository</h2></div></div></div><p>Out of the box Spring Cloud Data Flow offers an embedded instance of the H2 database.
The H2 is good for development purposes but is not recommended for production use.</p><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_configuring_the_task_execution_repository" href="#_configuring_the_task_execution_repository"></a>30.1&nbsp;Configuring the Task Execution Repository</h2></div></div></div><p>To add a driver for the database that will store the Task Execution information, a
dependency for the driver will need to be added to a maven pom file and the
Spring Cloud Data Flow will need to be rebuilt.  Since Spring Cloud Data Flow is comprised of an SPI for
each environment it supports, please review the SPI&#8217;s documentation on which POM should be
updated to add the dependency and how to build.  This document will cover how to setup the
dependency for local SPI.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_local" href="#_local"></a>30.1.1&nbsp;Local</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">Open the spring-cloud-dataflow-server-local/pom.xml in your IDE.</li><li class="listitem">In the <code class="literal">dependencies</code> section add the dependency for the database driver required.  In
the sample below postgresql has been chosen.</li></ol></div><pre class="screen">&lt;dependencies&gt;
...
    &lt;dependency&gt;
        &lt;groupId&gt;org.postgresql&lt;/groupId&gt;
        &lt;artifactId&gt;postgresql&lt;/artifactId&gt;
    &lt;/dependency&gt;
...
&lt;/dependencies&gt;</pre><div class="orderedlist"><ol class="orderedlist" start="3" type="1"><li class="listitem">Save the changed pom.xml</li><li class="listitem">Build the application as described here: <a class="link" href="appendix-building.xml#building" target="_top">Building Spring Cloud Data Flow</a></li></ol></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_datasource" href="#_datasource"></a>30.2&nbsp;Datasource</h2></div></div></div><p>To configure the datasource Add the following properties to the dataflow-server.yml or via
environment variables:</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">spring.datasource.url</li><li class="listitem">spring.datasource.username</li><li class="listitem">spring.datasource.password</li><li class="listitem">spring.datasource.driver-class-name</li></ol></div><p>For example adding postgres would look something like this:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Environment variables:</li></ul></div><pre class="screen">export spring_datasource_url=jdbc:postgresql://localhost:5432/mydb
export spring_datasource_username=myuser
export spring_datasource_password=mypass
export spring_datasource_driver-class-name="org.postgresql.Driver"</pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">dataflow-server.yml</li></ul></div><pre class="screen">spring:
  datasource:
    url: jdbc:postgresql://localhost:5432/mydb
    username: myuser
    password: mypass
    driver-class-name:org.postgresql.Driver</pre></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-task-events" href="#spring-cloud-dataflow-task-events"></a>31.&nbsp;Subscribing to Task/Batch Events</h2></div></div></div><p>You can also tap into various task/batch events when the task is launched.
If the task is enabled to generate task and/or batch events (with the additional dependencies <code class="literal">spring-cloud-task-stream</code> and <code class="literal">spring-cloud-stream-binder-kafka</code>, in the case of Kafka as the binder), those events are published during the task lifecycle.
By default, the destination names for those published events on the broker (rabbit, kafka etc.,) are the event names themselves (for instance: <code class="literal">task-events</code>, <code class="literal">job-execution-events</code> etc.,).</p><pre class="screen">dataflow:&gt;task create myTask --definition &#8220;myBatchJob"
dataflow:&gt;task launch myTask
dataflow:&gt;stream create task-event-subscriber1 --definition ":task-events &gt; log" --deploy</pre><p>You can control the destination name for those events by specifying explicit names when launching the task such as:</p><pre class="screen">dataflow:&gt;task launch myTask --properties "spring.cloud.stream.bindings.task-events.destination=myTaskEvents"
dataflow:&gt;stream create task-event-subscriber2 --definition ":myTaskEvents &gt; log" --deploy</pre><p>The default Task/Batch event and destination names on the broker are enumerated below:</p><div class="table"><a name="d0e2050" href="#d0e2050"></a><p class="title"><b>Table&nbsp;31.1.&nbsp;Task/Batch Event Destinations</b></p><div class="table-contents"><table summary="Task/Batch Event Destinations" style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="col_1"><col class="col_2"></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p><span class="strong"><strong>Event</strong></span></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p><span class="strong"><strong>Destination</strong></span></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Task events</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p><code class="literal">task-events</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Job Execution events</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p><code class="literal">job-execution-events</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Step Execution events</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p><code class="literal">step-execution-events</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Item Read events</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p><code class="literal">item-read-events</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Item Process events</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p><code class="literal">item-process-events</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Item Write events</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p><code class="literal">item-write-events</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; " align="left" valign="top"><p>Skip events</p></td><td style="" align="left" valign="top"><p><code class="literal">skip-events</code></p></td></tr></tbody></table></div></div><br class="table-break"></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="spring-cloud-dataflow-launch-tasks-from-stream" href="#spring-cloud-dataflow-launch-tasks-from-stream"></a>32.&nbsp;Launching Tasks from a Stream</h2></div></div></div><p>You can launch a task from a stream by using one of the available <code class="literal">task-launcher</code> sinks. Currently the only available
<code class="literal">task-launcher</code> sink is the <code class="literal">task-launcher-local</code> which will launch a task on your local machine.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p><code class="literal">task-launcher-local</code> is meant for development purposes only.</p></td></tr></table></div><p>A <code class="literal">task-launcher</code> sink expects a message containing a TaskLaunchRequest object in its payload. From the
TaskLaunchRequest object the task-launcher will obtain the URI of the artifact to be launched as well as the
properties and command line arguments to be used by the task.</p><p>The <code class="literal">task-launcher-local</code> can be added to the available sinks by executing the app register command as follows:</p><pre class="screen">app register --name task-launcher-local --type sink --uri maven://org.springframework.cloud.stream.app:task-launcher-local-sink-kafka:jar:1.0.0.BUILD-SNAPSHOT</pre><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_triggertask" href="#_triggertask"></a>32.1&nbsp;TriggerTask</h2></div></div></div><p>One way to launch a task using the <code class="literal">task-launcher</code> is to use the <code class="literal">triggertask</code> source. The <code class="literal">triggertask</code> source
will emit a message with a TaskLaunchRequest object containing the required launch information. An example of this
would be to launch the timestamp task once every 5 seconds, the stream to implement this would look like:</p><pre class="screen">stream create foo --definition "triggertask --triggertask.uri=maven://org.springframework.cloud.task.app:timestamp-task:jar:1.0.0.BUILD-SNAPSHOT --trigger.fixed-delay=5 | task-launcher-local" --deploy</pre></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_translator" href="#_translator"></a>32.2&nbsp;Translator</h2></div></div></div><p>Another option to start a task using the <code class="literal">task-launcher</code> would be to create a stream using a your own translator
(as a processor) to translate a message payload to a TaskLaunchRequest.  For example:</p><pre class="screen">http --server.port=9000 | my-task-processor | task-launcher-local</pre></div></div></div><div class="part"><div class="titlepage"><div><div><h1 class="title"><a name="dashboard" href="#dashboard"></a>Part&nbsp;VI.&nbsp;Dashboard</h1></div></div></div><div class="partintro"><div></div><p>This section describe how to use the Dashboard of Spring Cloud Data Flow.</p></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="dashboard-introduction" href="#dashboard-introduction"></a>33.&nbsp;Introduction</h2></div></div></div><p>Spring Cloud Data Flow provides a browser-based GUI which currently has 6 sections:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><span class="strong"><strong>Apps</strong></span> Lists all available applications and provides the control to register/unregister them</li><li class="listitem"><span class="strong"><strong>Runtime</strong></span> Provides the Data Flow cluster view with the list of all running applications</li><li class="listitem"><span class="strong"><strong>Streams</strong></span> Deploy/undeploy Stream Definitions</li><li class="listitem"><span class="strong"><strong>Tasks</strong></span> List, create, launch and destroy Task Definitions</li><li class="listitem"><span class="strong"><strong>Jobs</strong></span> Perform Batch Job related functions</li><li class="listitem"><span class="strong"><strong>Analytics</strong></span> Create data visualizations for the various analytics applications</li></ul></div><p>Upon starting Spring Cloud Data Flow, the Dashboard is available at:</p><p><code class="literal">http://&lt;host&gt;:&lt;port&gt;/dashboard</code></p><p>For example: <a class="link" href="http://localhost:9393/dashboard" target="_top">http://localhost:9393/dashboard</a></p><p>If you have enabled https, then it will be located at <code class="literal">https://localhost:9393/dashboard</code>.
If you have enabled security, a login form is available at <code class="literal">http://localhost:9393/dashboard/#/login</code>.</p><p><span class="strong"><strong>Note</strong></span>: The default Dashboard server port is <code class="literal">9393</code></p><div class="figure"><a name="d0e2244" href="#d0e2244"></a><p class="title"><b>Figure&nbsp;33.1.&nbsp;The Spring Cloud Data Flow Dashboard</b></p><div class="figure-contents"><div class="mediaobject"><img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.0.1.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-dashboard-about.png" alt="The Spring Cloud Data Flow Dashboard"></div></div></div><br class="figure-break"></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="dashboard-apps" href="#dashboard-apps"></a>34.&nbsp;Apps</h2></div></div></div><p>The <span class="emphasis"><em>Apps</em></span> section of the Dashboard lists all the available applications and
provides the control to register/unregister them (if applicable). By clicking on
the magnifying glass, you will get a listing of available definition properties.</p><div class="figure"><a name="d0e2261" href="#d0e2261"></a><p class="title"><b>Figure&nbsp;34.1.&nbsp;List of Available Applications</b></p><div class="figure-contents"><div class="mediaobject"><img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.0.1.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-available-apps-list.png" alt="List of available applications"></div></div></div><br class="figure-break"></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="dashboard-runtime" href="#dashboard-runtime"></a>35.&nbsp;Runtime</h2></div></div></div><p>The <span class="emphasis"><em>Runtime</em></span> section of the Dashboard application shows the Spring Cloud Data Flow
cluster view with the list of all running applications. For each runtime app the
state of the deployment and the number of deployed instances is shown.
A list of the used deployment properties is available by clicking on the
app id.</p><div class="figure"><a name="d0e2278" href="#d0e2278"></a><p class="title"><b>Figure&nbsp;35.1.&nbsp;List of Running Applications</b></p><div class="figure-contents"><div class="mediaobject"><img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.0.1.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-runtime.png" alt="List of running applications"></div></div></div><br class="figure-break"></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="dashboard-streams" href="#dashboard-streams"></a>36.&nbsp;Streams</h2></div></div></div><p>The <span class="emphasis"><em>Streams</em></span> section of the Dashboard provides the <span class="emphasis"><em>Definitions</em></span> tab that provides
a listing of Stream definitions. There you have the option to <span class="strong"><strong>deploy</strong></span> or <span class="strong"><strong>undeploy</strong></span>
those stream definitions. Additionally you can remove the definition by clicking on <span class="strong"><strong>destroy</strong></span>.</p><div class="figure"><a name="d0e2307" href="#d0e2307"></a><p class="title"><b>Figure&nbsp;36.1.&nbsp;List of Stream Definitions</b></p><div class="figure-contents"><div class="mediaobject"><img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.0.1.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-streams-list-definitions.png" alt="List of Stream Definitions"></div></div></div><br class="figure-break"></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="dashboard-flo-streams-designer" href="#dashboard-flo-streams-designer"></a>37.&nbsp;Create Stream</h2></div></div></div><p>The <span class="emphasis"><em>Create Stream</em></span> section of the Dashboard includes the <a class="link" href="https://github.com/spring-projects/spring-flo" target="_top">Spring Flo</a> designer tab that provides the canvas application, offering a interactive graphical interface for creating data pipelines.</p><p>In this tab, you can:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Create, manage, and visualize stream pipelines using DSL, a graphical canvas, or both</li><li class="listitem">Write pipelines via DSL with content-assist and auto-complete</li><li class="listitem">Use auto-adjustment and grid-layout capabilities in the GUI for simpler and interactive organization of pipelines</li></ul></div><p>Watch this <a class="link" href="https://www.youtube.com/watch?v=78CgV46OstI" target="_top">screencast</a> that highlights some of the "Flo for Spring Cloud Data Flow" capabilities. Spring Flo <a class="link" href="https://github.com/spring-projects/spring-flo/wiki" target="_top">wiki</a> includes more detailed content on core Flo capabilities.</p><div class="figure"><a name="d0e2347" href="#d0e2347"></a><p class="title"><b>Figure&nbsp;37.1.&nbsp;Flo for Spring Cloud Data Flow</b></p><div class="figure-contents"><div class="mediaobject"><img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.0.1.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-flo-create-stream.png" alt="Flo for Spring Cloud Data Flo"></div></div></div><br class="figure-break"></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="dashboard-tasks" href="#dashboard-tasks"></a>38.&nbsp;Tasks</h2></div></div></div><p>The <span class="emphasis"><em>Tasks</em></span> section of the Dashboard currently has three tabs:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Apps</li><li class="listitem">Definitions</li><li class="listitem">Executions</li></ul></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="dashboard-tasks-apps" href="#dashboard-tasks-apps"></a>38.1&nbsp;Apps</h2></div></div></div><p><span class="emphasis"><em>Apps</em></span> encapsulate a unit of work into a reusable component. Within the Data Flow
runtime environment <span class="emphasis"><em>Apps</em></span> allow users to create definitions for <span class="emphasis"><em>Streams</em></span> as
well as <span class="emphasis"><em>Tasks</em></span>. Consequently, the <span class="emphasis"><em>Apps</em></span> tab within the <span class="emphasis"><em>Tasks</em></span> section
allows users to create <span class="emphasis"><em>Task</em></span> definitions.</p><p><span class="strong"><strong>Note</strong></span>: You will also use this tab to create Batch Jobs.</p><div class="figure"><a name="d0e2403" href="#d0e2403"></a><p class="title"><b>Figure&nbsp;38.1.&nbsp;List of Task Apps</b></p><div class="figure-contents"><div class="mediaobject"><img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.0.1.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-task-apps-list.png" alt="List of Task Apps"></div></div></div><br class="figure-break"><p>On this screen you can perform the following actions:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">View details such as the task app options.</li><li class="listitem">Create a Task Definition from the respective App.</li></ul></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_create_a_task_definition_from_a_selected_task_app" href="#_create_a_task_definition_from_a_selected_task_app"></a>38.1.1&nbsp;Create a Task Definition from a selected Task App</h3></div></div></div><p>On this screen you can create a new Task Definition. As a minimum you must provide a
name for the new definition. You will also have the option
to specify various properties that are used during the deployment of the app.</p><p><span class="strong"><strong>Note</strong></span>: Each parameter is only included if the <span class="emphasis"><em>Include</em></span> checkbox is selected.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_view_task_app_details" href="#_view_task_app_details"></a>38.1.2&nbsp;View Task App Details</h3></div></div></div><p>On this page you can view the details of a selected task app,
including the list of available options (properties) for that app.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="dashboard-task-definition" href="#dashboard-task-definition"></a>38.2&nbsp;Definitions</h2></div></div></div><p>This page lists the Data Flow Task definitions and provides actions to <span class="strong"><strong>launch</strong></span>
or <span class="strong"><strong>destroy</strong></span> those tasks.</p><div class="figure"><a name="d0e2449" href="#d0e2449"></a><p class="title"><b>Figure&nbsp;38.2.&nbsp;List of Task Definitions</b></p><div class="figure-contents"><div class="mediaobject"><img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.0.1.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-task-definitions-list.png" alt="List of Task Definitions"></div></div></div><br class="figure-break"><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_launching_tasks" href="#_launching_tasks"></a>38.2.1&nbsp;Launching Tasks</h3></div></div></div><p>Once the task definition is created, they can be launched through the Dashboard
as well. Navigate to the <span class="strong"><strong>Definitions</strong></span> tab. Select the Task you want to launch by
pressing <code class="literal">Launch</code>.</p><p>On the following screen, you can define one or more Task parameters by entering:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Parameter Key</li><li class="listitem">Parameter Value</li></ul></div><p>Task parameters are not typed.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="dashboard-tasks-executions" href="#dashboard-tasks-executions"></a>38.3&nbsp;Executions</h2></div></div></div><div class="figure"><a name="d0e2483" href="#d0e2483"></a><p class="title"><b>Figure&nbsp;38.3.&nbsp;List of Task Executions</b></p><div class="figure-contents"><div class="mediaobject"><img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.0.1.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-task-executions-list.png" alt="List of Task Executions"></div></div></div><br class="figure-break"></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="dashboard-jobs" href="#dashboard-jobs"></a>39.&nbsp;Jobs</h2></div></div></div><p>The <span class="emphasis"><em>Jobs</em></span> section of the Dashboard allows you to inspect <span class="strong"><strong>Batch Jobs</strong></span>. The main
section of the screen provides a list of Job Executions. <span class="strong"><strong>Batch Jobs</strong></span>
are <span class="strong"><strong>Tasks</strong></span> that were executing one or more <span class="strong"><strong>Batch Job</strong></span>. As such each
Job Execution has a back reference to the <span class="strong"><strong>Task Execution Id</strong></span> (Task Id).</p><p>In case of a failed job, you can also restart the task. When dealing with long-running
Batch Jobs, you can also request to stop it.</p><div class="figure"><a name="d0e2517" href="#d0e2517"></a><p class="title"><b>Figure&nbsp;39.1.&nbsp;List of Job Executions</b></p><div class="figure-contents"><div class="mediaobject"><img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.0.1.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-job-executions-list.png" alt="List of Job Executions"></div></div></div><br class="figure-break"><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="dashboard-job-executions-list" href="#dashboard-job-executions-list"></a>39.1&nbsp;List job executions</h2></div></div></div><p>This page lists the Batch Job Executions and provides the option to <span class="strong"><strong>restart</strong></span> or <span class="strong"><strong>stop</strong></span> a specific job execution, provided the operation is available.
Furthermore, you have the option to view the Job execution details.</p><p>The list of Job Executions also shows the state of the underlying Job Definition.
Thus, if the underlying definition has been deleted, <span class="emphasis"><em>deleted</em></span> will be shown.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="dashboard-job-executions-details" href="#dashboard-job-executions-details"></a>39.1.1&nbsp;Job execution details</h3></div></div></div><div class="figure"><a name="d0e2545" href="#d0e2545"></a><p class="title"><b>Figure&nbsp;39.2.&nbsp;Job Execution Details</b></p><div class="figure-contents"><div class="mediaobject"><img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.0.1.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-jobs-job-execution-details.png" alt="Job Execution Details"></div></div></div><br class="figure-break"><p>The Job Execution Details screen also contains a list of the executed steps. You can
further drill into the <span class="emphasis"><em>Step Execution Details</em></span> by clicking onto the magnifying glass.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="dashboard-job-executions-steps" href="#dashboard-job-executions-steps"></a>39.1.2&nbsp;Step execution details</h3></div></div></div><p>On the top of the page, you will see progress indicator the respective step, with
the option to refresh the indicator. Furthermore, a link is provided to view the
<span class="emphasis"><em>step execution history</em></span>.</p><p>The Step Execution details screen provides a complete list of all Step Execution
Context key/value pairs.</p><div class="important" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="images/important.png"></td><th align="left">Important</th></tr><tr><td align="left" valign="top"><p>In case of exceptions, the <span class="emphasis"><em>Exit Description</em></span> field will contain
additional error information. Please be aware, though, that this field can only
have a maximum of <span class="strong"><strong>2500 characters</strong></span>. Therefore, in case of long exception
stacktraces, trimming of error messages may occur. In that case, please refer to
the server log files for further details.</p></td></tr></table></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="dashboard-job-executions-steps-progress" href="#dashboard-job-executions-steps-progress"></a>39.1.3&nbsp;Step Execution Progress</h3></div></div></div><p>On this screen, you can see a progress bar indicator in regards to the execution
of the current step. Under the <span class="strong"><strong>Step Execution History</strong></span>, you can also view various
metrics associated with the selected step such as <span class="strong"><strong>duration</strong></span>, <span class="strong"><strong>read counts</strong></span>, <span class="strong"><strong>write
counts</strong></span> etc.</p><div class="figure"><a name="d0e2595" href="#d0e2595"></a><p class="title"><b>Figure&nbsp;39.3.&nbsp;Step Execution History</b></p><div class="figure-contents"><div class="mediaobject"><img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-dataflow/v1.0.1.RELEASE/spring-cloud-dataflow-docs/src/main/asciidoc/images/dataflow-step-execution-history.png" alt="Step Execution History"></div></div></div><br class="figure-break"></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a name="dashboard-analytics" href="#dashboard-analytics"></a>40.&nbsp;Analytics</h2></div></div></div><p>The <span class="emphasis"><em>Analytics</em></span> section of the Dashboard provided data visualization capabilities
for the various analytics applications available in <span class="emphasis"><em>Spring Cloud Data Flow</em></span>:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Counters</li><li class="listitem">Field-Value Counters</li></ul></div><p>For example, if you have created the <code class="literal">springtweets</code> stream and the corresponding
counter in the <a class="link" href="#">Counter chapter</a>, you can now easily create the corresponding
graph from within the <span class="strong"><strong>Dashboard</strong></span> tab:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">Under <code class="literal">Metric Type</code>, select <code class="literal">Counters</code> from the select box</li><li class="listitem">Under <code class="literal">Stream</code>, select <code class="literal">tweetcount</code></li><li class="listitem">Under <code class="literal">Visualization</code>, select the desired chart option, <code class="literal">Bar Chart</code></li></ol></div><p>Using the icons to the right, you can add additional charts to the Dashboard,
re-arange the order of created dashboards or remove data visualizations.</p></div></div><div class="part"><div class="titlepage"><div><div><h1 class="title"><a name="appendix" href="#appendix"></a>Part&nbsp;VII.&nbsp;Appendices</h1></div></div></div><div class="appendix"><div class="titlepage"><div><div><h2 class="title"><a name="migration-guide" href="#migration-guide"></a>Appendix&nbsp;A.&nbsp;Migrating from Spring XD to Spring Cloud Data Flow</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_terminology_changes" href="#_terminology_changes"></a>A.1&nbsp;Terminology Changes</h2></div></div></div><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; " width="100%"><colgroup><col class="col_1"><col class="col_2"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top">Old</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="top">New</th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>XD-Admin</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Server (<span class="emphasis"><em>implementations</em></span>: local, cloud foundry, apache yarn, kubernetes, and apache mesos)</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>XD-Container</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>N/A</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Modules</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Applications</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Admin UI</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Dashboard</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Message Bus</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Binders</p></td></tr><tr><td style="border-right: 0.5pt solid ; " align="left" valign="top"><p>Batch / Job</p></td><td style="" align="left" valign="top"><p>Task</p></td></tr></tbody></table></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_modules_to_applications" href="#_modules_to_applications"></a>A.2&nbsp;Modules to Applications</h2></div></div></div><p>If you have custom Spring XD modules, you&#8217;d have to refactor them to use Spring Cloud
Stream and Spring Cloud Task annotations, with updated dependencies and built as normal
Spring Boot "applications".</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_custom_applications" href="#_custom_applications"></a>A.2.1&nbsp;Custom Applications</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Spring XD&#8217;s stream and batch modules are refactored into <a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-app-starters" target="_top">Spring Cloud Stream</a> and <a class="link" href="https://github.com/spring-cloud/spring-cloud-task-app-starters" target="_top">Spring
Cloud Task</a> application-starters, respectively. These applications can be used as the reference while refactoring Spring XD modules</li><li class="listitem">There are also some samples for <a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-samples" target="_top">Stream</a> and <a class="link" href="https://github.com/spring-cloud/spring-cloud-task/tree/master/spring-cloud-task-samples" target="_top">Task</a> applications for reference</li><li class="listitem">If you&#8217;d like to create a brand new custom application, use the getting started guide for <a class="link" href="https://github.com/spring-cloud/spring-cloud-stream/blob/master/spring-cloud-stream-docs/src/main/asciidoc/spring-cloud-stream-overview.adoc#getting-started" target="_top">Stream</a> and <a class="link" href="https://github.com/spring-cloud/spring-cloud-task/blob/master/spring-cloud-task-docs/src/main/asciidoc/getting-started.adoc#developing-your-first-spring-cloud-task-application" target="_top">Task</a> applications and as well as  review the development <a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-app-starters/blob/master/spring-cloud-stream-app-starters-docs/src/main/asciidoc/overview.adoc#creating-your-own-applications" target="_top">guide</a></li><li class="listitem">Alternatively, if you&#8217;d like to patch any of the out-of-the-box stream applications, you can
follow the procedure <a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-app-starters/blob/master/spring-cloud-stream-app-starters-docs/src/main/asciidoc/overview.adoc#patching-pre-built-applications" target="_top">here</a></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_application_registration" href="#_application_registration"></a>A.2.2&nbsp;Application Registration</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Custom Stream/Task application requires being installed to a maven repository for Local, YARN, and
CF implementations or as docker images, when deploying to Kubernetes and Mesos. Other than maven and
docker resolution, you can also resolve application artifacts from <code class="literal">http</code>, <code class="literal">file</code>, or as <code class="literal">hdfs</code>
coordinates</li><li class="listitem">Unlike Spring XD, you do not have to upload the application bits while registering custom applications anymore; instead, you&#8217;re expected to <a class="link" href="http://docs.spring.io/spring-cloud-dataflow/docs/current/reference/html/_dsl_syntax.html#_register_a_stream_app" target="_top">register</a> the application coordinates that are hosted in the maven repository or by other means as discussed in the previous bullet</li><li class="listitem">By default, none of the out-of-the-box applications are preloaded already. It is intentionally designed to
provide the flexibility to register app(s), as you find appropriate for the given use-case requirement</li><li class="listitem">Depending on the binder choice, you can manually add the appropriate binder dependency to build
applications specific to that binder-type. Alternatively, you can follow the Spring Initialzr <a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-app-starters/blob/master/spring-cloud-stream-app-starters-docs/src/main/asciidoc/overview.adoc#using-the-starters-to-create-custom-components" target="_top">procedure</a>
to create an application with binder embedded in it</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_application_properties_2" href="#_application_properties_2"></a>A.2.3&nbsp;Application Properties</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p class="simpara">counter-sink:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem">The peripheral <code class="literal">redis</code> is not required in Spring Cloud Data Flow. If you intend to use the <code class="literal">counter-sink</code>, then <code class="literal">redis</code> becomes required, and you&#8217;re expected to have your own running <code class="literal">redis</code> cluster</li></ul></div></li><li class="listitem"><p class="simpara">field-value-counter-sink:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem">The peripheral <code class="literal">redis</code> is not required in Spring Cloud Data Flow. If you intend to use the <code class="literal">field-value-counter-sink</code>, then <code class="literal">redis</code> becomes required, and you&#8217;re expected to have your own running <code class="literal">redis</code> cluster</li></ul></div></li><li class="listitem"><p class="simpara">aggregate-counter-sink:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem">The peripheral <code class="literal">redis</code> is not required in Spring Cloud Data Flow. If you intend to use the <code class="literal">aggregate-counter-sink</code>, then <code class="literal">redis</code> becomes required, and you&#8217;re expected to have your own running <code class="literal">redis</code> cluster</li></ul></div></li></ul></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_message_bus_to_binders" href="#_message_bus_to_binders"></a>A.3&nbsp;Message Bus to Binders</h2></div></div></div><p>Terminology wise, in Spring Cloud Data Flow, the message bus implementation is commonly referred to
as binders.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_message_bus" href="#_message_bus"></a>A.3.1&nbsp;Message Bus</h3></div></div></div><p>Similar to Spring XD, there&#8217;s an abstraction available to extend the binder interface. By default,
we take the opinionated view of <a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-binder-kafka" target="_top">Apache Kafka</a> and <a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-binder-rabbit" target="_top">RabbitMQ</a> as the
production-ready binders and are available as GA releases. We also have an experimental version of the <a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-binder-gemfire" target="_top">Gemfire</a> binder.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_binders" href="#_binders"></a>A.3.2&nbsp;Binders</h3></div></div></div><p>Selecting a binder is as simple as providing the right binder dependency in the classpath. If you&#8217;re
to choose Kafka as the binder, you&#8217;d register stream applications that are pre-built with Kafka binder
in it. If you were to create a custom application with Kafka binder, you&#8217;d add the following
dependency in the classpath.</p><pre class="programlisting"><span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;dependency&gt;</span>
    <span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;groupId&gt;</span>org.springframework.cloud<span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;/groupId&gt;</span>
    <span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;artifactId&gt;</span>spring-cloud-stream-binder-kafka<span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;/artifactId&gt;</span>
    <span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;version&gt;</span>1.0.2.RELEASE<span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;/version&gt;</span>
<span xmlns:d="http://docbook.org/ns/docbook" class="hl-tag">&lt;/dependency&gt;</span></pre><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Spring Cloud Stream supports <a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-binder-kafka" target="_top">Apache Kafka</a>, <a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-binder-rabbit" target="_top">RabbitMQ</a> and an experimental
<a class="link" href="https://github.com/spring-cloud/spring-cloud-stream-binder-gemfire" target="_top">Gemfire</a> binder implementation. All binder implementations are maintained and managed in their individual repositories</li><li class="listitem">Every Stream/Task application can be built with a binder implementation of your choice.
All the out-of-the-box applications are pre-built for both Kafka and Rabbit and they&#8217;re
readily available for use as maven artifacts [<a class="link" href="http://repo.spring.io/libs-milestone/org/springframework/cloud/stream/app/" target="_top">stream</a> / <a class="link" href="http://repo.spring.io/libs-milestone/org/springframework/cloud/task/app/" target="_top">task</a>] or docker images [<a class="link" href="https://hub.docker.com/r/springcloudstream/" target="_top">stream</a> / <a class="link" href="https://hub.docker.com/r/springcloudtask/" target="_top">task</a>]
Changing the binder requires selecting the right binder <a class="link" href="https://github.com/spring-cloud/spring-cloud-stream/blob/master/spring-cloud-stream-docs%2Fsrc%2Fmain%2Fasciidoc%2Fspring-cloud-stream-overview.adoc#binder-selection" target="_top">dependency</a>. Alternatively, you can download the pre-built application from this version of <a class="link" href="http://start-scs.cfapps.io/" target="_top">Spring Initializr</a> with the desired &#8220;binder-starter&#8221; dependency</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_named_channels" href="#_named_channels"></a>A.3.3&nbsp;Named Channels</h3></div></div></div><p>Fundamentally, all the messaging channels are backed by pub/sub semantics. Unlike Spring XD, the
messaging channels are backed only by <code class="literal">topics</code> or <code class="literal">topic-exchange</code> and there&#8217;s no representation of
<code class="literal">queues</code> in the new architecture.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><code class="literal">${xd.module.index}</code> is not supported anymore; instead, you can directly interact with named
destinations</li><li class="listitem"><p class="simpara"><code class="literal">stream.index</code> changes to <code class="literal">:&lt;stream-name&gt;.&lt;label/app-name&gt;</code></p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><span class="emphasis"><em>for instance:</em></span> <code class="literal">ticktock.0</code> changes to <code class="literal">:ticktock.time</code></li></ul></div></li><li class="listitem"><p class="simpara">&#8220;topic/queue&#8221; prefixes are not required to interact with named-channels</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><span class="emphasis"><em>for instance:</em></span> <code class="literal">topic:foo</code> changes to <code class="literal">:foo</code></li><li class="listitem"><span class="emphasis"><em>for instance:</em></span> <code class="literal">stream create stream1 --definition ":foo &gt; log"</code></li></ul></div></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_directed_graphs" href="#_directed_graphs"></a>A.3.4&nbsp;Directed Graphs</h3></div></div></div><p>If you&#8217;re building non-linear streams, you could take advantage of named destinations to build
directed graphs.</p><p><span class="emphasis"><em>for instance, in Spring XD:</em></span></p><pre class="programlisting">stream create f --definition "queue:foo &gt; transform --expression=payload+'-foo' | log" --deploy
stream create b --definition "queue:bar &gt; transform --expression=payload+'-bar' | log" --deploy
stream create r --definition "http | router --expression=payload.contains('a')?'queue:foo':'queue:bar'" --deploy</pre><p><span class="emphasis"><em>for instance, in Spring Cloud Data Flow:</em></span></p><pre class="programlisting">stream create f --definition ":foo &gt; transform --expression=payload+'-foo' | log" --deploy
stream create b --definition ":bar &gt; transform --expression=payload+'-bar' | log" --deploy
stream create r --definition "http | router --expression=payload.contains('a')?':foo':':bar'" --deploy</pre></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_batch_to_tasks" href="#_batch_to_tasks"></a>A.4&nbsp;Batch to Tasks</h2></div></div></div><p>A Task by definition, is any application that does not run forever, including Spring Batch jobs, and they
end/stop at some point. Task applications can be majorly used for on-demand use-cases such as database migration, machine learning, scheduled operations etc. Using <a class="link" href="http://cloud.spring.io/spring-cloud-task/" target="_top">Spring Cloud Task</a>, users can build Spring Batch jobs as microservice applications.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Spring Batch <a class="link" href="http://docs.spring.io/spring-xd/docs/current-SNAPSHOT/reference/html/#jobs" target="_top">jobs</a>
from Spring XD are being refactored to Spring Boot applications a.k.a link: Spring Cloud Task
<a class="link" href="https://github.com/spring-cloud/spring-cloud-task-app-starters" target="_top">applications</a></li><li class="listitem">Unlike Spring XD, these &#8220;Tasks&#8221; don&#8217;t require explicit deployment; instead, a task is ready to be
launched directly once the definition is declared</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_shell_dsl_commands" href="#_shell_dsl_commands"></a>A.5&nbsp;Shell/DSL Commands</h2></div></div></div><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; " width="100%"><colgroup><col class="col_1"><col class="col_2"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top">Old Command</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="top">New Command</th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>module upload</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>app register / app import</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>module list</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>app list</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>module info</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>app info</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>admin config server</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>dataflow config server</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>job create</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>task create</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>job launch</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>task launch</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>job list</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>task list</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>job status</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>task status</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>job display</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>task display</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>job destroy</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>task destroy</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>job execution list</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>task execution list</p></td></tr><tr><td style="border-right: 0.5pt solid ; " align="left" valign="top"><p>runtime modules</p></td><td style="" align="left" valign="top"><p>runtime apps</p></td></tr></tbody></table></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_rest_api" href="#_rest_api"></a>A.6&nbsp;REST-API</h2></div></div></div><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; " width="70%"><colgroup><col class="col_1"><col class="col_2"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top">Old API</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="top">New API</th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>/modules</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>/apps</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>/runtime/modules</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>/runtime/apps</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>/runtime/modules/(moduleId}</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>/runtime/apps/{appId}</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>/jobs/definitions</p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>/task/definitions</p></td></tr><tr><td style="border-right: 0.5pt solid ; " align="left" valign="top"><p>/jobs/deployments</p></td><td style="" align="left" valign="top"><p>/task/deployments</p></td></tr></tbody></table></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_ui_flo" href="#_ui_flo"></a>A.7&nbsp;UI / Flo</h2></div></div></div><p>The Admin-UI is now renamed as Dashboard. The URI for accessing the Dashboard is changed from
<a class="link" href="http://localhost:9393/admin-ui" target="_top">localhost:9393/admin-ui</a> to <a class="link" href="http://localhost:9393/dashboard" target="_top">localhost:9393/dashboard</a></p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><span class="emphasis"><em>(New)</em></span> Apps: Lists all the registered applications that are available for use. This view includes informational details such as the URI and the properties supported by each application. You can also register/unregister applications from this view</li><li class="listitem">Runtime: Container changes to Runtime. The notion of <code class="literal">xd-container</code> is gone, replaced by out-of-the-box applications running as autonomous Spring Boot applications. The Runtime tab displays the applications
running in the runtime platforms (<span class="emphasis"><em>implementations:</em></span> cloud foundry, apache yarn, apache mesos, or
kubernetes). You can click on each application to review relevant details about the application such
as where it is running with, and what resources etc.</li><li class="listitem"><a class="link" href="https://github.com/spring-projects/spring-flo" target="_top">Spring Flo</a> is now an OSS product. Flo for
Spring Cloud Data Flow&#8217;s &#8220;Create Stream&#8221;, the designer-tab comes pre-built in the Dashboard</li><li class="listitem"><p class="simpara"><span class="emphasis"><em>(New)</em></span> Tasks:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem">The sub-tab &#8220;Modules&#8221; is renamed to &#8220;Apps&#8221;</li><li class="listitem">The sub-tab &#8220;Definitions&#8221; lists all the Task definitions, including Spring Batch jobs that are
orchestrated as Tasks</li><li class="listitem">The sub-tab &#8220;Executions&#8221; lists all the Task execution details similar to Spring XD&#8217;s Job executions</li></ul></div></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_architecture_components" href="#_architecture_components"></a>A.8&nbsp;Architecture Components</h2></div></div></div><p>Spring Cloud Data Flow comes with a significantly simplified architecture. In fact, when compared with
Spring XD, there are less peripherals that are necessary to operationalize Spring Cloud Data Flow.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_zookeeper" href="#_zookeeper"></a>A.8.1&nbsp;ZooKeeper</h3></div></div></div><p>ZooKeeper is not used in the new architecture.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_rdbms" href="#_rdbms"></a>A.8.2&nbsp;RDBMS</h3></div></div></div><p>Spring Cloud Data Flow uses an RDBMS instead of Redis for stream/task definitions, application
registration, and for job repositories.The default configuration uses an embedded H2 instance, but Oracle, SqlServer, MySQL/MariaDB, PostgreSQL, H2, and HSQLDB databases are supported. To use Oracle and
SqlServer you will need to create your own Data Flow Server using <a class="link" href="https://start.spring.io/" target="_top">Spring Initializr</a> and add the appropriate JDBC driver dependency.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_redis" href="#_redis"></a>A.8.3&nbsp;Redis</h3></div></div></div><p>Running a Redis cluster is only required for analytics functionality. Specifically, when the <code class="literal">counter-sink</code>,
<code class="literal">field-value-counter-sink</code>, or <code class="literal">aggregate-counter-sink</code> applications are used, it is expected to also
have a running instance of Redis cluster.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_cluster_topology" href="#_cluster_topology"></a>A.8.4&nbsp;Cluster Topology</h3></div></div></div><p>Spring XD&#8217;s <code class="literal">xd-admin</code> and <code class="literal">xd-container</code> server components are replaced by stream and task
applications themselves running as autonomous Spring Boot applications. The applications run natively
on various platforms including Cloud Foundry, Apache YARN, Apache Mesos, or Kubernetes. You can develop,
test, deploy, scale +/-, and interact with (Spring Boot) applications individually, and they can
evolve in isolation.</p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_central_configuration" href="#_central_configuration"></a>A.9&nbsp;Central Configuration</h2></div></div></div><p>To support centralized and consistent management of an application&#8217;s configuration properties,
<a class="link" href="https://cloud.spring.io/spring-cloud-config/" target="_top">Spring Cloud Config</a> client libraries have been
included into the Spring Cloud Data Flow server as well as the Spring Cloud Stream applications provided
by the Spring Cloud Stream App Starters. You can also <a class="link" href="http://docs.spring.io/spring-cloud-dataflow/docs/1.0.0.RC1/reference/htmlsingle/#spring-cloud-dataflow-global-properties" target="_top">pass common application properties</a>
to all streams when the Data Flow Server starts.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_distribution" href="#_distribution"></a>A.10&nbsp;Distribution</h2></div></div></div><p>Spring Cloud Data Flow is a Spring Boot application. Depending on the platform of your choice, you
can download the respective release uber-jar and deploy/push it to the runtime platform
(cloud foundry, apache yarn, kubernetes, or apache mesos). For example, if you&#8217;re running Spring
Cloud Data Flow on Cloud Foundry, you&#8217;d download the Cloud Foundry server implementation and do a
<code class="literal">cf push</code> as explained in the <a class="link" href="http://docs.spring.io/spring-cloud-dataflow-server-cloudfoundry/docs/current-SNAPSHOT/reference/htmlsingle/#getting-started" target="_top">reference guide</a>.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_hadoop_distribution_compatibility" href="#_hadoop_distribution_compatibility"></a>A.11&nbsp;Hadoop Distribution Compatibility</h2></div></div></div><p>The <code class="literal">hdfs-sink</code> application builds upon Spring Hadoop 2.4.0 release, so this application is compatible
with following Hadoop distributions.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Cloudera - cdh5</li><li class="listitem">Pivotal Hadoop - phd30</li><li class="listitem">Hortonworks Hadoop - hdp24</li><li class="listitem">Hortonworks Hadoop - hdp23</li><li class="listitem">Vanilla Hadoop - hadoop26</li><li class="listitem">Vanilla Hadoop - 2.7.x (default)</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_yarn_deployment" href="#_yarn_deployment"></a>A.12&nbsp;YARN Deployment</h2></div></div></div><p>Spring Cloud Data Flow can be deployed and used with Apche YARN in two different ways.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Deploy the server <a class="link" href="http://docs.spring.io/spring-cloud-dataflow-server-yarn/docs/current-SNAPSHOT/reference/htmlsingle/#_deploying_on_yarn" target="_top">directly</a> in a YARN cluster</li><li class="listitem">Leverage Apache Ambari <a class="link" href="http://docs.spring.io/spring-cloud-dataflow-server-yarn/docs/current-SNAPSHOT/reference/htmlsingle/#_deploying_on_ambari" target="_top">plugin to provision</a> Spring Cloud Data Flow as
a service</li></ul></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_use_case_comparison" href="#_use_case_comparison"></a>A.13&nbsp;Use Case Comparison</h2></div></div></div><p>Let&#8217;s review some use-cases to compare and contrast the differences between Spring XD and Spring
Cloud Data Flow.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_use_case_1" href="#_use_case_1"></a>A.13.1&nbsp;Use Case #1</h3></div></div></div><p>(<span class="emphasis"><em>It is assumed both XD and SCDF distributions are already downloaded</em></span>)</p><p>Description: Simple <code class="literal">ticktock</code> example using local/singlenode.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; " width="100%"><colgroup><col class="col_1"><col class="col_2"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top">Spring XD</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="top">Spring Cloud Data Flow</th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Start <code class="literal">xd-singlenode</code> server from CLI
</p><p><code class="literal">&#8594; xd-singlenode</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Start a binder of your choice
</p><p>Start <code class="literal">local-server</code> implementation of SCDF from the CLI
</p><p><code class="literal">&#8594; java -jar spring-cloud-dataflow-server-local-1.0.0.BUILD-SNAPSHOT.jar</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Start <code class="literal">xd-shell</code> server from the CLI
</p><p><code class="literal">&#8594; xd-shell</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Start <code class="literal">dataflow-shell</code> server from the CLI
</p><p><code class="literal">&#8594; java -jar spring-cloud-dataflow-shell-1.0.0.BUILD-SNAPSHOT.jar</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Create <code class="literal">ticktock</code> stream
</p><p><code class="literal">xd:&gt;stream create ticktock --definition &#8220;time | log&#8221; --deploy</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Create <code class="literal">ticktock</code> stream
</p><p><code class="literal">dataflow:&gt;stream create ticktock --definition &#8220;time | log&#8221; --deploy</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; " align="left" valign="top"><p>Review <code class="literal">ticktock</code> results in the <code class="literal">xd-singlenode</code> server console</p></td><td style="" align="left" valign="top"><p>Review <code class="literal">ticktock</code> results by tailing the <code class="literal">ticktock.log/stdout_log</code> application logs</p></td></tr></tbody></table></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_use_case_2" href="#_use_case_2"></a>A.13.2&nbsp;Use Case #2</h3></div></div></div><p>(<span class="emphasis"><em>It is assumed both XD and SCDF distributions are already downloaded</em></span>)</p><p>Description: Stream with custom module/application.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; " width="100%"><colgroup><col class="col_1"><col class="col_2"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top">Spring XD</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="top">Spring Cloud Data Flow</th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Start <code class="literal">xd-singlenode</code> server from CLI
</p><p><code class="literal">&#8594; xd-singlenode</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Start a binder of your choice
</p><p>Start <code class="literal">local-server</code> implementation of SCDF from the CLI
</p><p><code class="literal">&#8594; java -jar spring-cloud-dataflow-server-local-1.0.0.BUILD-SNAPSHOT.jar</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Start <code class="literal">xd-shell</code> server from the CLI
</p><p><code class="literal">&#8594; xd-shell</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Start <code class="literal">dataflow-shell</code> server from the CLI
</p><p><code class="literal">&#8594; java -jar spring-cloud-dataflow-shell-1.0.0.BUILD-SNAPSHOT.jar</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Register custom &#8220;processor&#8221; module to transform payload to a desired format
</p><p><code class="literal">xd:&gt;module upload --name toupper --type processor --file &lt;CUSTOM_JAR_FILE_LOCATION&gt;</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Register custom &#8220;processor&#8221; application to transform payload to a desired format
</p><p><code class="literal">dataflow:&gt;app register --name toupper --type processor --uri &lt;MAVEN_URI_COORDINATES&gt;</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Create a stream with custom module
</p><p><code class="literal">xd:&gt;stream create testupper --definition &#8220;http | toupper | log&#8221; --deploy</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Create a stream with custom application
</p><p><code class="literal">dataflow:&gt;stream create testupper --definition &#8220;http | toupper | log&#8221; --deploy</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; " align="left" valign="top"><p>Review results in the <code class="literal">xd-singlenode</code> server console</p></td><td style="" align="left" valign="top"><p>Review results by tailing the <code class="literal">testupper.log/stdout_log</code> application logs</p></td></tr></tbody></table></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_use_case_3" href="#_use_case_3"></a>A.13.3&nbsp;Use Case #3</h3></div></div></div><p>(<span class="emphasis"><em>It is assumed both XD and SCDF distributions are already downloaded</em></span>)</p><p>Description: Simple batch-job.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; " width="100%"><colgroup><col class="col_1"><col class="col_2"></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top">Spring XD</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="top">Spring Cloud Data Flow</th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Start <code class="literal">xd-singlenode</code> server from CLI
</p><p><code class="literal">&#8594; xd-singlenode</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Start <code class="literal">local-server</code> implementation of SCDF from the CLI
</p><p><code class="literal">&#8594; java -jar spring-cloud-dataflow-server-local-1.0.0.BUILD-SNAPSHOT.jar</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Start <code class="literal">xd-shell</code> server from the CLI
</p><p><code class="literal">&#8594; xd-shell</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Start <code class="literal">dataflow-shell</code> server from the CLI
</p><p><code class="literal">&#8594; java -jar spring-cloud-dataflow-shell-1.0.0.BUILD-SNAPSHOT.jar</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Register custom &#8220;batch-job&#8221; module
</p><p><code class="literal">xd:&gt;module upload --name simple-batch --type job --file &lt;CUSTOM_JAR_FILE_LOCATION&gt;</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Register
custom &#8220;batch-job&#8221; as task application
</p><p><code class="literal">dataflow:&gt;app register --name simple-batch --type task --uri &lt;MAVEN_URI_COORDINATES&gt;</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Create a job with custom batch-job module
</p><p><code class="literal">xd:&gt;job create batchtest --definition &#8220;simple-batch&#8221;</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Create a task with custom batch-job application
</p><p><code class="literal">dataflow:&gt;task create batchtest --definition &#8220;simple-batch&#8221;</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Deploy job
</p><p><code class="literal">xd:&gt;job deploy batchtest</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>NA</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Launch job
</p><p><code class="literal">xd:&gt;job launch batchtest</code></p></td><td style="border-bottom: 0.5pt solid ; " align="left" valign="top"><p>Launch task
</p><p><code class="literal">dataflow:&gt;task launch batchtest</code></p></td></tr><tr><td style="border-right: 0.5pt solid ; " align="left" valign="top"><p>Review results in the <code class="literal">xd-singlenode</code> server console as well as Jobs tab in UI
(executions sub-tab should include all step details)</p></td><td style="" align="left" valign="top"><p>Review results by tailing the <code class="literal">batchtest/stdout_log</code> application logs as well as Task tab in UI (executions sub-tab should include all step details)</p></td></tr></tbody></table></div></div></div></div><div class="appendix"><div class="titlepage"><div><div><h2 class="title"><a name="test-cluster" href="#test-cluster"></a>Appendix&nbsp;B.&nbsp;Test Cluster</h2></div></div></div><p>Here are brief setup instructions for setting up a local Vagrant DC/OS Mesos cluster. The DC/OS Dashboard endpoint will be <a class="link" href="http://m1.dcos" target="_top">http://m1.dcos</a>, the Mesos endpoint will be <a class="link" href="http://m1.dcos/mesos" target="_top">http://m1.dcos/mesos</a>, the Marathon endpoint will be <a class="link" href="http://m1.dcos/marathon" target="_top">http://m1.dcos/marathon</a> and the Chronos endpoint will be <a class="link" href="http://m1.dcos/service/chronos" target="_top">http://m1.dcos/service/chronos</a>.</p><p>Detailed installation instructions are here: <a class="link" href="https://github.com/dcos/dcos-vagrant" target="_top">https://github.com/dcos/dcos-vagrant</a></p><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_clone_and_download_files" href="#_clone_and_download_files"></a>B.1&nbsp;Clone and Download files</h2></div></div></div><p>First clone the <code class="literal">dcos-vagrant</code> project:</p><pre class="literallayout">git clone https://github.com/dcos/dcos-vagrant.git</pre><p>Download the DC/OS version you want to use, we used version 1.8.1 from <a class="link" href="https://dcos.io/releases/1.8.1/" target="_top">https://dcos.io/releases/1.8.1/</a>, and place this <code class="literal">dcos_generate_config.sh</code> file in the root directory of the cloned <code class="literal">dcos-project</code>.</p><p>Copy the config example file. From the root directory of the <code class="literal">dcos-vagrant</code> project execute:</p><pre class="literallayout">cp VagrantConfig.yaml.example VagrantConfig.yaml</pre><p>Update the <code class="literal">etc/config-1.8.yaml</code> file. Add a line with</p><p><code class="literal">oauth_enabled: 'false'</code></p><p>to the end of this file and save it. This will disable any security for the cluster.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_start_vagrant_vms_for_the_mesos_cluster" href="#_start_vagrant_vms_for_the_mesos_cluster"></a>B.2&nbsp;Start Vagrant VMs for the Mesos cluster</h2></div></div></div><p>You need to install Vagrant and VirtualBox. We used the DC/OS version 1.8.1 with Vagrant version 1.8.4.</p><p>Install Vagrant Host Manager Plugin if it isn&#8217;t already installed:</p><pre class="literallayout">vagrant plugin install vagrant-hostmanager</pre><p>Before we start the cluster we should define the config file we want to use:</p><pre class="literallayout">export DCOS_CONFIG_PATH=etc/config-1.8.yaml</pre><p>Now we are ready to start the Vagrant VMs. You can select different cluster configurations depending on your needs. We have tested using a single master, three private agents and one bootstrap node using this command:</p><pre class="literallayout">vagrant up m1 a1 a2 a3 boot</pre><p>When you are done with the Mesos cluster you can destroy it using:</p><pre class="literallayout">vagrant destroy -f</pre></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_install_chronos_as_a_service_running_in_marathon" href="#_install_chronos_as_a_service_running_in_marathon"></a>B.3&nbsp;Install Chronos as a Service running in Marathon</h2></div></div></div><p>You can install Chronos by using the following <code class="literal">dcos</code> command. This requires that you install the  <a class="link" href="https://docs.mesosphere.com/1.7/usage/cli/install/" target="_top">DC/OS CLI</a>.</p><pre class="literallayout">dcos package install chronos</pre></div></div><div class="appendix"><div class="titlepage"><div><div><h2 class="title"><a name="building" href="#building"></a>Appendix&nbsp;C.&nbsp;Building</h2></div></div></div><p>To build the source you will need to install JDK 1.7.</p><p>The build uses the Maven wrapper so you don&#8217;t have to install a specific
version of Maven.  To enable the tests for Redis you should run the server
before bulding.  See below for more information on how run Redis.</p><p>The main build command is</p><pre class="screen">$ ./mvnw clean install</pre><p>You can also add '-DskipTests' if you like, to avoid running the tests.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>You can also install Maven (&gt;=3.3.3) yourself and run the <code class="literal">mvn</code> command
in place of <code class="literal">./mvnw</code> in the examples below. If you do that you also
might need to add <code class="literal">-P spring</code> if your local Maven settings do not
contain repository declarations for spring pre-release artifacts.</p></td></tr></table></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Be aware that you might need to increase the amount of memory
available to Maven by setting a <code class="literal">MAVEN_OPTS</code> environment variable with
a value like <code class="literal">-Xmx512m -XX:MaxPermSize=128m</code>. We try to cover this in
the <code class="literal">.mvn</code> configuration, so if you find you have to do it to make a
build succeed, please raise a ticket to get the settings added to
source control.</p></td></tr></table></div><p>The projects that require middleware generally include a
<code class="literal">docker-compose.yml</code>, so consider using
<a class="link" href="http://compose.docker.io/" target="_top">Docker Compose</a> to run the middeware servers
in Docker containers. See the README in the
<a class="link" href="https://github.com/spring-cloud-samples/scripts" target="_top">scripts demo
repository</a> for specific instructions about the common cases of mongo,
rabbit and redis.</p><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_documentation" href="#_documentation"></a>C.1&nbsp;Documentation</h2></div></div></div><p>There is a "full" profile that will generate documentation. You can build just the documentation by executing</p><p><code class="literal">$ ./mvnw clean package -DskipTests -P full -pl spring-cloud-dataflow-server-mesos-docs -am</code></p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_working_with_the_code" href="#_working_with_the_code"></a>C.2&nbsp;Working with the code</h2></div></div></div><p>If you don&#8217;t have an IDE preference we would recommend that you use
<a class="link" href="http://www.springsource.com/developer/sts" target="_top">Spring Tools Suite</a> or
<a class="link" href="http://eclipse.org" target="_top">Eclipse</a> when working with the code. We use the
<a class="link" href="http://eclipse.org/m2e/" target="_top">m2eclipe</a> eclipse plugin for maven support. Other IDEs and tools
should also work without issue.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_importing_into_eclipse_with_m2eclipse" href="#_importing_into_eclipse_with_m2eclipse"></a>C.2.1&nbsp;Importing into eclipse with m2eclipse</h3></div></div></div><p>We recommend the <a class="link" href="http://eclipse.org/m2e/" target="_top">m2eclipe</a> eclipse plugin when working with
eclipse. If you don&#8217;t already have m2eclipse installed it is available from the "eclipse
marketplace".</p><p>Unfortunately m2e does not yet support Maven 3.3, so once the projects
are imported into Eclipse you will also need to tell m2eclipse to use
the <code class="literal">.settings.xml</code> file for the projects.  If you do not do this you
may see many different errors related to the POMs in the
projects.  Open your Eclipse preferences, expand the Maven
preferences, and select User Settings.  In the User Settings field
click Browse and navigate to the Spring Cloud project you imported
selecting the <code class="literal">.settings.xml</code> file in that project.  Click Apply and
then OK to save the preference changes.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Alternatively you can copy the repository settings from <a class="link" href="https://github.com/spring-cloud/spring-cloud-build/blob/master/.settings.xml" target="_top"><code class="literal">.settings.xml</code></a> into your own <code class="literal">~/.m2/settings.xml</code>.</p></td></tr></table></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="_importing_into_eclipse_without_m2eclipse" href="#_importing_into_eclipse_without_m2eclipse"></a>C.2.2&nbsp;Importing into eclipse without m2eclipse</h3></div></div></div><p>If you prefer not to use m2eclipse you can generate eclipse project metadata using the
following command:</p><pre class="screen">$ ./mvnw eclipse:eclipse</pre><p>The generated eclipse projects can be imported by selecting <code class="literal">import existing projects</code>
from the <code class="literal">file</code> menu.</p></div></div></div><div class="appendix"><div class="titlepage"><div><div><h2 class="title"><a name="contributing" href="#contributing"></a>Appendix&nbsp;D.&nbsp;Contributing</h2></div></div></div><p>Spring Cloud is released under the non-restrictive Apache 2.0 license,
and follows a very standard Github development process, using Github
tracker for issues and merging pull requests into master. If you want
to contribute even something trivial please do not hesitate, but
follow the guidelines below.</p><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_sign_the_contributor_license_agreement" href="#_sign_the_contributor_license_agreement"></a>D.1&nbsp;Sign the Contributor License Agreement</h2></div></div></div><p>Before we accept a non-trivial patch or pull request we will need you to sign the
<a class="link" href="https://support.springsource.com/spring_committer_signup" target="_top">contributor&#8217;s agreement</a>.
Signing the contributor&#8217;s agreement does not grant anyone commit rights to the main
repository, but it does mean that we can accept your contributions, and you will get an
author credit if we do.  Active contributors might be asked to join the core team, and
given the ability to merge pull requests.</p></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_code_conventions_and_housekeeping" href="#_code_conventions_and_housekeeping"></a>D.2&nbsp;Code Conventions and Housekeeping</h2></div></div></div><p>None of these is essential for a pull request, but they will all help.  They can also be
added after the original pull request but before a merge.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">Use the Spring Framework code format conventions. If you use Eclipse
you can import formatter settings using the
<code class="literal">eclipse-code-formatter.xml</code> file from the
<a class="link" href="https://github.com/spring-cloud/spring-cloud-build/blob/master/spring-cloud-dependencies-parent/eclipse-code-formatter.xml" target="_top">Spring
Cloud Build</a> project. If using IntelliJ, you can use the
<a class="link" href="http://plugins.jetbrains.com/plugin/6546" target="_top">Eclipse Code Formatter
Plugin</a> to import the same file.</li><li class="listitem">Make sure all new <code class="literal">.java</code> files to have a simple Javadoc class comment with at least an
<code class="literal">@author</code> tag identifying you, and preferably at least a paragraph on what the class is
for.</li><li class="listitem">Add the ASF license header comment to all new <code class="literal">.java</code> files (copy from existing files
in the project)</li><li class="listitem">Add yourself as an <code class="literal">@author</code> to the .java files that you modify substantially (more
than cosmetic changes).</li><li class="listitem">Add some Javadocs and, if you change the namespace, some XSD doc elements.</li><li class="listitem">A few unit tests would help a lot as well&#8201;&#8212;&#8201;someone has to do it.</li><li class="listitem">If no-one else is using your branch, please rebase it against the current master (or
other target branch in the main project).</li><li class="listitem">When writing a commit message please follow <a class="link" href="http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html" target="_top">these conventions</a>,
if you are fixing an existing issue please add <code class="literal">Fixes gh-XXXX</code> at the end of the commit
message (where XXXX is the issue number).</li></ul></div></div></div></div></div></body></html>