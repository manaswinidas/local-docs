<!DOCTYPE html>
<html lang="en-US" >
<head>
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta charset="utf-8">
<meta http-equiv="Content-Type" content="UTF-8" />
<title>Testing and Tuning Models</title>
<meta name="generator" content="DITA Open Toolkit version 1.8.5 (Mode = doc)" />
<meta name="description" content="Testing a model enables you to estimate how accurate the predictions of a model are. You can test Classification models and Regression models, and tune Classification models." />
<meta name="keywords" content="classification, test, Classification models, metrics for Classification Models, Predictive Confidence, average accuracy, overall accuracy, cost, performance measure, performance matrix, Receiver Operating Characteristics, ROC, using, Lift, Profit and ROI, example, use case, Classification Model, test viewer, Performance Matrix, graph, Profit, profit setting, tuning, Classification Models, removing, tune settings, Costs and Benefits, benefit, tab, Tuning tab, tuning steps, regression, testing, Residual Plot, Regression Statistics, Residual" />
<meta name="dcterms.created" content="2018-03-26T22:30:41Z" />
<meta name="robots" content="all" />
<meta name="dcterms.title" content="Data Miner User's Guide" />
<meta name="dcterms.identifier" content="E94763_02" />
<meta name="dcterms.isVersionOf" content="DMRUG" />
<meta name="dcterms.rights" content="Copyright&nbsp;&copy;&nbsp;2015, 2018, Oracle&nbsp;and/or&nbsp;its&nbsp;affiliates.&nbsp;All&nbsp;rights&nbsp;reserved." />
<link rel="Start" href="../index.htm" title="Home" type="text/html" />
<link rel="Copyright" href="../dcommon/html/cpyr.htm" title="Copyright" type="text/html" />

<script type="application/javascript"  src="../dcommon/js/headfoot.js"></script>
<script type="application/javascript"  src="../nav/js/doccd.js" charset="UTF-8"></script>
<link rel="Contents" href="toc.htm" title="Contents" type="text/html" />
<link rel="Index" href="index.htm" title="Index" type="text/html" />
<link rel="Prev" href="text-nodes.htm" title="Previous" type="text/html" />
<link rel="Next" href="data-mininig-algorithms.htm" title="Next" type="text/html" />
<link rel="alternate" href="DMRUG.pdf" title="PDF version" type="application/pdf" />
<link rel="schema.dcterms" href="http://purl.org/dc/terms/" />
<link rel="stylesheet" href="../dcommon/css/fusiondoc.css">
<link rel="stylesheet" type="text/css"  href="../dcommon/css/header.css">
<link rel="stylesheet" type="text/css"  href="../dcommon/css/footer.css">
<link rel="stylesheet" type="text/css"  href="../dcommon/css/fonts.css">
<link rel="stylesheet" href="../dcommon/css/foundation.css">
<link rel="stylesheet" href="../dcommon/css/codemirror.css">
<link rel="stylesheet" type="text/css" title="Default" href="../nav/css/html5.css">
<link rel="stylesheet" href="../dcommon/css/respond-480-tablet.css">
<link rel="stylesheet" href="../dcommon/css/respond-768-laptop.css">
<link rel="stylesheet" href="../dcommon/css/respond-1140-deskop.css">
<script type="application/javascript" src="../dcommon/js/modernizr.js"></script>
<script type="application/javascript" src="../dcommon/js/codemirror.js"></script>
<script type="application/javascript" src="../dcommon/js/jquery.js"></script>
<script type="application/javascript" src="../dcommon/js/foundation.min.js"></script>
<script type="application/javascript" src="../dcommon/js/jqfns.js"></script>
<script type="application/javascript" src="../dcommon/js/ohc-inline-videos.js"></script>
<!-- Add fancyBox -->
<link rel="stylesheet" href="../dcommon/fancybox/jquery.fancybox.css?v=2.1.5" type="text/css" media="screen" />
<script type="text/javascript" src="../dcommon/fancybox/jquery.fancybox.pack.js?v=2.1.5"></script>
<!-- Optionally add helpers - button, thumbnail and/or media -->
<link rel="stylesheet"  href="../dcommon/fancybox/helpers/jquery.fancybox-buttons.css?v=1.0.5"  type="text/css" media="screen" />
<script type="text/javascript" src="../dcommon/fancybox/helpers/jquery.fancybox-buttons.js?v=1.0.5"></script>
<script type="text/javascript" src="../dcommon/fancybox/helpers/jquery.fancybox-media.js?v=1.0.6"></script>
<link rel="stylesheet"  href="../dcommon/fancybox/helpers/jquery.fancybox-thumbs.css?v=1.0.7"  type="text/css" media="screen" />
<script type="text/javascript" src="../dcommon/fancybox/helpers/jquery.fancybox-thumbs.js?v=1.0.7"></script>
<script type="text/javascript" src="target.js"></script>
</head>
<body>
<a href="#BEGIN" class="accessibility-top skipto" tabindex="0">Go to main content</a><header><!--
<div class="zz-skip-header"><a id="top" href="#BEGIN">Go to main content</a>--></header>
<div class="row" id="CONTENT">
<div class="IND large-9 medium-8 columns" dir="ltr">
<a id="BEGIN" name="BEGIN"></a>
<a id="GUID-4B6BA33E-BCD6-4AA1-AF9F-3890E8D22356"></a> <span id="PAGE" style="display:none;">18/20</span> <!-- End Header -->
<a id="DMRUG816"></a><a id="test_tune"></a>
<h1 id="DMRUG-GUID-4B6BA33E-BCD6-4AA1-AF9F-3890E8D22356" class="sect1"><span class="enumeration_chapter">12</span> Testing and Tuning Models</h1>
<div>
<p>Testing a model enables you to estimate how accurate the predictions of a model are. You can test Classification models and Regression models, and tune Classification models.</p>
<p>This section contains the following topics:</p>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-1B8084C5-6857-427B-BE37-599C4363F964">Testing Classification Models</a><br />
Classification models are tested by comparing the predicted values to known target values in a set of test data.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-89B07B20-C62C-4124-BD83-50F020F2E6F3">Tuning Classification Models</a><br />
When you tune a model, you create a derived cost matrix to use for subsequent Test and Apply operations.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-193209C6-6440-4881-9492-65D25DD401EC">Testing Regression Models</a><br />
Regression models are tested by comparing the predicted values to known target values in a set of test data.</li>
</ul>
</div>
<a id="DMRUG817"></a><a id="class_test"></a>
<div class="props_rev_3"><a id="GUID-1B8084C5-6857-427B-BE37-599C4363F964"></a>
<h2 id="DMRUG-GUID-1B8084C5-6857-427B-BE37-599C4363F964" class="sect2">Testing Classification Models</h2>
<div>
<p>Classification models are tested by comparing the predicted values to known target values in a set of test data.</p>
<p>The historical data for a Classification project is typically divided into two data sets:</p>
<ul style="list-style-type: disc;">
<li>
<p>One for building the model</p>
</li>
<li>
<p>One for testing the model</p>
</li>
</ul>
<p>The test data must be compatible with the data used to build the model and must be prepared in the same way that the build data was prepared.</p>
<p>These are the ways to test Classification and Regression models:</p>
<ul style="list-style-type: disc;">
<li>
<p>By splitting the input data into build data and test data. This is the default. The test data is created by randomly splitting the build data into two subsets. 40 percent of the input data is used for test data.</p>
</li>
<li>
<p>By using all the build data as test data.</p>
</li>
<li>
<p>By attaching two Data Source nodes to the build node.</p>
<ul style="list-style-type: disc;">
<li>
<p>The first data source that you connect to the build node is the source of the build data.</p>
</li>
<li>
<p>The second node that you connect is the source of the test data.</p>
</li>
</ul>
</li>
<li>
<p>By deselecting <span class="bold">Perform Test</span> in the <span class="wintitle">Test</span> section of the <span class="wintitle">Properties</span> pane and using a Test node. The <span class="wintitle">Test</span> section define how tests are done. By default, all Classification and Regression models are tested.</p>
</li>
</ul>
<p>Oracle Data Miner provides test metrics for Classification models so that you can evaluate the model.</p>
<p>After testing, you can tune the models.</p>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-E8D2043A-03AE-498F-BE0A-6CED607AB8FF">Test Metrics for Classification Models</a><br />
Test metrics assess how accurately the model predicts the known values.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-372B9138-0A9A-4F1C-8339-046E29F03417">Compare Classification Test Results</a><br />
By using the <span class="bold">Compare Test Result</span> context menu option in a Test node and Classification node, you can compare test results of a Classification model that are tested in a Test node, and for models that are tested after running the Classification node respectively.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-5F4220DD-498D-47CA-98DB-5216D0F241A7">Classification Model Test Viewer</a><br />
The Classification Model Test viewer displays all information related to the Classification Model test results.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-0ED07A23-3DB5-4888-9C6D-7FFB16C8D544">Viewing Test Results</a><br />
You can view results of models that are tested in a Classification node and a Test node.</li>
</ul>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="model-operations.htm#GUID-2C1D2C12-03C5-40E7-85AB-DA46C9CFA858">Test Node</a></li>
<li><a href="testing-and-tuning-models.htm#GUID-89B07B20-C62C-4124-BD83-50F020F2E6F3">Tuning Classification Models</a></li>
</ul>
</div>
</div>
<a id="DMRUG818"></a><a id="class_test_metrics"></a>
<div class="props_rev_3"><a id="GUID-E8D2043A-03AE-498F-BE0A-6CED607AB8FF"></a>
<h3 id="DMRUG-GUID-E8D2043A-03AE-498F-BE0A-6CED607AB8FF" class="sect3">Test Metrics for Classification Models</h3>
<div>
<p>Test metrics assess how accurately the model predicts the known values.</p>
<p>Test settings specify the metrics to be calculated and control the calculation of the metrics. By default, Oracle Data Miner calculates the following metrics for Classification models:</p>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-D2321495-79D8-4D69-8B40-62CC074D0267">Performance</a><br />
The performance measures that are calculated are Predictive Confidence, Average Accuracy, Overall Accuracy, Cost and Cost.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-D464F430-E7B7-412E-B6F5-B928095F3746">Performance Matrix</a><br />
A Performance Matrix displays the number of correct and incorrect predictions made by the model compared with the actual classifications in the test data.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-19BDA41C-5E78-485E-8A2D-88AFDBB5B433">Receiver Operating Characteristics (ROC)</a><br />
Receiver Operating Characteristics (ROC) analysis is a useful method for evaluating Classification models. ROC applies to binary classification only.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-972672DA-2E50-4215-BBB3-1E487490F8CD">Lift</a><br />
Lift measures the degree to which the predictions of a Classification model are better than randomly-generated predictions. Lift applies to binary classification and non-binary classifications.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-2E338EE4-6ED3-49B1-ABBA-EA7BE458DB52">Profit and ROI</a><br />
Profit uses user-supplied values for startup cost, incremental revenue, incremental cost, budget, and population to maximize the profit.</li>
</ul>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-5F4220DD-498D-47CA-98DB-5216D0F241A7">Classification Model Test Viewer</a></li>
<li><a href="data-mininig-algorithms.htm#unique_1725539885">Viewing Models in Model Viewer</a></li>
<li><a href="data-miner-gui.htm#GUID-D5184A12-E8F8-4B4B-A9BF-265882B09E93" title="You can set preferences for Oracle Data Miner in the Preference option in the Tools menu.">Data Miner Preferences</a></li>
</ul>
</div>
</div>
<a id="DMRUG819"></a>
<div class="props_rev_3"><a id="GUID-D2321495-79D8-4D69-8B40-62CC074D0267"></a>
<h4 id="DMRUG-GUID-D2321495-79D8-4D69-8B40-62CC074D0267" class="sect4">Performance</h4>
<div>
<p>The performance measures that are calculated are Predictive Confidence, Average Accuracy, Overall Accuracy, Cost and Cost.</p>
<div class="section"></div>
<!-- class="section" -->
<div class="section">
<p>You can view these values separately, and also view all of them at the same time. To view the performance measures:</p>
</div>
<!-- class="section" -->
<ol>
<li class="stepexpand"><span>Select the measure that you want to display. Alternately, click <span class="bold">All Measures</span> from the <span class="bold">Measures</span> list.</span></li>
<li class="stepexpand"><span>Use the <span class="bold">Sort By</span> lists to sort the measures. You can sort by:</span>
<div>
<ul style="list-style-type: disc;">
<li>
<p>Name (default)</p>
</li>
<li>
<p>Measures</p>
</li>
<li>
<p>Creation date</p>
<p>The sort can be by descending order (default) or ascending order.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-2744EB07-B54D-44C9-9E9B-C8D84465824C">Predictive Confidence</a><br />
Predictive Confidence provides an estimate of how accurate the model is. Predictive Confidence is a number between 0 and 1.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-F6AD27B5-A937-479D-B54C-55FD9CAF0825">Average Accuracy</a><br />
Average Accuracy refers to the percentage of correct predictions made by the model when compared with the actual classifications in the test data.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-BFEAE664-4C80-412B-A2C2-48F89BDB5CB9">Overall Accuracy</a><br />
Overall Accuracy refers to the percentage of correct predictions made by the model when compared with the actual classifications in the test data.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-4EA3DE97-7FBD-43E2-B44D-38D008ECF63A">Cost</a><br />
In a Classification model, it is important to specify the costs involved in making an incorrect decision. By doing so, it can be useful when the costs of different misclassifications vary significantly.</li>
</ul>
</div>
<a id="DMRUG820"></a>
<div class="props_rev_3"><a id="GUID-2744EB07-B54D-44C9-9E9B-C8D84465824C"></a>
<h5 id="DMRUG-GUID-2744EB07-B54D-44C9-9E9B-C8D84465824C" class="sect5">Predictive Confidence</h5>
<div>
<p>Predictive Confidence provides an estimate of how accurate the model is. Predictive Confidence is a number between 0 and 1.</p>
<p>Oracle Data Miner displays Predictive Confidence as a percentage. For example, the Predictive Confidence of 59 means that the Predictive Confidence is 59 percent (0.59).</p>
<p>Predictive Confidence indicates how much better the predictions made by the tested model are than predictions made by a naive model. The Naive Bayes model always predicts the mean for numerical targets and the mode for categorical targets.</p>
<p><a id="d101138e533" class="indexterm-anchor"></a>Predictive Confidence is defined by the following formula:</p>
<pre dir="ltr">
Predictive Confidence = MAX[(1-Error of model/Error of Naive Model),0]X100 
</pre>
<p>Where:</p>
<p>Error of Model is (1 - Average Accuracy/100)</p>
<p>Error of Naive Model is (Number of target classes - 1) / Number of target classes</p>
<ul style="list-style-type: disc;">
<li>
<p>If the Predictive Confidence is <code class="codeph">0</code><code class="codeph">,</code> then it indicates that the predictions of the model are no better than the predictions made by using the naive model.</p>
</li>
<li>
<p>If the Predictive Confidence is <code class="codeph">1,</code> then it indicates that the predictions are perfect.</p>
</li>
<li>
<p>If the Predictive Confidence is <code class="codeph">0.5,</code> then it indicates that the model has reduced the error of a naive model by 50 percent.</p>
</li>
</ul>
</div>
</div>
<a id="DMRUG821"></a>
<div class="props_rev_3"><a id="GUID-F6AD27B5-A937-479D-B54C-55FD9CAF0825"></a>
<h5 id="DMRUG-GUID-F6AD27B5-A937-479D-B54C-55FD9CAF0825" class="sect5">Average Accuracy</h5>
<div>
<p>Average Accuracy refers to the percentage of correct predictions made by the model when compared with the actual classifications in the test data.</p>
<p>The formula to calculate the Average Accuracy is:</p>
<pre dir="ltr">
Average Accuracy = (TP/(TP+FP)+TN/(FN+TN))/Number of classes*100
</pre>
<p>Where:</p>
<ul style="list-style-type: disc;">
<li>
<p>TP is True Positive.</p>
</li>
<li>
<p>TN is True Negative.</p>
</li>
<li>
<p>FP is False Positive.</p>
</li>
<li>
<p>FN is False Negative.</p>
</li>
</ul>
<p>The average per-class accuracy achieved at a specific probability threshold is greater than the accuracy achieved at all other possible thresholds.</p>
</div>
</div>
<a id="DMRUG822"></a>
<div class="props_rev_3"><a id="GUID-BFEAE664-4C80-412B-A2C2-48F89BDB5CB9"></a>
<h5 id="DMRUG-GUID-BFEAE664-4C80-412B-A2C2-48F89BDB5CB9" class="sect5">Overall Accuracy</h5>
<div>
<p>Overall Accuracy refers to the percentage of correct predictions made by the model when compared with the actual classifications in the test data.</p>
<p>The formula to calculate the Overall Accuracy is:</p>
<pre dir="ltr">
Overall Accuracy = (TP+TN)/(TP+FP+FN+TN)*100
</pre>
<p>Where:</p>
<ul style="list-style-type: disc;">
<li>
<p>TP is True Positive.</p>
</li>
<li>
<p>TN is True Negative.</p>
</li>
<li>
<p>FP is False Positive.</p>
</li>
<li>
<p>FN is False Negative.</p>
</li>
</ul>
</div>
</div>
<a id="DMRUG823"></a>
<div class="props_rev_3"><a id="GUID-4EA3DE97-7FBD-43E2-B44D-38D008ECF63A"></a>
<h5 id="DMRUG-GUID-4EA3DE97-7FBD-43E2-B44D-38D008ECF63A" class="sect5">Cost</h5>
<div>
<p>In a Classification model, it is important to specify the costs involved in making an incorrect decision. By doing so, it can be useful when the costs of different misclassifications vary significantly.</p>
<p>For example, suppose the problem is to predict whether a user is likely to respond to a promotional mailing. The target has two categories: YES (the customer responds) and NO (the customer does not respond). Suppose a positive response to the promotion generates $500 and that it costs $5 to do the mailing. Then, the scenarios are:</p>
<ul style="list-style-type: disc;">
<li>
<p>If the model predicts YES, and the actual value is YES, then the cost of misclassification is $0.</p>
</li>
<li>
<p>If the model predicts YES, and the actual value is NO, then the cost of misclassification is $5.</p>
</li>
<li>
<p>If the model predicts NO, and the actual value is YES, then the cost of misclassification is $500.</p>
</li>
<li>
<p>If the model predicts NO, and the actual value is NO, then the cost of misclassification is $0.</p>
</li>
</ul>
<p>Algorithms for Classification model use cost matrix during scoring to propose the least expensive solution. If you do not specify a cost matrix, then all misclassifications are counted as equally important.</p>
<p>If you are building an SVM model, then you must specify costs using model weights instead of a cost matrix.</p>
</div>
</div>
</div>
<a id="DMRUG824"></a><a id="test_performance_matrix"></a>
<div class="props_rev_3"><a id="GUID-D464F430-E7B7-412E-B6F5-B928095F3746"></a>
<h4 id="DMRUG-GUID-D464F430-E7B7-412E-B6F5-B928095F3746" class="sect4">Performance Matrix</h4>
<div>
<p>A Performance Matrix displays the number of correct and incorrect predictions made by the model compared with the actual classifications in the test data.</p>
<p>Performance Matrix is calculated by applying the model to a hold-out sample (the test set, created during the split step in a classification activity) taken from the build data. The values of the target are known. The known values are compared with the values predicted by the model. Performance Matrix does the following:</p>
<ul style="list-style-type: disc;">
<li>
<p>Measures the likelihood of the model to predict incorrect and correct values</p>
</li>
<li>
<p>Indicates the types of errors that the model is likely to make</p>
</li>
</ul>
<p>The columns are predicted values and the rows are actual values. For example, if you are predicting a target with values 0 and 1, then the number in the upper right cell of the matrix indicates the false-positive predictions, that is, predictions of 1 when the actual value is 0.</p>
</div>
</div>
<a id="DMRUG825"></a>
<div class="props_rev_3"><a id="GUID-19BDA41C-5E78-485E-8A2D-88AFDBB5B433"></a>
<h4 id="DMRUG-GUID-19BDA41C-5E78-485E-8A2D-88AFDBB5B433" class="sect4">Receiver Operating Characteristics (ROC)</h4>
<div>
<p>Receiver Operating Characteristics (ROC) analysis is a useful method for evaluating Classification models. ROC applies to binary classification only.</p>
<p>ROC is plotted as a curve. The area under the ROC curve m<a id="d101138e909" class="indexterm-anchor"></a><a id="d101138e911" class="indexterm-anchor"></a>easures the discriminating ability of a binary Classification model. The correct value for the ROC threshold depends on the problem that the model is trying to solve.</p>
<p><a id="d101138e917" class="indexterm-anchor"></a>ROC curves are similar to lift charts in that they provide a means of comparison between individual models and determine thresholds that yield a high proportion of positive results. An ROC curve does the following:</p>
<ul style="list-style-type: disc;">
<li>
<p>Provides a means to compare individual models and determine thresholds that yield a high proportion of positive results.</p>
</li>
<li>
<p>Provides insight into the decision-making ability of the model. For example, you can determine how likely the model is to accurately predict the negative or the positive class.</p>
</li>
<li>
<p>Compares predicted and actual target values in a Classification model.</p>
</li>
</ul>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-E03BF4DB-9BAF-4785-BA3A-D251E1C449B1">How to Use ROC</a><br />
Receiver Operating Characteristics (ROC) supports what-if analysis.</li>
</ul>
</div>
<a id="DMRUG826"></a>
<div class="props_rev_3"><a id="GUID-E03BF4DB-9BAF-4785-BA3A-D251E1C449B1"></a>
<h5 id="DMRUG-GUID-E03BF4DB-9BAF-4785-BA3A-D251E1C449B1" class="sect5">How to Use ROC</h5>
<div>
<p>Receiver Operating Characteristics (ROC) supports what-if analysis.</p>
<div class="section">
<p>You can use ROC to experiment with modified model settings to observe the effect on the Performance Matrix. For example, assume that a business problem requires that the false-negative value be reduced as much as possible within the confines of the requirement that the number of positive predictions be less than or equal to some fixed number. You might offer an incentive to each customer predicted to be high-value, but you are constrained by a budget with a maximum of 170 incentives. On the other hand, the false negatives represent missed opportunities, so you want to avoid such mistakes.</p>
<p>To view the changes in the Performance Matrix:</p>
</div>
<!-- class="section" -->
<ol>
<li><span>Click <span class="bold">Edit Custom Operating Point</span> at the upper right corner. The <span class="wintitle">Specify Custom Threshold</span> dialog box opens.</span></li>
<li><span>In the <span class="wintitle">Specify Custom Threshold</span> dialog box, mention the desired settings, and view the changes in the <span class="bold">Custom Accuracy</span> field.</span></li>
</ol>
<div class="section">
<p>As you change the Performance Matrix, you are changing the probability that result in a positive prediction. Typically, the probability assigned to each case is examined and if the probability is 0.5 or higher, then a positive prediction is made. Changing the cost matrix changes the positive prediction threshold to some value other than 0.5, and the changed value is displayed in the first column of the table beneath the graph.</p>
</div>
<!-- class="section" --></div>
</div>
</div>
<a id="DMRUG827"></a>
<div class="props_rev_3"><a id="GUID-972672DA-2E50-4215-BBB3-1E487490F8CD"></a>
<h4 id="DMRUG-GUID-972672DA-2E50-4215-BBB3-1E487490F8CD" class="sect4">Lift</h4>
<div>
<p>Lift measures the degree to which the predictions of a Classification model are better than randomly-generated predictions. Lift applies to binary classification and non-binary classifications.</p>
<p>Lift measures how rapidly the model finds the actual positive target values. For example, lift enables you to figure how much of the customer database you must contact to get 50 percent of the customers likely to respond to an offer.</p>
<p>The x-axis of the graph is divided into quantiles. To view exact values, place the cursor over the graph. Below the graph, you can select the quantile of interest using <span class="bold">Selected Quantile.</span> The default quantile is quantile <code class="codeph">1.</code></p>
<p>To calculate lift, Oracle Data Mining does the following:</p>
<ul style="list-style-type: disc;">
<li>
<p>Applies the model to test data to gather predicted and actual target values. This is the same data used to calculate the Performance Matrix.</p>
</li>
<li>
<p>Sorts the predicted results by probability, that is, the confidence in a positive prediction.</p>
</li>
<li>
<p>Divides the ranked list into equal parts, quantiles. The default is <code class="codeph">100.</code></p>
</li>
<li>
<p>Counts the actual positive values in each quantile.</p>
</li>
</ul>
<p>You can graph the lift as either Cumulative Lift or as Cumulative Positive Cases (default). To change the graph, select the appropriate value from the <span class="bold">Display</span> list. You can also select a target value in the <span class="bold">Target Value</span> list.</p>
</div>
</div>
<a id="DMRUG828"></a>
<div class="props_rev_3"><a id="GUID-2E338EE4-6ED3-49B1-ABBA-EA7BE458DB52"></a>
<h4 id="DMRUG-GUID-2E338EE4-6ED3-49B1-ABBA-EA7BE458DB52" class="sect4">Profit and ROI</h4>
<div>
<p>Profit uses user-supplied values for startup cost, incremental revenue, incremental cost, budget, and population to maximize the profit.</p>
<p>Oracle Data Miner calculates profit as follows:</p>
<pre dir="ltr">
Profit = -1 * Startup Cost + (Incremental Revenue * Targets Cumulative - Incremental Cost * (Targets Cumulative + Non Targets Cumulative)) * Population / Total Targets
</pre>
<p>Profit can be positive or negative, that is, it can be a loss.</p>
<p>To view the profit predicted by this model, select the <span class="bold">Target Value</span> that you are interested in. You can change the <span class="bold">Selected Population%.</span> The default is 1 percent.</p>
<p>Return on Investment (ROI) is the ratio of money gained or lost (whether realized or unrealized) on an investment relative to the amount of money invested. Oracle Data Mining uses this formula:</p>
<pre dir="ltr">
ROI = ((profit - cost) / cost) * 100

where profit = Incremental Revenue * Targets Cumulative, cost = Incremental Cost * (Targets Cumulative + Non Targets Cumulative)
</pre></div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-CF9DDE14-312E-4A7A-9CE2-D571907927B6">Profit and ROI Example</a><br />
The Profit and ROI example illustrates how profit and ROI are calculated.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-74E2BFD6-C3BD-4C53-9C66-7F948806190F">Profit and ROI Use Case</a><br />
The Profit and ROI Use Case depicts how to interpret results for profit and ROI calculations.</li>
</ul>
</div>
<a id="DMRUG829"></a>
<div class="props_rev_3"><a id="GUID-CF9DDE14-312E-4A7A-9CE2-D571907927B6"></a>
<h5 id="DMRUG-GUID-CF9DDE14-312E-4A7A-9CE2-D571907927B6" class="sect5">Profit and ROI Example</h5>
<div>
<p>The Profit and ROI example illustrates how profit and ROI are calculated.</p>
<div class="section">
<p>To calculate profit:</p>
</div>
<!-- class="section" -->
<ol>
<li class="stepexpand"><span>Profit is calculated for a quantile. In this example, profit and ROI for quantile 20 is calculated.</span></li>
<li class="stepexpand"><span>Find the value of Targets Cumulative for Quantile 20 by looking at the lift chart data. Suppose that this value is <code class="codeph">18.</code></span></li>
<li class="stepexpand"><span>Suppose that the value of Non Targets Cumulative for Quantile 20 is <code class="codeph">2.</code> Find this value by looking at the lift chart.</span></li>
<li class="stepexpand"><span>Calculate Total Targets which is Targets Cumulative at last Quantile plus Non Targets Cumulative at last Quantile. Suppose that this value is <code class="codeph">100.</code></span></li>
<li class="stepexpand"><span>These values are all user provided. You must provide values based on the business problem:</span>
<div>
<ul style="list-style-type: disc;">
<li>
<p>Startup cost = 1000</p>
</li>
<li>
<p>Incremental revenue = 10</p>
</li>
<li>
<p>Incremental cost = 5</p>
</li>
<li>
<p>Budget = 10000</p>
</li>
<li>
<p>Population = 2000</p>
</li>
</ul>
</div>
</li>
<li class="stepexpand"><span>Calculate profit using this formula</span>
<div>
<pre dir="ltr">
Profit = -1 * Startup Cost + (Incremental Revenue * Targets Cumulative - Incremental Cost * (Targets Cumulative + Non Targets Cumulative)) * Population / Total Targets
</pre>
<p>Substituting the values in this example results in</p>
<pre dir="ltr">
Profit = -1 * 1000 + (10 * 18 - 5 * (18 + 2) * 2000 / 100 = 600
</pre></div>
</li>
</ol>
<div class="section">
<p>To calculate ROI, use the formula</p>
<pre dir="ltr">
ROI = ((profit - cost) / cost) * 100

profit = Incremental Revenue * Targets Cumulative, cost = Incremental Cost * (Targets Cumulative + Non Targets Cumulative)
</pre>
<p>Substituting the values in this example results in</p>
<pre dir="ltr">
ROI = ((180 - 100) / 100) * 100 = 80
</pre></div>
<!-- class="section" --></div>
</div>
<a id="DMRUG830"></a>
<div class="props_rev_3"><a id="GUID-74E2BFD6-C3BD-4C53-9C66-7F948806190F"></a>
<h5 id="DMRUG-GUID-74E2BFD6-C3BD-4C53-9C66-7F948806190F" class="sect5">Profit and ROI Use Case</h5>
<div>
<p>The Profit and ROI Use Case depicts how to interpret results for profit and ROI calculations.</p>
<p>Suppose you run a mail order campaign. You will mail each customer a catalog. You want to mail catalogs to those customers who are likely to purchase things from the catalog.</p>
<p>Here is the input data from Profit and ROI example:</p>
<ul style="list-style-type: disc;">
<li>
<p>Startup cost = 1000. This is the total cost to start the campaign.</p>
</li>
<li>
<p>Incremental revenue = 10. This is estimated revenue that results from a sale or new customer.</p>
</li>
<li>
<p>Budget = 10000. This is the total amount of money that you can spend.</p>
</li>
<li>
<p>Population = 2000. This is the total number of cases.</p>
</li>
</ul>
<p>Therefore, each quantile contains 20 cases:</p>
<pre dir="ltr">
total population /number of quantiles = 2000/100 = 20
</pre>
<p>The cost to promote a sale in each quantile is (Incremental Cost * number of cases per quantile) = $5 * 20 = $100).</p>
<p>The cumulative costs per quantile are as follows:</p>
<ul style="list-style-type: disc;">
<li>
<p>Quantile 1 costs $1000 (startup cost) + $100 (cost to promote a sale in Quantile 1) = $1100.</p>
</li>
<li>
<p>Quantile 2 costs $1100 (cost of Quantile 1) + $100 (cost in Quantile 2).</p>
</li>
<li>
<p>Quantile 3 costs $1200.</p>
</li>
</ul>
<p>If you calculate all of the intermediate values, then the cumulative costs for Quantile 90 is $10,000 and for Quantile 100 is $11,000. The budget is $10,000. If you look at the graph for profit in Oracle Data Miner, then you should see the budget line drawn in the profit chart on the 90th quantile.</p>
<p>In the Profit and ROI example, the calculated profit is $600 and ROI is 80 percent, which means that if you mail catalogs to first 20 quantiles of the population (400), then the campaign will generate a profit of $600 (which has ROI of 80 percent).</p>
<p>If you randomly mail the catalogs to first 20 quantiles of customers, then the profit is</p>
<pre dir="ltr">
Profit = -1 * Startup Cost
         + (Incremental Revenue * Targets Cumulative - Incremental Cost
           * (Targets Cumulative + Non Targets Cumulative)) 
           * Population / Total Targets
Profit = -1 * 1000 + (10 * 10 - 5 * (10 + 10)) * 2000 / 100 = -$1000
</pre>
<p>In other words, there is no profit.</p>
</div>
<div>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-CF9DDE14-312E-4A7A-9CE2-D571907927B6">Profit and ROI Example</a></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<a id="DMRUG845"></a><a id="compare_class_test_resutls"></a>
<div class="props_rev_3"><a id="GUID-372B9138-0A9A-4F1C-8339-046E29F03417"></a>
<h3 id="DMRUG-GUID-372B9138-0A9A-4F1C-8339-046E29F03417" class="sect3">Compare Classification Test Results</h3>
<div>
<p>By using the <span class="bold">Compare Test Result</span> context menu option in a Test node and Classification node, you can compare test results of a Classification model that are tested in a Test node, and for models that are tested after running the Classification node respectively.</p>
<div class="section">
<p>To compare test results for all of the models in a Classification Build node:</p>
</div>
<!-- class="section" -->
<div class="section">
<ul style="list-style-type: disc;">
<li>
<p>If you tested the models when you ran the Classification node: Right-click the Classification node that contains the models and select <span class="bold">Compare Test Results.</span></p>
</li>
<li>
<p>If you tested the Classification models in a Test node: Right-click the Test node that tests the models and select <span class="bold">Compare Test Results.</span></p>
</li>
</ul>
<p>The <span class="wintitle">Classification Model Test</span> viewer that compares the test results, opens. The comparison enables you to select the model that best solves a business problem.</p>
<p>The graphs in the <span class="bold">Performance</span> tab for different models are in different colors. In the other tabs, the same color is used for the line indicating measures such as lift.</p>
<p>The color associated with each model is displayed in the bottom page of each tab.</p>
</div>
<!-- class="section" --></div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-6B35714A-7F38-4FA3-9CC6-F1F0F520C75C">Compare Test Results</a><br />
The comparison of Test results for a Classification node are displayed under different categories for performance, performance Matrix, ROC, lift, and profit.</li>
</ul>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-5F4220DD-498D-47CA-98DB-5216D0F241A7">Classification Model Test Viewer</a></li>
</ul>
</div>
</div>
<a id="DMRUG846"></a><a id="class_test_compare_results"></a>
<div class="props_rev_3"><a id="GUID-6B35714A-7F38-4FA3-9CC6-F1F0F520C75C"></a>
<h4 id="DMRUG-GUID-6B35714A-7F38-4FA3-9CC6-F1F0F520C75C" class="sect4">Compare Test Results</h4>
<div>
<p>The comparison of Test results for a Classification node are displayed under different categories for performance, performance Matrix, ROC, lift, and profit.</p>
<p>Compare Test Results for Classification are displayed in these tabs:</p>
<ul style="list-style-type: disc;">
<li>
<p><span class="bold">Performance:</span> Compares performance results in the top pane for the models listed in the bottom panel.</p>
<p>To edit the list of models, click <img width="13" height="16" src="img/GUID-246FB100-36EE-4322-9F80-FE5873B74DBE-default.png" alt="compare" title="compare" /> above pane that lists the models. This opens the <span class="bold"><span class="wintitle">Edit Test Selection (Classification and Regression)</span></span> dialog box. By default, test results for all models are compared.</p>
</li>
<li>
<p><span class="bold">Performance Matrix:</span> Displays the Performance Matrix for each model. You can display either Compare models (a comparison of the performance matrices) or Details (the Performance Matrix for a selected model).</p>
</li>
<li>
<p><span class="bold">ROC:</span> Compares the ROC curves for the models listed in the lower pane.</p>
<p>To see information for a curve, select a model and click.</p>
<p>To edit the list of models, click <img width="13" height="16" src="img/GUID-246FB100-36EE-4322-9F80-FE5873B74DBE-default.png" alt="compare" title="compare" /> above pane that lists the models. This opens the <span class="wintitle">Edit Test Selection (Classification and Regression)</span> dialog box.</p>
</li>
<li>
<p><span class="bold">Lift:</span> Compares the lift for the models listed in the lower pane. For more information about lift, see <a href="testing-and-tuning-models.htm#GUID-972672DA-2E50-4215-BBB3-1E487490F8CD" title="Lift measures the degree to which the predictions of a Classification model are better than randomly-generated predictions. Lift applies to binary classification and non-binary classifications.">Lift</a>.</p>
<p>To edit the list of models, click <img width="13" height="16" src="img/GUID-246FB100-36EE-4322-9F80-FE5873B74DBE-default.png" alt="compare" title="compare" /> above pane that lists the models. This opens the <span class="wintitle">Edit Test Selection (Classification and Regression)</span> dialog box.</p>
</li>
<li>
<p><span class="bold">Profit:</span> Compares the profit curves for the models listed in the lower pane.</p>
<p>To edit the list of models, click <img width="13" height="16" src="img/GUID-246FB100-36EE-4322-9F80-FE5873B74DBE-default.png" alt="compare" title="compare" /> above pane that lists the models. This opens the <span class="wintitle">Edit Test Selection (Classification and Regression)</span> dialog box.</p>
</li>
</ul>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-1EC39895-8D19-4BCE-9F13-0BA56C2009CC">Edit Test Selection (Classification and Regression)</a><br />
By default, test results for all successfully built models in the build node are selected.</li>
</ul>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-2E338EE4-6ED3-49B1-ABBA-EA7BE458DB52">Profit and ROI</a></li>
<li><a href="testing-and-tuning-models.htm#GUID-D2321495-79D8-4D69-8B40-62CC074D0267">Performance</a></li>
<li><a href="testing-and-tuning-models.htm#GUID-D464F430-E7B7-412E-B6F5-B928095F3746">Performance Matrix</a></li>
<li><a href="testing-and-tuning-models.htm#GUID-19BDA41C-5E78-485E-8A2D-88AFDBB5B433">Receiver Operating Characteristics (ROC)</a></li>
</ul>
</div>
</div>
<a id="DMRUG847"></a><a id="edit_test_selection_compare"></a>
<div class="props_rev_3"><a id="GUID-1EC39895-8D19-4BCE-9F13-0BA56C2009CC"></a>
<h5 id="DMRUG-GUID-1EC39895-8D19-4BCE-9F13-0BA56C2009CC" class="sect5">Edit Test Selection (Classification and Regression)</h5>
<div>
<p>By default, test results for all successfully built models in the build node are selected.</p>
<div class="section">
<p>If you do not want to view test results for a model, then deselect the model. Click <span class="bold">OK</span> when you have finished.</p>
</div>
<!-- class="section" --></div>
</div>
</div>
</div>
<a id="DMRUG832"></a><a id="class_test_viewer"></a>
<div class="props_rev_3"><a id="GUID-5F4220DD-498D-47CA-98DB-5216D0F241A7"></a>
<h3 id="DMRUG-GUID-5F4220DD-498D-47CA-98DB-5216D0F241A7" class="sect3">Classification Model Test Viewer</h3>
<div>
<p>The Classification Model Test viewer displays all information related to the Classification Model test results.</p>
<p>Open the test viewer by selecting either <span class="bold">View Test Results</span> or <span class="bold">Compare Test Results</span> in the context menu for a Classification node or a Test node that tests Classification models. Select the results to view.</p>
<div class="p">In the Classification Model Test viewer, you can select the comparison levels for:
<ul style="list-style-type: disc;">
<li>
<p><span class="bold">Models:</span> (Default)</p>
</li>
<li>
<p><span class="bold">Partitions:</span></p>
<ul style="list-style-type: disc;">
<li>
<p>If a partition has never been selected, then the <a href="testing-and-tuning-models.htm#GUID-3058AA31-35EB-4D45-A237-69F6FBDBFC40" title="In the Select Partition dialog box, you can view filtered partitions based on the Partition keys.">Select Partition</a> dialog box opens.</p>
</li>
<li>
<p>If a partition has been previously selected, then it will be loaded. Click the Partition name that is displayed in the Search field, to view the details.</p>
</li>
<li>
<p>To change the selected partition, click <img width="16" height="16" src="img/GUID-8E676C2C-814A-47FD-9B26-633AB619624C-default.png" alt="search icon" title="search icon" />. This opens the <a href="testing-and-tuning-models.htm#GUID-3058AA31-35EB-4D45-A237-69F6FBDBFC40" title="In the Select Partition dialog box, you can view filtered partitions based on the Partition keys.">Select Partition</a> dialog box.</p>
</li>
</ul>
</li>
</ul>
</div>
<p>The Classification model test viewer shows the following tabs:</p>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-612E8FA7-2294-4AB4-959A-C7CC3C8117C2">Performance</a><br />
The <span class="bold">Performance</span> tab provides an overall summary of the performance of each model generated.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-A315381A-21BD-41D5-992C-B02ABD405567">Performance Matrix</a><br />
The Performance Matrix displays the number of correct and incorrect predictions made by the model compared with the actual classifications in the test data.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-548CA0DA-5DD8-4C12-BB8B-8B1D69FAF82A">ROC</a><br />
Receiver Operating Characteristics (ROC) compares predicted and actual target values in a binary Classification model.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-34CDE01C-78FD-47B5-AA32-5701126AF91C">Lift</a><br />
The Lift graph shows the lift from the model (or models) and also shows the lift from a naive model (Random) and the ideal lift.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-7D5D42E9-97A5-4D85-B4F1-1D3C76148DF8">Profit</a><br />
The Profit graph displays information related to profit, budget, and threshold for one or more models.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-3EC74675-1AE3-4D23-B763-45DBCE8D8DA4">Model Partitions</a><br />
The Model Partition tab displays the information of the model partitions on a node. The number of partitions can be very large, a fetch size limit will be added.</li>
</ul>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-E8D2043A-03AE-498F-BE0A-6CED607AB8FF">Test Metrics for Classification Models</a></li>
</ul>
</div>
</div>
<a id="DMRUG833"></a><a id="class_test_viewer_performance"></a>
<div class="props_rev_3"><a id="GUID-612E8FA7-2294-4AB4-959A-C7CC3C8117C2"></a>
<h4 id="DMRUG-GUID-612E8FA7-2294-4AB4-959A-C7CC3C8117C2" class="sect4">Performance</h4>
<div>
<p>The <span class="bold">Performance</span> tab provides an overall summary of the performance of each model generated.</p>
<div class="section">
<p>It displays test results for several common test metrics:</p>
<ul style="list-style-type: disc;">
<li>
<p>All Measures (default). The <span class="bold">Measure</span> list enables you to select the measures to display. By default, all measures are displayed. The selected measures are displayed as graphs. If you are comparing test results for two or more models, then different models have graphs in different colors.</p>
</li>
<li>
<p>Predictive Confidence</p>
</li>
<li>
<p>Average Accuracy</p>
</li>
<li>
<p>Overall Accuracy</p>
</li>
<li>
<p>Cost, if you specified costs or the system calculated costs</p>
</li>
</ul>
<p>In the <span class="bold">Sort By</span> fields. you can specify the sort attribute and sort order. The first list is the sort attribute: measure, creation date, or name (the default). The second list is the sort order: ascending or descending (default).</p>
<p>Below the graphs, the Models table supplements the information presented in the graph. You can minimize the table using the splitter line. The <span class="wintitle">Models</span> table in the lower panel summarizes the data in the histograms:</p>
<ul style="list-style-type: disc;">
<li>
<p>Name, the name of the model along with color of the model in the graphs</p>
</li>
<li>
<p>Predictive Confidence percent</p>
</li>
<li>
<p>Overall Accuracy percent</p>
</li>
<li>
<p>Average Accuracy percent</p>
</li>
<li>
<p>Cost, if you specified cost (costs are calculated by Oracle Data Miner for decision trees)</p>
</li>
<li>
<p>Algorithm (used to build the model)</p>
</li>
<li>
<p>Build Rows</p>
</li>
<li>
<p>Test Rows</p>
</li>
<li>
<p>Creation date</p>
</li>
</ul>
<p>By default, results for the selected model are displayed. To change the list of models, click <img width="13" height="16" src="img/GUID-246FB100-36EE-4322-9F80-FE5873B74DBE-default.png" alt="compare" title="compare" /> and deselect any models for which you do not want to see results. If you deselect a model, then both the histogram and the summary information are removed.</p>
<p>To view the model, select <img width="16" height="16" src="img/GUID-97516EB4-F6DA-440C-A0C5-8EC2CBB33057-default.png" alt="view models" title="view models" /></p>
</div>
<!-- class="section" --></div>
<div>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-2744EB07-B54D-44C9-9E9B-C8D84465824C">Predictive Confidence</a></li>
<li><a href="testing-and-tuning-models.htm#GUID-F6AD27B5-A937-479D-B54C-55FD9CAF0825">Average Accuracy</a></li>
<li><a href="testing-and-tuning-models.htm#GUID-BFEAE664-4C80-412B-A2C2-48F89BDB5CB9">Overall Accuracy</a></li>
<li><a href="testing-and-tuning-models.htm#GUID-4EA3DE97-7FBD-43E2-B44D-38D008ECF63A">Cost</a></li>
</ul>
</div>
</div>
</div>
<a id="DMRUG834"></a><a id="class_test_performance_matrix"></a>
<div class="props_rev_3"><a id="GUID-A315381A-21BD-41D5-992C-B02ABD405567"></a>
<h4 id="DMRUG-GUID-A315381A-21BD-41D5-992C-B02ABD405567" class="sect4">Performance Matrix</h4>
<div>
<p>The Performance Matrix displays the number of correct and incorrect predictions made by the model compared with the actual classifications in the test data.</p>
<div class="section">
<p>You can either view the detail for a selected model, or you can compare performance matrices for all models.</p>
<ul style="list-style-type: disc;">
<li>
<p>Click <span class="bold">Show Details</span> to view test results for one model.</p>
</li>
<li>
<p>Click <span class="bold">Compare Nodes</span> to compare test results.</p>
</li>
</ul>
</div>
<!-- class="section" --></div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-681EAAE7-C61C-4A7A-9C9C-54A794A8DF49">Show Detail</a><br />
The <span class="wintitle">Show Detail</span> view displays all information related to the selected model.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-1CED4EE4-2D2E-4B0F-82A8-57B68354227C">Compare Models</a><br />
<span class="wintitle">Compare Models</span> compares performance information for all models in the node that were tested.</li>
</ul>
</div>
<a id="DMRUG835"></a>
<div class="props_rev_3"><a id="GUID-681EAAE7-C61C-4A7A-9C9C-54A794A8DF49"></a>
<h5 id="DMRUG-GUID-681EAAE7-C61C-4A7A-9C9C-54A794A8DF49" class="sect5">Show Detail</h5>
<div>
<p>The <span class="wintitle">Show Detail</span> view displays all information related to the selected model.</p>
<div class="section">
<p>First, select a model. If you are viewing test results for one model, then the details for that model are displayed automatically.</p>
<ul style="list-style-type: disc;">
<li>
<p>In the top pane, Average Accuracy and Overall Accuracy are displayed with a grid that displays the correct predictions for each target value. Cost information is displayed if you have specified costs.</p>
</li>
<li>
<p>In the bottom pane, a Performance Matrix with rows showing actual values and columns showing predicted values is displayed for the selected model. The percentage correct and cost are displayed for each column.</p>
</li>
</ul>
<p>Select <span class="bold">Show totals and cost</span> to see the total, the percentage correct, and cost for correct and incorrect predictions.</p>
<p>Click <img width="16" height="16" src="img/GUID-8E676C2C-814A-47FD-9B26-633AB619624C-default.png" alt="filter" title="filter" /> to filter your search based on a target.</p>
</div>
<!-- class="section" --></div>
</div>
<a id="DMRUG836"></a>
<div class="props_rev_3"><a id="GUID-1CED4EE4-2D2E-4B0F-82A8-57B68354227C"></a>
<h5 id="DMRUG-GUID-1CED4EE4-2D2E-4B0F-82A8-57B68354227C" class="sect5">Compare Models</h5>
<div>
<p><span class="wintitle">Compare Models</span> compares performance information for all models in the node that were tested.</p>
<div class="section">
<ul style="list-style-type: disc;">
<li>
<p>The top pane lists the following for each model:</p>
<ul style="list-style-type: disc;">
<li>
<p>Percentage of correct predictions</p>
</li>
<li>
<p>Count of correct predictions</p>
</li>
<li>
<p>Total case count</p>
</li>
<li>
<p>Cost information</p>
</li>
</ul>
<p>To see more detail, select a model and click <img width="16" height="16" src="img/GUID-97516EB4-F6DA-440C-A0C5-8EC2CBB33057-default.png" alt="view selected model" title="view selected model" />.</p>
</li>
<li>
<p>The bottom pane displays the target value details for the model selected in the top pane. Select the measure. To filter your search by target value, click <img width="16" height="16" src="img/GUID-8E676C2C-814A-47FD-9B26-633AB619624C-default.png" alt="filter by target" title="filter by target" /></p>
<ul style="list-style-type: disc;">
<li>
<p><span class="bold">Correct Predictions</span> (default): Displays correct predictions for each value of the target attribute</p>
</li>
<li>
<p><span class="bold">Costs:</span> Displays costs for each value of the target</p>
</li>
</ul>
</li>
</ul>
</div>
<!-- class="section" --></div>
</div>
</div>
<a id="DMRUG837"></a><a id="class_test_viewer_roc"></a>
<div class="props_rev_3"><a id="GUID-548CA0DA-5DD8-4C12-BB8B-8B1D69FAF82A"></a>
<h4 id="DMRUG-GUID-548CA0DA-5DD8-4C12-BB8B-8B1D69FAF82A" class="sect4">ROC</h4>
<div>
<p>Receiver Operating Characteristics (ROC) compares predicted and actual target values in a binary Classification model.</p>
<div class="section">
<p>To edit and view an ROC:</p>
</div>
<!-- class="section" -->
<ol>
<li class="stepexpand"><span>Select the <span class="bold">Target Value.</span> The ROC curves for that value are displayed.</span></li>
<li class="stepexpand"><span>Click <span class="bold">Edit Custom Operating Point</span> to change the operating point. The ROC graph displays a line showing ROC for each model. Points are marked on the graph indicating the values shown in the key at the bottom of the graph. Below the graph, the ROC Summary results table supplements the information presented in the graph. You can minimize the table using the splitter line.</span>
<div>
<ul style="list-style-type: disc;">
<li>
<p>The <span class="wintitle">Models</span> grid, in the lower pane, contains the following summary information:</p>
<ul style="list-style-type: disc;">
<li>
<p>Name</p>
</li>
<li>
<p>Area under the curve</p>
</li>
<li>
<p>Maximum Overall Accuracy Percentage</p>
</li>
<li>
<p>Maximum Average Accuracy Percentage</p>
</li>
<li>
<p>Custom Accuracy Percentage</p>
</li>
<li>
<p>Model Accuracy Percentage</p>
</li>
<li>
<p>Algorithm</p>
</li>
<li>
<p>Build Rows</p>
</li>
<li>
<p>Test Rows</p>
</li>
<li>
<p>Creation Date and Time</p>
</li>
</ul>
</li>
<li>
<p>Select a model and click <img width="16" height="16" src="img/GUID-8E676C2C-814A-47FD-9B26-633AB619624C-default.png" alt="browse" title="browse" /> to see the <span class="wintitle">ROC Details</span> dialog box, which displays the statistics for probability thresholds.</p>
</li>
<li>
<p>To change the list of models, click <img width="13" height="16" src="img/GUID-246FB100-36EE-4322-9F80-FE5873B74DBE-default.png" alt="compare" title="compare" /> to open the <span class="wintitle">Edit Test Result Selection</span> dialog box. By default, results for all models in the node are displayed.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-9B00A459-F2F6-4A0A-8A3F-313C161F8D80">Edit Test Result Selection</a><br />
In the <span class="wintitle">Edit test Result Selection</span> dialog box, you can select specific models that you want to compare.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-199D2212-6CEC-49C2-AD72-9E013A273E44">ROC Detail Dialog</a><br />
The ROC Detail Dialog displays statistics for probability thresholds.</li>
</ul>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-E03BF4DB-9BAF-4785-BA3A-D251E1C449B1" title="Receiver Operating Characteristics (ROC) supports what-if analysis.">How to Use ROC</a></li>
</ul>
</div>
</div>
<a id="DMRUG838"></a><a id="edit_test_result_select"></a>
<div class="props_rev_3"><a id="GUID-9B00A459-F2F6-4A0A-8A3F-313C161F8D80"></a>
<h5 id="DMRUG-GUID-9B00A459-F2F6-4A0A-8A3F-313C161F8D80" class="sect5">Edit Test Result Selection</h5>
<div>
<p>In the <span class="wintitle">Edit test Result Selection</span> dialog box, you can select specific models that you want to compare.</p>
<div class="section">
<p>By default, all models are selected in the <span class="wintitle">Edit test Result Selection</span> dialog box. Deselect the check box for those models for which you do not want to see results. If you deselect a model, then both the ROC curve and the details for that model are not displayed.</p>
<p>Click <span class="bold">OK</span> when you have finished.</p>
</div>
<!-- class="section" --></div>
</div>
<a id="DMRUG839"></a><a id="roc_detail_dialog"></a>
<div class="props_rev_3"><a id="GUID-199D2212-6CEC-49C2-AD72-9E013A273E44"></a>
<h5 id="DMRUG-GUID-199D2212-6CEC-49C2-AD72-9E013A273E44" class="sect5">ROC Detail Dialog</h5>
<div>
<p>The ROC Detail Dialog displays statistics for probability thresholds.</p>
<p>For each probability threshold, the following are displayed:</p>
<ul style="list-style-type: disc;">
<li>
<p>True Positive</p>
</li>
<li>
<p>False Negative</p>
</li>
<li>
<p>False Positive</p>
</li>
<li>
<p>True Negative</p>
</li>
<li>
<p>True Positive Fraction</p>
</li>
<li>
<p>False Positive Fraction</p>
</li>
<li>
<p>Overall Accuracy</p>
</li>
<li>
<p>Average Accuracy</p>
</li>
</ul>
<p>Click <span class="bold">OK</span> to dismiss the dialog box.</p>
</div>
</div>
</div>
<a id="DMRUG840"></a><a id="class_test_viewer_lift"></a>
<div class="props_rev_3"><a id="GUID-34CDE01C-78FD-47B5-AA32-5701126AF91C"></a>
<h4 id="DMRUG-GUID-34CDE01C-78FD-47B5-AA32-5701126AF91C" class="sect4">Lift</h4>
<div>
<p>The Lift graph shows the lift from the model (or models) and also shows the lift from a naive model (Random) and the ideal lift.</p>
<p>The x-axis of the graph is divided into quantiles. The lift graph displays at least three lines:</p>
<ul style="list-style-type: disc;">
<li>
<p>A line showing the lift for each model</p>
</li>
<li>
<p>A red line for the random model</p>
</li>
<li>
<p>A vertical blue line for threshold</p>
</li>
</ul>
<p>The Lift viewer compares lift results for a given target value in two or more models. It displays either the Cumulative Positive Cases or the Cumulative Lift.</p>
<p>If you are comparing the lift for two or more models, then the lines for different models are in different colors. The table below the graph shows the name of the model and the color used to display results for that model.</p>
<p>The viewer has the following controls:</p>
<ul style="list-style-type: disc;">
<li>
<p><span class="bold">Display:</span> Selects the display option, either <span class="bold">Cumulative Positive Cases</span> (default) or <span class="bold">Cumulative Lift.</span></p>
</li>
<li>
<p><span class="bold">Target Value:</span> Selects the target value for comparison. The default target value is the least frequently occurring target value.</p>
</li>
</ul>
<p>The threshold is a blue vertical line used to select a quantile. As the threshold moves, the details for each test result in the Lift Detail table changes to the point on the Lift Chart that corresponds to the selected quantile. You move the threshold by dragging the indicator on the quantile line. Here is the quantile set to 20:</p>
<div class="figure" id="GUID-34CDE01C-78FD-47B5-AA32-5701126AF91C__GUID-C666924F-787B-44AD-AEE0-F689E1A42591"><img width="858" height="53" src="img/GUID-AA9935FF-A71D-47D1-852D-957383A6D695-default.gif" alt="Description of GUID-AA9935FF-A71D-47D1-852D-957383A6D695-print.eps follows" title="Description of GUID-AA9935FF-A71D-47D1-852D-957383A6D695-print.eps follows" /><br />
<a href="img_text/GUID-AA9935FF-A71D-47D1-852D-957383A6D695-print.htm">Description of the illustration GUID-AA9935FF-A71D-47D1-852D-957383A6D695-print.eps</a></div>
<!-- class="figure" -->
<p>Below the graph, a data table supplements the information presented in the graph. You can minimize the table using the splitter line.</p>
<p>The table has the following columns:</p>
<ul style="list-style-type: disc;">
<li>
<p>Name, the name of the model along with color of the model in the graph</p>
</li>
<li>
<p>Lift Cumulative</p>
</li>
<li>
<p>Gain Cumulative Percentage</p>
</li>
<li>
<p>Percentage Records Cumulative</p>
</li>
<li>
<p>Target Density Cumulative</p>
</li>
<li>
<p>Algorithm</p>
</li>
<li>
<p>Build Rows</p>
</li>
<li>
<p>Test Rows</p>
</li>
<li>
<p>Creation Date (date and time)</p>
</li>
</ul>
<p>Above the Models grid is the Lift Detail Dialog icon <img width="16" height="16" src="img/GUID-8E676C2C-814A-47FD-9B26-633AB619624C-default.png" alt="browse" title="browse" />. Select a model and click the icon to open the <span class="wintitle">Lift Detail</span> dialog box, which displays lift details for 100 quantiles.</p>
<p>To change the list of models, click <img width="13" height="16" src="img/GUID-246FB100-36EE-4322-9F80-FE5873B74DBE-default.png" alt="compare" title="compare" /> and deselect any models for which you do not want to see results. If you deselect a model, then both the lift curve and the detail information for that model are not displayed. By default, results for all models in the node are displayed.</p>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-A9BF1FB9-D80E-4783-98F9-81977B608298">Lift Detail</a><br />
The <span class="wintitle">Lift Detail</span> dialog box displays statistics for each quantile from 1 to 100.</li>
</ul>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-E8D2043A-03AE-498F-BE0A-6CED607AB8FF">Test Metrics for Classification Models</a></li>
</ul>
</div>
</div>
<a id="DMRUG841"></a><a id="lift_detail"></a>
<div class="props_rev_3"><a id="GUID-A9BF1FB9-D80E-4783-98F9-81977B608298"></a>
<h5 id="DMRUG-GUID-A9BF1FB9-D80E-4783-98F9-81977B608298" class="sect5">Lift Detail</h5>
<div>
<p>The <span class="wintitle">Lift Detail</span> dialog box displays statistics for each quantile from 1 to 100.</p>
<div class="section">
<p>Threshold probability does not always reflect standard probability. For example, the Classification node enables you to specify three different performance settings:</p>
</div>
<!-- class="section" -->
<div class="section">
<ul style="list-style-type: disc;">
<li>
<p><span class="bold">Balanced:</span> Apply balance weighting to all target class values.</p>
</li>
<li>
<p><span class="bold">Natural:</span> Do not apply any weighting.</p>
</li>
<li>
<p><span class="bold">Custom:</span> Apply user- created custom weights file.</p>
</li>
</ul>
<p>The default for Classification models is <code class="codeph">Balanced.</code> Balanced is implemented by passing weights or costs into the model, depending on the algorithm used.</p>
<p>The threshold probability actually reflects cost rather than standard probability.</p>
<p>To see the difference between Balanced and Natural:</p>
</div>
<!-- class="section" -->
<ol>
<li class="stepexpand"><span>Create a Classification model.</span></li>
<li class="stepexpand"><span>Select the performance setting options and view the lift details:</span>
<div>
<ul style="list-style-type: disc;">
<li>
<p><span class="bold">Natural:</span> The threshold probability values are the greatest probabilities for each quantile.</p>
</li>
<li>
<p><span class="bold">Balanced:</span> The threshold reflects cost. You see the lowest cost value for each quantile.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
</div>
<a id="DMRUG842"></a><a id="class_test_viewer_profit"></a>
<div class="props_rev_3"><a id="GUID-7D5D42E9-97A5-4D85-B4F1-1D3C76148DF8"></a>
<h4 id="DMRUG-GUID-7D5D42E9-97A5-4D85-B4F1-1D3C76148DF8" class="sect4">Profit</h4>
<div>
<p>The Profit graph displays information related to profit, budget, and threshold for one or more models.</p>
<p>The Profit graph displays at least three lines:</p>
<ul style="list-style-type: disc;">
<li>
<p>A line showing the profit for each model</p>
</li>
<li>
<p>A line indicating the budget</p>
</li>
<li>
<p>A line indicating the threshold</p>
</li>
</ul>
<p>The threshold is a blue vertical line used to select a quantile. As the threshold moves, the details for each test result in the Lift Detail table changes to the point on the Lift Chart that corresponds to the selected quantile. You can move the threshold by dragging the indicator on the quantile line. Here is the quantile set to 20:</p>
<div class="figure" id="GUID-7D5D42E9-97A5-4D85-B4F1-1D3C76148DF8__GUID-12FE05A7-C930-4E5F-B128-8D178897A597"><img width="858" height="53" src="img/GUID-AA9935FF-A71D-47D1-852D-957383A6D695-default.gif" alt="Description of GUID-AA9935FF-A71D-47D1-852D-957383A6D695-print.eps follows" title="Description of GUID-AA9935FF-A71D-47D1-852D-957383A6D695-print.eps follows" /><br />
<a href="img_text/GUID-AA9935FF-A71D-47D1-852D-957383A6D695-print.htm">Description of the illustration GUID-AA9935FF-A71D-47D1-852D-957383A6D695-print.eps</a></div>
<!-- class="figure" -->
<p>To specify the values for profit, click <span class="bold">Profit Settings</span> to open the Profit Setting dialog box.</p>
<p>If you are comparing the profit for two or more models, then the lines for different models are different colors. The table below the graph shows the name of the model and the color used to display results for that model.</p>
<p>The bottom pane contains the Models grid and supplements the information presented in the graph. You can minimize the table using the splitter line.</p>
<p>The table has the following columns:</p>
<ul style="list-style-type: disc;">
<li>
<p>Name, the name of the model along with color of the model in the graphs</p>
</li>
<li>
<p>Profit</p>
</li>
<li>
<p>ROI Percentage</p>
</li>
<li>
<p>Records Cumulative Percentage</p>
</li>
<li>
<p>Target Density Cumulative</p>
</li>
<li>
<p>Maximum Profit</p>
</li>
<li>
<p>Maximum Profit Population Percentage</p>
</li>
<li>
<p>Algorithm</p>
</li>
<li>
<p>Build Rows</p>
</li>
<li>
<p>Test Rows</p>
</li>
<li>
<p>Creation Date (and time)</p>
</li>
</ul>
<p>Above the Models grid is the Browse Detail icon. Select a model and click <img width="16" height="16" src="img/GUID-8E676C2C-814A-47FD-9B26-633AB619624C-default.png" alt="browse" title="browse" /> to see the <span class="wintitle">Profit Detail</span> dialog box which displays statistics for each quantile from 1 to 100.</p>
<p>To change the list of models, click <img width="13" height="16" src="img/GUID-246FB100-36EE-4322-9F80-FE5873B74DBE-default.png" alt="compare" title="compare" /> and deselect any models for which you do not want to see results. If you deselect a model, then both the profit curve and the detail information for that model are not displayed. By default, results for all models in the node are displayed.</p>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-BD9156A4-BA36-42F3-ADEA-623E1ABC3A22">Profit Detail Dialog</a><br />
The <span class="wintitle">Profit Detail</span> dialog box displays statistics about profit for quantiles 1 to 100.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-6794F8C2-A190-4C24-8BC1-3D009EF6DB3D">Profit Setting Dialog</a><br />
In the <span class="wintitle">Profit Settings</span> dialog box, you can provide values for profit settings such as budget, increment cost and so on.</li>
</ul>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-2E338EE4-6ED3-49B1-ABBA-EA7BE458DB52">Profit and ROI</a></li>
<li><a href="testing-and-tuning-models.htm#GUID-CF9DDE14-312E-4A7A-9CE2-D571907927B6">Profit and ROI Example</a></li>
<li><a href="testing-and-tuning-models.htm#GUID-74E2BFD6-C3BD-4C53-9C66-7F948806190F">Profit and ROI Use Case</a></li>
<li><a href="testing-and-tuning-models.htm#GUID-E8D2043A-03AE-498F-BE0A-6CED607AB8FF">Test Metrics for Classification Models</a></li>
</ul>
</div>
</div>
<a id="DMRUG843"></a><a id="profit_detail"></a>
<div class="props_rev_3"><a id="GUID-BD9156A4-BA36-42F3-ADEA-623E1ABC3A22"></a>
<h5 id="DMRUG-GUID-BD9156A4-BA36-42F3-ADEA-623E1ABC3A22" class="sect5">Profit Detail Dialog</h5>
<div>
<p>The <span class="wintitle">Profit Detail</span> dialog box displays statistics about profit for quantiles 1 to 100.</p>
<div class="section">
<p>Click <span class="bold">OK</span> to dismiss the dialog box.</p>
</div>
<!-- class="section" --></div>
</div>
<div class="sect5"><a id="GUID-6794F8C2-A190-4C24-8BC1-3D009EF6DB3D"></a>
<h5 id="DMRUG-GUID-6794F8C2-A190-4C24-8BC1-3D009EF6DB3D" class="sect5">Profit Setting Dialog</h5>
<div>
<p>In the <span class="wintitle">Profit Settings</span> dialog box, you can provide values for profit settings such as budget, increment cost and so on.</p>
<div class="section">To edit profit settings:</div>
<!-- class="section" -->
<ol>
<li><span>Click <span class="bold">Profit Settings</span> to change the following values:</span>
<ul>
<li><span class="bold">Startup Cost:</span> The cost of starting the process that creates the profit. The default is <code class="codeph">1.</code></li>
<li><span class="bold">Incremental Revenue:</span> Incremental revenue earned for each correct prediction. The default is <code class="codeph">1.</code></li>
<li><span class="bold">Incremental Cost:</span> The cost of each additional item. The default is <code class="codeph">1.</code></li>
<li><span class="bold">Budget:</span> A total cost that cannot be exceeded. The default value is <code class="codeph">1.</code></li>
<li><span class="bold">Population:</span> The number of individual cases that the model is applied to. The default is <code class="codeph">100.</code></li>
</ul>
</li>
<li><span>Click <span class="bold">OK.</span></span></li>
</ol>
</div>
</div>
</div>
<a id="DMRUG1250"></a>
<div class="props_rev_3"><a id="GUID-3EC74675-1AE3-4D23-B763-45DBCE8D8DA4"></a>
<h4 id="DMRUG-GUID-3EC74675-1AE3-4D23-B763-45DBCE8D8DA4" class="sect4">Model Partitions</h4>
<div>
<p>The Model Partition tab displays the information of the model partitions on a node. The number of partitions can be very large, a fetch size limit will be added.</p>
<div class="p">The Model Partition tab displays the following information about the partitioned model:
<ul style="list-style-type: disc;">
<li>
<p>Model name</p>
</li>
<li>
<p>Partition ID</p>
</li>
<li>
<p>Partition Name</p>
</li>
<li>
<p>Predictive Confidence</p>
</li>
<li>
<p>Overall Accuracy</p>
</li>
<li>
<p>Average Accuracy</p>
</li>
<li>
<p>Build Rows</p>
</li>
<li>
<p>Test Rows</p>
</li>
<li>
<p>Cost</p>
</li>
<li>
<p>Algorithm type</p>
</li>
<li>
<p>Creation Date</p>
</li>
</ul>
You can perform the following tasks:
<ul style="list-style-type: disc;">
<li>
<p>Sort data: To sort data, click <img width="26" height="26" src="img/GUID-BD2A8CFF-4330-4F06-BC22-2007A32A728B-default.png" alt="icon to indicate sort" title="icon to indicate sort" /></p>
</li>
<li>
<p>Pin partition: The icon to pin or select a partition is enabled when you select a row. Select a row and click <img width="25" height="25" src="img/GUID-A9D78493-3345-42E4-B423-1251683E0F1A-default.png" alt="pin partition" title="pin partition" /></p>
to mark the selected partitioned as pinned in all the Test Result editors. This means that the partition will be loaded when the editor is opened.</li>
<li>
<p>View partition details: Double click the partition name or click <img width="16" height="16" src="img/GUID-8E676C2C-814A-47FD-9B26-633AB619624C-default.png" alt="magnifying glass icon" title="magnifying glass icon" /> to view the details of the partition such as Partition ID, Partition Name, Partition Details Table, and Table Filtering.</p>
</li>
<li>
<p>View model details: Click <img width="16" height="16" src="img/GUID-97516EB4-F6DA-440C-A0C5-8EC2CBB33057-default.png" alt="view icon" title="view icon" /> to view the specific partition model details in the Model Viewer.</p>
</li>
<li>
<p>Select and view models in the Edit Test Result Selection dialog box: Click <img width="13" height="16" src="img/GUID-246FB100-36EE-4322-9F80-FE5873B74DBE-default.png" alt="compare" title="compare" /> to select models and view them in the <span class="wintitle">Edit Test Result Selection</span> dialog box</p>
</li>
<li>
<p>Filter model partition: You can filter and sort model partitions based on the model name, partition name, algorithm, and partition keys.</p>
</li>
</ul>
</div>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-3058AA31-35EB-4D45-A237-69F6FBDBFC40">Select Partition</a><br />
In the <span class="wintitle">Select Partition</span> dialog box, you can view filtered partitions based on the Partition keys.</li>
</ul>
</div>
<div class="sect5"><a id="GUID-3058AA31-35EB-4D45-A237-69F6FBDBFC40"></a>
<h5 id="DMRUG-GUID-3058AA31-35EB-4D45-A237-69F6FBDBFC40" class="sect5">Select Partition</h5>
<div>
<p>In the <span class="wintitle">Select Partition</span> dialog box, you can view filtered partitions based on the Partition keys.</p>
<div class="section">The filtered partitions are displayed in the partition table in the lower panel of the dialog box. To query and view filtered partitions:</div>
<!-- class="section" -->
<ol>
<li class="stepexpand"><span>In the <span class="bold">Fetch</span> field, select a number by clicking the arrows.</span>
<div>This limits the number of partitions displayed to the number that you have entered.</div>
</li>
<li class="stepexpand"><span>Select any one of the following options:</span>
<ul>
<li><span class="bold">Match all of the following:</span> To consider all the Partition Keys as the matching criteria to fetch the data.</li>
<li><span class="bold">Match any of the following</span> To consider the selected Partition Key as the matching criteria to fetch the data.</li>
</ul>
</li>
<li class="stepexpand"><span>Click <span class="bold">Query.</span></span>
<div>This displays the list of filtered partitioned based on the query.</div>
</li>
<li class="stepexpand"><span>Click <span class="bold">OK.</span></span></li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect3"><a id="GUID-0ED07A23-3DB5-4888-9C6D-7FFB16C8D544"></a>
<h3 id="DMRUG-GUID-0ED07A23-3DB5-4888-9C6D-7FFB16C8D544" class="sect3">Viewing Test Results</h3>
<div>
<p>You can view results of models that are tested in a Classification node and a Test node.</p>
<div class="p">Test the model in a Classification Node.</div>
<!-- class="section" -->
<div class="section">You can change the defaults using the preference setting.</div>
<!-- class="section" -->
<ol>
<li class="stepexpand"><span>Run the Classification node.</span></li>
<li class="stepexpand"><span>Right-click the node and select <span class="bold">View Test Results.</span></span></li>
<li class="stepexpand"><span>To view the model, select the model that you are interested in.</span>
<div>The <span class="wintitle">Classification Model Test Viewer</span> opens.</div>
</li>
<li class="stepexpand"><span>To compare the test results for all models in the node, select <span class="bold">Compare Test Results.</span></span>
<div>
<div class="infobox-note" id="GUID-0ED07A23-3DB5-4888-9C6D-7FFB16C8D544__GUID-6C5B701E-C7C1-41E2-9884-C0CFBFF58071">
<p class="notep1">Note:</p>
To view the test results of models tested in a Test node, you must test the model in a Test node and run the Test node.</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<a id="DMRUG848"></a><a id="class_tune_select"></a>
<div class="props_rev_3"><a id="GUID-89B07B20-C62C-4124-BD83-50F020F2E6F3"></a>
<h2 id="DMRUG-GUID-89B07B20-C62C-4124-BD83-50F020F2E6F3" class="sect2">Tuning Classification Models</h2>
<div>
<p>When you tune a model, you create a derived cost matrix to use for subsequent Test and Apply operations.</p>
<div class="p">Select one of several ways to create a derived cost matrix when tuning a model.</div>
<!-- class="section" -->
<div class="section">The derived cost matrix is used for any subsequent test and apply operations. Each tuning dialog box has a different goal in how the tuning is performed.</div>
<!-- class="section" -->
<div class="section">
<div class="infobox-note" id="GUID-89B07B20-C62C-4124-BD83-50F020F2E6F3__GUID-D2324883-B656-46E5-8080-6317D4107878">
<p class="notep1">Note:</p>
<p>To tune models, you must test the models in the same node that you build them.</p>
</div>
<p>If necessary, you can remove tuning and then re-run the node.</p>
<p>To tune a model:</p>
</div>
<!-- class="section" -->
<ol>
<li class="stepexpand"><span>Open the <span class="wintitle">Properties</span> pane for the Build node. Right-click the node and select <span class="bold">Go to Properties.</span></span></li>
<li class="stepexpand"><span>Go to the Test section. Select <span class="bold">Generate Select Test Results for Model Tuning.</span> The model test operation then generates the unbiased test results (no cost matrix used) for the corresponding test results. The unbiased test results are determined by the <span class="wintitle">Tune Settings</span> dialog box to initialize tuning options. For example, if only ROC is selected for Test Results, then the test operation generates the regular ROC result and the unbiased ROC result.</span></li>
<li class="stepexpand"><span>Run the Build node. You must test the models in the Build node. This is the default behavior of the Classification Build node.</span></li>
<li class="stepexpand"><span>In <span class="wintitle">Properties</span> pane for the Build node, go to the <span class="wintitle">Models</span> section. Select the models that you want to tune and click the tune icon <img width="16" height="16" src="img/GUID-342E87F0-3554-422E-8133-9EF07302D670-default.png" alt="tune selected models" title="tune selected models" /> in the menu bar.</span>
<div>
<p>Select <span class="bold">Tune</span> from the drop-down list.</p>
<div class="figure" id="GUID-89B07B20-C62C-4124-BD83-50F020F2E6F3__GUID-F3D4482F-DA5B-412A-9BBF-01921FF4D495"><img width="864" height="218" src="img/GUID-C3F2D84A-CB2E-4080-9C11-132863651AC6-default.gif" alt="Description of GUID-C3F2D84A-CB2E-4080-9C11-132863651AC6-print.eps follows" title="Description of GUID-C3F2D84A-CB2E-4080-9C11-132863651AC6-print.eps follows" /><br />
<a href="img_text/GUID-C3F2D84A-CB2E-4080-9C11-132863651AC6-print.htm">Description of the illustration GUID-C3F2D84A-CB2E-4080-9C11-132863651AC6-print.eps</a></div>
<!-- class="figure" --></div>
</li>
<li class="stepexpand"><span>The <span class="wintitle">Tune Settings</span> dialog box opens with all the available test results.</span>
<div>
<p>You can tune a model using one technique. For example, you can tune using costs or using lift, but not using both costs and lift at the same time.</p>
</div>
</li>
<li class="stepexpand"><span>If you are tuning more than one model, then select a model from the <span class="wintitle">Models</span> list in the bottom pane of the dialog box. After you tune the first model, return to this pane and select another model.</span></li>
<li class="stepexpand"><span>Click the tab for the test result to tune. The tabs are:</span>
<div>
<ul style="list-style-type: disc;">
<li>
<p>Cost</p>
</li>
<li>
<p>Benefit</p>
</li>
<li>
<p>ROC</p>
</li>
<li>
<p>Lift</p>
</li>
<li>
<p>Profit</p>
</li>
</ul>
</div>
</li>
<li class="stepexpand"><span>When you finish tuning a model, click <span class="bold">Tune</span> in the pane on the right to generate tuning. In the <span class="wintitle">Models</span> list in the bottom pane, the Tune setting changes from Automatic to the new setting.</span></li>
<li class="stepexpand"><span>Tune as many models in the node you want. Go to other tuning tabs and perform tuning from there. When you have finished tuning, click <span class="bold">OK.</span></span></li>
<li class="stepexpand"><span>All models that have tuning specifications changed during the session have their test results marked as not run. When you run the node again:</span>
<div>
<ul style="list-style-type: disc;">
<li>
<p>The new cost matrix is generated and inserted into the model.</p>
</li>
<li>
<p>A new test result is generated showing full test result information for the behavior of the current model.</p>
</li>
</ul>
</div>
</li>
<li class="stepexpand"><span>Run the tuned model. After running of the model is complete, the Models section in Properties indicates how each model was tuned. For example, if you tune a model by changing costs, then the Tune entry for that model is Tune - Cost.</span></li>
<li class="stepexpand"><span>Right-click the Build node and select <span class="bold">View Test Results</span> for the tuned model to see the effects of the tuning.</span></li>
</ol>
<div class="section">
<p>You may have to repeat the tuning steps several times to get the desired results. If necessary, you can remove tuning for a model.</p>
</div>
<!-- class="section" --></div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-0264B563-618A-48BC-8ADF-E71BAFC61506">Remove Tuning</a><br />
You can remove tuning of a Classification by selecting the <span class="bold">Automatic</span> option.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-9AB480C2-2A88-4936-B01A-90A0666D49F0">Cost</a><br />
The <span class="bold">Cost</span> tab in <span class="bold">Tune Settings</span> enables you to specify costs for target for scoring purposes.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-5D20C102-CDA3-4195-81BE-D0378E6CAEC5">Benefit</a><br />
In the <span class="bold">Benefit</span> tab, you can specify a benefit for each value of the target. Specifying benefits is useful when there are many target values.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-7FC8A45B-BCFA-4CF2-AB6E-37A91F5B6E7D">ROC</a><br />
ROC is only supported for binary models.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-48FCB234-AEB4-445C-94BC-19DD3E1889C7">Lift</a><br />
Lift measures the degree to which the predictions of a Classification model are better than randomly generated predictions.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-190FE2F0-C9C8-4FDC-BE2F-D6F8D2BA78BD">Profit</a><br />
The <span class="bold">Profit</span> tab provides a method for maximizing profit.</li>
</ul>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-1B8084C5-6857-427B-BE37-599C4363F964">Testing Classification Models</a></li>
</ul>
</div>
</div>
<a id="DMRUG849"></a>
<div class="props_rev_3"><a id="GUID-0264B563-618A-48BC-8ADF-E71BAFC61506"></a>
<h3 id="DMRUG-GUID-0264B563-618A-48BC-8ADF-E71BAFC61506" class="sect3">Remove Tuning</h3>
<div>
<p>You can remove tuning of a Classification by selecting the <span class="bold">Automatic</span> option.</p>
<div class="section">
<p>To remove tuning for a model:</p>
</div>
<!-- class="section" -->
<ol>
<li><span>Right-click the node and select <span class="bold">Go to Properties.</span></span></li>
<li><span>Go to the <span class="bold">Models</span> section and click <img width="16" height="16" src="img/GUID-342E87F0-3554-422E-8133-9EF07302D670-default.png" alt="tune" title="tune" />.</span></li>
<li><span>Select <span class="bold">Automatic.</span></span></li>
<li><span>Run the node.</span></li>
</ol>
</div>
</div>
<a id="DMRUG850"></a><a id="class_tune_cost"></a>
<div class="props_rev_3"><a id="GUID-9AB480C2-2A88-4936-B01A-90A0666D49F0"></a>
<h3 id="DMRUG-GUID-9AB480C2-2A88-4936-B01A-90A0666D49F0" class="sect3">Cost</h3>
<div>
<p>The <span class="bold">Cost</span> tab in <span class="bold">Tune Settings</span> enables you to specify costs for target for scoring purposes.</p>
<div class="section">
<p>By default the cost matrix is initially generated based on all the known target values in the Build Data Source. The cost matrix is set to cost values of 1 to start with.</p>
<p>To specify costs:</p>
</div>
<!-- class="section" -->
<ol>
<li class="stepexpand"><span>Open the <span class="wintitle">Properties</span> pane for the Build node. Right-click the node and select <span class="bold">Go to Properties.</span></span></li>
<li class="stepexpand"><span>In the <span class="wintitle">Test</span> section, select <span class="bold">Generate Select Test Results for Model Tuning</span> and run the node.</span></li>
<li class="stepexpand"><span>In the <span class="wintitle">Models</span> section, select the models that you want to tune and click <img width="16" height="16" src="img/GUID-342E87F0-3554-422E-8133-9EF07302D670-default.png" alt="tune selected models" title="tune selected models" />.</span></li>
<li class="stepexpand"><span>Select <span class="bold">Tune</span> from the drop-down list. The <span class="wintitle">Tune Settings</span> dialog box opens.</span></li>
<li class="stepexpand"><span>In the <span class="wintitle">Tune Settings</span> dialog box, go to the <span class="bold">Cost</span> tab.</span></li>
<li class="stepexpand"><span>If you are tuning more than one model, then select a model from the <span class="bold">Models</span> list in the bottom pane. After you tune the first model, return to this pane and select another model.</span></li>
<li class="stepexpand"><span>Select the target value for which to specify costs.</span></li>
<li class="stepexpand"><span>Select the appropriate option:</span>
<div>
<ul style="list-style-type: disc;">
<li>
<p><span class="bold">False Positive: Incorrectly identifying a case as a target.</span> (Default)</p>
</li>
<li>
<p><span class="bold">False Negative: Incorrectly identifying a case as a non-target.</span></p>
</li>
</ul>
</div>
</li>
<li class="stepexpand"><span>In the <span class="bold">Weight</span> field, specify a weight for the cost.</span></li>
<li class="stepexpand"><span>Click <span class="bold">Apply</span> to add the cost that you just specified to the cost matrix.</span></li>
<li class="stepexpand"><span>Define costs for all target values that you are concerned about.</span></li>
<li class="stepexpand"><span>To apply the matrix, click <span class="bold">Tune</span> in the upper right pane.</span></li>
<li class="stepexpand"><span>Click <span class="bold">Derived Matrix</span> to view the cost matrix that you created. Examine the derived cost matrix. You can continue tuning by changing any selections that you made.</span></li>
<li class="stepexpand"><span>When you have finished, click <span class="bold">OK</span> to accept the tuning. Click <span class="bold">Cancel</span> to cancel the tuning</span></li>
</ol>
<div class="section">
<p>To cancel the tuning, click <span class="bold">Reset.</span> Tuning returns to Automatic.</p>
<p>To see the impact of the tuning, rerun the model node.</p>
</div>
<!-- class="section" --></div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-A1F7B9AE-F264-4F47-8DD5-995B0FC54922">Costs and Benefits</a><br />
In a classification problem, you must specify the cost or benefit associated with correct or incorrect classifications.</li>
</ul>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-89B07B20-C62C-4124-BD83-50F020F2E6F3">Tuning Classification Models</a></li>
</ul>
</div>
</div>
<a id="DMRUG851"></a>
<div class="props_rev_3"><a id="GUID-A1F7B9AE-F264-4F47-8DD5-995B0FC54922"></a>
<h4 id="DMRUG-GUID-A1F7B9AE-F264-4F47-8DD5-995B0FC54922" class="sect4">Costs and Benefits</h4>
<div>
<p>In a classification problem, you must specify the cost or benefit associated with correct or incorrect classifications.</p>
<p>By doing so, it is valuable when the cost of different misclassification varies significantly.</p>
<p>You can create a cost matrix to bias the model to minimize the cost or maximize the benefit. The cost/benefit matrix is taken into consideration when the model is scored.</p>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-D8F80215-FF7F-43F5-84B0-E1933E9A109A">Costs</a><br /></li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-697AB3C7-A418-47F7-8E17-AD733A1259D2">Benefits</a><br /></li>
</ul>
</div>
<a id="DMRUG852"></a>
<div class="props_rev_3"><a id="GUID-D8F80215-FF7F-43F5-84B0-E1933E9A109A"></a>
<h5 id="DMRUG-GUID-D8F80215-FF7F-43F5-84B0-E1933E9A109A" class="sect5">Costs</h5>
<div>
<div class="section">
<p>Suppose the problem is to predict whether a customer is likely to respond to a promotional mail. The target has two categories: YES (the customer responds) and NO (the customer does not respond). Suppose a positive response to the promotion generates $500 and that it costs $5 to do the mailing. After building the model, you compare the model predictions with actual data held aside for testing. At this point, you can evaluate the relative cost of different misclassifications:</p>
<ul style="list-style-type: disc;">
<li>
<p>If the model predicts YES and the actual value is YES, then the cost of misclassification is $0.</p>
</li>
<li>
<p>If the model predicts YES and the actual value is NO, then the cost of misclassification is $5.</p>
</li>
<li>
<p>If the model predicts NO and the actual value is YES, then the cost of misclassification is $495.</p>
</li>
<li>
<p>If the model predicts NO and the actual value is NO, then the cost is $0.</p>
</li>
</ul>
</div>
<!-- class="section" --></div>
</div>
<a id="DMRUG853"></a>
<div class="props_rev_3"><a id="GUID-697AB3C7-A418-47F7-8E17-AD733A1259D2"></a>
<h5 id="DMRUG-GUID-697AB3C7-A418-47F7-8E17-AD733A1259D2" class="sect5">Benefits</h5>
<div>
<div class="section">
<p>Using the same costs, you can approach the relative value of the outcomes from a benefits perspective. When you correctly predict a YES (a responder), the benefit is $495. When you correctly predict a NO (a non-responder), the benefit is $5.00 because you can avoid sending out the mailing. Because the goal is to find the lowest cost solution, benefits are represented as negative numbers.</p>
</div>
<!-- class="section" --></div>
</div>
</div>
</div>
<a id="DMRUG854"></a><a id="class_tune_benefit"></a>
<div class="props_rev_3"><a id="GUID-5D20C102-CDA3-4195-81BE-D0378E6CAEC5"></a>
<h3 id="DMRUG-GUID-5D20C102-CDA3-4195-81BE-D0378E6CAEC5" class="sect3">Benefit</h3>
<div>
<p>In the <span class="bold">Benefit</span> tab, you can specify a benefit for each value of the target. Specifying benefits is useful when there are many target values.</p>
<div class="section">
<p>The <span class="bold">Benefit</span> tab enables you to:</p>
</div>
<!-- class="section" -->
<div class="section">
<ul style="list-style-type: disc;">
<li>
<p>Specify a benefit for each value of the target. The values specified are applied to the cost benefit matrix.</p>
</li>
<li>
<p>Indicate the most important values.</p>
</li>
</ul>
<p>To tune a model using the <span class="bold">Benefit</span> tab:</p>
</div>
<!-- class="section" -->
<ol>
<li class="stepexpand"><span>Open the <span class="bold">Properties</span> pane for the Build node. Right-click the node and select <span class="bold">Go to Properties.</span></span></li>
<li class="stepexpand"><span>In the <span class="bold">Test</span> section, select <span class="bold">Generate Select Test Results for Model Tuning</span> and run the node.</span></li>
<li class="stepexpand"><span>In the <span class="bold">Models</span> section, select the models that you want to tune and click <img width="16" height="16" src="img/GUID-342E87F0-3554-422E-8133-9EF07302D670-default.png" alt="tune selected models" title="tune selected models" />.</span></li>
<li class="stepexpand"><span>Select <span class="bold">Tune</span> from the drop-down list. The <span class="wintitle">Tune Settings</span> dialog box opens in a new tab.</span></li>
<li class="stepexpand"><span>In the <span class="wintitle">Tune Settings</span> dialog box, click <span class="bold">Benefit.</span></span></li>
<li class="stepexpand"><span>If you are tuning more than one model, then select a model from the <span class="bold">Models</span> list in the bottom pane. After you tune the first model, return to this pane and select another model.</span></li>
<li class="stepexpand"><span>Select the target value for tuning from the <span class="bold">Target Value</span> list.</span></li>
<li class="stepexpand"><span>Specify benefit values for the target value selected. Benefit values can be positive or negative. If there is more benefit from a target value, then the benefit value should be higher than other benefit values. The default benefit value for each target value is <code class="codeph">0.</code></span>
<div>
<p>Enter the benefit value for the selected target in the <span class="bold">Benefit</span> box and click <span class="bold">Apply</span> to update the Cost Benefits matrix.</p>
</div>
</li>
<li class="stepexpand"><span>When you have finished specifying benefit values, click <span class="bold">Tune</span> in the right-hand column.</span></li>
<li class="stepexpand"><span>Click <span class="bold">View</span> to see the derived cost matrix.</span></li>
<li class="stepexpand"><span>When you have finished, click <span class="bold">OK</span> to accept the tuning, or click <span class="bold">Cancel</span> to cancel the tuning.</span></li>
</ol>
</div>
<div>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-A1F7B9AE-F264-4F47-8DD5-995B0FC54922">Costs and Benefits</a></li>
<li><a href="testing-and-tuning-models.htm#GUID-89B07B20-C62C-4124-BD83-50F020F2E6F3">Tuning Classification Models</a></li>
</ul>
</div>
</div>
</div>
<a id="DMRUG855"></a><a id="class_tune_roc"></a>
<div class="props_rev_3"><a id="GUID-7FC8A45B-BCFA-4CF2-AB6E-37A91F5B6E7D"></a>
<h3 id="DMRUG-GUID-7FC8A45B-BCFA-4CF2-AB6E-37A91F5B6E7D" class="sect3">ROC</h3>
<div>
<p>ROC is only supported for binary models.</p>
<p>The <span class="bold">ROC Tuning</span> tab adds a side panel to the standard ROC Test Viewer. The following information is displayed:</p>
<ul style="list-style-type: disc;">
<li>
<p>Performance Matrix in the upper right pane, displays these matrices:</p>
<ul style="list-style-type: disc;">
<li>
<p>Overall Accuracy: Cost matrix for the maximum Overall Accuracy point on the ROC chart.</p>
</li>
<li>
<p>Average Accuracy: Cost matrix for the maximum Average Accuracy point.</p>
</li>
<li>
<p>Custom Accuracy: Cost matrix for the custom operating point.</p>
<p>You must specify a custom operating point for this option to be available.</p>
</li>
<li>
<p>Model Accuracy: The current Performance Matrix (approximately) of the current model.</p>
<p>You can use the following calculation to derive Model Accuracy from the ROC result provided:</p>
<p>If there is no embedded cost matrix, then find the 50 percent threshold point or the closest one to it. If there is an embedded cost matrix, then find the lowest cost point. For a model to have an embedded cost matrix, it must have either been tuned or it has a cost matrix or cost benefit defined by the default settings of the Build node.</p>
</li>
</ul>
</li>
<li>
<p>The <span class="wintitle">Performance Matrix</span> grid shows the performance matrix for the option selected.</p>
</li>
<li>
<p>Click <span class="bold">Tune</span> to:</p>
<ul style="list-style-type: disc;">
<li>
<p>Select the current performance option as the one to use to tune the model.</p>
</li>
<li>
<p>Derive a cost matrix from the ROC result at that probability threshold.</p>
</li>
</ul>
<p>Tune Settings, in the lower part of this panel, is updated to display the new matrix.</p>
</li>
<li>
<p>Click <span class="bold">Clear</span> to clear any tuning specifications and set tuning to Automatic. In other words, no tuning is performed.</p>
</li>
</ul>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-040CCD55-BC16-4900-83C3-1EC05BB908EA">ROC Tuning Steps</a><br />
Lists the procedure to perform ROC tuning.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-3522EBC2-B262-4264-8600-423B671B6BE1">Receiver Operating Characteristics</a><br />
Receiver Operating Characteristics (ROC) is a method for experimenting with changes in the probability threshold and observing the resultant effect on the predictive power of the model.</li>
</ul>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-36BBB501-3A2C-4112-8367-3E2ACA77677C">Select Custom Operating Point</a></li>
</ul>
</div>
</div>
<a id="DMRUG856"></a><a id="class_tune_roc_steps"></a>
<div class="props_rev_3"><a id="GUID-040CCD55-BC16-4900-83C3-1EC05BB908EA"></a>
<h4 id="DMRUG-GUID-040CCD55-BC16-4900-83C3-1EC05BB908EA" class="sect4">ROC Tuning Steps</h4>
<div>
<p>Lists the procedure to perform ROC tuning.</p>
<div class="section">
<p>To perform ROC tuning:</p>
</div>
<!-- class="section" -->
<ol>
<li class="stepexpand"><span>Open the <span class="bold">Properties</span> pane for the Build node. Right-click the node and select <span class="bold">Go to Properties.</span></span></li>
<li class="stepexpand"><span>In the <span class="bold">Test</span> section, select <span class="bold">Generate Select Test Results for Model Tuning</span> and run the node.</span></li>
<li class="stepexpand"><span>In the <span class="bold">Models</span> section, select the models that you want to tune and click <img width="16" height="16" src="img/GUID-342E87F0-3554-422E-8133-9EF07302D670-default.png" alt="tune selected models" title="tune selected models" />.</span></li>
<li class="stepexpand"><span>Select <span class="bold">Tune</span> from the drop-down list. The <span class="wintitle">Tune Settings</span> dialog box opens in a new tab.</span></li>
<li class="stepexpand"><span>In the <span class="wintitle">Tune Settings</span> dialog box, go to the <span class="bold">ROC</span> tab.</span></li>
<li class="stepexpand"><span>If you are tuning more than one model, then select a model from the <span class="bold">Models</span> list in the bottom pane. After you tune the first model, return to this pane and select another model.</span></li>
<li class="stepexpand"><span>Select a target value. In the case of ROC, there are only two values.</span></li>
<li class="stepexpand"><span>Select a custom operating point if you do no want to use the default point.</span></li>
<li class="stepexpand"><span>Select the kind of Performance Matrix to use:</span>
<div>
<ul style="list-style-type: disc;">
<li>
<p><span class="bold">Overall Accuracy</span> (default)</p>
</li>
<li>
<p><span class="bold">Average Accuracy</span></p>
</li>
<li>
<p><span class="bold">Custom Accuracy.</span> Fill in the values for the Performance Matrix if you select this option.</p>
</li>
<li>
<p><span class="bold">Model Accuracy</span></p>
</li>
</ul>
</div>
</li>
<li class="stepexpand"><span>Click <span class="bold">Tune.</span> New tune settings are displayed in the same panel as the Performance Matrix. Examine the Derived Cost Matrix. You can continue tuning by changing any selections that you made.</span></li>
<li class="stepexpand"><span>When you have finished, click <span class="bold">OK</span> to accept the tuning, or click <span class="bold">Cancel</span> to cancel the tuning.</span>
<div>
<ul style="list-style-type: disc;">
<li>
<p>To reset the tuning, click <span class="bold">Reset.</span></p>
</li>
<li>
<p>To see the impact of the tuning, run the Model node.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-36BBB501-3A2C-4112-8367-3E2ACA77677C">Select Custom Operating Point</a><br />
The <span class="wintitle">Specify Custom Threshold</span> dialog box allows you to edit the custom operating point for all the models in the node.</li>
</ul>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-89B07B20-C62C-4124-BD83-50F020F2E6F3">Tuning Classification Models</a></li>
</ul>
</div>
</div>
<a id="DMRUG857"></a><a id="class_tune_roc_cust_op"></a>
<div class="props_rev_3"><a id="GUID-36BBB501-3A2C-4112-8367-3E2ACA77677C"></a>
<h5 id="DMRUG-GUID-36BBB501-3A2C-4112-8367-3E2ACA77677C" class="sect5">Select Custom Operating Point</h5>
<div>
<p>The <span class="wintitle">Specify Custom Threshold</span> dialog box allows you to edit the custom operating point for all the models in the node.</p>
<div class="section">
<ul style="list-style-type: disc;">
<li>
<p>To change the <span class="bold">Hit Rate</span> or <span class="bold">False Alarm,</span> click the appropriate option and adjust the value that you want to use.</p>
</li>
<li>
<p>Alternatively, you can specify the <span class="bold">False Positive</span> or <span class="bold">False Negative</span> ratio. To do this, click the appropriate option and specify the ratio.</p>
</li>
</ul>
<p>Click <span class="bold">OK</span> when you have finished.</p>
</div>
<!-- class="section" --></div>
</div>
</div>
<a id="DMRUG858"></a>
<div class="props_rev_3"><a id="GUID-3522EBC2-B262-4264-8600-423B671B6BE1"></a>
<h4 id="DMRUG-GUID-3522EBC2-B262-4264-8600-423B671B6BE1" class="sect4">Receiver Operating Characteristics</h4>
<div>
<p>Receiver Operating Characteristics (ROC) is a method for experimenting with changes in the probability threshold and observing the resultant effect on the predictive power of the model.</p>
<ul style="list-style-type: disc;">
<li>
<p>The horizontal axis of an ROC graph measures the False Positive Rate as a percentage.</p>
</li>
<li>
<p>The vertical axis shows the True Positive Rate.</p>
</li>
<li>
<p>The top left corner is the optimal location in an ROC curve, indicating a high TP (True Positive) rate versus low FP (False Positive) rate.</p>
</li>
<li>
<p>The area under the ROC curve measures the discriminating ability of a binary Classification model. This measure is especially useful for data sets with an unbalanced target distribution (one target class dominates the other). The larger the area under the curve, the higher the likelihood that an actual positive case is assigned a higher probability of being positive than an actual negative case.</p>
</li>
</ul>
<p>ROC curves are similar to lift charts in that they provide a means of comparison between individual models, and then determine thresholds that yield a high proportion of positive hits. ROC was originally used in signal detection theory to gauge the true hit versus false alarm ratio when sending signals over a noisy channel.</p>
</div>
</div>
</div>
<a id="DMRUG859"></a><a id="class_tune_lift"></a>
<div class="props_rev_3"><a id="GUID-48FCB234-AEB4-445C-94BC-19DD3E1889C7"></a>
<h3 id="DMRUG-GUID-48FCB234-AEB4-445C-94BC-19DD3E1889C7" class="sect3">Lift</h3>
<div>
<p>Lift measures the degree to which the predictions of a Classification model are better than randomly generated predictions.</p>
<div class="section">
<p>To tune a model using Lift:</p>
</div>
<!-- class="section" -->
<ol>
<li class="stepexpand"><span>Open the <span class="bold">Properties</span> pane for the Build node. Right-click the node and select <span class="bold">Go to Properties</span>.</span></li>
<li class="stepexpand"><span>In the <span class="bold">Test</span> section, select <span class="bold">Generate Select Test Results for Model Tuning</span> and run the node.</span></li>
<li class="stepexpand"><span>In the <span class="bold">Models</span> section, select the models that you want to tune and click <img width="16" height="16" src="img/GUID-342E87F0-3554-422E-8133-9EF07302D670-default.png" alt="tune selected models" title="tune selected models" />.</span></li>
<li class="stepexpand"><span>Select <span class="bold">Tune</span> from the drop-down list. The <span class="wintitle">Tune Settings</span> dialog box opens in a new tab.</span></li>
<li class="stepexpand"><span>In <span class="wintitle">Tune Settings</span> dialog box, go to the <span class="bold">Lift</span> tab.</span></li>
<li class="stepexpand"><span>If you are tuning more than one model, then select a model from the Models list in the bottom pane. After you tune the first model, return to this pane and select another model.</span></li>
<li class="stepexpand"><span>Select the target value for tuning from the <span class="bold">Target Value</span> list.</span></li>
<li class="stepexpand"><span>Decide whether to tune using the Cumulative Positive Cases chart, the default or the Cumulative Lift Chart. Select the chart from the <span class="bold">Display</span> list.</span>
<div>
<p>Either chart displays several curves: the lift curve for the model that you are tuning, ideal lift, and random lift, which is the lift from a model where predictions are random.</p>
<p>The chart also displays a blue vertical line that indicates the threshold, the quantile of interest.</p>
</div>
</li>
<li class="stepexpand"><span>Selected a quantile using the slider in the quantile display below the lift chart. As you move the slider, the blue vertical bar moves to that quantile, and the tuning panel is updated with the Performance Matrix for that point.</span></li>
<li class="stepexpand"><span>Click <span class="bold">Tune,</span> below the Performance Matrix. New tune settings are displayed in the same panel as the Performance Matrix. Examine the Derived Cost Matrix. You can continue tuning by changing any selections that you made.</span></li>
<li class="stepexpand"><span>When you have finished, click <span class="bold">OK</span> to accept the tuning, or click <span class="bold">Cancel</span> to cancel the tuning.</span>
<div>
<ul style="list-style-type: disc;">
<li>
<p>To reset the tuning, click <span class="bold">Reset.</span></p>
</li>
<li>
<p>To see the impact of the tuning, run the Model node.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-766E8BD7-7981-4EFD-B9D9-0B7CDAB348EA">About Lift</a><br />
Lift is the ratio of positive responders in a segment to the positive responders in the population as a whole.</li>
</ul>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-89B07B20-C62C-4124-BD83-50F020F2E6F3">Tuning Classification Models</a></li>
</ul>
</div>
</div>
<a id="DMRUG860"></a>
<div class="props_rev_3"><a id="GUID-766E8BD7-7981-4EFD-B9D9-0B7CDAB348EA"></a>
<h4 id="DMRUG-GUID-766E8BD7-7981-4EFD-B9D9-0B7CDAB348EA" class="sect4">About Lift</h4>
<div>
<p>Lift is the ratio of positive responders in a segment to the positive responders in the population as a whole.</p>
<p>For example, if a population has a predicted response rate of 20 percent, but one segment of the population has a predicted response rate of 60 percent, then the lift of that segment is 3 (60 percent/20 percent). Lift measures the following:</p>
<ul style="list-style-type: disc;">
<li>
<p>The concentration of positive predictions within segments of the population and specifies the improvement over the rate of positive predictions in the population as a whole.</p>
</li>
<li>
<p>The performance of targeting models in marketing applications. The purpose of a targeting model is to identify segments of the population with potentially high concentrations of positive responders to a marketing campaign.</p>
</li>
</ul>
<p>The notion of lift implies a binary target: either a Responder or a Non-responder, which means either YES or NO. Lift can be computed for multiclass targets by designating a preferred positive class and combining all other target class values, effectively turning a multiclass target into a binary target. Lift can be applied to both binary and non-binary classifications as well.</p>
<p>The calculation of lift begins by applying the model to test data in which the target values are already known. Then, the predicted results are sorted in order of probability, from highest to lowest Predictive Confidence. The ranked list is divided into quantiles (equal parts). The default number of quantiles is <code class="codeph">100.</code></p>
</div>
</div>
</div>
<a id="DMRUG861"></a><a id="class_tune_profit"></a>
<div class="props_rev_3"><a id="GUID-190FE2F0-C9C8-4FDC-BE2F-D6F8D2BA78BD"></a>
<h3 id="DMRUG-GUID-190FE2F0-C9C8-4FDC-BE2F-D6F8D2BA78BD" class="sect3">Profit</h3>
<div>
<p>The <span class="bold">Profit</span> tab provides a method for maximizing profit.</p>
<div class="section">
<p>To tune a model<a id="d101138e5217" class="indexterm-anchor"></a>:</p>
</div>
<!-- class="section" -->
<ol>
<li class="stepexpand"><span>Open <span class="bold">Properties</span> for the Build node. Right-click the node and select <span class="bold">Go to Properties.</span></span></li>
<li class="stepexpand"><span>In the <span class="bold">Test</span> section, select <span class="bold">Generate Select Test Results for Model Tuning</span> and run the node.</span></li>
<li class="stepexpand"><span>In the <span class="bold">Models</span> section, select the models that you want to tune and click <img width="16" height="16" src="img/GUID-342E87F0-3554-422E-8133-9EF07302D670-default.png" alt="tune selected models" title="tune selected models" />.</span></li>
<li class="stepexpand"><span>Select <span class="bold">Tune</span> from the drop-down list. The <span class="wintitle">Tune Settings</span> dialog box opens in a new tab.</span></li>
<li class="stepexpand"><span>In the <span class="wintitle">Tune Settings</span> dialog box, go to the <span class="bold">Profit</span> tab.</span></li>
<li class="stepexpand"><span>If you are tuning more than one model, then select a model from the <span class="bold">Models</span> list in the bottom pane. After you tune the first model, return to this pane and select another model.</span></li>
<li class="stepexpand"><span>Select the target value for tuning from the <span class="bold">Target Value</span> list.</span></li>
<li class="stepexpand"><span>Click <span class="bold">Profit Settings</span> and specify the values in the <span class="wintitle">Profit Settings</span> dialog box.</span></li>
<li class="stepexpand"><span>After you specify Profit Settings, the graph reflects the values that you specified.</span></li>
<li class="stepexpand"><span>Use the slider below the chart to adjust the Threshold (blue vertical line).</span></li>
<li class="stepexpand"><span>Click <span class="bold">Tune,</span> below the Performance Matrix. New tune settings are displayed in the same panel as the Performance Matrix. Examine the Derived Cost Matrix. You can continue tuning by changing any selections that you made.</span></li>
<li class="stepexpand"><span>When you have finished, click <span class="bold">OK</span> to accept the tuning, or click <span class="bold">Cancel</span> to cancel the tuning.</span>
<div>
<ul style="list-style-type: disc;">
<li>
<p>To reset tuning, click <span class="bold">Reset.</span></p>
</li>
<li>
<p>To see the impact of the tuning, run the Model node.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-062BF458-32C4-4490-A548-073FD481BE82">Profit Setting</a><br />
In the <span class="wintitle">Profit Setting</span> dialog box, you can change default values for profit related settings.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-660EC43B-F319-4047-ADF7-554497810530">Profit</a><br />
Profit provides a method for maximizing profit.</li>
</ul>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-89B07B20-C62C-4124-BD83-50F020F2E6F3">Tuning Classification Models</a></li>
</ul>
</div>
</div>
<a id="DMRUG862"></a><a id="class_tune_profit_setting"></a>
<div class="props_rev_3"><a id="GUID-062BF458-32C4-4490-A548-073FD481BE82"></a>
<h4 id="DMRUG-GUID-062BF458-32C4-4490-A548-073FD481BE82" class="sect4">Profit Setting</h4>
<div>
<p>In the <span class="wintitle">Profit Setting</span> dialog box, you can change default values for profit related settings.</p>
<div class="section">
<p>The default values for Startup Cost, Incremental Revenue, Incremental Cost, and Budget are all <code class="codeph">1.</code> The default value for Population is <code class="codeph">100.</code> Change these values to ones appropriate for your business problem.</p>
<p>Click <span class="bold">OK.</span></p>
</div>
<!-- class="section" --></div>
</div>
<a id="DMRUG863"></a>
<div class="props_rev_3"><a id="GUID-660EC43B-F319-4047-ADF7-554497810530"></a>
<h4 id="DMRUG-GUID-660EC43B-F319-4047-ADF7-554497810530" class="sect4">Profit</h4>
<div>
<p>Profit provides a method for maximizing profit.</p>
<p>You can specify the information listed below. Oracle Data Miner uses these information to create a <a id="d101138e5467" class="indexterm-anchor"></a>cost matrix that optimizes profit:</p>
<ul style="list-style-type: disc;">
<li>
<p>Startup cost</p>
</li>
<li>
<p>Incremental revenue</p>
</li>
<li>
<p>Incremental cost</p>
</li>
<li>
<p>Budget</p>
</li>
<li>
<p>Population</p>
</li>
</ul>
</div>
<div>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-6794F8C2-A190-4C24-8BC1-3D009EF6DB3D">Profit Setting Dialog</a></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<a id="DMRUG864"></a><a id="regress_test"></a>
<div class="props_rev_3"><a id="GUID-193209C6-6440-4881-9492-65D25DD401EC"></a>
<h2 id="DMRUG-GUID-193209C6-6440-4881-9492-65D25DD401EC" class="sect2">Testing Regression Models</h2>
<div>
<p>Regression models are tested by comparing the predicted values to known target values in a set of test data.</p>
<p>The historical data for a regression project is typically divided into two data sets:</p>
<ul style="list-style-type: disc;">
<li>
<p>One for building the model</p>
</li>
<li>
<p>One for testing the model</p>
</li>
</ul>
<p>The test data must be compatible with the data used to build the model and must be prepared in the same way that the build data was prepared.</p>
<p>The ways to test Classification and Regression models:</p>
<ul style="list-style-type: disc;">
<li>
<p>By splitting the input data into build data and test data. This is the default. The test data is created by randomly splitting the build data into two subsets. 40 percent of the input data is used for test data.</p>
</li>
<li>
<p>By using all the build data as the test data.</p>
</li>
<li>
<p>By attaching two Data Source nodes to the build node.</p>
<ul style="list-style-type: disc;">
<li>
<p>The first data source, that you connect to the build node, is the source of build data.</p>
</li>
<li>
<p>The second node that you connect is the source of test data.</p>
</li>
</ul>
</li>
<li>
<p>By deselecting <span class="bold">Perform Test</span> in the <span class="wintitle">Test</span> section of the <span class="wintitle">Properties</span> pane and then using a Test node. By default, all Classification and Regression models are tested.</p>
</li>
</ul>
<p>Test settings specify which metrics to calculate and control the calculation of the metrics.</p>
<p>Oracle Data Mining provides several kinds of information to assess Regression models:</p>
<ul style="list-style-type: disc;">
<li>
<p>Residual Plot</p>
</li>
<li>
<p>Regression Statistics</p>
</li>
<li>
<p>Regression Model Test Viewer</p>
</li>
<li>
<p>Compare Regression test Results</p>
</li>
</ul>
<p>To view test results, first test the model or models in the node:</p>
<ul style="list-style-type: disc;">
<li>
<p>If you tested the models using the default test in the Regression node, then run the node and then right-click the node. Select <span class="bold">View Test Results</span> and select the model that you are interest in. The Regression Model Test viewer opens. To compare the test results for all models in the node, select <span class="bold">Compare Test Results.</span></p>
</li>
<li>
<p>If you tested the models using a Test node, then run the Test node and then right-click the node. Select <span class="bold">View Test Results</span> and select the model that you are interested in. The Regression Model Test viewer opens. To compare the test results for all models in the node, select <span class="bold">Compare Test Results.</span></p>
</li>
</ul>
<p>You can also compare test results by going to the <span class="wintitle">Models</span> section of the <span class="wintitle">Properties</span> pane of the Build node where you tested the models and click <img width="13" height="16" src="img/GUID-246FB100-36EE-4322-9F80-FE5873B74DBE-default.png" alt="compare" title="compare" />.</p>
</div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-773A6D4D-BD05-49EA-901B-8141A37899D0">Residual Plot</a><br />
The residual plot is a scatter plot of the residuals.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-E90846D1-B49C-4DCB-9546-5DDD9CA7607E">Regression Statistics</a><br />
Oracle Data Mining calculates the statistics Root Mean Squared Error and Mean Absolute Error to help the assessment of the overall quality of Regressions models.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-4C9A2836-7D77-4FF4-8BAA-A31E3F1EF38B">Compare Regression Test Results</a><br />
You can compare the results of a Regression test for all models that are in a Regression node as well as in a Test node.</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-CF9CEFB7-EA94-4DE1-9846-DF43A7B046F1">Regression Model Test Viewer</a><br />
You can view the results of a regression model test in the <span class="wintitle">Regression Model Test Viewer.</span></li>
</ul>
</div>
<a id="DMRUG865"></a>
<div class="props_rev_3"><a id="GUID-773A6D4D-BD05-49EA-901B-8141A37899D0"></a>
<h3 id="DMRUG-GUID-773A6D4D-BD05-49EA-901B-8141A37899D0" class="sect3">Residual Plot</h3>
<div>
<p>The residual plot is a scatter plot of the residuals.</p>
<p>Each residual is the difference between the actual value and the value predicted by the model. Residuals can be positive or negative. If residuals are small (close to 0), then the predictions are accurate. A residual plot may indicate that predictions are better for some classes of values than others.</p>
</div>
</div>
<a id="DMRUG866"></a><a id="regress_test_stats"></a>
<div class="props_rev_3"><a id="GUID-E90846D1-B49C-4DCB-9546-5DDD9CA7607E"></a>
<h3 id="DMRUG-GUID-E90846D1-B49C-4DCB-9546-5DDD9CA7607E" class="sect3">Regression Statistics</h3>
<div>
<p>Oracle Data Mining calculates the statistics Root Mean Squared Error and Mean Absolute Error to help the assessment of the overall quality of Regressions models.</p>
<ul style="list-style-type: disc;">
<li>
<p>Root Mean Squared Error: The square root of the average squared distance of a data point from the fitted line.</p>
</li>
<li>
<p>Mean Absolute Error: The average of the absolute value of the residuals (error). The Mean Absolute Error is very similar to the Root Mean Square Error but is less sensitive to large errors.</p>
</li>
</ul>
</div>
</div>
<a id="DMRUG871"></a><a id="regress_test_results"></a>
<div class="props_rev_3"><a id="GUID-4C9A2836-7D77-4FF4-8BAA-A31E3F1EF38B"></a>
<h3 id="DMRUG-GUID-4C9A2836-7D77-4FF4-8BAA-A31E3F1EF38B" class="sect3">Compare Regression Test Results</h3>
<div>
<p>You can compare the results of a Regression test for all models that are in a Regression node as well as in a Test node.</p>
<div class="section">
<p>To compare test results for all the models in a Regression Build node:</p>
</div>
<!-- class="section" -->
<div class="section">
<ul style="list-style-type: disc;">
<li>
<p>If you tested the models when you ran the Regression node, then:</p>
<ul style="list-style-type: disc;">
<li>
<p>Right-click the Regression node that contains the models.</p>
</li>
<li>
<p>Select <span class="bold">Compare Test Results.</span></p>
</li>
</ul>
</li>
<li>
<p>If you tested the Regression models in a Test node, then:</p>
<ul style="list-style-type: disc;">
<li>
<p>Right-click the Test node that tests the models.</p>
</li>
<li>
<p>Select <span class="bold">Compare Test Results.</span></p>
</li>
</ul>
</li>
</ul>
</div>
<!-- class="section" --></div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-5582CEE1-1E65-4FE4-A205-CB2B02829B51">Compare Test Results</a><br />
When you compare test results for two or more Regression models, each model has a color associated with it. This color indicates the results for that model.</li>
</ul>
</div>
<a id="DMRUG872"></a><a id="regress_test_results_compare"></a>
<div class="props_rev_3"><a id="GUID-5582CEE1-1E65-4FE4-A205-CB2B02829B51"></a>
<h4 id="DMRUG-GUID-5582CEE1-1E65-4FE4-A205-CB2B02829B51" class="sect4">Compare Test Results</h4>
<div>
<p>When you compare test results for two or more Regression models, each model has a color associated with it. This color indicates the results for that model.</p>
<p>For example, if model M1 has purple associated with it, then the bar graphs on the <span class="bold">Performance</span> tab for M1 is displayed in purple.</p>
<p>By default, test results for all models in the node are compared. If you do not want to compare all test results, then click <img width="13" height="16" src="img/GUID-246FB100-36EE-4322-9F80-FE5873B74DBE-default.png" alt="compare" title="compare" />. The <span class="wintitle">Edit Test Results Selection</span> dialog box opens. Deselect results that you do not want to see. Click <span class="bold">OK</span> when you have finished.</p>
<p>Compare Test Results opens in a new tab. Results are displayed in two tabs:</p>
<ul style="list-style-type: disc;">
<li>
<p><span class="bold">Performance</span> tab: The following metrics are compared on the <span class="bold">Performance</span> tab:</p>
<ul style="list-style-type: disc;">
<li>
<p>Predictive Confidence for Classification Models</p>
</li>
<li>
<p>Mean Absolute Error</p>
</li>
<li>
<p>Mean Predicted Value</p>
</li>
</ul>
<p>By default, test results for all models are compared. To edit the list of models, click <img width="13" height="16" src="img/GUID-246FB100-36EE-4322-9F80-FE5873B74DBE-default.png" alt="compare" title="compare" /> above pane that lists the models to open the <span class="wintitle">Edit Test Selection (Classification and Regression</span> dialog box.</p>
</li>
<li>
<p><span class="bold">Residual</span> tab: Displays the residual plot for each model.</p>
<ul style="list-style-type: disc;">
<li>
<p>You can compare two plots side by side. By default, test results for all models are compared.</p>
</li>
<li>
<p>To edit the list of models, click <img width="13" height="16" src="img/GUID-246FB100-36EE-4322-9F80-FE5873B74DBE-default.png" alt="compare" title="compare" /> above pane that lists the models to open the <span class="wintitle">Edit Test Selection (Classification and Regression</span> dialog box.</p>
</li>
</ul>
</li>
</ul>
</div>
<div>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-E90846D1-B49C-4DCB-9546-5DDD9CA7607E">Regression Statistics</a></li>
<li><a href="testing-and-tuning-models.htm#GUID-2744EB07-B54D-44C9-9E9B-C8D84465824C">Predictive Confidence</a></li>
<li><a href="testing-and-tuning-models.htm#GUID-1EC39895-8D19-4BCE-9F13-0BA56C2009CC">Edit Test Selection (Classification and Regression)</a></li>
</ul>
</div>
</div>
</div>
</div>
<a id="DMRUG868"></a><a id="regress_test_viewer"></a>
<div class="props_rev_3"><a id="GUID-CF9CEFB7-EA94-4DE1-9846-DF43A7B046F1"></a>
<h3 id="DMRUG-GUID-CF9CEFB7-EA94-4DE1-9846-DF43A7B046F1" class="sect3">Regression Model Test Viewer</h3>
<div>
<p>You can view the results of a regression model test in the <span class="wintitle">Regression Model Test Viewer.</span></p>
<div class="section">
<p>To view information in the <span class="wintitle">Regression Model Test Viewer:</span></p>
</div>
<!-- class="section" -->
<ol>
<li class="stepexpand"><span>Right-click a Regression node or a Test node (that tests Regression Models) and select <span class="bold">View Test Results</span> or <span class="bold">Compare Test Results.</span></span></li>
<li class="stepexpand"><span>The <span class="wintitle">Regression Model Test Viewer</span> opens, and displays the following tabs:</span>
<div>
<ul style="list-style-type: disc;">
<li>
<p>Performance</p>
</li>
<li>
<p>Residual</p>
</li>
</ul>
</div>
</li>
<li class="stepexpand"><span>Click <span class="bold">OK.</span></span></li>
</ol>
<div class="section"></div>
<!-- class="section" --></div>
<div>
<ul class="ullinks">
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-8EF5F0F3-F4C2-4B34-9938-BADA0C8A81A4">Performance (Regression)</a><br />
The <span class="bold">Performance</span> tab displays the test results for several common test metrics. For Regression models, it displays the measures for all models:</li>
<li class="ulchildlink"><a href="testing-and-tuning-models.htm#GUID-4E59DBAD-BBD5-4BA9-B977-76DFB0EC72F6">Residual</a><br />
The <span class="bold">Residual Plot</span> tab show the residual plot on a per-model basis.</li>
</ul>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-E90846D1-B49C-4DCB-9546-5DDD9CA7607E" title="Oracle Data Mining calculates the statistics Root Mean Squared Error and Mean Absolute Error to help the assessment of the overall quality of Regressions models.">Regression Statistics</a></li>
</ul>
</div>
</div>
<a id="DMRUG869"></a><a id="regress_test_viewer_performance"></a>
<div class="props_rev_3"><a id="GUID-8EF5F0F3-F4C2-4B34-9938-BADA0C8A81A4"></a>
<h4 id="DMRUG-GUID-8EF5F0F3-F4C2-4B34-9938-BADA0C8A81A4" class="sect4">Performance (Regression)</h4>
<div>
<p>The <span class="bold">Performance</span> tab displays the test results for several common test metrics. For Regression models, it displays the measures for all models:</p>
<p>The test metrics are:</p>
<ul style="list-style-type: disc;">
<li>
<p><span class="bold">All Measures</span> (default). The <span class="bold">Measure</span> list enables you to select the measures to display. By default, All Measures are displayed. The selected measures are displayed as graphs. If you are comparing test results for two or more models, then the different models have graphs in different colors.</p>
</li>
<li>
<p><span class="bold">Predictive Confidence:</span> Measures how much better the predictions of the mode are than those of the naive model. Predictive Confidence for regression is the same measure as Predictive Confidence for classification.</p>
</li>
<li>
<p><span class="bold">Mean Absolute Error</span></p>
</li>
<li>
<p><span class="bold">Root Mean Square Error</span></p>
</li>
<li>
<p><span class="bold">Mean Predicted Value:</span> The average of the predicted values.</p>
</li>
<li>
<p><span class="bold">Mean Actual Value:</span> The average of the actual values.</p>
</li>
</ul>
<p>Two <span class="bold">Sort By</span> lists specify sort attribute and sort order. The first <span class="bold">Sort By</span> list contains Measure, Creation Date, or Name (the default). The second <span class="bold">Sort By</span> list contains the sort order: ascending or descending (default).</p>
<p>The top pane displays these measures as histograms.</p>
<p>The bottom pane contains the Models grid that supplements the information presented in the graphs. You can minimize the table using the splitter line.</p>
<p>The Models grid has the following columns:</p>
<ul style="list-style-type: disc;">
<li>
<p>Name, the name of the model along with color of the model in the graphs.</p>
</li>
<li>
<p>Predictive Confidence</p>
</li>
<li>
<p>Mean Absolute Error</p>
</li>
<li>
<p>Root Mean Square Error</p>
</li>
<li>
<p>Mean Predicted Value</p>
</li>
<li>
<p>Mean Actual Value</p>
</li>
<li>
<p>Algorithm</p>
</li>
<li>
<p>Creation Date (and time)</p>
</li>
</ul>
<p>By default, results for the selected model are displayed. To change the list of models, click <img width="13" height="16" src="img/GUID-246FB100-36EE-4322-9F80-FE5873B74DBE-default.png" alt="compare" title="compare" /> and deselect any models for which you do not want to see results. If you deselect a model, then both the histograms and the detail information for that model are not displayed.</p>
</div>
<div>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-2744EB07-B54D-44C9-9E9B-C8D84465824C">Predictive Confidence</a></li>
<li><a href="testing-and-tuning-models.htm#GUID-E90846D1-B49C-4DCB-9546-5DDD9CA7607E">Regression Statistics</a></li>
</ul>
</div>
</div>
</div>
<a id="DMRUG870"></a><a id="regress_test_reidual"></a>
<div class="props_rev_3"><a id="GUID-4E59DBAD-BBD5-4BA9-B977-76DFB0EC72F6"></a>
<h4 id="DMRUG-GUID-4E59DBAD-BBD5-4BA9-B977-76DFB0EC72F6" class="sect4">Residual</h4>
<div>
<p>The <span class="bold">Residual Plot</span> tab show the residual plot on a per-model basis.</p>
<div class="section">
<p>By default, the residual plots are displayed as graph.</p>
<ul style="list-style-type: disc;">
<li>
<p>To see numeric results, click <img width="16" height="16" src="img/GUID-8E676C2C-814A-47FD-9B26-633AB619624C-default.png" alt="browse" title="browse" />.</p>
</li>
<li>
<p>To change the display back to a graph, click <img width="16" height="16" src="img/GUID-904BBEA7-5CF8-4724-921E-AF6CABA6FDBB-default.png" alt="ena" title="ena" />.</p>
</li>
<li>
<p>To see the plot for another model, select the model from the <span class="bold">Show</span> list and click <span class="bold">Query.</span></p>
</li>
</ul>
<p>You can control how the plot is displayed in several ways:</p>
<ul style="list-style-type: disc;">
<li>
<p>Select the information displayed on the y-axis and on the x-axis. The default depictions are:</p>
<ul style="list-style-type: disc;">
<li>
<p>X axis: Predicted Value</p>
</li>
<li>
<p>Y axis: Residual</p>
</li>
</ul>
<p>To change this, select information from the lists.</p>
</li>
<li>
<p>The default sample size is <code class="codeph">2000.</code> You can make this value larger or smaller.</p>
</li>
<li>
<p>You can compare plots side by side. The default is to not compare plots.</p>
</li>
</ul>
<p>If you change any of these fields, then click <span class="bold">Query</span> to see the results.</p>
<p>To compare plots side by side, select the model to compare with the current model from the <span class="bold">Compare</span> list and click <span class="bold">Query.</span> The residual plots are displayed side by side.</p>
<p>The bottom pane show the <span class="wintitle">Residual result summary table.</span> The table contains the Models grid which supplements the information presented in the plots. You can minimize the table using the splitter line.</p>
<p>The table has the following columns:</p>
<ul style="list-style-type: disc;">
<li>
<p>Model, the name of the model along with color of the model in the graphs</p>
</li>
<li>
<p>Predictive Confidence</p>
</li>
<li>
<p>Mean Absolute Error</p>
</li>
<li>
<p>Root Mean Square Error</p>
</li>
<li>
<p>Mean Predicted Value</p>
</li>
<li>
<p>Mean Actual Value</p>
</li>
<li>
<p>Algorithm</p>
</li>
<li>
<p>Creation Date (and time)</p>
</li>
</ul>
<p>By default, results for all models in the node are displayed. To change the list of models, click <img width="13" height="16" src="img/GUID-246FB100-36EE-4322-9F80-FE5873B74DBE-default.png" alt="compare" title="compare" /> to open the Edit Test Selection dialog box.</p>
</div>
<!-- class="section" --></div>
<div>
<div class="relinfo">
<p><strong>Related Topics</strong></p>
<ul>
<li><a href="testing-and-tuning-models.htm#GUID-773A6D4D-BD05-49EA-901B-8141A37899D0">Residual Plot</a></li>
<li><a href="testing-and-tuning-models.htm#GUID-1EC39895-8D19-4BCE-9F13-0BA56C2009CC">Edit Test Selection (Classification and Regression)</a></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<!-- class="ind" --><!-- Start Footer -->
</div>
<!-- add extra wrapper close div-->
<footer><!--
<hr />
<table class="cellalignment56">
<tr>
<td class="cellalignment63">
<table class="cellalignment61">
<tr>
<td class="cellalignment60"><a href="text-nodes.htm"><img width="24" height="24" src="../dcommon/gifs/leftnav.gif" alt="Go to previous page" /><br />
<span class="icon">Previous</span></a></td>
<td class="cellalignment60"><a href="data-mininig-algorithms.htm"><img width="24" height="24" src="../dcommon/gifs/rightnav.gif" alt="Go to next page" /><br />
<span class="icon">Next</span></a></td>
</tr>
</table>
</td>
<td class="cellalignment-copyrightlogo"><img width="144" height="18" src="../dcommon/gifs/oracle.gif" alt="Oracle" /><br />
Copyright&nbsp;&copy;&nbsp;2015, 2018, Oracle&nbsp;and/or&nbsp;its&nbsp;affiliates.&nbsp;All&nbsp;rights&nbsp;reserved.<br />
<a href="../dcommon/html/cpyr.htm">Legal Notices</a></td>
<td class="cellalignment65">
<table class="cellalignment59">
<tr>
<td class="cellalignment60"><a href="../index.htm"><img width="24" height="24" src="../dcommon/gifs/doclib.gif" alt="Go to Documentation Home" /><br />
<span class="icon">Home</span></a></td>
<td class="cellalignment60"><a href="../nav/portal_booklist.htm"><img width="24" height="24" src="../dcommon/gifs/booklist.gif" alt="Go to Book List" /><br />
<span class="icon">Book List</span></a></td>
<td class="cellalignment60"><a href="toc.htm"><img width="24" height="24" src="../dcommon/gifs/toc.gif" alt="Go to Table of Contents" /><br />
<span class="icon">Contents</span></a></td>
<td class="cellalignment60"><a href="index.htm"><img width="24" height="24" src="../dcommon/gifs/index.gif" alt="Go to Index" /><br />
<span class="icon">Index</span></a></td>
<td class="cellalignment60"><a href="../nav/mindx.htm"><img width="24" height="24" src="../dcommon/gifs/masterix.gif" alt="Go to Master Index" /><br />
<span class="icon">Master Index</span></a></td>
<td class="cellalignment60"><a href="../dcommon/html/feedback.htm"><img width="24" height="24" src="../dcommon/gifs/feedbck2.gif" alt="Go to Feedback page" /><br />
<span class="icon">Contact Us</span></a></td>
</tr>
</table>
</td>
</tr>
</table>
--></footer>
<noscript>
<p>Scripting on this page enhances content navigation, but does not change the content in any way.</p>
</noscript>
</body>
</html>
