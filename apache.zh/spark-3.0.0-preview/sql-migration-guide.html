<html class="no-js" ><head></head><body >﻿<!--<![endif]-->
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>迁移指南：SQL，数据集和DataFrame-Spark 3.0.0-预览文档</title>
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        
        <!-- Google analytics script -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-32518208-2']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
        </script>
        

    
    
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html"><img src="img/spark-logo-hd.png" style="height:50px"></a> <span class="version">3.0.0预览版</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">总览</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">编程指南<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">快速开始</a></li>
                                <li><a href="rdd-programming-guide.html">RDD，累加器，广播变量</a></li>
                                <li><a href="sql-programming-guide.html">SQL，数据框和数据集</a></li>
                                <li><a href="structured-streaming-programming-guide.html">结构化流</a></li>
                                <li><a href="streaming-programming-guide.html">火花流（DStreams）</a></li>
                                <li><a href="ml-guide.html">MLlib（机器学习）</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX（图形处理）</a></li>
                                <li><a href="sparkr.html">SparkR（Spark上的R）</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API文件<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">斯卡拉</a></li>
                                <li><a href="api/java/index.html">爪哇</a></li>
                                <li><a href="api/python/index.html">蟒蛇</a></li>
                                <li><a href="api/R/index.html">[R</a></li>
                                <li><a href="api/sql/index.html">SQL，内置函数</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">部署中<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">总览</a></li>
                                <li><a href="submitting-applications.html">提交申请</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark独立</a></li>
                                <li><a href="running-on-mesos.html">梅索斯</a></li>
                                <li><a href="running-on-yarn.html">纱</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">更多<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">组态</a></li>
                                <li><a href="monitoring.html">监控方式</a></li>
                                <li><a href="tuning.html">调音指南</a></li>
                                <li><a href="job-scheduling.html">作业调度</a></li>
                                <li><a href="security.html">安全</a></li>
                                <li><a href="hardware-provisioning.html">硬件配置</a></li>
                                <li><a href="migration-guide.html">迁移指南</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">建筑火花</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">为Spark贡献</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">第三方项目</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v3.0.0-preview</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                
                    <div class="left-menu-wrapper">
    <div class="left-menu">
        <h3><a href="migration-guide.html">迁移指南</a></h3>
        
<ul>

    <li>
        <a href="core-migration-guide.html">火花芯</a>
    </li>
    
    

    <li>
        <a href="sql-migration-guide.html">
            
                <b>SQL，数据集和数据框</b>
            
        </a>
    </li>
    
    

    <li>
        <a href="ss-migration-guide.html">结构化流</a>
    </li>
    
    

    <li>
        <a href="ml-migration-guide.html">MLlib（机器学习）</a>
    </li>
    
    

    <li>
        <a href="pyspark-migration-guide.html">PySpark（Spark上的Python）</a>
    </li>
    
    

    <li>
        <a href="sparkr-migration-guide.html">SparkR（Spark上的R）</a>
    </li>
    
    

</ul>

    </div>
</div>

                
                <input id="nav-trigger" class="nav-trigger" type="checkbox" checked>
                <label for="nav-trigger"></label>
                <div class="content-with-sidebar" id="content">
                    
                        <h1 class="title">迁移指南：SQL，数据集和DataFrame</h1>
                    

                    <ul id="markdown-toc">
  <li><a href="#upgrading-from-spark-sql-24-to-30" id="markdown-toc-upgrading-from-spark-sql-24-to-30">从Spark SQL 2.4升级到3.0</a></li>
  <li><a href="#upgrading-from-spark-sql-24-to-241" id="markdown-toc-upgrading-from-spark-sql-24-to-241">从Spark SQL 2.4升级到2.4.1</a></li>
  <li><a href="#upgrading-from-spark-sql-23-to-24" id="markdown-toc-upgrading-from-spark-sql-23-to-24">从Spark SQL 2.3升级到2.4</a></li>
  <li><a href="#upgrading-from-spark-sql-22-to-23" id="markdown-toc-upgrading-from-spark-sql-22-to-23">从Spark SQL 2.2升级到2.3</a></li>
  <li><a href="#upgrading-from-spark-sql-21-to-22" id="markdown-toc-upgrading-from-spark-sql-21-to-22">从Spark SQL 2.1升级到2.2</a></li>
  <li><a href="#upgrading-from-spark-sql-20-to-21" id="markdown-toc-upgrading-from-spark-sql-20-to-21">从Spark SQL 2.0升级到2.1</a></li>
  <li><a href="#upgrading-from-spark-sql-16-to-20" id="markdown-toc-upgrading-from-spark-sql-16-to-20">从Spark SQL 1.6升级到2.0</a></li>
  <li><a href="#upgrading-from-spark-sql-15-to-16" id="markdown-toc-upgrading-from-spark-sql-15-to-16">从Spark SQL 1.5升级到1.6</a></li>
  <li><a href="#upgrading-from-spark-sql-14-to-15" id="markdown-toc-upgrading-from-spark-sql-14-to-15">从Spark SQL 1.4升级到1.5</a></li>
  <li><a href="#upgrading-from-spark-sql-13-to-14" id="markdown-toc-upgrading-from-spark-sql-13-to-14">从Spark SQL 1.3升级到1.4</a></li>
  <li><a href="#upgrading-from-spark-sql-10-12-to-13" id="markdown-toc-upgrading-from-spark-sql-10-12-to-13">从Spark SQL 1.0-1.2升级到1.3</a></li>
  <li><a href="#compatibility-with-apache-hive" id="markdown-toc-compatibility-with-apache-hive">与Apache Hive的兼容性</a></li>
</ul>

<h2 id="upgrading-from-spark-sql-24-to-30">从Spark SQL 2.4升级到3.0</h2>
<ul>
  <li>
    <p>从Spark 3.0开始，将值插入具有不同数据类型的表列中时，将根据ANSI SQL标准执行类型强制转换。某些不合理的类型转换，例如转换<code>string</code>至<code>int</code>和<code>double</code>至<code>boolean</code>被禁止。如果该列的数据类型的值超出范围，则将引发运行时异常。在Spark 2.4及更低版本中，只要在表插入期间有效，就允许类型转换<code>Cast</code> 。当将超出范围的值插入整数字段时，将插入该值的低位（与Java / Scala数字类型转换相同）。例如，如果将257插入到字节类型的字段中，则结果为1。行为由选项控制<code>spark.sql.storeAssignmentPolicy</code> ，默认值为“ ANSI”。将选项设置为“旧版”可恢复以前的行为。</p>
  </li>
  <li>
    <p>在Spark 3.0中，不推荐使用的方法<code>SQLContext.createExternalTable</code>和<code>SparkSession.createExternalTable</code>已被移除以支持替换， <code>createTable</code> 。</p>
  </li>
  <li>
    <p>在Spark 3.0中，已弃用<code>HiveContext</code>类已删除。使用<code>SparkSession.builder.enableHiveSupport()</code>代替。</p>
  </li>
  <li>
    <p>从Spark 3.0开始，配置<code>spark.sql.crossJoin.enabled</code>成为内部配置，默认情况下为true，因此默认情况下，spark不会在具有隐式交叉联接的sql上引发异常。</p>
  </li>
  <li>
    <p>从Spark 3.0开始，我们将trim函数的参数顺序从<code>TRIM(trimStr, str)</code>至<code>TRIM(str, trimStr)</code>与其他数据库兼容。</p>
  </li>
  <li>
    <p>在Spark 2.4及更低版本中，SQL查询（例如<code>FROM <table></code>要么<code>FROM <table> UNION ALL FROM <table></code>被意外支持。蜂巢式<code>FROM <table> SELECT <expr></code> ， <code>SELECT</code>条款不可忽略。Hive和Presto都不支持此语法。因此，自Spark 3.0起，我们将这些查询视为无效。</p>
  </li>
  <li>
    <p>从Spark 3.0开始，Dataset和DataFrame API <code>unionAll</code>不再被弃用。这是别名<code>union</code> 。</p>
  </li>
  <li>
    <p>在Spark 2.4和更低版本中，JSON数据源的解析器将某些数据类型的空字符串视为null，例如<code>IntegerType</code> 。对于<code>FloatType</code>和<code>DoubleType</code> ，它在空字符串上失败并引发异常。从Spark 3.0开始，我们禁止使用空字符串，并且会抛出数据类型以外的异常<code>StringType</code>和<code>BinaryType</code> 。</p>
  </li>
  <li>
    <p>自Spark 3.0起， <code>from_json</code>功能支持两种模式- <code>PERMISSIVE</code>和<code>FAILFAST</code> 。可以通过<code>mode</code>选项。默认模式变为<code>PERMISSIVE</code> 。在以前的版本中， <code>from_json</code>不符合<code>PERMISSIVE</code>也不<code>FAILFAST</code> ，尤其是在处理格式错误的JSON记录中。例如，JSON字符串<code>{"a" 1}</code>与模式<code>a INT</code>转换为<code>null</code>以前的版本，但Spark 3.0会将其转换为<code>Row(null)</code> 。</p>
  </li>
  <li>
    <p>的<code>ADD JAR</code>该命令先前返回的结果集具有单个值0。现在，它返回一个空结果集。</p>
  </li>
  <li>
    <p>在Spark版本2.4及更早版本中，用户可以通过内置类型的函数使用地图类型键创建地图值<code>CreateMap</code> ， <code>MapFromArrays</code>等等。从Spark 3.0开始，不允许使用这些内置函数使用地图类型键创建地图值。尽管它们不是很有用，但用户仍然可以使用来自数据源或Java / Scala集合的地图类型键来读取地图值。</p>
  </li>
  <li>
    <p>在Spark 2.4及更低版本中， <code>Dataset.groupByKey</code>如果键是非结构类型（例如int，字符串，数组等），则将结果返回到键属性错误命名为“值”的分组数据集。这是违反直觉的，并且使聚合查询的模式变得怪异。例如， <code>ds.groupByKey(...).count()</code>是<code>(value, count)</code> 。从Spark 3.0开始，我们将分组属性命名为“ key”。旧行为保留在新添加的配置下<code>spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue</code>默认值为<code>false</code> 。</p>
  </li>
  <li>
    <p>在Spark 2.4和更早版本中，float / double -0.0在语义上等于0.0，但在聚合分组键，窗口分区键和联接键中使用-0.0和0.0时，它们被视为不同的值。从Spark 3.0开始，此错误已修复。例如， <code>Seq(-0.0, 0.0).toDF("d").groupBy("d").count()</code>退货<code>[(0.0, 2)]</code>在Spark 3.0中，以及<code>[(0.0, 1), (-0.0, 1)]</code>在Spark 2.4及更早版本中。</p>
  </li>
  <li>
    <p>在Spark 2.4和更早版本中，用户可以通过内置功能（例如， <code>CreateMap</code> ， <code>StringToMap</code>等具有重复键的映射的行为是不确定的，例如，映射查找首先考虑重复键的出现， <code>Dataset.collect</code>只保留重复的密钥出现在最后， <code>MapKeys</code>返回重复的键，等等。从Spark 3.0开始，这些内置函数将使用“最后获胜”策略删除重复的地图键。用户仍然可以从不强制执行此操作的数据源（例如Parquet）中读取具有重复键的映射值，该行为将是不确定的。</p>
  </li>
  <li>
    <p>在Spark 2.4及更早版本中，如果无法将分区列值强制转换为相应的用户提供的模式，则将其转换为null。从3.0开始，将使用用户提供的架构来验证分区列的值。如果验证失败，将引发异常。您可以通过设置禁用此类验证<code>spark.sql.sources.validatePartitionColumns</code>至<code>false</code> 。</p>
  </li>
  <li>
    <p>在Spark 2.4及更低版本中， <code>SET</code>即使指定的键用于<code>SparkConf</code>条目，它无效，因为该命令不会更新<code>SparkConf</code> ，但是该行为可能会使用户感到困惑。从3.0开始，如果<code>SparkConf</code>使用密钥。您可以通过设置禁用此检查<code>spark.sql.legacy.setCommandRejectsSparkCoreConfs</code>至<code>false</code> 。</p>
  </li>
  <li>
    <p>在Spark 2.4及更低版本中，CSV数据源将格式错误的CSV字符串转换为包含所有内容的行<code>null</code>处于PERMISSIVE模式。从Spark 3.0开始，返回的行可以包含非<code>null</code>字段，如果某些CSV列值已解析并成功转换为所需类型。</p>
  </li>
  <li>
    <p>在Spark版本2.4及更低版本中，JSON数据源和JSON函数例如<code>from_json</code>将所有错误的JSON记录转换为一行<code>null</code>当指定架构为PERMISSIVE模式时<code>StructType</code> 。从Spark 3.0开始，返回的行可以包含非<code>null</code>字段，如果已解析某些JSON列值并将其成功转换为所需的类型。</p>
  </li>
  <li>
    <p>刷新缓存的表将触发表取消缓存操作，然后触发表缓存（延迟）操作。在Spark 2.4和更早版本中，在取消缓存操作之前不会保留缓存名称和存储级别。因此，缓存名称和存储级别可能会意外更改。从Spark 3.0开始，将首先保留缓存名称和存储级别以用于缓存重新创建。它有助于在刷新表时保持一致的缓存行为。</p>
  </li>
  <li>
    <p>从Spark 3.0开始，JSON数据源和JSON功能<code>schema_of_json</code>如果字符串值与JSON选项定义的模式匹配，则从字符串值推断TimestampType <code>timestampFormat</code> 。设置JSON选项<code>inferTimestamp</code>至<code>false</code>禁用这种类型推断。</p>
  </li>
  <li>
    <p>在Spark 2.4和更低版本中，如果<code>org.apache.spark.sql.functions.udf(Any, DataType)</code>获取带有原始类型参数的Scala闭包，如果输入值为null，则返回的UDF将返回null。从Spark 3.0开始，如果输入值为null，则UDF将返回Java类型的默认值。例如， <code>val f = udf((x: Int) => x, IntegerType)</code> ， <code>f($"x")</code>如果列在Spark 2.4和更早版本中将返回null <code>x</code>为null，并在Spark 3.0中返回0。引入此行为更改是因为默认情况下，Spark 3.0是使用Scala 2.12构建的。</p>
  </li>
  <li>
    <p>从Spark 3.0开始，Proleptic Gregorian日历用于解析，格式化和转换日期和时间戳记，以及提取子组件（例如年，日等）。Spark3.0使用来自java.time包的Java 8 API类，该类基于ISO年表（https://docs.oracle.com/javase/8/docs/api/java/time/chrono/IsoChronology.html）。在Spark 2.4及更早版本中，这些操作是通过使用混合日历（Julian + Gregorian，请参阅https://docs.oracle.com/javase/7/docs/api/java/util/GregorianCalendar.html）执行的。更改会影响1582年10月15日之前的结果（Gregorian），并且会影响以下Spark 3.0 API：</p>

    <ul>
      <li>
        <p>CSV / JSON数据源使用java.time API来解析和生成CSV / JSON内容。在Spark 2.4和更低版本中，为java.text。SimpleDateFormat用于与Spark 2.0和1.x的解析机制相同的目的。例如， <code>2018-12-08 10:39:21.123</code>与模式<code>yyyy-MM-dd'T'HH:mm:ss.SSS</code>自从Spark 3.0起就无法解析，因为时间戳与模式不匹配，但是由于回退到<code>Timestamp.valueOf</code> 。为了解析自Spark 3.0之后的相同时间戳，该模式应为<code>yyyy-MM-dd HH:mm:ss.SSS</code> 。</p>
      </li>
      <li>
        <p>的<code>unix_timestamp</code> ， <code>date_format</code> ， <code>to_unix_timestamp</code> ， <code>from_unixtime</code> ， <code>to_date</code> ， <code>to_timestamp</code>功能。新的实现支持模式格式，如https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html所述，并对其输入进行严格检查。例如， <code>2015-07-22 10:00:00</code>如果模式为，则无法解析时间戳<code>yyyy-MM-dd</code>因为解析器不会消耗整个输入。另一个例子是<code>31/01/2015 00:00</code>输入不能被解析<code>dd/MM/yyyy hh:mm</code>模式，因为<code>hh</code>假设范围内的小时数<code>1-12</code> 。</p>
      </li>
      <li>
        <p>的<code>weekofyear</code> ， <code>weekday</code> ， <code>dayofweek</code> ， <code>date_trunc</code> ， <code>from_utc_timestamp</code> ， <code>to_utc_timestamp</code>和<code>unix_timestamp</code>函数使用java.time API来计算年的周数，周的天数，以及在UTC时区中与TimestampType值之间的转换。</p>
      </li>
      <li>
        <p>JDBC选项<code>lowerBound</code>和<code>upperBound</code>以将字符串转换为TimestampType / DateType值的相同方式转换为TimestampType / DateType值。转换基于公历日历和SQL配置定义的时区<code>spark.sql.session.timeZone</code> 。在Spark 2.4和更早版本中，转换基于混合日历（Julian + Gregorian）和默认系统时区。</p>
      </li>
      <li>
        <p>格式化<code>TIMESTAMP</code>和<code>DATE</code>文字。</p>
      </li>
    </ul>
  </li>
  <li>
    <p>在Spark 2.4和更早版本中，无效的时区ID会被静默忽略，并由GMT时区代替，例如，在from_utc_timestamp函数中。从Spark 3.0开始，此类时区ID被拒绝，Spark抛出<code>java.time.DateTimeException</code> 。</p>
  </li>
  <li>
    <p>在Spark 2.4及更低版本中， <code>current_timestamp</code>函数仅返回毫秒级的时间戳。从Spark 3.0开始，如果系统上可用的基础时钟提供这样的分辨率，则该函数可以以微秒的分辨率返回结果。</p>
  </li>
  <li>
    <p>在Spark 2.4和更早版本中，当读取带有Spark本机数据源（parquet / orc）的Hive Serde表时，Spark将推断实际的文件架构并更新metastore中的表架构。从Spark 3.0开始，Spark不再推断架构。这不会给最终用户造成任何问题，但如果确实如此，请设置<code>spark.sql.hive.caseSensitiveInferenceMode</code>至<code>INFER_AND_SAVE</code> 。</p>
  </li>
  <li>
    <p>自Spark 3.0起， <code>TIMESTAMP</code>使用SQL配置将文字转换为字符串<code>spark.sql.session.timeZone</code> 。在Spark 2.4和更早版本中，转换使用Java虚拟机的默认时区。</p>
  </li>
  <li>
    <p>在Spark版本2.4中，当通过创建Spark会话时<code>cloneSession()</code> ，新创建的spark会话会从其父级继承其配置<code>SparkContext</code>即使相同的配置在其父Spark会话中可能存在不同的值。从Spark 3.0开始，父级的配置<code>SparkSession</code>优先于父级<code>SparkContext</code> 。可以通过设置恢复旧的行为<code>spark.sql.legacy.sessionInitWithConfigDefaults</code>至<code>true</code> 。</p>
  </li>
  <li>
    <p>自Spark 3.0起，实木复合地板逻辑类型<code>TIMESTAMP_MICROS</code>保存时默认使用<code>TIMESTAMP</code>列。在Spark 2.4及更低版本中， <code>TIMESTAMP</code>列另存为<code>INT96</code>在实木复合地板文件中。设置<code>INT96</code>至<code>spark.sql.parquet.outputTimestampType</code>恢复以前的行为。</p>
  </li>
  <li>
    <p>从Spark 3.0开始，如果<code>hive.default.fileformat</code>在中找不到<code>Spark SQL configuration</code>那么它将回退到当前存在的hive-site.xml <code>Hadoop configuration</code>的<code>SparkContext</code> 。</p>
  </li>
  <li>
    <p>从Spark 3.0开始，Spark将投放<code>String</code>至<code>Date/TimeStamp</code>与日期/时间戳进行二进制比较。铸造的先前行为<code>Date/Timestamp</code>至<code>String</code>可以通过设置恢复<code>spark.sql.legacy.typeCoercion.datetimeToString</code>至<code>true</code> 。</p>
  </li>
  <li>
    <p>从Spark 3.0开始，当使用用户提供的架构写入Avro文件时，字段将由催化剂架构和avro架构之间的字段名称而不是位置进行匹配。</p>
  </li>
  <li>
    <p>从Spark 3.0开始，当使用用户提供的不可为空的架构写入Avro文件时，即使催化剂架构也为可为空，Spark仍然能够写入文件。但是，如果任何记录包含null，Spark都会抛出运行时NPE。</p>
  </li>
  <li>
    <p>从Spark 3.0开始，我们使用新协议来获取洗牌块，对于外部洗牌服务用户，我们需要相应地升级服务器。否则，我们会收到错误消息<code>UnsupportedOperationException: Unexpected message: FetchShuffleBlocks</code> 。如果现在很难升级随机播放服务，则仍然可以通过以下方式使用旧协议： <code>spark.shuffle.useOldFetchProtocol</code>至<code>true</code> 。</p>
  </li>
  <li>
    <p>自Spark 3.0起，具有更高阶的功能<code>exists</code>遵循三值布尔逻辑，即<code>predicate</code>返回任何<code>null</code> s和否<code>true</code>获得，然后<code>exists</code>将返回<code>null</code>代替<code>false</code> 。例如， <code>exists(array(1, null, 3), x -> x % 2 == 0)</code>将会<code>null</code> 。可以通过设置恢复以前的行为<code>spark.sql.legacy.arrayExistsFollowsThreeValuedLogic</code>至<code>false</code> 。</p>
  </li>
  <li>
    <p>从Spark 3.0开始，如果文件或子目录在递归目录列表期间消失（即，它们出现在中间列表中，但由于并发文件删除或对象存储一致性问题而在递归目录列表的后续阶段无法读取或列出），则除非失败，否则该列表将失败<code>spark.sql.files.ignoreMissingFiles</code>是<code>true</code> （默认<code>false</code> ）。在以前的版本中，这些丢失的文件或子目录将被忽略。请注意，这种行为更改仅适用于初始表文件列表（或<code>REFRESH TABLE</code> ），而不是在查询执行期间：净变化是<code>spark.sql.files.ignoreMissingFiles</code>现在，不仅在查询执行时，在表文件列表/查询计划期间也要遵守。</p>
  </li>
  <li>
    <p>从Spark 3.0开始，更改了嵌套WITH子句的替换顺序，并且内部CTE定义优先于外部CTE定义。在2.4及更低版本中， <code>WITH t AS (SELECT 1), t2 AS (WITH t AS (SELECT 2) SELECT * FROM t) SELECT * FROM t2</code>退货<code>1</code>而在3.0版中，它会返回<code>2</code> 。可以通过设置恢复以前的行为<code>spark.sql.legacy.ctePrecedence.enabled</code>至<code>true</code> 。</p>
  </li>
  <li>
    <p>自Spark 3.0起， <code>add_months</code>如果原始日期是月份的最后一天，则函数不会将结果日期调整为月份的最后一天。例如， <code>select add_months(DATE'2019-02-28', 1)</code>结果<code>2019-03-28</code> 。在Spark 2.4和更早版本中，当原始日期是几个月的最后一天时，将调整结果日期。例如，将一个月添加到<code>2019-02-28</code>结果是<code>2019-03-31</code> 。</p>
  </li>
  <li>
    <p>从Spark 3.0开始，参数0的Java UDF在执行器端与其他UDF相同地执行。在Spark 2.4和更早版本中，仅在驱动程序侧执行了0参数的Java UDF，并且结果传播给了执行程序，这在某些情况下可能会提高性能，但在某些情况下会导致与正确性问题不一致。</p>
  </li>
  <li>
    <p>的结果<code>java.lang.Math</code>的<code>log</code> ， <code>log1p</code> ， <code>exp</code> ， <code>expm1</code>和<code>pow</code>可能因平台而异。在Spark 3.0中，等效SQL函数的结果（包括相关的SQL函数，例如<code>LOG10</code> ）传回的值与<code>java.lang.StrictMath</code> 。在几乎所有情况下，返回值都没有差异，差异很小，但可能不完全匹配<code>java.lang.Math</code>例如在x86平台上<code>log(3.0)</code> ，其值在<code>Math.log()</code>和<code>StrictMath.log()</code> 。</p>
  </li>
  <li>
    <p>从Spark 3.0开始，如果数据集查询包含由自连接引起的模糊列引用，则该数据集查询将失败。一个典型的例子： <code>val df1 = ...; val df2 = df1.filter(...);</code> ， 然后<code>df1.join(df2, df1("a") > df2("a"))</code>返回一个很混乱的空结果。这是因为Spark无法解析指向自连接表的数据集列引用，并且<code>df1("a")</code>与...完全相同<code>df2("a")</code>在Spark中。要恢复Spark 3.0之前的行为，可以设置<code>spark.sql.analyzer.failAmbiguousSelfJoin</code>至<code>false</code> 。</p>
  </li>
  <li>自Spark 3.0起， <code>Cast</code>函数在将字面量转换为<code>Double</code>要么<code>Float</code>类型以确保与其他数据库系统的更大兼容性。下表说明了此行为更改：<table class="table">
    <tbody><tr>
      <th>
        <b>运作方式</b>
      </th>
      <th>
        <b>Spark 3.0之前的结果</b>
      </th>
      <th>
        <b>结果启动Spark 3.0</b>
      </th>
    </tr>
    <tr>
      <td>CAST（“无限”为双）<br>CAST（'+ infinity'AS DOUBLE）<br>CAST（“ inf”为Double）<br>CAST（“ + inf”为双）<br>
      </td>
      <td>空值</td>
      <td>双。正无穷大</td>
    </tr>
    <tr>
      <td>CAST（“-infinity”为DOUBLE）<br>CAST（“-inf”为Double）<br>
      </td>
      <td>空值</td>
      <td>双。负无穷大</td>
    </tr>
    <tr>
      <td>CAST（“无穷大”浮动）<br>CAST（'+ infinity'AS FLOAT）<br>CAST（“ inf”作为浮动）<br>CAST（'+ inf'AS FLOAT）<br>
      </td>
      <td>空值</td>
      <td>浮动。正无穷大</td>
    </tr>
    <tr>
      <td>CAST（“-infinity”浮点数）<br>CAST（'-inf'AS FLOAT）<br>
      </td>
      <td>空值</td>
      <td>浮动。负无穷大</td>
    </tr>
    <tr>
      <td>CAST（“南”为双）</td>
      <td>空值</td>
      <td>双。N</td>
    </tr>
    <tr>
      <td>CAST（“南部”浮动）</td>
      <td>空值</td>
      <td>浮动。N</td>
    </tr>
</tbody></table>
  </li>
  <li>从Spark 3.0开始，从字符串到日期和时间戳的转换中支持特殊值。这些值只是简写的简写形式，读取时将转换为普通的日期或时间戳记值。日期支持以下字符串值：<ul>
      <li><code>epoch [zoneId]</code> -1970-01-01</li>
      <li><code>today [zoneId]</code> -由指定的时区中的当前日期<code>spark.sql.session.timeZone</code></li>
      <li><code>yesterday [zoneId]</code> -当前日期-1</li>
      <li><code>tomorrow [zoneId]</code> -当前日期+ 1</li>
      <li><code>now</code> -运行当前查询的日期。它与今天的概念相同<code>SELECT date 'tomorrow' - date 'yesterday';</code>应该输出<code>2</code> 。这是特殊的时间戳记值：</li>
      <li><code>epoch [zoneId]</code> -1970-01-01 00：00：00 + 00（Unix系统时间为零）</li>
      <li><code>today [zoneId]</code> -今天午夜</li>
      <li><code>yesterday [zoneId]</code> -昨天午夜</li>
      <li><code>tomorrow [zoneId]</code> -明天午夜</li>
      <li><code>now</code> -当前查询开始时间，例如<code>SELECT timestamp 'tomorrow';</code> 。</li>
    </ul>
  </li>
  <li>自Spark 3.0起， <code>size</code>函数返回<code>NULL</code>为了<code>NULL</code>输入。在Spark 2.4和更早的版本中，此函数提供<code>-1</code>对于相同的输入。要恢复Spark 3.0之前的行为，可以设置<code>spark.sql.legacy.sizeOfNull</code>至<code>true</code> 。</li>
</ul>

<h2 id="upgrading-from-spark-sql-24-to-241">从Spark SQL 2.4升级到2.4.1</h2>

<ul>
  <li>
    <p>的价值<code>spark.executor.heartbeatInterval</code>在未指定单位（例如“ 30”而不是“ 30s”）时，在Spark 2.4.0中代码的不同部分不一致地解释为秒和毫秒。现在，无单位值始终被解释为毫秒。设置为“ 30”的值的应用程序现在需要以“ 30s”为单位指定一个值，以避免被解释为毫秒。否则，产生的极短间隔可能会导致应用程序失败。</p>
  </li>
  <li>
    <p>将一个数据集转换为另一个数据集时，Spark会将原始数据集中的字段向上转换为目标数据集中相应字段的类型。在2.4版或更早的版本中，这种转换不是很严格，例如<code>Seq("str").toDS.as[Int]</code>失败，但是<code>Seq("str").toDS.as[Boolean]</code>在执行期间工作并抛出NPE。在Spark 3.0中，强制转换更加严格，并且不允许将String转换为其他内容，即<code>Seq("str").toDS.as[Boolean]</code>在分析期间将失败。</p>
  </li>
</ul>

<h2 id="upgrading-from-spark-sql-23-to-24">从Spark SQL 2.3升级到2.4</h2>

<ul>
  <li>在Spark 2.3及更早版本中，array_contains函数的第二个参数隐式提升为第一个数组类型参数的元素类型。这种类型的促销可能会造成损失，并可能导致<code>array_contains</code>函数返回错误的结果。通过使用更安全的类型升级机制，已在2.4中解决了此问题。这可能会导致某些行为变化，如下表所示。
    <table class="table">
    <tbody><tr>
      <th>
        <b>询问</b>
      </th>
      <th>
        <b>Spark 2.3或更低版本</b>
      </th>
      <th>
        <b>火花2.4</b>
      </th>
      <th>
        <b>备注</b>
      </th>
    </tr>
    <tr>
      <td>
        <code>SELECT array_contains(array(1), 1.34D);</code>
      </td>
      <td>
        <code>true</code>
      </td>
      <td>
        <code>false</code>
      </td>
      <td>在Spark 2.4中，left和right参数分别提升为double类型和double类型的数组类型。
      </td>
    </tr>
    <tr>
      <td>
        <code>SELECT array_contains(array(1), '1');</code>
      </td>
      <td>
        <code>true</code>
      </td>
      <td>
        <code>AnalysisException</code>被抛出。
      </td>
      <td>可以在参数中使用显式强制转换以避免异常。在Spark 2.4中， <code>AnalysisException</code>因为不能以无损方式将整数类型提升为字符串类型，所以将抛出此异常。
      </td>
    </tr>
    <tr>
      <td>
        <code>SELECT array_contains(array(1), 'anystring');</code>
      </td>
      <td>
        <code>null</code>
      </td>
      <td>
        <code>AnalysisException</code>被抛出。
      </td>
      <td>可以在参数中使用显式强制转换以避免异常。在Spark 2.4中， <code>AnalysisException</code>因为不能以无损方式将整数类型提升为字符串类型，所以将抛出此异常。
      </td>
    </tr>
</tbody></table>
  </li>
  <li>
    <p>从Spark 2.4开始，当子查询之前IN运算符前面有一个struct字段时，内部查询也必须包含一个struct字段。相反，在以前的版本中，将结构的字段与内部查询的输出进行了比较。例如。如果<code>a</code>是一个<code>struct(a string, b int)</code> ，在Spark 2.4中<code>a in (select (1 as a, 'a' as b) from range(1))</code>是有效的查询，而<code>a in (select 1, 'a' from range(1))</code>不是。在以前的版本中则相反。</p>
  </li>
  <li>
    <p>在2.2.1+和2.3版本中，如果<code>spark.sql.caseSensitive</code>设置为true，则<code>CURRENT_DATE</code>和<code>CURRENT_TIMESTAMP</code>函数不正确地区分大小写，并且会解析为列（除非以小写形式键入）。在Spark 2.4中，此问题已得到修复，功能不再区分大小写。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，Spark将按照SQL标准遵循优先级规则来评估查询中引用的设置操作。如果括号中未指定顺序，则设置操作从左到右执行，除了所有INTERSECT操作在所有UNION，EXCEPT或MINUS操作之前执行。在所有新增的设置操作中赋予相同优先级的旧行为保留在新添加的配置下<code>spark.sql.legacy.setopsPrecedence.enabled</code>默认值为<code>false</code> 。当此属性设置为<code>true</code> ，由于没有使用括号强制执行任何显式排序，因此spark将在查询中从左到右评估集合运算符。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，当值是1970年1月1日时，Spark将表描述列的Last Access值显示为UNKNOWN。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，默认情况下，Spark会最大化使用矢量化ORC阅读器处理ORC文件。要做到这一点， <code>spark.sql.orc.impl</code>和<code>spark.sql.orc.filterPushdown</code>将其默认值更改为<code>native</code>和<code>true</code>分别。一些旧的Apache Hive版本无法读取由本机ORC编写器创建的ORC文件。使用<code>spark.sql.orc.impl=hive</code>创建与Hive 2.1.1和更早版本共享的文件。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，将空数据帧写入目录将启动至少一个写入任务，即使该数据帧实际上没有分区。这引入了一个小的行为更改，对于自描述文件格式（例如Parquet和Orc），Spark在写入0分区数据帧时在目标目录中创建仅元数据文件，因此，如果用户以后读取该目录，则模式推断仍然可以工作。关于写入空数据框，新行为更合理，更一致。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，UDF参数中的表达式ID不会出现在列名中。例如，Spark 2.4中的列名称不是<code>UDF:f(col0 AS colA#28)</code>但<code>UDF:f(col0 AS `colA`)</code> 。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，不允许使用任何文件格式（镶木地板，orc，json，文本，csv等）编写具有空或嵌套空模式的数据框。尝试使用空模式写入数据帧时引发异常。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，在将双方都提升为TIMESTAMP之后，Spark会将DATE类型与TIMESTAMP类型进行比较。设置<code>false</code>至<code>spark.sql.legacy.compareDateTimestampInTimestamp</code>恢复以前的行为。此选项将在Spark 3.0中删除。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，不允许创建具有非空位置的托管表。尝试创建具有非空位置的托管表时将引发异常。设置<code>true</code>至<code>spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation</code>恢复以前的行为。此选项将在Spark 3.0中删除。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，不允许将托管表重命名为现有位置。尝试将托管表重命名到现有位置时将引发异常。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，类型强制规则可以自动将可变参数SQL函数的参数类型（例如IN / COALESCE）提升为最广泛的通用类型，而不管输入参数的排序如何。在以前的Spark版本中，升级可能会按某些特定顺序失败（例如，TimestampType，IntegerType和StringType）并引发异常。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，除了传统的缓存失效机制之外，Spark还启用了非级联SQL缓存失效。非级联高速缓存失效机制使用户可以删除高速缓存，而不会影响其从属高速缓存。此新的缓存失效机制用于要删除的缓存数据仍然有效的场景，例如，在数据集上调用unpersist（）或删除临时视图。这使用户可以释放内存并同时保持所需的缓存有效。</p>
  </li>
  <li>
    <p>在2.3及更早版本中，Spark默认会转换Parquet Hive表，但会忽略表属性，例如<code>TBLPROPERTIES (parquet.compression 'NONE')</code> 。对于ORC Hive表属性，例如<code>TBLPROPERTIES (orc.compress 'NONE')</code>的情况下<code>spark.sql.hive.convertMetastoreOrc=true</code>也一样从Spark 2.4开始，Spark在转换Parquet / ORC Hive表时会遵循Parquet / ORC特定的表属性。举个例子， <code>CREATE TABLE t(id int) STORED AS PARQUET TBLPROPERTIES (parquet.compression 'NONE')</code>在Spark 2.3中插入时会生成Snappy实木复合地板文件，而在Spark 2.4中，结果将是未压缩的实木复合地板文件。</p>
  </li>
  <li>
    <p>从Spark 2.0开始，Spark默认会转换Parquet Hive表以获得更好的性能。从Spark 2.4开始，Spark也会默认转换ORC Hive表。这意味着默认情况下，Spark使用其自己的ORC支持而不是Hive SerDe。举个例子， <code>CREATE TABLE t(id int) STORED AS ORC</code>将在Spark 2.3中使用Hive SerDe进行处理，在Spark 2.4中将其转换为Spark的ORC数据源表，并应用ORC向量化。设置<code>false</code>至<code>spark.sql.hive.convertMetastoreOrc</code>恢复以前的行为。</p>
  </li>
  <li>
    <p>在2.3版及更早版本中，如果行中的至少一个列值格式错误，则认为CSV行格式错误。CSV解析器在DROPMALFORMED模式下丢弃了这样的行，或者在FAILFAST模式下输出了错误。从Spark 2.4开始，仅当CSV行包含从CSV数据源请求的格式错误的列值时，才将其视为格式错误的其他值。例如，CSV文件包含“ id，name”标头和一行“ 1234”。在Spark 2.4中，对id列的选择由具有一个列值1234的行组成，但在Spark 2.3及更早版本中，在DROPMALFORMED模式下为空。要恢复以前的行为，请设置<code>spark.sql.csv.parser.columnPruning.enabled</code>至<code>false</code> 。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，默认情况下将并行进行计算统计信息的文件列表。可以通过设置禁用<code>spark.sql.statistics.parallelFileListingInStatsComputation.enabled</code>至<code>False</code> 。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，在统计信息计算过程中计算表大小时，元数据文件（例如Parquet摘要文件）和临时文件不计为数据文件。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，将空字符串保存为带引号的空字符串<code>""</code> 。在版本2.3和更早版本中，空字符串等于<code>null</code>值，并且不反映保存的CSV文件中的任何字符。例如， <code>"a", null, "", 1</code>被写成<code>a,,,1</code> 。从Spark 2.4开始，同一行另存为<code>a,,"",1</code> 。要恢复以前的行为，请设置CSV选项<code>emptyValue</code>清空（不加引号）字符串。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，LOAD DATA命令支持通配符<code>?</code>和<code>*</code> ，分别与任何一个字符和零个或多个字符匹配。例： <code>LOAD DATA INPATH '/tmp/folder*/'</code>要么<code>LOAD DATA INPATH '/tmp/part-?'</code> 。特殊字符，例如<code>space</code>现在也可以在路径中工作。例： <code>LOAD DATA INPATH '/tmp/folder name/'</code> 。</p>
  </li>
  <li>
    <p>在Spark 2.3及更早版本中，没有GROUP BY的HAVING被视为WHERE。这意味着， <code>SELECT 1 FROM range(10) HAVING true</code>被执行为<code>SELECT 1 FROM range(10) WHERE true</code>并返回10行。这违反了SQL标准，并已在Spark 2.4中修复。从Spark 2.4开始，没有GROUP BY的HAVING被视为全局集合，这意味着<code>SELECT 1 FROM range(10) HAVING true</code>将仅返回一行。要恢复以前的行为，请设置<code>spark.sql.legacy.parser.havingWithoutGroupByAsWhere</code>至<code>true</code> 。</p>
  </li>
  <li>在2.3版及更早版本中，从Parquet数据源表中读取数据时，无论Hive Metastore模式和Parquet模式中的列名使用不同的字母大小写，Spark始终返回null <code>spark.sql.caseSensitive</code>被设定为<code>true</code>要么<code>false</code> 。从2.4开始<code>spark.sql.caseSensitive</code>被设定为<code>false</code> ，Spark在Hive Metastore模式和Parquet模式之间执行不区分大小写的列名解析，因此即使列名大小写不同，Spark也会返回相应的列值。如果存在歧义，即匹配多个Parquet列，则会引发异常。此更改也适用于Parquet Hive表，当<code>spark.sql.hive.convertMetastoreParquet</code>被设定为<code>true</code> 。</li>
</ul>

<h2 id="upgrading-from-spark-sql-22-to-23">从Spark SQL 2.2升级到2.3</h2>

<ul>
  <li>
    <p>从Spark 2.3开始，当引用的列仅包含内部损坏的记录列（命名为）时，不允许从原始JSON / CSV文件进行查询<code>_corrupt_record</code>默认）。例如， <code>spark.read.schema(schema).json(file).filter($"_corrupt_record".isNotNull).count()</code>和<code>spark.read.schema(schema).json(file).select("_corrupt_record").show()</code> 。相反，您可以缓存或保存已解析的结果，然后发送相同的查询。例如， <code>val df = spark.read.schema(schema).json(file).cache()</code>然后<code>df.filter($"_corrupt_record".isNotNull).count()</code> 。</p>
  </li>
  <li>
    <p>的<code>percentile_approx</code>函数先前接受的数字类型输入和输出双精度类型结果。现在，它支持日期类型，时间戳类型和数字类型作为输入类型。结果类型也更改为与输入类型相同，这对于百分位数而言更为合理。</p>
  </li>
  <li>
    <p>从Spark 2.3开始，如果可能，在第一个非确定性谓词之后的Join / Filter的确定性谓词也会被下推/通过子运算符。在以前的Spark版本中，这些过滤器不适合进行谓词下推。</p>
  </li>
  <li>分区列推断先前为不同的推断类型发现了不正确的通用类型，例如，先前它以double type作为double type和date type的通用类型而结束。现在，它会为此类冲突找到正确的通用类型。解决冲突的方法如下表所示：<table class="table">
  <tbody><tr>
    <th>
      <b>输入A \输入B</b>
    </th>
    <th>
      <b>空类型</b>
    </th>
    <th>
      <b>整数类型</b>
    </th>
    <th>
      <b>长型</b>
    </th>
    <th>
      <b>DecimalType（38,0）*</b>
    </th>
    <th>
      <b>双重类型</b>
    </th>
    <th>
      <b>日期类型</b>
    </th>
    <th>
      <b>时间戳类型</b>
    </th>
    <th>
      <b>StringType</b>
    </th>
  </tr>
  <tr>
    <td>
      <b>空类型</b>
    </td>
    <td>空类型</td>
    <td>整数类型</td>
    <td>长型</td>
    <td>DecimalType（38,0）</td>
    <td>双重类型</td>
    <td>日期类型</td>
    <td>时间戳类型</td>
    <td>StringType</td>
  </tr>
  <tr>
    <td>
      <b>整数类型</b>
    </td>
    <td>整数类型</td>
    <td>整数类型</td>
    <td>长型</td>
    <td>DecimalType（38,0）</td>
    <td>双重类型</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
  </tr>
  <tr>
    <td>
      <b>长型</b>
    </td>
    <td>长型</td>
    <td>长型</td>
    <td>长型</td>
    <td>DecimalType（38,0）</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
  </tr>
  <tr>
    <td>
      <b>DecimalType（38,0）*</b>
    </td>
    <td>DecimalType（38,0）</td>
    <td>DecimalType（38,0）</td>
    <td>DecimalType（38,0）</td>
    <td>DecimalType（38,0）</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
  </tr>
  <tr>
    <td>
      <b>双重类型</b>
    </td>
    <td>双重类型</td>
    <td>双重类型</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>双重类型</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
  </tr>
  <tr>
    <td>
      <b>日期类型</b>
    </td>
    <td>日期类型</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>日期类型</td>
    <td>时间戳类型</td>
    <td>StringType</td>
  </tr>
  <tr>
    <td>
      <b>时间戳类型</b>
    </td>
    <td>时间戳类型</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>时间戳类型</td>
    <td>时间戳类型</td>
    <td>StringType</td>
  </tr>
  <tr>
    <td>
      <b>StringType</b>
    </td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
  </tr>
</tbody></table>

    <p>请注意，对于<b>DecimalType（38,0）*</b> ，上面的表有意不涵盖小数位数和精度的所有其他组合，因为当前我们仅推断像<code>BigInteger</code> / <code>BigInt</code> 。例如，将1.1推断为double类型。</p>
  </li>
  <li>
    <p>从Spark 2.3开始，当适用广播哈希联接或广播嵌套循环联接时，我们更喜欢广播在广播提示中显式指定的表。有关详细信息，请参阅“ <a href="sql-performance-tuning.html#broadcast-hint-for-sql-queries">广播提示</a>和<a href="https://issues.apache.org/jira/browse/SPARK-22489">SPARK-22489”部分</a> 。</p>
  </li>
  <li>
    <p>从Spark 2.3开始，当所有输入均为二进制时， <code>functions.concat()</code>返回二进制输出。否则，它将作为字符串返回。在Spark 2.3之前，无论输入类型如何，它始终以字符串形式返回。要保留旧的行为，请设置<code>spark.sql.function.concatBinaryAsString</code>至<code>true</code> 。</p>
  </li>
  <li>
    <p>从Spark 2.3开始，当所有输入均为二进制时，SQL <code>elt()</code>返回二进制输出。否则，它将作为字符串返回。在Spark 2.3之前，无论输入类型如何，它始终以字符串形式返回。要保留旧的行为，请设置<code>spark.sql.function.eltOutputAsString</code>至<code>true</code> 。</p>
  </li>
  <li>
    <p>从Spark 2.3开始，默认情况下，如果不可能使用精确的表示形式，则小数点之间的算术运算将返回舍入值（而不是返回NULL）。这符合SQL ANSI 2011规范以及Hive 2.2（HIVE-15331）中引入的Hive新行为。这涉及以下变化</p>

    <ul>
      <li>
        <p>确定算术运算结果类型的规则已更新。特别是，如果所需的精度/小数位数超出了可用值的范围，则将小数位数减小到6，以防止小数的整数部分被截断。所有算术运算均受更改影响，即。加法（ <code>+</code> ），减法（ <code>-</code> ），乘法（ <code>*</code> ），部门（ <code>/</code> ），余数（ <code>%</code> ）和肯定模块（ <code>pmod</code> ）。</p>
      </li>
      <li>
        <p>SQL操作中使用的文字值将以它们所需的精确度和小数位数转换为DECIMAL。</p>
      </li>
      <li>
        <p>配置<code>spark.sql.decimalOperations.allowPrecisionLoss</code>已经介绍了。默认为<code>true</code> ，表示此处描述的新行为；如果设置为<code>false</code> ，Spark使用以前的规则，即。它不会调整所需的小数位数来表示值，并且如果不可能精确表示该值，则返回NULL。</p>
      </li>
    </ul>
  </li>
  <li>
    <p>未混淆的子查询的语义尚未通过混乱的行为很好地定义。从Spark 2.3开始，我们使这种令人困惑的情况无效，例如： <code>SELECT v.i from (SELECT i FROM v)</code> ，在这种情况下，Spark会引发分析异常，因为用户不应在子查询中使用限定符。有关更多详细信息，请参见<a href="https://issues.apache.org/jira/browse/SPARK-20690">SPARK-20690</a>和<a href="https://issues.apache.org/jira/browse/SPARK-21335">SPARK-21335</a> 。</p>
  </li>
  <li>创建一个<code>SparkSession</code>与<code>SparkSession.builder.getOrCreate()</code> ，如果存在<code>SparkContext</code> ，构建者正在尝试更新<code>SparkConf</code>现有的<code>SparkContext</code>具有为建造者指定的配置，但是<code>SparkContext</code>被所有人共享<code>SparkSession</code> s，因此我们不应该更新它们。从2.3开始，构建器开始不更新配置。如果要更新它们，则需要先更新它们，然后再创建<code>SparkSession</code> 。</li>
</ul>

<h2 id="upgrading-from-spark-sql-21-to-22">从Spark SQL 2.1升级到2.2</h2>

<ul>
  <li>
    <p>Spark 2.1.1引入了新的配置密钥： <code>spark.sql.hive.caseSensitiveInferenceMode</code> 。它的默认设置为<code>NEVER_INFER</code> ，其行为与2.1.0相同。但是，Spark 2.2.0将此设置的默认值更改为<code>INFER_AND_SAVE</code>恢复与读取其基础文件架构具有大小写混合的列名称的Hive Metastore表的兼容性。随着<code>INFER_AND_SAVE</code>配置值，在首次访问时，Spark将在尚未为其存储已推断模式的任何Hive Metastore表上执行模式推断。请注意，对于具有数千个分区的表，模式推断可能是非常耗时的操作。如果不考虑与大小写混合的列名称兼容，则可以安全地设置<code>spark.sql.hive.caseSensitiveInferenceMode</code>至<code>NEVER_INFER</code>以避免架构推断的初始开销。请注意，使用新的默认设置<code>INFER_AND_SAVE</code>设置，模式推断的结果将另存为元存储密钥，以备将来使用。因此，初始模式推断仅在表的首次访问时发生。</p>
  </li>
  <li>
    <p>从Spark 2.2.1和2.3.0开始，当数据源表具有分区模式和数据模式中都存在的列时，总是在运行时推断模式。推断的架构没有分区列。在读取表时，Spark会考虑这些重叠列的分区值，而不是存储在数据源文件中的值。在2.2.0和2.1.x版本中，推断的架构已分区，但表的数据对用户不可见（即，结果集为空）。</p>
  </li>
  <li>
    <p>从Spark 2.2开始，视图定义的存储方式与以前的版本不同。这可能会导致Spark无法读取以前版本创建的视图。在这种情况下，您需要使用以下命令重新创建视图<code>ALTER VIEW AS</code>要么<code>CREATE OR REPLACE VIEW AS</code>使用更新的Spark版本。</p>
  </li>
</ul>

<h2 id="upgrading-from-spark-sql-20-to-21">从Spark SQL 2.0升级到2.1</h2>

<ul>
  <li>
    <p>现在，数据源表将分区元数据存储在Hive元存储中。这意味着Hive DDL如<code>ALTER TABLE PARTITION ... SET LOCATION</code>现在可用于使用数据源API创建的表。</p>

    <ul>
      <li>
        <p>可以通过以下方式将旧版数据源表迁移为这种格式： <code>MSCK REPAIR TABLE</code>命令。建议迁移旧表以利用Hive DDL支持和改进的计划性能。</p>
      </li>
      <li>
        <p>要确定表是否已迁移，请查找<code>PartitionProvider: Catalog</code>发行时的属性<code>DESCRIBE FORMATTED</code>在桌子上。</p>
      </li>
    </ul>
  </li>
  <li>
    <p>更改为<code>INSERT OVERWRITE TABLE ... PARTITION ...</code>数据源表的行为。</p>

    <ul>
      <li>
        <p>在以前的Spark版本中<code>INSERT OVERWRITE</code>即使给定了分区规范，也会覆盖整个数据源表。现在仅覆盖符合规范的分区。</p>
      </li>
      <li>
        <p>请注意，这仍然与Hive表的行为不同，后者仅覆盖与新插入的数据重叠的分区。</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="upgrading-from-spark-sql-16-to-20">从Spark SQL 1.6升级到2.0</h2>

<ul>
  <li>
    <p><code>SparkSession</code>现在是Spark的新切入点，取代了旧的<code>SQLContext</code>和</p>

    <p><code>HiveContext</code> 。请注意，保留旧的SQLContext和HiveContext是为了向后兼容。一个新的<code>catalog</code>界面可从以下位置访问<code>SparkSession</code> -有关数据库和表访问的现有API，例如<code>listTables</code> ， <code>createExternalTable</code> ， <code>dropTempView</code> ， <code>cacheTable</code>搬到这里了</p>
  </li>
  <li>
    <p>数据集API和DataFrame API是统一的。在斯卡拉， <code>DataFrame</code>成为其的类型别名<code>Dataset[Row]</code> ，而Java API用户必须替换<code>DataFrame</code>与<code>Dataset<Row></code> 。两种类型的转换（例如， <code>map</code> ， <code>filter</code>和<code>groupByKey</code> ）和无类型的转换（例如， <code>select</code>和<code>groupBy</code> ）在数据集类上可用。由于Python和R中的编译时类型安全不是语言功能，因此数据集的概念不适用于这些语言的API。代替， <code>DataFrame</code>仍然是主要的编程抽象，类似于这些语言中的单节点数据帧概念。</p>
  </li>
  <li>
    <p>数据集和DataFrame API <code>unionAll</code>已不推荐使用，并已替换为<code>union</code></p>
  </li>
  <li>
    <p>数据集和DataFrame API <code>explode</code>已被弃用，或者使用<code>functions.explode()</code>与<code>select</code>要么<code>flatMap</code></p>
  </li>
  <li>
    <p>数据集和DataFrame API <code>registerTempTable</code>已不推荐使用，并已替换为<code>createOrReplaceTempView</code></p>
  </li>
  <li>
    <p>更改为<code>CREATE TABLE ... LOCATION</code> Hive表的行为。</p>

    <ul>
      <li>
        <p>从Spark 2.0开始， <code>CREATE TABLE ... LOCATION</code>相当于<code>CREATE EXTERNAL TABLE ... LOCATION</code>为了防止意外丢失现有数据在用户提供的位置。这意味着，在Spark SQL中创建且用户指定位置的Hive表始终是Hive外部表。删除外部表不会删除数据。不允许用户为Hive托管表指定位置。请注意，这与Hive行为不同。</p>
      </li>
      <li>
        <p>结果是， <code>DROP TABLE</code>这些表上的语句不会删除数据。</p>
      </li>
    </ul>
  </li>
  <li>
    <p><code>spark.sql.parquet.cacheMetadata</code>不再使用。有关详细信息，请参见<a href="https://issues.apache.org/jira/browse/SPARK-13664">SPARK-13664</a> 。</p>
  </li>
</ul>

<h2 id="upgrading-from-spark-sql-15-to-16">从Spark SQL 1.5升级到1.6</h2>

<ul>
  <li>默认情况下，从Spark 1.6开始，Thrift服务器以多会话模式运行。这意味着每个JDBC / ODBC连接都拥有自己的SQL配置和临时功能注册表的副本。缓存的表仍然共享。如果您希望在旧的单会话模式下运行Thrift服务器，请设置选项<code>spark.sql.hive.thriftServer.singleSession</code>至<code>true</code> 。您可以将此选项添加到<code>spark-defaults.conf</code> ，或将其传递给<code>start-thriftserver.sh</code>通过<code>--conf</code> ：</li>
</ul>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>   ./sbin/start-thriftserver.sh <span class="se">\</span>
     --conf spark.sql.hive.thriftServer.singleSession<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
     ...
   </code></pre></figure>

<ul>
  <li>从Spark 1.6开始，LongType转换为TimestampType的时间期望为秒而不是微秒。进行此更改以使Hive 1.2的行为与从数字类型到TimestampType的更一致的类型转换匹配。有关详细信息，请参见<a href="https://issues.apache.org/jira/browse/SPARK-11724">SPARK-11724</a> 。</li>
</ul>

<h2 id="upgrading-from-spark-sql-14-to-15">从Spark SQL 1.4升级到1.5</h2>

<ul>
  <li>
    <p>现在默认启用使用手动管理的内存（Tungsten）的优化执行，并生成用于表达式评估的代码。这些功能都可以通过设置禁用<code>spark.sql.tungsten.enabled</code>至<code>false</code> 。</p>
  </li>
  <li>
    <p>默认情况下，不再启用Parquet模式合并。可以通过设置重新启用<code>spark.sql.parquet.mergeSchema</code>至<code>true</code> 。</p>
  </li>
  <li>
    <p>默认情况下，内存中列式存储分区修剪功能处于打开状态。可以通过设置禁用<code>spark.sql.inMemoryColumnarStorage.partitionPruning</code>至<code>false</code> 。</p>
  </li>
  <li>
    <p>不再支持无限制的精度十进制列，相反，Spark SQL强制将最大精度设为38。从中推断架构时<code>BigDecimal</code>对象，现在使用的精度为（38，18）。如果在DDL中未指定精度，则保留默认值<code>Decimal(10, 0)</code> 。</p>
  </li>
  <li>
    <p>时间戳现在以1us而不是1ns的精度存储</p>
  </li>
  <li>
    <p>在里面<code>sql</code>方言，浮点数现在解析为十进制。HiveQL解析保持不变。</p>
  </li>
  <li>
    <p>现在，SQL / DataFrame函数的规范名称为小写（例如，sum与SUM）。</p>
  </li>
  <li>
    <p>JSON数据源不会自动加载其他应用程序创建的新文件（即未通过Spark SQL插入到数据集中的文件）。对于JSON持久性表（即表的元数据存储在Hive Metastore中），用户可以使用<code>REFRESH TABLE</code> SQL命令或<code>HiveContext</code>的<code>refreshTable</code>方法，以将那些新文件包括到表中。对于表示JSON数据集的DataFrame，用户需要重新创建DataFrame，新的DataFrame将包含新文件。</p>
  </li>
</ul>

<h2 id="upgrading-from-spark-sql-13-to-14">从Spark SQL 1.3升级到1.4</h2>

<h4 class="no_toc" id="dataframe-data-readerwriter-interface">DataFrame数据读取器/写入器接口</h4>

<p>根据用户的反馈，我们创建了一个新的，更流畅的API来读取（ <code>SQLContext.read</code> ）并将数据写出（ <code>DataFrame.write</code> ），并弃用了旧版API（例如， <code>SQLContext.parquetFile</code> ， <code>SQLContext.jsonFile</code> ）。</p>

<p>请参阅API文档以获取<code>SQLContext.read</code> （ <a href="api/scala/index.html#org.apache.spark.sql.SQLContext@read:DataFrameReader">Scala</a> ， <a href="api/java/org/apache/spark/sql/SQLContext.html#read()">Java</a> ， <a href="api/python/pyspark.sql.html#pyspark.sql.SQLContext.read">Python</a> ）和<code>DataFrame.write</code> （ <a href="api/scala/index.html#org.apache.spark.sql.DataFrame@write:DataFrameWriter">Scala</a> ， <a href="api/java/org/apache/spark/sql/Dataset.html#write()">Java</a> ， <a href="api/python/pyspark.sql.html#pyspark.sql.DataFrame.write">Python</a> ）更多信息。</p>

<h4 class="no_toc" id="dataframegroupby-retains-grouping-columns">DataFrame.groupBy保留分组列</h4>

<p>根据用户反馈，我们更改了<code>DataFrame.groupBy().agg()</code>在结果中保留分组列<code>DataFrame</code> 。要使行为保持在1.3中，请设置<code>spark.sql.retainGroupColumns</code>至<code>false</code> 。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// In 1.3.x, in order for the grouping column &quot;department&quot; to show up,</span>
<span class="c1">// it must be included explicitly as part of the agg function call.</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&quot;department&quot;</span><span class="o">).</span><span class="n">agg</span><span class="o">(</span><span class="n">$</span><span class="s">&quot;department&quot;</span><span class="o">,</span> <span class="n">max</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">),</span> <span class="n">sum</span><span class="o">(</span><span class="s">&quot;expense&quot;</span><span class="o">))</span>

<span class="c1">// In 1.4+, grouping column &quot;department&quot; is included automatically.</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&quot;department&quot;</span><span class="o">).</span><span class="n">agg</span><span class="o">(</span><span class="n">max</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">),</span> <span class="n">sum</span><span class="o">(</span><span class="s">&quot;expense&quot;</span><span class="o">))</span>

<span class="c1">// Revert to 1.3 behavior (not retaining grouping column) by:</span>
<span class="n">sqlContext</span><span class="o">.</span><span class="n">setConf</span><span class="o">(</span><span class="s">&quot;spark.sql.retainGroupColumns&quot;</span><span class="o">,</span> <span class="s">&quot;false&quot;</span><span class="o">)</span></code></pre></figure>

  </div>

<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// In 1.3.x, in order for the grouping column &quot;department&quot; to show up,</span>
<span class="c1">// it must be included explicitly as part of the agg function call.</span>
<span class="n">df</span><span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="s">&quot;department&quot;</span><span class="o">).</span><span class="na">agg</span><span class="o">(</span><span class="n">col</span><span class="o">(</span><span class="s">&quot;department&quot;</span><span class="o">),</span> <span class="n">max</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">),</span> <span class="n">sum</span><span class="o">(</span><span class="s">&quot;expense&quot;</span><span class="o">));</span>

<span class="c1">// In 1.4+, grouping column &quot;department&quot; is included automatically.</span>
<span class="n">df</span><span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="s">&quot;department&quot;</span><span class="o">).</span><span class="na">agg</span><span class="o">(</span><span class="n">max</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">),</span> <span class="n">sum</span><span class="o">(</span><span class="s">&quot;expense&quot;</span><span class="o">));</span>

<span class="c1">// Revert to 1.3 behavior (not retaining grouping column) by:</span>
<span class="n">sqlContext</span><span class="o">.</span><span class="na">setConf</span><span class="o">(</span><span class="s">&quot;spark.sql.retainGroupColumns&quot;</span><span class="o">,</span> <span class="s">&quot;false&quot;</span><span class="o">);</span></code></pre></figure>

  </div>

<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">as</span> <span class="nn">func</span>

<span class="c1"># In 1.3.x, in order for the grouping column &quot;department&quot; to show up,</span>
<span class="c1"># it must be included explicitly as part of the agg function call.</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;department&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;department&quot;</span><span class="p">],</span> <span class="n">func</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">),</span> <span class="n">func</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;expense&quot;</span><span class="p">))</span>

<span class="c1"># In 1.4+, grouping column &quot;department&quot; is included automatically.</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;department&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">),</span> <span class="n">func</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;expense&quot;</span><span class="p">))</span>

<span class="c1"># Revert to 1.3.x behavior (not retaining grouping column) by:</span>
<span class="n">sqlContext</span><span class="o">.</span><span class="n">setConf</span><span class="p">(</span><span class="s2">&quot;spark.sql.retainGroupColumns&quot;</span><span class="p">,</span> <span class="s2">&quot;false&quot;</span><span class="p">)</span></code></pre></figure>

  </div>

</div>

<h4 class="no_toc" id="behavior-change-on-dataframewithcolumn">在DataFrame.withColumn上的行为更改</h4>

<p>在1.4之前的版本中，DataFrame.withColumn（）仅支持添加列。该列将始终以其指定名称作为新列添加到结果DataFrame中，即使可能存在相同名称的任何现有列也是如此。从1.4开始，DataFrame.withColumn（）支持添加名称与所有现有列名称不同的列，或替换具有相同名称的现有列。</p>

<p>请注意，此更改仅适用于Scala API，不适用于PySpark和SparkR。</p>

<h2 id="upgrading-from-spark-sql-10-12-to-13">从Spark SQL 1.0-1.2升级到1.3</h2>

<p>在Spark 1.3中，我们从Spark SQL中删除了“ Alpha”标签，并以此清理了可用的API。从Spark 1.3开始，Spark SQL将提供与1.中其他版本的二进制兼容性。X系列。此兼容性保证不包括被明确标记为不稳定的API（即DeveloperAPI或实验性）。</p>

<h4 class="no_toc" id="rename-of-schemardd-to-dataframe">将SchemaRDD重命名为DataFrame</h4>

<p>用户升级到Spark SQL 1.3时会注意到的最大变化是<code>SchemaRDD</code>已重命名为<code>DataFrame</code> 。这主要是因为DataFrames不再直接从RDD继承，而是通过其自己的实现提供RDD提供的大多数功能。仍然可以通过调用DataFrame将其转换为RDD。 <code>.rdd</code>方法。</p>

<p>在Scala中，存在来自的类型别名<code>SchemaRDD</code>至<code>DataFrame</code>为某些用例提供源兼容性。仍然建议用户更新其代码以使用<code>DataFrame</code>代替。Java和Python用户将需要更新其代码。</p>

<h4 class="no_toc" id="unification-of-the-java-and-scala-apis">Java和Scala API的统一</h4>

<p>在Spark 1.3之前，存在单独的Java兼容类（ <code>JavaSQLContext</code>和<code>JavaSchemaRDD</code> ）的镜像Scala API。在Spark 1.3中，Java API和Scala API已统一。两种语言的用户都应使用<code>SQLContext</code>和<code>DataFrame</code> 。通常，这些类尝试使用两种语言都可以使用的类型（即<code>Array</code>而不是特定语言的集合）。在某些不存在公共类型的情况下（例如，用于传递闭包或Maps），使用函数重载代替。</p>

<p>此外，已删除了Java特定类型的API。Scala和Java的用户都应使用存在于<code>org.apache.spark.sql.types</code>以编程方式描述架构。</p>

<h4 class="no_toc" id="isolation-of-implicit-conversions-and-removal-of-dsl-package-scala-only">隐式转换的隔离和dsl软件包的删除（仅Scala）</h4>

<p>Spark 1.3之前的许多代码示例均始于<code>import sqlContext._</code> ，这将sqlContext的所有功能引入了作用域。在Spark 1.3中，我们隔离了用于转换的隐式转换<code>RDD</code>入<code>DataFrame</code>进入对象内部的对象<code>SQLContext</code> 。用户现在应该写<code>import sqlContext.implicits._</code> 。</p>

<p>此外，隐式转换现在仅会增加由以下内容组成的RDD： <code>Product</code> s（即案例类或元组）的方法<code>toDF</code> ，而不是自动应用。</p>

<p>在DSL内部使用功能时（现已替换为<code>DataFrame</code> API）用户用来导入<code>org.apache.spark.sql.catalyst.dsl</code> 。相反，应该使用公共数据框函数API： <code>import org.apache.spark.sql.functions._</code> 。</p>

<h4 class="no_toc" id="removal-of-the-type-aliases-in-orgapachesparksql-for-datatype-scala-only">删除org.apache.spark.sql中用于DataType的类型别名（仅限Scala）</h4>

<p>Spark 1.3删除了以下基本sql包中存在的类型别名<code>DataType</code> 。用户应改为将类导入<code>org.apache.spark.sql.types</code></p>

<h4 class="no_toc" id="udf-registration-moved-to-sqlcontextudf-java--scala">UDF注册已移至<code>sqlContext.udf</code> （Java和Scala）</h4>

<p>用于注册UDF的功能（已在DataFrame DSL或SQL中使用）已移至的udf对象中。 <code>SQLContext</code> 。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">sqlContext</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">register</span><span class="o">(</span><span class="s">&quot;strLen&quot;</span><span class="o">,</span> <span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="o">.</span><span class="n">length</span><span class="o">())</span></code></pre></figure>

  </div>

<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">sqlContext</span><span class="o">.</span><span class="na">udf</span><span class="o">().</span><span class="na">register</span><span class="o">(</span><span class="s">&quot;strLen&quot;</span><span class="o">,</span> <span class="o">(</span><span class="n">String</span> <span class="n">s</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">s</span><span class="o">.</span><span class="na">length</span><span class="o">(),</span> <span class="n">DataTypes</span><span class="o">.</span><span class="na">IntegerType</span><span class="o">);</span></code></pre></figure>

  </div>

</div>

<p>Python UDF注册保持不变。</p>

<h2 id="compatibility-with-apache-hive">与Apache Hive的兼容性</h2>

<p>Spark SQL旨在与Hive Metastore，SerDes和UDF兼容。当前，Hive SerDes和UDF基于Hive 1.2.1，Spark SQL可以连接到不同版本的Hive Metastore（从0.12.0到2.3.6和3.0.0到3.1.2。另请参阅<a href="sql-data-sources-hive-tables.html#interacting-with-different-versions-of-hive-metastore">与Hive Metastore的不同版本进行交互</a> ）。</p>

<h4 class="no_toc" id="deploying-in-existing-hive-warehouses">在现有的Hive仓库中进行部署</h4>

<p>Spark SQL Thrift JDBC服务器被设计为与现有的Hive安装“开箱即用”兼容。您不需要修改现有的Hive Metastore或更改表的数据放置或分区。</p>

<h3 class="no_toc" id="supported-hive-features">支持的Hive功能</h3>

<p>Spark SQL支持绝大多数Hive功能，例如：</p>

<ul>
  <li>配置单元查询语句，包括：<ul>
      <li><code>SELECT</code></li>
      <li><code>GROUP BY</code></li>
      <li><code>ORDER BY</code></li>
      <li><code>CLUSTER BY</code></li>
      <li><code>SORT BY</code></li>
    </ul>
  </li>
  <li>所有Hive运算符，包括：<ul>
      <li>关系运算符（ <code>=</code> ， <code><=></code> ， <code>==</code> ， <code><></code> ， <code><</code> ， <code>></code> ， <code>>=</code> ， <code><=</code>等）</li>
      <li>算术运算符（ <code>+</code> ， <code>-</code> ， <code>*</code> ， <code>/</code> ， <code>%</code>等）</li>
      <li>逻辑运算符（ <code>AND</code> ， <code>&&</code> ， <code>OR</code> ， <code>||</code>等）</li>
      <li>复杂类型构造函数</li>
      <li>数学函数（ <code>sign</code> ， <code>ln</code> ， <code>cos</code>等）</li>
      <li>字符串函数（ <code>instr</code> ， <code>length</code> ， <code>printf</code>等）</li>
    </ul>
  </li>
  <li>用户定义函数（UDF）</li>
  <li>用户定义的聚合功能（UDAF）</li>
  <li>用户定义的序列化格式（SerDes）</li>
  <li>视窗功能</li>
  <li>加入<ul>
      <li><code>JOIN</code></li>
      <li><code>{LEFT|RIGHT|FULL} OUTER JOIN</code></li>
      <li><code>LEFT SEMI JOIN</code></li>
      <li><code>CROSS JOIN</code></li>
    </ul>
  </li>
  <li>工会</li>
  <li>子查询<ul>
      <li><code>SELECT col FROM ( SELECT a + b AS col from t1) t2</code></li>
    </ul>
  </li>
  <li>采样</li>
  <li>说明</li>
  <li>分区表，包括动态分区插入</li>
  <li>视图<ul>
      <li>
        <p>如果在视图定义查询中未指定列别名，则Spark和Hive都将生成别名，但方式不同。为了使Spark能够读取Hive创建的视图，用户应在视图定义查询中显式指定列别名。例如，Spark无法读取<code>v1</code>由Hive如下创建。</p>

        <pre><code>CREATE VIEW v1 AS SELECT * FROM (SELECT c + 1 FROM (SELECT 1 c) t1) t2;
</code></pre>

        <p>相反，您应该创建<code>v1</code>如下所示，并明确指定了列别名。</p>

        <pre><code>CREATE VIEW v1 AS SELECT * FROM (SELECT c + 1 AS inc_c FROM (SELECT 1 c) t1) t2;
</code></pre>
      </li>
    </ul>
  </li>
  <li>所有Hive DDL功能，包括：<ul>
      <li><code>CREATE TABLE</code></li>
      <li><code>CREATE TABLE AS SELECT</code></li>
      <li><code>ALTER TABLE</code></li>
    </ul>
  </li>
  <li>大多数Hive数据类型，包括：<ul>
      <li><code>TINYINT</code></li>
      <li><code>SMALLINT</code></li>
      <li><code>INT</code></li>
      <li><code>BIGINT</code></li>
      <li><code>BOOLEAN</code></li>
      <li><code>FLOAT</code></li>
      <li><code>DOUBLE</code></li>
      <li><code>STRING</code></li>
      <li><code>BINARY</code></li>
      <li><code>TIMESTAMP</code></li>
      <li><code>DATE</code></li>
      <li><code>ARRAY<></code></li>
      <li><code>MAP<></code></li>
      <li><code>STRUCT<></code></li>
    </ul>
  </li>
</ul>

<h3 class="no_toc" id="unsupported-hive-functionality">不受支持的Hive功能</h3>

<p>以下是我们尚不支持的Hive功能列表。这些功能中的大多数很少在Hive部署中使用。</p>

<p><strong>蜂巢的主要功能</strong></p>

<ul>
  <li>带存储桶的表：存储桶是Hive表分区内的哈希分区。Spark SQL还不支持存储桶。</li>
</ul>

<p><strong>神秘的蜂巢功能</strong></p>

<ul>
  <li><code>UNION</code>类型</li>
  <li>独特的加入</li>
  <li>列统计信息收集：Spark SQL目前不搭载扫描来收集列统计信息，仅支持填充配置单元元存储的sizeInBytes字段。</li>
</ul>

<p><strong>配置单元输入/输出格式</strong></p>

<ul>
  <li>CLI的文件格式：为了将结果显示回CLI，Spark SQL仅支持TextOutputFormat。</li>
  <li>Hadoop档案</li>
</ul>

<p><strong>蜂巢优化</strong></p>

<p>Spark中尚未包含一些Hive优化。由于Spark SQL的内存中计算模型，其中一些（例如索引）不太重要。其他版本已投放到Spark SQL的将来版本中。</p>

<ul>
  <li>块级位图索引和虚拟列（用于构建索引）</li>
  <li>自动确定用于联接和groupby的reducer的数量：当前，在Spark SQL中，您需要使用“ <code>SET spark.sql.shuffle.partitions=[num_tasks];</code> ”。</li>
  <li>仅元数据查询：对于仅可使用元数据回答的查询，Spark SQL仍会启动任务以计算结果。</li>
  <li>偏斜数据标志：Spark SQL不遵循Hive中的偏斜数据标志。</li>
  <li><code>STREAMTABLE</code>加入提示：Spark SQL不遵循<code>STREAMTABLE</code>暗示。</li>
  <li>合并多个小文件以获取查询结果：如果结果输出包含多个小文件，则Hive可以选择将小文件合并为较少的大文件，以避免HDFS元数据溢出。Spark SQL不支持该功能。</li>
</ul>

<p><strong>蜂巢UDF / UDTF / UDAF</strong></p>

<p>Spark SQL并不支持Hive UDF / UDTF / UDAF的所有API。以下是不受支持的API：</p>

<ul>
  <li><code>getRequiredJars</code>和<code>getRequiredFiles</code> （ <code>UDF</code>和<code>GenericUDF</code> ）是自动包含此UDF所需的其他资源的功能。</li>
  <li><code>initialize(StructObjectInspector)</code>在<code>GenericUDTF</code>目前尚不支持。Spark SQL当前使用不推荐使用的接口<code>initialize(ObjectInspector[])</code>只要。</li>
  <li><code>configure</code> （ <code>GenericUDF</code> ， <code>GenericUDTF</code>和<code>GenericUDAFEvaluator</code> ）是用于初始化函数的函数<code>MapredContext</code> ，不适用于Spark。</li>
  <li><code>close</code> （ <code>GenericUDF</code>和<code>GenericUDAFEvaluator</code> ）是释放关联资源的功能。任务完成后，Spark SQL不会调用此函数。</li>
  <li><code>reset</code> （ <code>GenericUDAFEvaluator</code> ）是用于重新初始化聚合以重用相同聚合的功能。Spark SQL当前不支持聚合的重用。</li>
  <li><code>getWindowingEvaluator</code> （ <code>GenericUDAFEvaluator</code> ）是通过评估固定窗口上的汇总来优化汇总的功能。</li>
</ul>

<h3 class="no_toc" id="incompatible-hive-udf">不兼容的Hive UDF</h3>

<p>以下是Hive和Spark产生不同结果的方案：</p>

<ul>
  <li><code>SQRT(n)</code>如果n <0，则Hive返回null，Spark SQL返回NaN。</li>
  <li><code>ACOS(n)</code>如果n <-1或n> 1，则Hive返回null，Spark SQL返回NaN。</li>
  <li><code>ASIN(n)</code>如果n <-1或n> 1，则Hive返回null，Spark SQL返回NaN。</li>
</ul>


                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-3.4.1.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    

</body></html>