<html class="no-js" ><head></head><body >﻿<!--<![endif]-->
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>RDD编程指南-Spark 3.0.0-预览文档</title>
        
          <meta name="description" content="Spark 3.0.0-preview programming guide in Java, Scala and Python">
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        
        <!-- Google analytics script -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-32518208-2']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
        </script>
        

    
    
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html"><img src="img/spark-logo-hd.png" style="height:50px"></a> <span class="version">3.0.0预览版</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">总览</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">编程指南<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">快速开始</a></li>
                                <li><a href="rdd-programming-guide.html">RDD，累加器，广播变量</a></li>
                                <li><a href="sql-programming-guide.html">SQL，数据框和数据集</a></li>
                                <li><a href="structured-streaming-programming-guide.html">结构化流</a></li>
                                <li><a href="streaming-programming-guide.html">火花流（DStreams）</a></li>
                                <li><a href="ml-guide.html">MLlib（机器学习）</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX（图形处理）</a></li>
                                <li><a href="sparkr.html">SparkR（Spark上的R）</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API文件<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">斯卡拉</a></li>
                                <li><a href="api/java/index.html">爪哇</a></li>
                                <li><a href="api/python/index.html">蟒蛇</a></li>
                                <li><a href="api/R/index.html">[R</a></li>
                                <li><a href="api/sql/index.html">SQL，内置函数</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">部署中<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">总览</a></li>
                                <li><a href="submitting-applications.html">提交申请</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark独立</a></li>
                                <li><a href="running-on-mesos.html">梅索斯</a></li>
                                <li><a href="running-on-yarn.html">纱</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">更多<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">组态</a></li>
                                <li><a href="monitoring.html">监控方式</a></li>
                                <li><a href="tuning.html">调音指南</a></li>
                                <li><a href="job-scheduling.html">作业调度</a></li>
                                <li><a href="security.html">安全</a></li>
                                <li><a href="hardware-provisioning.html">硬件配置</a></li>
                                <li><a href="migration-guide.html">迁移指南</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">建筑火花</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">为Spark贡献</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">第三方项目</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v3.0.0-preview</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                <div class="content" id="content">
                    
                        <h1 class="title">RDD编程指南</h1>
                    

                    <ul id="markdown-toc">
  <li><a href="#overview" id="markdown-toc-overview">总览</a></li>
  <li><a href="#linking-with-spark" id="markdown-toc-linking-with-spark">与Spark链接</a></li>
  <li><a href="#initializing-spark" id="markdown-toc-initializing-spark">初始化Spark</a>    <ul>
      <li><a href="#using-the-shell" id="markdown-toc-using-the-shell">使用外壳</a></li>
    </ul>
  </li>
  <li><a href="#resilient-distributed-datasets-rdds" id="markdown-toc-resilient-distributed-datasets-rdds">弹性分布式数据集（RDD）</a>    <ul>
      <li><a href="#parallelized-collections" id="markdown-toc-parallelized-collections">并行集合</a></li>
      <li><a href="#external-datasets" id="markdown-toc-external-datasets">外部数据集</a></li>
      <li><a href="#rdd-operations" id="markdown-toc-rdd-operations">RDD操作</a>        <ul>
          <li><a href="#basics" id="markdown-toc-basics">基本</a></li>
          <li><a href="#passing-functions-to-spark" id="markdown-toc-passing-functions-to-spark">将函数传递给Spark</a></li>
          <li><a href="#understanding-closures-" id="markdown-toc-understanding-closures-">了解闭包</a> <a name="ClosuresLink"></a>            <ul>
              <li><a href="#example" id="markdown-toc-example">例</a></li>
              <li><a href="#local-vs-cluster-modes" id="markdown-toc-local-vs-cluster-modes">本地与集群模式</a></li>
              <li><a href="#printing-elements-of-an-rdd" id="markdown-toc-printing-elements-of-an-rdd">RDD的打印元素</a></li>
            </ul>
          </li>
          <li><a href="#working-with-key-value-pairs" id="markdown-toc-working-with-key-value-pairs">使用键值对</a></li>
          <li><a href="#transformations" id="markdown-toc-transformations">转变</a></li>
          <li><a href="#actions" id="markdown-toc-actions">动作</a></li>
          <li><a href="#shuffle-operations" id="markdown-toc-shuffle-operations">随机操作</a>            <ul>
              <li><a href="#background" id="markdown-toc-background">背景</a></li>
              <li><a href="#performance-impact" id="markdown-toc-performance-impact">绩效影响</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#rdd-persistence" id="markdown-toc-rdd-persistence">RDD持久性</a>        <ul>
          <li><a href="#which-storage-level-to-choose" id="markdown-toc-which-storage-level-to-choose">选择哪个存储级别？</a></li>
          <li><a href="#removing-data" id="markdown-toc-removing-data">删除资料</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#shared-variables" id="markdown-toc-shared-variables">共享变量</a>    <ul>
      <li><a href="#broadcast-variables" id="markdown-toc-broadcast-variables">广播变量</a></li>
      <li><a href="#accumulators" id="markdown-toc-accumulators">蓄能器</a></li>
    </ul>
  </li>
  <li><a href="#deploying-to-a-cluster" id="markdown-toc-deploying-to-a-cluster">部署到集群</a></li>
  <li><a href="#launching-spark-jobs-from-java--scala" id="markdown-toc-launching-spark-jobs-from-java--scala">从Java / Scala启动Spark作业</a></li>
  <li><a href="#unit-testing" id="markdown-toc-unit-testing">单元测试</a></li>
  <li><a href="#where-to-go-from-here" id="markdown-toc-where-to-go-from-here">从这往哪儿走</a></li>
</ul>

<h1 id="overview">总览</h1>

<p>从总体上讲，每个Spark应用程序都包含一个运行用户<em>程序</em>的<em>驱动程序</em> <code>main</code>功能并在集群上执行各种<em>并行操作</em> 。Spark提供的主要抽象是<em>弹性分布式数据集</em> （RDD），它是跨集群节点划分的元素的集合，可以并行操作。通过从Hadoop文件系统（或任何其他Hadoop支持的文件系统）中的文件或驱动程序中现有的Scala集合开始并进行转换来创建RDD。用户还可以要求Spark将RDD <em>保留</em>在内存中，从而使其能够在并行操作中有效地重用。最后，RDD自动从节点故障中恢复。</p>

<p>Spark中的第二个抽象是可以在并行操作中使用的<em>共享变量</em> 。默认情况下，当Spark作为一组任务在不同节点上并行运行一个函数时，它会将函数中使用的每个变量的副本传送给每个任务。有时，需要在任务之间或任务与驱动程序之间共享变量。Spark支持两种类型的共享变量： <em>广播变量</em> （可用于在所有节点上的内存中缓存值）和<em>累加器（accumulator）</em> ，这些变量仅被“添加”到其上，例如计数器和总和。</p>

<p>本指南以Spark的每种受支持语言显示了这些功能。如果启动Spark的交互式Shell，最简单的方法是- <code>bin/spark-shell</code>用于Scala外壳或<code>bin/pyspark</code>对于Python之一。</p>

<h1 id="linking-with-spark">与Spark链接</h1>

<div class="codetabs">

<div data-lang="scala">

    <p>默认情况下，Spark 3.0.0-preview已构建并分发以与Scala 2.12一起使用。（也可以将Spark构建为与其他版本的Scala一起使用。）要在Scala中编写应用程序，您将需要使用兼容的Scala版本（例如2.12。X）。</p>

    <p>要编写Spark应用程序，您需要在Spark上添加Maven依赖项。可通过Maven Central在以下位置获得Spark：</p>

    <pre><code>groupId = org.apache.spark
artifactId = spark-core_2.12
version = 3.0.0-preview
</code></pre>

    <p>另外，如果您想访问HDFS集群，则需要添加对<code>hadoop-client</code>适用于您的HDFS版本。</p>

    <pre><code>groupId = org.apache.hadoop
artifactId = hadoop-client
version = &lt;your-hdfs-version&gt;
</code></pre>

    <p>最后，您需要将一些Spark类导入到程序中。添加以下行：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">org.apache.spark.SparkContext</span>
<span class="k">import</span> <span class="nn">org.apache.spark.SparkConf</span></code></pre></figure>

    <p>（在Spark 1.3.0之前，您需要显式<code>import org.apache.spark.SparkContext._</code>以实现必要的隐式转换。）</p>

  </div>

<div data-lang="java">

    <p>Spark 3.0.0-preview支持使用<a href="http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html">lambda表达式</a>来简洁地编写函数，否则，您可以使用<a href="api/java/index.html?org/apache/spark/api/java/function/package-summary.html">org.apache.spark.api.java.function</a>包中的类。</p>

    <p>请注意，在Spark 2.2.0中已删除了对Java 7的支持。</p>

    <p>要使用Java编写Spark应用程序，您需要添加对Spark的依赖。可通过Maven Central在以下位置获得Spark：</p>

    <pre><code>groupId = org.apache.spark
artifactId = spark-core_2.12
version = 3.0.0-preview
</code></pre>

    <p>另外，如果您想访问HDFS群集，则需要添加对<code>hadoop-client</code>适用于您的HDFS版本。</p>

    <pre><code>groupId = org.apache.hadoop
artifactId = hadoop-client
version = &lt;your-hdfs-version&gt;
</code></pre>

    <p>最后，您需要将一些Spark类导入到程序中。添加以下行：</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.api.java.JavaSparkContext</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.JavaRDD</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.SparkConf</span><span class="o">;</span></code></pre></figure>

  </div>

<div data-lang="python">

    <p>Spark 3.0.0-preview适用于Python 2.7+或Python 3.4+。它可以使用标准的CPython解释器，因此可以使用NumPy之类的C库。它还适用于PyPy 2.3+。</p>

    <p>请注意，自Spark 3.0.0起不推荐使用Python 2。</p>

    <p>Python中的Spark应用程序可以与<code>bin/spark-submit</code>在运行时包含Spark的脚本，或通过在setup.py中将其包含为：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span>    <span class="n">install_requires</span><span class="o">=</span><span class="p">[</span>
        <span class="s1">&#39;pyspark=={site.SPARK_VERSION}&#39;</span>
    <span class="p">]</span></code></pre></figure>

    <p>要在Python中运行Spark应用程序而无需pip安装PySpark，请使用<code>bin/spark-submit</code>脚本位于Spark目录中。该脚本将加载Spark的Java / Scala库，并允许您将应用程序提交到集群。您也可以使用<code>bin/pyspark</code>启动交互式Python Shell。</p>

    <p>如果您想访问HDFS数据，则需要使用PySpark的构建链接到您的HDFS版本。对于常用的HDFS版本，Spark主页上也提供了<a href="https://spark.apache.org/downloads.html">预构建的软件包</a> 。</p>

    <p>最后，您需要将一些Spark类导入到程序中。添加以下行：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span><span class="p">,</span> <span class="n">SparkConf</span></code></pre></figure>

    <p>PySpark在驱动程序和工作程序中都需要使用相同的次要版本的Python。它在PATH中使用默认的python版本，您可以指定要使用的Python版本<code>PYSPARK_PYTHON</code> ， 例如：</p>

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ <span class="nv">PYSPARK_PYTHON</span><span class="o">=</span>python3.4 bin/pyspark
$ <span class="nv">PYSPARK_PYTHON</span><span class="o">=</span>/opt/pypy-2.5/bin/pypy bin/spark-submit examples/src/main/python/pi.py</code></pre></figure>

  </div>

</div>

<h1 id="initializing-spark">初始化Spark</h1>

<div class="codetabs">

<div data-lang="scala">

    <p>Spark程序必须做的第一件事是创建一个<a href="api/scala/index.html#org.apache.spark.SparkContext">SparkContext</a>对象，该对象告诉Spark如何访问集群。创建一个<code>SparkContext</code>您首先需要构建一个<a href="api/scala/index.html#org.apache.spark.SparkConf">SparkConf</a>对象，其中包含有关您的应用程序的信息。</p>

    <p>每个JVM仅应激活一个SparkContext。你必须<code>stop()</code>创建新的活动SparkContext之前。</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">().</span><span class="n">setAppName</span><span class="o">(</span><span class="n">appName</span><span class="o">).</span><span class="n">setMaster</span><span class="o">(</span><span class="n">master</span><span class="o">)</span>
<span class="k">new</span> <span class="nc">SparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">)</span></code></pre></figure>

  </div>

<div data-lang="java">

    <p>Spark程序必须做的第一件事是创建一个<a href="api/java/index.html?org/apache/spark/api/java/JavaSparkContext.html">JavaSparkContext</a>对象，该对象告诉Spark如何访问集群。创建一个<code>SparkContext</code>您首先需要构建一个<a href="api/java/index.html?org/apache/spark/SparkConf.html">SparkConf</a>对象，其中包含有关您的应用程序的信息。</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">SparkConf</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SparkConf</span><span class="o">().</span><span class="na">setAppName</span><span class="o">(</span><span class="n">appName</span><span class="o">).</span><span class="na">setMaster</span><span class="o">(</span><span class="n">master</span><span class="o">);</span>
<span class="n">JavaSparkContext</span> <span class="n">sc</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaSparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">);</span></code></pre></figure>

  </div>

<div data-lang="python">

    <p>Spark程序必须做的第一件事是创建一个<a href="api/python/pyspark.html#pyspark.SparkContext">SparkContext</a>对象，该对象告诉Spark如何访问集群。创建一个<code>SparkContext</code>您首先需要构建一个<a href="api/python/pyspark.html#pyspark.SparkConf">SparkConf</a>对象，其中包含有关您的应用程序的信息。</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span><span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="n">appName</span><span class="p">)</span><span class="o">.</span><span class="n">setMaster</span><span class="p">(</span><span class="n">master</span><span class="p">)</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span></code></pre></figure>

  </div>

</div>

<p>的<code>appName</code>参数是您的应用程序在集群UI上显示的名称。 <code>master</code>是<a href="submitting-applications.html#master-urls">Spark，Mesos或YARN群集URL</a>或特殊的“本地”字符串，以本地模式运行。实际上，在群集上运行时，您将不希望硬编码<code>master</code>在程序中，而是<a href="submitting-applications.html">使用<code>spark-submit</code></a>并在那里收到。但是，对于本地测试和单元测试，您可以传递“ local”以在内部运行Spark。</p>

<h2 id="using-the-shell">使用外壳</h2>

<div class="codetabs">

<div data-lang="scala">

    <p>在Spark Shell中，已经为您创建了一个特殊的可识别解释器的SparkContext，该变量名为<code>sc</code> 。制作自己的SparkContext将不起作用。您可以使用<code>--master</code>参数，然后可以通过将逗号分隔的列表传递给<code>--jars</code>论点。您还可以通过将逗号分隔的Maven坐标列表提供给Shell会话，将依赖项（例如Spark Packages）添加到Shell会话中。 <code>--packages</code>论点。可能存在依赖关系的任何其他存储库（例如Sonatype）都可以传递给<code>--repositories</code>论点。例如运行<code>bin/spark-shell</code>在四个核心上使用：</p>

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/spark-shell --master local<span class="o">[</span><span class="m">4</span><span class="o">]</span></code></pre></figure>

    <p>或者，也要添加<code>code.jar</code>到其类路径，请使用：</p>

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/spark-shell --master local<span class="o">[</span><span class="m">4</span><span class="o">]</span> --jars code.jar</code></pre></figure>

    <p>要使用Maven坐标包含依赖项，请执行以下操作：</p>

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/spark-shell --master local<span class="o">[</span><span class="m">4</span><span class="o">]</span> --packages <span class="s2">&quot;org.example:example:0.1&quot;</span></code></pre></figure>

    <p>有关选项的完整列表，请运行<code>spark-shell --help</code> 。在幕后<code>spark-shell</code>调用更一般的<a href="submitting-applications.html"><code>spark-submit</code>脚本</a> 。</p>

  </div>

<div data-lang="python">

    <p>在PySpark Shell中，已经为您创建了一个特殊的可识别解释器的SparkContext，该变量名为<code>sc</code> 。制作自己的SparkContext将不起作用。您可以使用<code>--master</code>参数，然后可以通过将逗号分隔的列表传递到来将Python .zip，.egg或.py文件添加到运行时路径。 <code>--py-files</code> 。您还可以通过将逗号分隔的Maven坐标列表提供给Shell会话，将依赖项（例如Spark Packages）添加到Shell会话中。 <code>--packages</code>论点。可能存在依赖关系的任何其他存储库（例如Sonatype）都可以传递给<code>--repositories</code>论点。Spark软件包具有的所有Python依赖项（在该软件包的requirements.txt中列出）都必须使用手动安装<code>pip</code>必要时。例如运行<code>bin/pyspark</code>在四个核心上使用：</p>

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/pyspark --master local<span class="o">[</span><span class="m">4</span><span class="o">]</span></code></pre></figure>

    <p>或者，也要添加<code>code.py</code>到搜索路径（以便以后能够<code>import code</code> ）， 使用：</p>

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/pyspark --master local<span class="o">[</span><span class="m">4</span><span class="o">]</span> --py-files code.py</code></pre></figure>

    <p>有关选项的完整列表，请运行<code>pyspark --help</code> 。在幕后<code>pyspark</code>调用更一般的<a href="submitting-applications.html"><code>spark-submit</code>脚本</a> 。</p>

    <p>也可以在增强的Python解释器<a href="http://ipython.org">IPython中</a>启动PySpark Shell。PySpark可与IPython 1.0.0及更高版本一起使用。要使用IPython，请设置<code>PYSPARK_DRIVER_PYTHON</code>可变为<code>ipython</code>跑步时<code>bin/pyspark</code> ：</p>

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ <span class="nv">PYSPARK_DRIVER_PYTHON</span><span class="o">=</span>ipython ./bin/pyspark</code></pre></figure>

    <p>要使用Jupyter笔记本（以前称为IPython笔记本），</p>

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ <span class="nv">PYSPARK_DRIVER_PYTHON</span><span class="o">=</span>jupyter <span class="nv">PYSPARK_DRIVER_PYTHON_OPTS</span><span class="o">=</span>notebook ./bin/pyspark</code></pre></figure>

    <p>您可以自定义<code>ipython</code>要么<code>jupyter</code>通过设置命令<code>PYSPARK_DRIVER_PYTHON_OPTS</code> 。</p>

    <p>启动Jupyter Notebook服务器后，您可以从“文件”选项卡创建一个新的“ Python 2”笔记本。在笔记本中，您可以输入命令<code>%pylab inline</code>作为笔记本的一部分，然后再开始从Jupyter笔记本尝试Spark。</p>

  </div>

</div>

<h1 id="resilient-distributed-datasets-rdds">弹性分布式数据集（RDD）</h1>

<p>Spark围绕<em>弹性分布式数据集</em> （RDD）的概念展开，RDD是可并行操作的元素的容错集合。创建RDD的方法有两种： <em>并行化</em>驱动程序中的现有集合，或引用外部存储系统（例如共享文件系统，HDFS，HBase或提供Hadoop InputFormat的任何数据源）中的数据集。</p>

<h2 id="parallelized-collections">并行集合</h2>

<div class="codetabs">

<div data-lang="scala">

    <p>通过调用创建并行集合<code>SparkContext</code>的<code>parallelize</code>驱动程序中现有集合上的方法（Scala <code>Seq</code> ）。复制集合的元素以形成可以并行操作的分布式数据集。例如，以下是创建包含数字1到5的并行化集合的方法：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">5</span><span class="o">)</span>
<span class="k">val</span> <span class="n">distData</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">data</span><span class="o">)</span></code></pre></figure>

    <p>创建后，分布式数据集（ <code>distData</code> ）可以并行操作。例如，我们可能会打电话<code>distData.reduce((a, b) => a + b)</code>添加数组的元素。我们稍后将描述对分布式数据集的操作。</p>

  </div>

<div data-lang="java">

    <p>通过调用创建并行集合<code>JavaSparkContext</code>的<code>parallelize</code>现有方法<code>Collection</code>在您的驱动程序中。复制集合的元素以形成可以并行操作的分布式数据集。例如，以下是创建包含数字1到5的并行化集合的方法：</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">List</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">data</span> <span class="o">=</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">5</span><span class="o">);</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">distData</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">parallelize</span><span class="o">(</span><span class="n">data</span><span class="o">);</span></code></pre></figure>

    <p>创建后，分布式数据集（ <code>distData</code> ）可以并行操作。例如，我们可能会打电话<code>distData.reduce((a, b) -> a + b)</code>添加列表中的元素。我们稍后将描述对分布式数据集的操作。</p>

  </div>

<div data-lang="python">

    <p>通过调用创建并行集合<code>SparkContext</code>的<code>parallelize</code>驱动程序中现有迭代或集合上的方法。复制集合的元素以形成可以并行操作的分布式数据集。例如，以下是创建包含数字1到5的并行化集合的方法：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">distData</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span></code></pre></figure>

    <p>创建后，分布式数据集（ <code>distData</code> ）可以并行操作。例如，我们可以打电话<code>distData.reduce(lambda a, b: a + b)</code>添加列表中的元素。我们稍后将描述对分布式数据集的操作。</p>

  </div>

</div>

<p>并行集合的一个重要参数是将数据集切入的<em>分区</em>数。Spark将为集群的每个分区运行一个任务。通常，群集中的每个CPU都需要2-4个分区。通常，Spark会尝试根据您的集群自动设置分区数。但是，您也可以通过将其作为第二个参数传递给<code>parallelize</code> （例如<code>sc.parallelize(data, 10)</code> ）。注意：代码中的某些位置使用术语“切片”（分区的同义词）来保持向后兼容性。</p>

<h2 id="external-datasets">外部数据集</h2>

<div class="codetabs">

<div data-lang="scala">

    <p>Spark可以从Hadoop支持的任何存储源创建分布式数据集，包括您的本地文件系统，HDFS，Cassandra，HBase， <a href="http://wiki.apache.org/hadoop/AmazonS3">Amazon S3</a>等。Spark支持文本文件， <a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a>和任何其他Hadoop <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html">InputFormat</a> 。</p>

    <p>可以使用以下命令创建文本文件RDD <code>SparkContext</code>的<code>textFile</code>方法。此方法采用文件的URI（计算机上的本地路径或<code>hdfs://</code> ， <code>s3a://</code> ，例如URI），并将其作为行的集合读取。这是一个示例调用：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">distFile</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">)</span>
<span class="n">distFile</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="n">data</span><span class="o">.</span><span class="n">txt</span> <span class="nc">MapPartitionsRDD</span><span class="o">[</span><span class="err">10</span><span class="o">]</span> <span class="n">at</span> <span class="n">textFile</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">26</span></code></pre></figure>

    <p>创建完成后， <code>distFile</code>可以通过数据集操作来执行。例如，我们可以使用来累加所有行的大小<code>map</code>和<code>reduce</code>操作如下： <code>distFile.map(s => s.length).reduce((a, b) => a + b)</code> 。</p>

    <p>关于使用Spark读取文件的一些注意事项：</p>

    <ul>
      <li>
        <p>如果在本地文件系统上使用路径，则还必须在工作节点上的相同路径上访问该文件。将文件复制到所有工作服务器，或者使用网络安装的共享文件系统。</p>
      </li>
      <li>
        <p>Spark的所有基于文件的输入法，包括<code>textFile</code> ，也支持在目录，压缩文件和通配符上运行。例如，您可以使用<code>textFile("/my/directory")</code> ， <code>textFile("/my/directory/*.txt")</code>和<code>textFile("/my/directory/*.gz")</code> 。</p>
      </li>
      <li>
        <p>的<code>textFile</code>方法还采用了可选的第二个参数来控制文件的分区数。默认情况下，Spark为文件的每个块创建一个分区（HDFS中的块默认为128MB），但是您也可以通过传递更大的值来请求更大数量的分区。请注意，分区不能少于块。</p>
      </li>
    </ul>

    <p>除了文本文件，Spark的Scala API还支持其他几种数据格式：</p>

    <ul>
      <li>
        <p><code>SparkContext.wholeTextFiles</code>使您可以读取包含多个小文本文件的目录，并将每个小文本文件作为（文件名，内容）对返回。这与<code>textFile</code> ，这将在每个文件的每一行返回一条记录。分区由数据局部性决定，在某些情况下，数据局部性可能导致分区太少。对于这些情况， <code>wholeTextFiles</code>提供了一个可选的第二个参数，用于控制最小数量的分区。</p>
      </li>
      <li>
        <p>对于<a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a> ，请使用SparkContext的<code>sequenceFile[K, V]</code>方法在哪里<code>K</code>和<code>V</code>是文件中键和值的类型。这些应该是Hadoop的<a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html">Writable</a>接口的子类，例如<a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/IntWritable.html">IntWritable</a>和<a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Text.html">Text</a> 。此外，Spark允许您为一些常见的可写对象指定本机类型。例如， <code>sequenceFile[Int, String]</code>将自动读取IntWritables和Texts。</p>
      </li>
      <li>
        <p>对于其他Hadoop InputFormat，您可以使用<code>SparkContext.hadoopRDD</code>方法，需要任意<code>JobConf</code>输入格式类别，键类别和值类别。使用与使用输入源进行Hadoop作业相同的方式设置这些内容。您也可以使用<code>SparkContext.newAPIHadoopRDD</code>基于“新” MapReduce API的InputFormats（ <code>org.apache.hadoop.mapreduce</code> ）。</p>
      </li>
      <li>
        <p><code>RDD.saveAsObjectFile</code>和<code>SparkContext.objectFile</code>支持以包含序列化Java对象的简单格式保存RDD。尽管它不如Avro这样的专用格式有效，但它提供了一种保存任何RDD的简便方法。</p>
      </li>
    </ul>

  </div>

<div data-lang="java">

    <p>Spark可以从Hadoop支持的任何存储源创建分布式数据集，包括您的本地文件系统，HDFS，Cassandra，HBase， <a href="http://wiki.apache.org/hadoop/AmazonS3">Amazon S3</a>等。Spark支持文本文件， <a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a>和任何其他Hadoop <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html">InputFormat</a> 。</p>

    <p>可以使用以下命令创建文本文件RDD <code>SparkContext</code>的<code>textFile</code>方法。此方法采用文件的URI（计算机上的本地路径或<code>hdfs://</code> ， <code>s3a://</code> ，例如URI），并将其作为行的集合读取。这是一个示例调用：</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">distFile</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">);</span></code></pre></figure>

    <p>创建完成后， <code>distFile</code>可以通过数据集操作来执行。例如，我们可以使用来累加所有行的大小<code>map</code>和<code>reduce</code>操作如下： <code>distFile.map(s -> s.length()).reduce((a, b) -> a + b)</code> 。</p>

    <p>关于使用Spark读取文件的一些注意事项：</p>

    <ul>
      <li>
        <p>如果在本地文件系统上使用路径，则还必须在工作节点上的相同路径上访问该文件。将文件复制到所有工作服务器，或者使用网络安装的共享文件系统。</p>
      </li>
      <li>
        <p>Spark的所有基于文件的输入法，包括<code>textFile</code> ，也支持在目录，压缩文件和通配符上运行。例如，您可以使用<code>textFile("/my/directory")</code> ， <code>textFile("/my/directory/*.txt")</code>和<code>textFile("/my/directory/*.gz")</code> 。</p>
      </li>
      <li>
        <p>的<code>textFile</code>方法还采用了可选的第二个参数来控制文件的分区数。默认情况下，Spark为文件的每个块创建一个分区（HDFS中的块默认为128MB），但是您也可以通过传递更大的值来请求更大数量的分区。请注意，分区不能少于块。</p>
      </li>
    </ul>

    <p>除了文本文件，Spark的Java API还支持其他几种数据格式：</p>

    <ul>
      <li>
        <p><code>JavaSparkContext.wholeTextFiles</code>使您可以读取包含多个小文本文件的目录，并将每个小文本文件作为（文件名，内容）对返回。这与<code>textFile</code> ，这将在每个文件的每一行返回一条记录。</p>
      </li>
      <li>
        <p>对于<a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a> ，请使用SparkContext的<code>sequenceFile[K, V]</code>方法在哪里<code>K</code>和<code>V</code>是文件中键和值的类型。这些应该是Hadoop的<a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html">Writable</a>接口的子类，例如<a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/IntWritable.html">IntWritable</a>和<a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Text.html">Text</a> 。</p>
      </li>
      <li>
        <p>对于其他Hadoop InputFormat，您可以使用<code>JavaSparkContext.hadoopRDD</code>方法，需要任意<code>JobConf</code>输入格式类别，键类别和值类别。使用与使用输入源进行Hadoop作业相同的方式设置这些内容。您也可以使用<code>JavaSparkContext.newAPIHadoopRDD</code>基于“新” MapReduce API的InputFormats（ <code>org.apache.hadoop.mapreduce</code> ）。</p>
      </li>
      <li>
        <p><code>JavaRDD.saveAsObjectFile</code>和<code>JavaSparkContext.objectFile</code>支持以包含序列化Java对象的简单格式保存RDD。尽管它不如Avro这样的专用格式有效，但它提供了一种保存任何RDD的简便方法。</p>
      </li>
    </ul>

  </div>

<div data-lang="python">

    <p>PySpark可以从Hadoop支持的任何存储源创建分布式数据集，包括您的本地文件系统，HDFS，Cassandra，HBase， <a href="http://wiki.apache.org/hadoop/AmazonS3">Amazon S3</a>等。Spark支持文本文件， <a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a>和任何其他Hadoop <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html">InputFormat</a> 。</p>

    <p>可以使用以下命令创建文本文件RDD <code>SparkContext</code>的<code>textFile</code>方法。此方法采用文件的URI（计算机上的本地路径或<code>hdfs://</code> ， <code>s3a://</code> ，例如URI），并将其作为行的集合读取。这是一个示例调用：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">distFile</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;data.txt&quot;</span><span class="p">)</span></code></pre></figure>

    <p>创建完成后， <code>distFile</code>可以通过数据集操作来执行。例如，我们可以使用来累加所有行的大小<code>map</code>和<code>reduce</code>操作如下： <code>distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)</code> 。</p>

    <p>关于使用Spark读取文件的一些注意事项：</p>

    <ul>
      <li>
        <p>如果在本地文件系统上使用路径，则还必须在工作节点上的相同路径上访问该文件。将文件复制到所有工作服务器，或者使用网络安装的共享文件系统。</p>
      </li>
      <li>
        <p>Spark的所有基于文件的输入法，包括<code>textFile</code> ，也支持在目录，压缩文件和通配符上运行。例如，您可以使用<code>textFile("/my/directory")</code> ， <code>textFile("/my/directory/*.txt")</code>和<code>textFile("/my/directory/*.gz")</code> 。</p>
      </li>
      <li>
        <p>的<code>textFile</code>方法还采用了可选的第二个参数来控制文件的分区数。默认情况下，Spark为文件的每个块创建一个分区（HDFS中的块默认为128MB），但是您也可以通过传递更大的值来请求更大数量的分区。请注意，分区不能少于块。</p>
      </li>
    </ul>

    <p>除文本文件外，Spark的Python API还支持其他几种数据格式：</p>

    <ul>
      <li>
        <p><code>SparkContext.wholeTextFiles</code>使您可以读取包含多个小文本文件的目录，并将每个小文本文件作为（文件名，内容）对返回。这与<code>textFile</code> ，这将在每个文件的每一行返回一条记录。</p>
      </li>
      <li>
        <p><code>RDD.saveAsPickleFile</code>和<code>SparkContext.pickleFile</code>支持以包含腌制Python对象的简单格式保存RDD。批处理用于咸菜序列化，默认批处理大小为10。</p>
      </li>
      <li>
        <p>SequenceFile和Hadoop输入/输出格式</p>
      </li>
    </ul>

    <p><strong>请注意，</strong>此功能当前已标记<code>Experimental</code>面向高级用户。将来可能会替换为基于Spark SQL的读/写支持，在这种情况下，Spark SQL是首选方法。</p>

    <p><strong>可写支持</strong></p>

    <p>PySpark SequenceFile支持在Java内加载键-值对的RDD，将Writables转换为基本Java类型，并使用<a href="https://github.com/irmen/Pyrolite/">Pyrolite</a>腌制所得的Java对象。将键/值对的RDD保存到SequenceFile时，PySpark会执行相反的操作。它将Python对象分解为Java对象，然后将它们转换为Writables。以下可写对象将自动转换：</p>

    <table class="table">
<tbody><tr><th>可写类型</th><th>Python类型</th></tr>
<tr><td>文本</td><td>unicode str</td></tr>
<tr><td>可写</td><td>整型</td></tr>
<tr><td>浮动可写</td><td>浮动</td></tr>
<tr><td>双写</td><td>浮动</td></tr>
<tr><td>布尔可写</td><td>布尔</td></tr>
<tr><td>字节可写</td><td>字节数组</td></tr>
<tr><td>空可写</td><td>没有</td></tr>
<tr><td>MapWritable</td><td>字典</td></tr>
</tbody></table>

    <p>数组不是开箱即用的。用户需要指定自定义<code>ArrayWritable</code>读或写时的子类型。编写时，用户还需要指定将数组转换为自定义的自定义转换器<code>ArrayWritable</code>亚型。阅读时，默认转换器将转换自定义<code>ArrayWritable</code> Java的子类型<code>Object[]</code> ，然后将其腌制为Python元组。获得Python <code>array.array</code>对于基本类型的数组，用户需要指定自定义转换器。</p>

    <p><strong>保存和加载SequenceFile</strong></p>

    <p>与文本文件类似，可以通过指定路径来保存和加载SequenceFiles。可以指定键和值类，但是对于标准可写对象则不需要。</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span> <span class="o">*</span> <span class="n">x</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rdd</span><span class="o">.</span><span class="n">saveAsSequenceFile</span><span class="p">(</span><span class="s2">&quot;path/to/file&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">sequenceFile</span><span class="p">(</span><span class="s2">&quot;path/to/file&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;a&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;aa&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;aaa&#39;</span><span class="p">)]</span></code></pre></figure>

    <p><strong>保存和加载其他Hadoop输入/输出格式</strong></p>

    <p>对于“新”和“旧” Hadoop MapReduce API，PySpark还可以读取任何Hadoop InputFormat或编写任何Hadoop OutputFormat。如果需要，可以将Hadoop配置作为Python字典传递。这是使用Elasticsearch ESInputFormat的示例：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="err">$</span> <span class="o">./</span><span class="nb">bin</span><span class="o">/</span><span class="n">pyspark</span> <span class="o">--</span><span class="n">jars</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">elasticsearch</span><span class="o">-</span><span class="n">hadoop</span><span class="o">.</span><span class="n">jar</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">conf</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;es.resource&quot;</span> <span class="p">:</span> <span class="s2">&quot;index/type&quot;</span><span class="p">}</span>  <span class="c1"># assume Elasticsearch is running on localhost defaults</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">newAPIHadoopRDD</span><span class="p">(</span><span class="s2">&quot;org.elasticsearch.hadoop.mr.EsInputFormat&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;org.apache.hadoop.io.NullWritable&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;org.elasticsearch.hadoop.mr.LinkedMapWritable&quot;</span><span class="p">,</span>
                             <span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rdd</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>  <span class="c1"># the result is a MapWritable that is converted to a Python dict</span>
<span class="p">(</span><span class="sa">u</span><span class="s1">&#39;Elasticsearch ID&#39;</span><span class="p">,</span>
 <span class="p">{</span><span class="sa">u</span><span class="s1">&#39;field1&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
  <span class="sa">u</span><span class="s1">&#39;field2&#39;</span><span class="p">:</span> <span class="sa">u</span><span class="s1">&#39;Some Text&#39;</span><span class="p">,</span>
  <span class="sa">u</span><span class="s1">&#39;field3&#39;</span><span class="p">:</span> <span class="mi">12345</span><span class="p">})</span></code></pre></figure>

    <p>请注意，如果InputFormat仅取决于Hadoop配置和/或输入路径，并且可以根据上表轻松地转换键和值类，则这种方法在这种情况下应该很好用。</p>

    <p>如果您具有自定义的序列化二进制数据（例如从Cassandra / HBase加载数据），则首先需要将Scala / Java端的数据转换为Pyrolite的picker可以处理的数据。为此提供了<a href="api/scala/index.html#org.apache.spark.api.python.Converter">转换器</a>特征。只需扩展此特征并在<code>convert</code>方法。请记住要确保该类以及访问您的计算机所需的任何依赖项<code>InputFormat</code>打包到您的Spark作业jar中，并包含在PySpark类路径中。</p>

    <p>有关使用Cassandra / HBase的示例，请参见<a href="https://github.com/apache/spark/tree/master/examples/src/main/python">Python示例</a>和<a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/pythonconverters">Converter示例</a> 。 <code>InputFormat</code>和<code>OutputFormat</code>与自定义转换器。</p>

  </div>
</div>

<h2 id="rdd-operations">RDD操作</h2>

<p>RDD支持两种类型的操作： <em>转换</em> （从现有操作创建一个新的数据集）和<em>动作（action）</em> ，在对数据集执行计算后，将值返回给驱动程序。例如， <code>map</code>是一个转换，它将每个数据集元素通过一个函数传递，并返回一个代表结果的新RDD。另一方面， <code>reduce</code>是一种使用某些函数聚合RDD的所有元素并将最终结果返回给驱动程序的操作（尽管也有一个并行<code>reduceByKey</code>返回分布式数据集）。</p>

<p>Spark中的所有转换都是<i>惰性的</i> ，因为它们不会立即计算出结果。相反，他们只记得应用于某些基本数据集（例如文件）的转换。仅当动作要求将结果返回给驱动程序时才计算转换。这种设计使Spark可以更高效地运行。例如，我们可以认识到通过<code>map</code>将用于<code>reduce</code>并只返回结果<code>reduce</code>而不是较大的映射数据集。</p>

<p>默认情况下，每次在其上执行操作时，可能都会重新计算每个转换后的RDD。不过，您也可以使用<em>持久化</em> RDD内存<code>persist</code> （要么<code>cache</code> ）方法，在这种情况下，Spark会将元素保留在群集中，以便下次查询时可以更快地进行访问。还支持将RDD持久存储在磁盘上，或在多个节点之间复制。</p>

<h3 id="basics">基本</h3>

<div class="codetabs">

<div data-lang="scala">

    <p>为了说明RDD基础知识，请考虑以下简单程序：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">lineLengths</span> <span class="k">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">s</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="o">.</span><span class="n">length</span><span class="o">)</span>
<span class="k">val</span> <span class="n">totalLength</span> <span class="k">=</span> <span class="n">lineLengths</span><span class="o">.</span><span class="n">reduce</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">)</span></code></pre></figure>

    <p>第一行从外部文件定义基本RDD。该数据集未加载到内存中或未采取以下行动： <code>lines</code>仅仅是文件的指针。第二行定义<code>lineLengths</code>作为一个结果<code>map</code>转型。再次， <code>lineLengths</code>由于懒惰， <em>无法</em>立即计算。最后，我们运行<code>reduce</code> ，这是一个动作。此时，Spark将计算分解为任务，以在不同的机器上运行，每台机器都运行其映射的一部分和局部归约，仅将其答案返回给驱动程序。</p>

    <p>如果我们也想使用<code>lineLengths</code>再过一遍，我们可以添加：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">lineLengths</span><span class="o">.</span><span class="n">persist</span><span class="o">()</span></code></pre></figure>

    <p>之前<code>reduce</code> ，这会导致<code>lineLengths</code>第一次计算后将其保存在内存中。</p>

  </div>

<div data-lang="java">

    <p>为了说明RDD基础知识，请考虑以下简单程序：</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">);</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">lineLengths</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="n">s</span><span class="o">.</span><span class="na">length</span><span class="o">());</span>
<span class="kt">int</span> <span class="n">totalLength</span> <span class="o">=</span> <span class="n">lineLengths</span><span class="o">.</span><span class="na">reduce</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">);</span></code></pre></figure>

    <p>第一行从外部文件定义基本RDD。该数据集未加载到内存中或未采取以下行动： <code>lines</code>仅仅是文件的指针。第二行定义<code>lineLengths</code>作为一个结果<code>map</code>转型。再次， <code>lineLengths</code>由于懒惰， <em>无法</em>立即计算。最后，我们运行<code>reduce</code> ，这是一个动作。此时，Spark将计算分解为任务，以在不同的机器上运行，每台机器都运行其映射的一部分和局部归约，仅将其答案返回给驱动程序。</p>

    <p>如果我们也想使用<code>lineLengths</code>再过一遍，我们可以添加：</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">lineLengths</span><span class="o">.</span><span class="na">persist</span><span class="o">(</span><span class="n">StorageLevel</span><span class="o">.</span><span class="na">MEMORY_ONLY</span><span class="o">());</span></code></pre></figure>

    <p>之前<code>reduce</code> ，这会导致<code>lineLengths</code>第一次计算后将其保存在内存中。</p>

  </div>

<div data-lang="python">

    <p>为了说明RDD基础知识，请考虑以下简单程序：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;data.txt&quot;</span><span class="p">)</span>
<span class="n">lineLengths</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="n">totalLength</span> <span class="o">=</span> <span class="n">lineLengths</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span></code></pre></figure>

    <p>第一行从外部文件定义基本RDD。该数据集未加载到内存中或未采取以下行动： <code>lines</code>仅仅是文件的指针。第二行定义<code>lineLengths</code>作为一个结果<code>map</code>转型。再次， <code>lineLengths</code>由于懒惰， <em>无法</em>立即计算。最后，我们运行<code>reduce</code> ，这是一个动作。此时，Spark将计算分解为任务，以在不同的机器上运行，每台机器都运行其映射的一部分和局部归约，仅将其答案返回给驱动程序。</p>

    <p>如果我们也想使用<code>lineLengths</code>再过一遍，我们可以添加：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">lineLengths</span><span class="o">.</span><span class="n">persist</span><span class="p">()</span></code></pre></figure>

    <p>之前<code>reduce</code> ，这会导致<code>lineLengths</code>第一次计算后将其保存在内存中。</p>

  </div>

</div>

<h3 id="passing-functions-to-spark">将函数传递给Spark</h3>

<div class="codetabs">

<div data-lang="scala">

    <p>Spark的API在很大程度上依赖于在驱动程序中传递函数以在群集上运行。有两种推荐的方法可以做到这一点：</p>

    <ul>
      <li><a href="http://docs.scala-lang.org/tour/basics.html#functions">匿名函数语法</a> ，可用于简短的代码段。</li>
      <li>全局单例对象中的静态方法。例如，您可以定义<code>object MyFunctions</code>然后通过<code>MyFunctions.func1</code> ， 如下：</li>
    </ul>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">object</span> <span class="nc">MyFunctions</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">func1</span><span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
<span class="o">}</span>

<span class="n">myRdd</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="nc">MyFunctions</span><span class="o">.</span><span class="n">func1</span><span class="o">)</span></code></pre></figure>

    <p>请注意，虽然也可以在类实例中传递对方法的引用（与单例对象相对），但这需要将包含该类的对象与方法一起发送。例如，考虑：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">class</span> <span class="nc">MyClass</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">func1</span><span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
  <span class="k">def</span> <span class="n">doStuff</span><span class="o">(</span><span class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">func1</span><span class="o">)</span> <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

    <p>在这里，如果我们创建一个新的<code>MyClass</code>实例和调用<code>doStuff</code>在它上面<code>map</code>在里面引用<code>func1</code> <em>那个</em>方法<em><code>MyClass</code> instance</em> ，因此需要将整个对象发送到集群。类似于写作<code>rdd.map(x => this.func1(x))</code> 。</p>

    <p>以类似的方式，访问外部对象的字段将引用整个对象：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">class</span> <span class="nc">MyClass</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">field</span> <span class="k">=</span> <span class="s">&quot;Hello&quot;</span>
  <span class="k">def</span> <span class="n">doStuff</span><span class="o">(</span><span class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">field</span> <span class="o">+</span> <span class="n">x</span><span class="o">)</span> <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

    <p>相当于写作<code>rdd.map(x => this.field + x)</code> ，其中引用了所有<code>this</code> 。为避免此问题，最简单的方法是复制<code>field</code>放入局部变量，而不是从外部访问它：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">def</span> <span class="n">doStuff</span><span class="o">(</span><span class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">field_</span> <span class="k">=</span> <span class="k">this</span><span class="o">.</span><span class="n">field</span>
  <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">field_</span> <span class="o">+</span> <span class="n">x</span><span class="o">)</span>
<span class="o">}</span></code></pre></figure>

  </div>

<div data-lang="java">

    <p>Spark的API在很大程度上依赖于在驱动程序中传递函数以在群集上运行。在Java中，功能由实现<a href="api/java/index.html?org/apache/spark/api/java/function/package-summary.html">org.apache.spark.api.java.function</a>包中的接口的类表示。有两种创建此类功能的方法：</p>

    <ul>
      <li>在您自己的类中（作为匿名内部类或命名类）实现Function接口，并将其实例传递给Spark。</li>
      <li>使用<a href="http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html">lambda表达式</a>来简洁地定义一个实现。</li>
    </ul>

    <p>尽管本指南中的大部分内容都使用lambda语法来简化，但以长格式使用所有相同的API还是很容易的。例如，我们可以将上面的代码编写如下：</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">);</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">lineLengths</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="k">new</span> <span class="n">Function</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;()</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">Integer</span> <span class="nf">call</span><span class="o">(</span><span class="n">String</span> <span class="n">s</span><span class="o">)</span> <span class="o">{</span> <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="na">length</span><span class="o">();</span> <span class="o">}</span>
<span class="o">});</span>
<span class="kt">int</span> <span class="n">totalLength</span> <span class="o">=</span> <span class="n">lineLengths</span><span class="o">.</span><span class="na">reduce</span><span class="o">(</span><span class="k">new</span> <span class="n">Function2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;()</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">Integer</span> <span class="nf">call</span><span class="o">(</span><span class="n">Integer</span> <span class="n">a</span><span class="o">,</span> <span class="n">Integer</span> <span class="n">b</span><span class="o">)</span> <span class="o">{</span> <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">;</span> <span class="o">}</span>
<span class="o">});</span></code></pre></figure>

    <p>或者，如果内联编写函数很麻烦：</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kd">class</span> <span class="nc">GetLength</span> <span class="kd">implements</span> <span class="n">Function</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">Integer</span> <span class="nf">call</span><span class="o">(</span><span class="n">String</span> <span class="n">s</span><span class="o">)</span> <span class="o">{</span> <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="na">length</span><span class="o">();</span> <span class="o">}</span>
<span class="o">}</span>
<span class="kd">class</span> <span class="nc">Sum</span> <span class="kd">implements</span> <span class="n">Function2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="n">Integer</span> <span class="nf">call</span><span class="o">(</span><span class="n">Integer</span> <span class="n">a</span><span class="o">,</span> <span class="n">Integer</span> <span class="n">b</span><span class="o">)</span> <span class="o">{</span> <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">;</span> <span class="o">}</span>
<span class="o">}</span>

<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">);</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">lineLengths</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="k">new</span> <span class="n">GetLength</span><span class="o">());</span>
<span class="kt">int</span> <span class="n">totalLength</span> <span class="o">=</span> <span class="n">lineLengths</span><span class="o">.</span><span class="na">reduce</span><span class="o">(</span><span class="k">new</span> <span class="n">Sum</span><span class="o">());</span></code></pre></figure>

    <p>请注意，Java中的匿名内部类也可以访问封闭范围内的变量，只要它们被标记为<code>final</code> 。与其他语言一样，Spark会将这些变量的副本发送到每个工作程序节点。</p>

  </div>

<div data-lang="python">

    <p>Spark的API在很大程度上依赖于在驱动程序中传递函数以在群集上运行。建议使用三种方法来执行此操作：</p>

    <ul>
      <li><a href="https://docs.python.org/2/tutorial/controlflow.html#lambda-expressions">Lambda表达式</a> ，用于可以作为表达式编写的简单函数。（Lambda不支持多语句函数或不返回值的语句。）</li>
      <li>本地<code>def</code>在调用Spark的函数内部，用于更长的代码。</li>
      <li>模块中的顶级功能。</li>
    </ul>

    <p>例如，要传递比使用<code>lambda</code> ，请考虑以下代码：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="sd">&quot;&quot;&quot;MyScript.py&quot;&quot;&quot;</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">myFunc</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;file.txt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">myFunc</span><span class="p">)</span></code></pre></figure>

    <p>请注意，虽然也可以在类实例中传递对方法的引用（与单例对象相对），但这需要将包含该类的对象与方法一起发送。例如，考虑：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">class</span> <span class="nc">MyClass</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">s</span>
    <span class="k">def</span> <span class="nf">doStuff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">)</span></code></pre></figure>

    <p>在这里，如果我们创建一个<code>new MyClass</code>并打电话<code>doStuff</code>在它上面<code>map</code>在里面引用<code>func</code> <em>那个</em>方法<em><code>MyClass</code> instance</em> ，因此需要将整个对象发送到集群。</p>

    <p>以类似的方式，访问外部对象的字段将引用整个对象：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">class</span> <span class="nc">MyClass</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">field</span> <span class="o">=</span> <span class="s2">&quot;Hello&quot;</span>
    <span class="k">def</span> <span class="nf">doStuff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">field</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span></code></pre></figure>

    <p>为避免此问题，最简单的方法是复制<code>field</code>放入局部变量，而不是从外部访问它：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">def</span> <span class="nf">doStuff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
    <span class="n">field</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">field</span>
    <span class="k">return</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">field</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span></code></pre></figure>

  </div>

</div>

<h3 id="understanding-closures-">了解闭包<a name="ClosuresLink"></a></h3>
<p>关于Spark的难点之一是在跨集群执行代码时了解变量和方法的范围和生命周期。修改超出其范围的变量的RDD操作可能经常引起混乱。在下面的示例中，我们将看一下使用<code>foreach()</code>增加一个计数器，但是其他操作也会发生类似的问题。</p>

<h4 id="example">例</h4>

<p>考虑下面的朴素的RDD元素总和，其行为可能会有所不同，具体取决于执行是否在同一JVM中进行。一个常见的例子是在<code>local</code>模式（ <code>--master = local[n]</code> ），而不是将Spark应用程序部署到集群（例如，通过将Spark提交给YARN）：</p>

<div class="codetabs">

<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">var</span> <span class="n">counter</span> <span class="k">=</span> <span class="mi">0</span>
<span class="k">var</span> <span class="n">rdd</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="c1">// Wrong: Don&#39;t do this!!</span>
<span class="n">rdd</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">counter</span> <span class="o">+=</span> <span class="n">x</span><span class="o">)</span>

<span class="n">println</span><span class="o">(</span><span class="s">&quot;Counter value: &quot;</span> <span class="o">+</span> <span class="n">counter</span><span class="o">)</span></code></pre></figure>

  </div>

<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kt">int</span> <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">parallelize</span><span class="o">(</span><span class="n">data</span><span class="o">);</span>

<span class="c1">// Wrong: Don&#39;t do this!!</span>
<span class="n">rdd</span><span class="o">.</span><span class="na">foreach</span><span class="o">(</span><span class="n">x</span> <span class="o">-&gt;</span> <span class="n">counter</span> <span class="o">+=</span> <span class="n">x</span><span class="o">);</span>

<span class="n">println</span><span class="o">(</span><span class="s">&quot;Counter value: &quot;</span> <span class="o">+</span> <span class="n">counter</span><span class="o">);</span></code></pre></figure>

  </div>

<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Wrong: Don&#39;t do this!!</span>
<span class="k">def</span> <span class="nf">increment_counter</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">counter</span>
    <span class="n">counter</span> <span class="o">+=</span> <span class="n">x</span>
<span class="n">rdd</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">increment_counter</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Counter value: &quot;</span><span class="p">,</span> <span class="n">counter</span><span class="p">)</span></code></pre></figure>

  </div>

</div>

<h4 id="local-vs-cluster-modes">本地与集群模式</h4>

<p>上面的代码的行为是未定义的，可能无法按预期工作。为了执行作业，Spark将RDD操作的处理分解为任务，每个任务都由执行程序执行。在执行之前，Spark计算任务的<strong>闭包</strong> 。闭包是执行程序在RDD上执行计算所必须可见的那些变量和方法（在这种情况下， <code>foreach()</code> ）。此闭包被序列化并发送给每个执行器。</p>

<p>发送到每个执行接线盒中的变量，现在的副本，因此，当<strong>计数器</strong>内引用<code>foreach</code>功能，它不再是驱动程序节点上的<strong>计数器</strong> 。驱动程序节点的内存中仍然存在一个<strong>计数器</strong> ，但是执行者将不再看到该<strong>计数器</strong> ！执行者仅从序列化闭包中看到副本。因此，因为对<strong>计数器的</strong>所有操作都引用了序列化闭包内的值，所以<strong>counter</strong>的最终值仍将为零。</p>

<p>在本地模式下，在某些情况下， <code>foreach</code>函数实际上将在与驱动程序相同的JVM中执行，并且将引用相同的原始<strong>计数器</strong> ，并且实际上可能会对其进行更新。</p>

<p>为确保在此类情况下的行为明确，应使用<a href="#accumulators"><code>Accumulator</code></a> 。Spark中的累加器专门用于提供一种机制，用于在集群中的各个工作节点之间拆分执行时安全地更新变量。本指南的“累加器”部分将详细讨论这些内容。</p>

<p>通常，闭包-像循环或局部定义的方法之类的构造，不应用于突变某些全局状态。Spark不定义或保证从闭包外部引用的对象的突变行为。某些执行此操作的代码可能会在本地模式下工作，但这只是偶然的情况，此类代码在分布式模式下将无法正常运行。如果需要一些全局聚合，请使用累加器。</p>

<h4 id="printing-elements-of-an-rdd">RDD的打印元素</h4>
<p>另一个常见的习惯用法是尝试使用以下命令打印出RDD的元素<code>rdd.foreach(println)</code>要么<code>rdd.map(println)</code> 。在单台机器上，这将生成预期的输出并打印所有RDD的元素。但是，在<code>cluster</code>模式下，输出到<code>stdout</code>被执行人呼叫的人现在正在写信给执行人的<code>stdout</code>相反，不是驱动程序上的那个，所以<code>stdout</code>在驱动程序上不会显示这些！要在驱动程序上打印所有元素，可以使用<code>collect()</code>首先将RDD带到驱动程序节点的方法： <code>rdd.collect().foreach(println)</code> 。但是，这可能会导致驱动程序内存不足，因为<code>collect()</code>将整个RDD提取到一台计算机上；如果您只需要打印RDD的一些元素，则更安全的方法是使用<code>take()</code> ： <code>rdd.take(100).foreach(println)</code> 。</p>

<h3 id="working-with-key-value-pairs">使用键值对</h3>

<div class="codetabs">

<div data-lang="scala">

    <p>尽管大多数Spark操作可在包含任何类型的对象的RDD上运行，但一些特殊操作仅可用于键-值对的RDD。最常见的是分布式“混洗”操作，例如通过键对元素进行分组或聚合。</p>

    <p>在Scala中，这些操作可在包含<a href="http://www.scala-lang.org/api/2.12.10/index.html#scala.Tuple2">Tuple2</a>对象（该语言的内置元组，只需编写即可创建）的<a href="http://www.scala-lang.org/api/2.12.10/index.html#scala.Tuple2">RDD</a>上自动使用<code>(a, b)</code> ）。<a href="api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions">PairRDDFunctions</a>类中提供了<a href="api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions">键值</a>对操作，该类会自动包装RDD元组。</p>

    <p>例如，以下代码使用<code>reduceByKey</code>键值对上的操作以计算文件中每一行文本出现的次数：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">pairs</span> <span class="k">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">s</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">s</span><span class="o">,</span> <span class="mi">1</span><span class="o">))</span>
<span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKey</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">)</span></code></pre></figure>

    <p>我们也可以使用<code>counts.sortByKey()</code> ，例如，对字母对进行排序，最后<code>counts.collect()</code>将它们作为对象数组带回到驱动程序。</p>

    <p><strong>注意：</strong>在键值对操作中使用自定义对象作为键时，必须确保自定义<code>equals()</code>方法附带一个匹配项<code>hashCode()</code>方法。有关完整的详细信息，请参见<a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#hashCode--">Object.hashCode（）文档中</a>概述的合同。</p>

  </div>

<div data-lang="java">

    <p>尽管大多数Spark操作可在包含任何类型的对象的RDD上运行，但一些特殊操作仅可用于键-值对的RDD。最常见的是分布式“混洗”操作，例如通过键对元素进行分组或聚合。</p>

    <p>在Java中，键值对使用<a href="http://www.scala-lang.org/api/2.12.10/index.html#scala.Tuple2">scala表示</a><a href="http://www.scala-lang.org/api/2.12.10/index.html#scala.Tuple2">。Scala标准库中的Tuple2</a>类。您可以简单地致电<code>new Tuple2(a, b)</code>创建一个元组，并稍后使用<code>tuple._1()</code>和<code>tuple._2()</code> 。</p>

    <p>键值对的<a href="api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html">RDD</a>由<a href="api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html">JavaPairRDD</a>类表示。您可以使用特殊版本的JavaRDD从JavaRDD构造JavaPairRDD。 <code>map</code>操作，例如<code>mapToPair</code>和<code>flatMapToPair</code> 。JavaPairRDD将同时具有标准的RDD功能和特殊的键值功能。</p>

    <p>例如，以下代码使用<code>reduceByKey</code>键值对上的操作以计算文件中每一行文本出现的次数：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="nc">JavaRDD</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">);</span>
<span class="nc">JavaPairRDD</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">Integer</span><span class="o">&gt;</span> <span class="n">pairs</span> <span class="k">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">mapToPair</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="k">new</span> <span class="nc">Tuple2</span><span class="o">(</span><span class="n">s</span><span class="o">,</span> <span class="mi">1</span><span class="o">));</span>
<span class="nc">JavaPairRDD</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">Integer</span><span class="o">&gt;</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKey</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">);</span></code></pre></figure>

    <p>我们也可以使用<code>counts.sortByKey()</code> ，例如，对字母对进行排序，最后<code>counts.collect()</code>将它们作为对象数组带回到驱动程序。</p>

    <p><strong>注意：</strong>在键值对操作中使用自定义对象作为键时，必须确保自定义<code>equals()</code>方法附带一个匹配项<code>hashCode()</code>方法。有关完整的详细信息，请参见<a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#hashCode--">Object.hashCode（）文档中</a>概述的合同。</p>

  </div>

<div data-lang="python">

    <p>尽管大多数Spark操作可在包含任何类型的对象的RDD上运行，但一些特殊操作仅可用于键-值对的RDD。最常见的是分布式“混洗”操作，例如通过键对元素进行分组或聚合。</p>

    <p>在Python中，这些操作适用于包含内置Python元组的RDD，例如<code>(1, 2)</code> 。只需创建这样的元组，然后调用所需的操作即可。</p>

    <p>例如，以下代码使用<code>reduceByKey</code>键值对上的操作以计算文件中每一行文本出现的次数：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;data.txt&quot;</span><span class="p">)</span>
<span class="n">pairs</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span></code></pre></figure>

    <p>我们也可以使用<code>counts.sortByKey()</code> ，例如，对字母对进行排序，最后<code>counts.collect()</code>将它们作为对象列表带回到驱动程序。</p>

  </div>

</div>

<h3 id="transformations">转变</h3>

<p>下表列出了Spark支持的一些常见转换。有关详细信息，请参考RDD API文档（ <a href="api/scala/index.html#org.apache.spark.rdd.RDD">Scala</a> ， <a href="api/java/index.html?org/apache/spark/api/java/JavaRDD.html">Java</a> ， <a href="api/python/pyspark.html#pyspark.RDD">Python</a> ， <a href="api/R/index.html">R</a> ）和RDD函数对doc（ <a href="api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions">Scala</a> ， <a href="api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html">Java</a> ）。</p>

<table class="table">
<tbody><tr><th style="width:25%">转型</th><th>含义</th></tr>
<tr>
  <td> <b>地图</b> （ <i>func</i> ）</td>
  <td>返回一个新的分布式数据集，该数据集是通过将源的每个元素传递给函数<i>func形成的</i> 。</td>
</tr>
<tr>
  <td> <b>过滤器</b> （ <i>func</i> ）</td>
  <td>返回一个新的数据集，该数据集是通过选择源中<i>func</i>返回true的那些元素形成的。</td>
</tr>
<tr>
  <td> <b>flatMap</b> （ <i>func</i> ）</td>
  <td>与map相似，但是每个输入项都可以映射到0个或多个输出项（因此<i>func</i>应该返回Seq而不是单个项）。</td>
</tr>
<tr>
  <td> <b>mapPartitions</b> （ <i>func</i> ） <a name="MapPartLink"></a> </td>
  <td>与map相似，但是分别在RDD的每个分区（块）上运行，因此<t><u>当在T类型的RDD上运行时</u></t> <i>func</i>必须为Iterator <t>=> Iterator <u>类型。</u></t></td>
</tr>
<tr>
  <td> <b>mapPartitionsWithIndex</b> （ <i>func</i> ）</td>
  <td>与mapPartitions类似，但它还为<i>func</i>提供表示分区索引的整数值，因此<t><u>当在类型T的RDD上运行时</u></t> ， <i>func</i>必须为（Int，Iterator <t>）=> Iterator <u>类型。</u></t></td>
</tr>
<tr>
  <td> <b>样品</b> （ <i>withReplacement</i> ， <i>分数</i> ， <i>种子</i> ）</td>
  <td>使用给定的随机数生成器种子，对数据的<i>一部分</i>进行采样，无论是否进行替换。</td>
</tr>
<tr>
  <td> <b>联合</b> （ <i>otherDataset</i> ）</td>
  <td>返回一个新的数据集，其中包含源数据集中的元素和参数的并集。</td>
</tr>
<tr>
  <td> <b>相交</b> （ <i>otherDataset</i> ）</td>
  <td>返回一个新的RDD，其中包含源数据集中的元素与参数的交集。</td>
</tr>
<tr>
  <td> <b>不</b> <i>重复</i> （[ <i>numPartitions</i> ]））</td>
  <td>返回一个新的数据集，其中包含源数据集的不同元素。</td>
</tr>
<tr>
  <td> <b>groupByKey</b> （[ <i>numPartitions</i> ]） <a name="GroupByLink"></a> </td>
  <td>在（K，V）对的数据集上调用时，返回（K，Iterable <v>）对的数据集<v>。<br>
    <b>注意：</b>如果要分组以便对每个键执行汇总（例如总和或平均），请使用<code>reduceByKey</code>要么<code>aggregateByKey</code>将产生更好的性能。
    <br>
    <b>注意：</b>默认情况下，输出中的并行度取决于父RDD的分区数。您可以传递可选<code>numPartitions</code>参数来设置不同数量的任务。
  </v></v></td>
</tr>
<tr>
  <td> <b>reduceByKey</b> （ <i>func</i> ，[ <i>numPartitions</i> ]） <a name="ReduceByLink"></a> </td>
  <td>在（K，V）对的数据集上调用时，返回（K，V）对的数据集，其中每个键的值使用给定的reduce函数<i>func</i>进行汇总，该函数必须为（V，V）=>五，喜欢<code>groupByKey</code> ，可以通过可选的第二个参数配置reduce任务的数量。</td>
</tr>
<tr>
  <td> <b>aggregateByKey</b> （ <i>zeroValue</i> ）（ <i>seqOp</i> ， <i>combOp</i> ，[ <i>numPartitions</i> ]） <a name="AggregateByLink"></a> </td>
  <td>在（K，V）对的数据集上调用时，返回（K，U）对的数据集，其中每个键的值使用给定的Combine函数和中性的“零”值进行汇总。允许与输入值类型不同的聚合值类型，同时避免不必要的分配。像<code>groupByKey</code> ，可以通过可选的第二个参数配置reduce任务的数量。</td>
</tr>
<tr>
  <td> <b>sortByKey</b> （[ <i>升序</i> ]，[ <i>numPartitions</i> ]） <a name="SortByLink"></a> </td>
  <td>在由K实现Ordered的（K，V）对的数据集上调用时，返回按布尔值指定按键升序或降序对（K，V）对的数据集<code>ascending</code>论点。</td>
</tr>
<tr>
  <td> <b>加入</b> <i><i>（otherDataset，[numPartitions]）</i></i> <a name="JoinLink"></a> </td>
  <td>在（K，V）和（K，W）类型的数据集上调用时，返回（K，（V，W））对的数据集，其中每个键都有所有成对的元素。通过以下方式支持外部联接<code>leftOuterJoin</code> ， <code>rightOuterJoin</code>和<code>fullOuterJoin</code> 。
  </td>
</tr>
<tr>
  <td> <b>协同组</b> <i><i>（otherDataset，[numPartitions]）</i></i> <a name="CogroupLink"></a> </td>
  <td>在（K，V）和（K，W）类型的数据集上调用时，返回（K，（Iterable <v>，Iterable <w>））元组的数据集<v><w>。此操作也称为<code>groupWith</code> 。</w></v></w></v></td>
</tr>
<tr>
  <td> <b>笛卡尔</b> （ <i>otherDataset</i> ）</td>
  <td>在类型T和U的数据集上调用时，返回（T，U）对（所有元素对）的数据集。</td>
</tr>
<tr>
  <td> <b>管道</b> （ <i>命令</i> ， <i>[envVars]</i> ）</td>
  <td>通过外壳命令（例如Perl或bash脚本）通过管道传输RDD的每个分区。将RDD元素写入进程的stdin，并将输出到其stdout的行作为字符串的RDD返回。</td>
</tr>
<tr>
  <td> <b>合并</b> （ <i>numPartitions</i> ） <a name="CoalesceLink"></a> </td>
  <td>将RDD中的分区数减少到numPartitions。筛选大型数据集后，对于更有效地运行操作很有用。</td>
</tr>
<tr>
  <td> <b>重新分区</b> （ <i>numPartitions</i> ）</td>
  <td>随机地重新随机排列RDD中的数据，以创建更多或更少的分区，并在整个分区之间保持平衡。这总是会通过网络重新整理所有数据。<a name="RepartitionLink"></a></td>
</tr>
<tr>
  <td> <b>repartitionAndSortWithinPartitions</b> （ <i>分区程序</i> ）<a name="Repartition2Link"></a></td>
  <td>根据给定的分区程序对RDD重新分区，并在每个结果分区中，按其键对记录进行排序。这比打电话更有效<code>repartition</code>然后在每个分区内进行排序，因为它可以将排序向下推入洗牌机。</td>
</tr>
</tbody></table>

<h3 id="actions">动作</h3>

<p>下表列出了Spark支持的一些常见操作。请参考RDD API文档（ <a href="api/scala/index.html#org.apache.spark.rdd.RDD">Scala</a> ， <a href="api/java/index.html?org/apache/spark/api/java/JavaRDD.html">Java</a> ， <a href="api/python/pyspark.html#pyspark.RDD">Python</a> ， <a href="api/R/index.html">R</a> ）</p>

<p>并配对RDD函数doc（ <a href="api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions">Scala</a>和<a href="api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html">Java</a> ）以获取详细信息。</p>

<table class="table">
<tbody><tr><th>行动</th><th>含义</th></tr>
<tr>
  <td> <b>减少</b> （ <i>func</i> ）</td>
  <td>使用函数<i>func</i> （该函数接受两个参数并返回一个）来聚合数据集的元素。该函数应该是可交换的和关联的，以便可以并行正确地计算它。</td>
</tr>
<tr>
  <td> <b>收集</b> （）</td>
  <td>在驱动程序中将数据集的所有元素作为数组返回。这通常在返回足够小的数据子集的过滤器或其他操作之后很有用。</td>
</tr>
<tr>
  <td> <b>数</b> （）</td>
  <td>返回数据集中的元素数。</td>
</tr>
<tr>
  <td> <b>第一</b> （）</td>
  <td>返回数据集的第一个元素（类似于take（1））。</td>
</tr>
<tr>
  <td> <b>取</b> （ <i>n</i> ）</td>
  <td>返回具有数据集的前<i>n个</i>元素的数组。</td>
</tr>
<tr>
  <td> <b>takeSample</b> （ <i>withReplacement</i> ， <i>num</i> ，[ <i>种子</i> ]）</td>
  <td>返回带有数据集<i>num个</i>元素的随机样本的数组，带有或不带有替换，可以选择预先指定一个随机数生成器种子。</td>
</tr>
<tr>
  <td> <b>takeOrdered</b> （ <i>n</i> ， <i>[ordering]</i> ）</td>
  <td>使用自然顺序或自定义比较器返回RDD的前<i>n个</i>元素。</td>
</tr>
<tr>
  <td> <b>saveAsTextFile</b> （ <i>path</i> ）</td>
  <td>将数据集的元素以文本文件（或文本文件集）的形式写入本地文件系统，HDFS或任何其他Hadoop支持的文件系统中的给定目录中。Spark将在每个元素上调用toString，以将其转换为文件中的一行文本。</td>
</tr>
<tr>
  <td> <b>saveAsSequenceFile</b> （ <i>path</i> ）<br>（Java和Scala）</td>
  <td>在本地文件系统，HDFS或任何其他Hadoop支持的文件系统的给定路径中，将数据集的元素作为Hadoop SequenceFile写入。这在实现Hadoop的Writable接口的键/值对的RDD上可用。在Scala中，它也可用于隐式转换为Writable的类型（Spark包括对基本类型（如Int，Double，String等）的转换。</td>
</tr>
<tr>
  <td> <b>saveAsObjectFile</b> （ <i>path</i> ）<br>（Java和Scala）</td>
  <td>使用Java序列化以简单的格式编写数据集的元素，然后可以使用加载<code>SparkContext.objectFile()</code> 。</td>
</tr>
<tr>
  <td> <b>countByKey</b> （） <a name="CountByLink"></a> </td>
  <td>仅在类型（K，V）的RDD上可用。返回（K，Int）对的哈希图以及每个键的计数。</td>
</tr>
<tr>
  <td> <b>foreach</b> （ <i>func</i> ）</td>
  <td>在数据集的每个元素上运行函数<i>func</i> 。通常这样做是出于副作用，例如更新<a href="#accumulators">累加器</a>或与外部存储系统进行交互。
  <br><b>注意</b> ：在变量之外修改除累加器以外的变量<code>foreach()</code>可能导致不确定的行为。有关更多详细信息，请参见<a href="#understanding-closures-a-nameclosureslinka">了解闭包</a> 。</td>
</tr>
</tbody></table>

<p>Spark RDD API还公开了某些操作的异步版本，例如<code>foreachAsync</code>对于<code>foreach</code> ，立即返回<code>FutureAction</code>给调用者，而不是阻止操作完成。这可用于管理或等待动作的异步执行。</p>

<h3 id="shuffle-operations">随机操作</h3>

<p>Spark中的某些操作会触发一个称为随机播放的事件。改组是Spark的一种用于重新分配数据的机制，以便在分区之间对数据进行不同的分组。这通常涉及跨执行程序和机器复制数据，从而使改组成为复杂且昂贵的操作。</p>

<h4 id="background">背景</h4>

<p>要了解随机播放期间发生的情况，我们可以考虑以下示例<a href="#ReduceByLink"><code>reduceByKey</code></a>操作。的<code>reduceByKey</code>操作会生成一个新的RDD，其中将单个键的所有值组合为一个元组-键以及针对与该键关联的所有值执行reduce函数的结果。挑战在于，并非单个键的所有值都必须位于同一分区，甚至同一台计算机上，但是必须将它们放在同一位置才能计算结果。</p>

<p>在Spark中，数据通常不会跨分区分布在特定操作的必要位置。在计算过程中，单个任务将在单个分区上运行-因此，可以为单个组织所有数据<code>reduceByKey</code>减少要执行的任务，Spark需要执行所有操作。它必须从所有分区读取以找到所有键的所有值，然后将分区中的值汇总在一起以计算每个键的最终结果-这称为<strong>shuffle</strong> 。</p>

<p>尽管新改组后的数据的每个分区中的元素集都是确定性的，分区本身的顺序也是如此，但这些元素的顺序不是确定性的。如果人们希望在洗牌后能有规律地排序数据，那么可以使用：</p>

<ul>
  <li><code>mapPartitions</code>使用例如，对每个分区进行排序<code>.sorted</code></li>
  <li><code>repartitionAndSortWithinPartitions</code>在对分区进行有效排序的同时进行重新分区</li>
  <li><code>sortBy</code>制作全球订购的RDD</li>
</ul>

<p>可能导致<strong>改组的</strong>操作包括<strong>重新分区</strong>操作，例如<a href="#RepartitionLink"><code>repartition</code></a>和<a href="#CoalesceLink"><code>coalesce</code></a> ， <strong>'ByKey</strong>操作（计数除外）之类的<a href="#GroupByLink"><code>groupByKey</code></a>和<a href="#ReduceByLink"><code>reduceByKey</code></a> ，并<strong>加入</strong>类似的操作<a href="#CogroupLink"><code>cogroup</code></a>和<a href="#JoinLink"><code>join</code></a> 。</p>

<h4 id="performance-impact">绩效影响</h4>
<p><strong>随机播放</strong>是一项昂贵的操作，因为它涉及磁盘I / O，数据序列化和网络I / O。为了组织洗牌的数据，Spark生成任务集- <em>映射</em>任务以组织数据，以及一组<em>reduce</em>任务来聚合数据。此术语来自MapReduce，与Spark的名称没有直接关系<code>map</code>和<code>reduce</code>操作。</p>

<p>在内部，单个地图任务的结果会保留在内存中，直到无法容纳为止。然后，根据目标分区对它们进行排序并写入单个文件。在简化方面，任务读取相关的已排序块。</p>

<p>某些混洗操作会占用大量的堆内存，因为它们在转移它们之前或之后采用内存中的数据结构来组织记录。特别， <code>reduceByKey</code>和<code>aggregateByKey</code>在地图端创建这些结构，并<code>'ByKey</code>操作会在reduce端生成这些。当数据不适合内存时，Spark会将这些表溢出到磁盘上，从而产生磁盘I / O的额外开销并增加垃圾回收。</p>

<p>随机播放还会在磁盘上生成大量中间文件。从Spark 1.3开始，将保留这些文件，直到不再使用相应的RDD并进行垃圾回收为止。这样做是为了在重新计算沿袭时无需重新创建随机播放文件。如果应用程序保留了对这些RDD的引用，或者如果GC不经常启动，则垃圾收集可能仅在很长一段时间后才会发生。这意味着长时间运行的Spark作业可能会占用大量磁盘空间。临时存储目录由<code>spark.local.dir</code>配置Spark上下文时的配置参数。</p>

<p>可以通过调整各种配置参数来调整随机播放行为。请参阅《 <a href="configuration.html">Spark配置指南</a> 》中的“随机播放行为”部分。</p>

<h2 id="rdd-persistence">RDD持久性</h2>

<p>Spark中最重要的功能之一是跨操作将数据集<em>持久化</em> （或<em>缓存</em> ）在内存中。当您保留RDD时，每个节点都会将其计算的所有分区存储在内存中，并在该数据集（或从该数据集派生的数据集）上的其他操作中重用它们。这样可以使以后的操作更快（通常快10倍以上）。缓存是用于迭代算法和快速交互使用的关键工具。</p>

<p>您可以使用<code>persist()</code>要么<code>cache()</code>方法就可以了。第一次在操作中对其进行计算时，它将被保存在节点上的内存中。Spark的缓存是容错的–如果RDD的任何分区丢失，它将使用最初创建它的转换自动重新计算。</p>

<p>此外，每个持久化的RDD可以使用不同的<em>存储级别</em>进行存储，例如，允许您将数据集持久化在磁盘上，持久化在内存中，但作为序列化的Java对象（以节省空间）在节点之间复制。这些级别是通过传递一个<code>StorageLevel</code>对象（ <a href="api/scala/index.html#org.apache.spark.storage.StorageLevel">Scala</a> ， <a href="api/java/index.html?org/apache/spark/storage/StorageLevel.html">Java</a> ， <a href="api/python/pyspark.html#pyspark.StorageLevel">Python</a> ）到<code>persist()</code> 。的<code>cache()</code>方法是使用默认存储级别的简写，即<code>StorageLevel.MEMORY_ONLY</code> （将反序列化的对象存储在内存中）。完整的存储级别集是：</p>

<table class="table">
<tbody><tr><th style="width:23%">储存量</th><th>含义</th></tr>
<tr>
  <td>MEMORY_ONLY</td>
  <td>将RDD作为反序列化的Java对象存储在JVM中。如果RDD不能容纳在内存中，则某些分区将不会被缓存，并且每次需要时都会进行动态重新计算。这是默认级别。</td>
</tr>
<tr>
  <td>MEMORY_AND_DISK</td>
  <td>将RDD作为反序列化的Java对象存储在JVM中。如果RDD不能容纳在内存中，请存储磁盘上不适合的分区，并在需要时从那里读取它们。</td>
</tr>
<tr>
  <td>MEMORY_ONLY_SER<br>（Java和Scala）</td>
  <td>将RDD存储为<i>序列化的</i> Java对象（每个分区一个字节数组）。通常，这比反序列化的对象更节省空间，尤其是在使用<a href="tuning.html">快速序列化程序时</a> ，但读取时会占用更多CPU。
  </td>
</tr>
<tr>
  <td>MEMORY_AND_DISK_SER<br>（Java和Scala）</td>
  <td>与MEMORY_ONLY_SER类似，但是将内存中不适合的分区溢出到磁盘上，而不是在需要时即时对其进行重新计算。</td>
</tr>
<tr>
  <td>DISK_ONLY</td>
  <td>仅将RDD分区存储在磁盘上。</td>
</tr>
<tr>
  <td>MEMORY_ONLY_2，MEMORY_AND_DISK_2等。</td>
  <td>与上面的级别相同，但是在两个群集节点上复制每个分区。</td>
</tr>
<tr>
  <td>OFF_HEAP（实验性）</td>
  <td>与MEMORY_ONLY_SER类似，但是将数据存储在<a href="configuration.html#memory-management">堆外内存中</a> 。这需要启用堆外内存。</td>
</tr>
</tbody></table>

<p><strong>注意：</strong> <em>在Python中，存储的对象将始终使用<a href="https://docs.python.org/2/library/pickle.html">Pickle</a>库进行序列化，因此，是否选择序列化级别都无关紧要。Python中可用的存储级别包括<code>MEMORY_ONLY</code> ， <code>MEMORY_ONLY_2</code> ， <code>MEMORY_AND_DISK</code> ， <code>MEMORY_AND_DISK_2</code> ， <code>DISK_ONLY</code>和<code>DISK_ONLY_2</code> 。</em></p>

<p>Spark还可以在随机操作中自动保留一些中间数据（例如<code>reduceByKey</code> ），即使没有用户致电<code>persist</code> 。这样做是为了避免在混洗期间节点发生故障时重新计算整个输入。我们仍然建议用户致电<code>persist</code>如果他们打算重用它，请在生成的RDD上使用。</p>

<h3 id="which-storage-level-to-choose">选择哪个存储级别？</h3>

<p>Spark的存储级别旨在在内存使用率和CPU效率之间提供不同的权衡。我们建议通过以下过程选择一个：</p>

<ul>
  <li>
    <p>如果您的RDD适合默认存储级别（ <code>MEMORY_ONLY</code> ），让他们那样走。这是CPU效率最高的选项，允许RDD上的操作尽可能快地运行。</p>
  </li>
  <li>
    <p>如果没有，请尝试使用<code>MEMORY_ONLY_SER</code>并<a href="tuning.html">选择一个快速的序列化库</a>以使对象的空间利用率更高，但仍可以快速访问。（Java和Scala）</p>
  </li>
  <li>
    <p>除非用于计算数据集的函数很昂贵，否则它们会过滤到磁盘上，否则它们会过滤大量数据。否则，重新计算分区可能与从磁盘读取分区一样快。</p>
  </li>
  <li>
    <p>如果要快速恢复故障，请使用复制的存储级别（例如，如果使用Spark来处理来自Web应用程序的请求）。<em>所有</em>存储级别都通过重新计算丢失的数据来提供完全的容错能力，但是复制的存储级别使您可以继续在RDD上运行任务，而不必等待重新计算丢失的分区。</p>
  </li>
</ul>

<h3 id="removing-data">删除资料</h3>

<p>Spark自动监视每个节点上的缓存使用情况，并以最近最少使用（LRU）的方式丢弃旧的数据分区。如果您要手动删除RDD而不是等待它脱离缓存，请使用<code>RDD.unpersist()</code>方法。请注意，此方法默认情况下不会阻止。要阻塞直到释放资源，请指定<code>blocking=true</code>调用此方法时。</p>

<h1 id="shared-variables">共享变量</h1>

<p>通常，当函数传递给Spark操作时（例如<code>map</code>要么<code>reduce</code> ）是在远程群集节点上执行的，它可以在函数中使用的所有变量的单独副本上工作。这些变量将复制到每台计算机，并且远程计算机上的变量的更新都不会传播回驱动程序。在各个任务之间支持通用的读写共享变量将效率很低。但是，Spark确实为两种常用用法模式提供了两种有限类型的<em>共享变量</em> ：广播变量和累加器。</p>

<h2 id="broadcast-variables">广播变量</h2>

<p>广播变量使程序员可以在每台计算机上保留一个只读变量，而不用随任务一起发送它的副本。例如，可以使用它们以高效的方式为每个节点提供大型输入数据集的副本。Spark还尝试使用有效的广播算法分配广播变量，以降低通信成本。</p>

<p>火花动作是通过一组阶段执行的，这些阶段由分布式“随机”操作分隔。Spark自动广播每个阶段中任务所需的通用数据。在运行每个任务之前，以这种方式广播的数据以序列化形式缓存并反序列化。这意味着仅当跨多个阶段的任务需要相同数据或以反序列化形式缓存数据非常重要时，显式创建广播变量才有用。</p>

<p>广播变量是从变量创建的<code>v</code>通过打电话<code>SparkContext.broadcast(v)</code> 。广播变量是一个包装<code>v</code> ，可以通过调用<code>value</code>方法。下面的代码显示了这一点：</p>

<div class="codetabs">

<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">broadcastVar</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">broadcast</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">))</span>
<span class="n">broadcastVar</span><span class="k">:</span> <span class="kt">org.apache.spark.broadcast.Broadcast</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Int</span><span class="o">]]</span> <span class="k">=</span> <span class="nc">Broadcast</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">broadcastVar</span><span class="o">.</span><span class="n">value</span>
<span class="n">res0</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">)</span></code></pre></figure>

  </div>

<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">Broadcast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">[]&gt;</span> <span class="n">broadcastVar</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">broadcast</span><span class="o">(</span><span class="k">new</span> <span class="kt">int</span><span class="o">[]</span> <span class="o">{</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">});</span>

<span class="n">broadcastVar</span><span class="o">.</span><span class="na">value</span><span class="o">();</span>
<span class="c1">// returns [1, 2, 3]</span></code></pre></figure>

  </div>

<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">broadcastVar</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">broadcast</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="o">&lt;</span><span class="n">pyspark</span><span class="o">.</span><span class="n">broadcast</span><span class="o">.</span><span class="n">Broadcast</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x102789f10</span><span class="o">&gt;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">broadcastVar</span><span class="o">.</span><span class="n">value</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span></code></pre></figure>

  </div>

</div>

<p>创建广播变量后，应使用它代替值<code>v</code>在集群上运行的任何功能中<code>v</code>不会多次运送到节点。另外，对象<code>v</code>为了确保所有节点都具有相同的广播变量值（例如，如果变量稍后被传送到新节点），则不应在广播后对其进行修改。</p>

<p>要释放广播变量复制到执行程序上的资源，请调用<code>.unpersist()</code> 。如果此后再次使用广播，将重新广播。要永久释放广播变量使用的所有资源，请调用<code>.destroy()</code> 。此后不能使用广播变量。请注意，这些方法默认情况下不会阻止。要阻塞直到释放资源，请指定<code>blocking=true</code>打电话给他们时。</p>

<h2 id="accumulators">蓄能器</h2>

<p>累加器是仅通过关联和交换操作“累加”的变量，因此可以有效地并行支持。它们可用于实现计数器（如MapReduce中的计数器）或总和。Spark本身支持数字类型的累加器，程序员可以添加对新类型的支持。</p>

<p>作为用户，您可以创建命名或未命名的累加器。如下图所示，一个命名的累加器（在这种情况下<code>counter</code> ）将在网络界面中显示修改该累加器的阶段。Spark在“任务”表中显示由任务修改的每个累加器的值。</p>

<p style="text-align:center">
  <img src="img/spark-webui-accumulators.png" title="Spark UI中的累加器" alt="Spark UI中的累加器">
</p>

<p>UI中的跟踪累加器对于了解运行阶段的进度很有用（注意：Python尚不支持此功能）。</p>

<div class="codetabs">

<div data-lang="scala">

    <p>可以通过调用创建数字累加器<code>SparkContext.longAccumulator()</code>要么<code>SparkContext.doubleAccumulator()</code>分别累积Long或Double类型的值。然后，可以使用<code>add</code>方法。但是，他们无法读取其值。只有驱动程序可以使用累加器的值读取累加器的值<code>value</code>方法。</p>

    <p>下面的代码显示了一个累加器，用于累加一个数组的元素：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">accum</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">longAccumulator</span><span class="o">(</span><span class="s">&quot;My Accumulator&quot;</span><span class="o">)</span>
<span class="n">accum</span><span class="k">:</span> <span class="kt">org.apache.spark.util.LongAccumulator</span> <span class="o">=</span> <span class="nc">LongAccumulator</span><span class="o">(</span><span class="n">id</span><span class="k">:</span> <span class="err">0</span><span class="o">,</span> <span class="n">name</span><span class="k">:</span> <span class="kt">Some</span><span class="o">(</span><span class="kt">My</span> <span class="kt">Accumulator</span><span class="o">),</span> <span class="n">value</span><span class="k">:</span> <span class="err">0</span><span class="o">)</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">)).</span><span class="n">foreach</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">accum</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">x</span><span class="o">))</span>
<span class="o">...</span>
<span class="mi">10</span><span class="o">/</span><span class="mi">09</span><span class="o">/</span><span class="mi">29</span> <span class="mi">18</span><span class="k">:</span><span class="err">41</span><span class="kt">:</span><span class="err">08</span> <span class="kt">INFO</span> <span class="kt">SparkContext:</span> <span class="kt">Tasks</span> <span class="kt">finished</span> <span class="kt">in</span> <span class="err">0</span><span class="kt">.</span><span class="err">317106</span> <span class="kt">s</span>

<span class="n">scala</span><span class="o">&gt;</span> <span class="n">accum</span><span class="o">.</span><span class="n">value</span>
<span class="n">res2</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">10</span></code></pre></figure>

    <p>尽管此代码使用了对Long类型的累加器的内置支持，但程序员也可以通过对<a href="api/scala/index.html#org.apache.spark.util.AccumulatorV2">AccumulatorV2</a>进行子类化来创建自己的类型。AccumulatorV2抽象类具有几种必须重写的方法： <code>reset</code>用于将累加器重置为零， <code>add</code>用于向累加器中添加另一个值， <code>merge</code>用于将另一个相同类型的累加器合并到此累加器中。<a href="api/scala/index.html#org.apache.spark.util.AccumulatorV2">API文档</a>中包含其他必须重写的方法。例如，假设我们有一个<code>MyVector</code>代表数学向量的类，我们可以这样写：</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">class</span> <span class="nc">VectorAccumulatorV2</span> <span class="k">extends</span> <span class="nc">AccumulatorV2</span><span class="o">[</span><span class="kt">MyVector</span>, <span class="kt">MyVector</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">private</span> <span class="k">val</span> <span class="n">myVector</span><span class="k">:</span> <span class="kt">MyVector</span> <span class="o">=</span> <span class="nc">MyVector</span><span class="o">.</span><span class="n">createZeroVector</span>

  <span class="k">def</span> <span class="n">reset</span><span class="o">()</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">myVector</span><span class="o">.</span><span class="n">reset</span><span class="o">()</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">add</span><span class="o">(</span><span class="n">v</span><span class="k">:</span> <span class="kt">MyVector</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">myVector</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">v</span><span class="o">)</span>
  <span class="o">}</span>
  <span class="o">...</span>
<span class="o">}</span>

<span class="c1">// Then, create an Accumulator of this type:</span>
<span class="k">val</span> <span class="n">myVectorAcc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">VectorAccumulatorV2</span>
<span class="c1">// Then, register it into spark context:</span>
<span class="n">sc</span><span class="o">.</span><span class="n">register</span><span class="o">(</span><span class="n">myVectorAcc</span><span class="o">,</span> <span class="s">&quot;MyVectorAcc1&quot;</span><span class="o">)</span></code></pre></figure>

    <p>请注意，当程序员定义自己的AccumulatorV2类型时，结果类型可能与所添加元素的类型不同。</p>

  </div>

<div data-lang="java">

    <p>可以通过调用创建数字累加器<code>SparkContext.longAccumulator()</code>要么<code>SparkContext.doubleAccumulator()</code>分别累积Long或Double类型的值。然后，可以使用<code>add</code>方法。但是，他们无法读取其值。只有驱动程序可以使用累加器的值读取累加器的值<code>value</code>方法。</p>

    <p>下面的代码显示了一个累加器，用于累加一个数组的元素：</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">LongAccumulator</span> <span class="n">accum</span> <span class="o">=</span> <span class="n">jsc</span><span class="o">.</span><span class="na">sc</span><span class="o">().</span><span class="na">longAccumulator</span><span class="o">();</span>

<span class="n">sc</span><span class="o">.</span><span class="na">parallelize</span><span class="o">(</span><span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">)).</span><span class="na">foreach</span><span class="o">(</span><span class="n">x</span> <span class="o">-&gt;</span> <span class="n">accum</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">x</span><span class="o">));</span>
<span class="c1">// ...</span>
<span class="c1">// 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s</span>

<span class="n">accum</span><span class="o">.</span><span class="na">value</span><span class="o">();</span>
<span class="c1">// returns 10</span></code></pre></figure>

    <p>尽管此代码使用了对Long类型的累加器的内置支持，但程序员也可以通过对<a href="api/scala/index.html#org.apache.spark.util.AccumulatorV2">AccumulatorV2</a>进行子类化来创建自己的类型。AccumulatorV2抽象类具有几种必须重写的方法： <code>reset</code>用于将累加器重置为零， <code>add</code>用于向累加器中添加另一个值， <code>merge</code>用于将另一个相同类型的累加器合并到此累加器中。<a href="api/scala/index.html#org.apache.spark.util.AccumulatorV2">API文档</a>中包含其他必须重写的方法。例如，假设我们有一个<code>MyVector</code>代表数学向量的类，我们可以这样写：</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kd">class</span> <span class="nc">VectorAccumulatorV2</span> <span class="kd">implements</span> <span class="n">AccumulatorV2</span><span class="o">&lt;</span><span class="n">MyVector</span><span class="o">,</span> <span class="n">MyVector</span><span class="o">&gt;</span> <span class="o">{</span>

  <span class="kd">private</span> <span class="n">MyVector</span> <span class="n">myVector</span> <span class="o">=</span> <span class="n">MyVector</span><span class="o">.</span><span class="na">createZeroVector</span><span class="o">();</span>

  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">reset</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">myVector</span><span class="o">.</span><span class="na">reset</span><span class="o">();</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">add</span><span class="o">(</span><span class="n">MyVector</span> <span class="n">v</span><span class="o">)</span> <span class="o">{</span>
    <span class="n">myVector</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">v</span><span class="o">);</span>
  <span class="o">}</span>
  <span class="o">...</span>
<span class="o">}</span>

<span class="c1">// Then, create an Accumulator of this type:</span>
<span class="n">VectorAccumulatorV2</span> <span class="n">myVectorAcc</span> <span class="o">=</span> <span class="k">new</span> <span class="n">VectorAccumulatorV2</span><span class="o">();</span>
<span class="c1">// Then, register it into spark context:</span>
<span class="n">jsc</span><span class="o">.</span><span class="na">sc</span><span class="o">().</span><span class="na">register</span><span class="o">(</span><span class="n">myVectorAcc</span><span class="o">,</span> <span class="s">&quot;MyVectorAcc1&quot;</span><span class="o">);</span></code></pre></figure>

    <p>请注意，当程序员定义自己的AccumulatorV2类型时，结果类型可能与所添加元素的类型不同。</p>

    <p><em>警告</em> ：当Spark任务完成时，Spark将尝试将该任务中累积的更新合并到累加器中。如果失败，Spark将忽略该失败，并仍将任务标记为成功并继续运行其他任务。因此，越野车累加器不会影响Spark作业，但是尽管Spark作业成功，但它可能无法正确更新。</p>

  </div>

<div data-lang="python">

    <p>从初始值创建累加器<code>v</code>通过打电话<code>SparkContext.accumulator(v)</code> 。然后，可以使用<code>add</code>方法或<code>+=</code>操作员。但是，他们无法读取其值。只有驱动程序可以使用累加器的值读取累加器的值<code>value</code>方法。</p>

    <p>下面的代码显示了一个累加器，用于累加一个数组的元素：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">accum</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">accum</span>
<span class="n">Accumulator</span><span class="o">&lt;</span><span class="nb">id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="o">&gt;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">accum</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="o">...</span>
<span class="mi">10</span><span class="o">/</span><span class="mi">09</span><span class="o">/</span><span class="mi">29</span> <span class="mi">18</span><span class="p">:</span><span class="mi">41</span><span class="p">:</span><span class="mi">08</span> <span class="n">INFO</span> <span class="n">SparkContext</span><span class="p">:</span> <span class="n">Tasks</span> <span class="n">finished</span> <span class="ow">in</span> <span class="mf">0.317106</span> <span class="n">s</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">accum</span><span class="o">.</span><span class="n">value</span>
<span class="mi">10</span></code></pre></figure>

    <p>尽管此代码使用了对Int类型的累加器的内置支持，但程序员也可以通过将<a href="api/python/pyspark.html#pyspark.AccumulatorParam">AccumulatorParam</a>子类化来创建自己的类型。AccumulatorParam接口有两种方法： <code>zero</code>用于为您的数据类型提供“零值”，以及<code>addInPlace</code>用于将两个值加在一起。例如，假设我们有一个<code>Vector</code>代表数学向量的类，我们可以这样写：</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">class</span> <span class="nc">VectorAccumulatorParam</span><span class="p">(</span><span class="n">AccumulatorParam</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">zero</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initialValue</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Vector</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">initialValue</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">addInPlace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">):</span>
        <span class="n">v1</span> <span class="o">+=</span> <span class="n">v2</span>
        <span class="k">return</span> <span class="n">v1</span>

<span class="c1"># Then, create an Accumulator of this type:</span>
<span class="n">vecAccum</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="p">(</span><span class="n">Vector</span><span class="p">(</span><span class="o">...</span><span class="p">),</span> <span class="n">VectorAccumulatorParam</span><span class="p">())</span></code></pre></figure>

  </div>

</div>

<p>对于<b>仅</b>在<b>操作</b>内部<b>执行的</b>累加器更新，Spark保证每个任务对累加器的更新将仅应用一次，即重新启动的任务将不会更新该值。在转换中，用户应意识到，如果重新执行任务或作业阶段，则可能不止一次应用每个任务的更新。</p>

<p>累加器不会更改Spark的惰性评估模型。如果在RDD上的操作中对其进行更新，则仅当将RDD计算为操作的一部分时才更新它们的值。因此，在像这样的惰性转换中进行累加器更新时，不能保证执行更新<code>map()</code> 。下面的代码片段演示了此属性：</p>

<div class="codetabs">

<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">accum</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">longAccumulator</span>
<span class="n">data</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="n">accum</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">x</span><span class="o">);</span> <span class="n">x</span> <span class="o">}</span>
<span class="c1">// Here, accum is still 0 because no actions have caused the map operation to be computed.</span></code></pre></figure>

  </div>

<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">LongAccumulator</span> <span class="n">accum</span> <span class="o">=</span> <span class="n">jsc</span><span class="o">.</span><span class="na">sc</span><span class="o">().</span><span class="na">longAccumulator</span><span class="o">();</span>
<span class="n">data</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">x</span> <span class="o">-&gt;</span> <span class="o">{</span> <span class="n">accum</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">x</span><span class="o">);</span> <span class="k">return</span> <span class="n">f</span><span class="o">(</span><span class="n">x</span><span class="o">);</span> <span class="o">});</span>
<span class="c1">// Here, accum is still 0 because no actions have caused the `map` to be computed.</span></code></pre></figure>

  </div>

<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">accum</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">accum</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
<span class="c1"># Here, accum is still 0 because no actions have caused the `map` to be computed.</span></code></pre></figure>

  </div>

</div>

<h1 id="deploying-to-a-cluster">部署到集群</h1>

<p><a href="submitting-applications.html">应用程序提交指南</a>介绍了如何将应用程序提交到集群。简而言之，将应用程序打包到JAR（用于Java / Scala）或一组<code>.py</code>要么<code>.zip</code>文件（对于Python）， <code>bin/spark-submit</code>脚本可让您将其提交给任何受支持的集群管理器。</p>

<h1 id="launching-spark-jobs-from-java--scala">从Java / Scala启动Spark作业</h1>

<p><a href="api/java/index.html?org/apache/spark/launcher/package-summary.html">org.apache.spark.launcher</a>软件包提供了一些类，用于使用简单的Java API将Spark作业作为子进程启动。</p>

<h1 id="unit-testing">单元测试</h1>

<p>Spark非常适合使用任何流行的单元测试框架进行单元测试。只需创建一个<code>SparkContext</code>在测试中将主URL设置为<code>local</code> ，运行您的操作，然后致电<code>SparkContext.stop()</code>拆掉它。确保您在<code>finally</code>块或测试框架的<code>tearDown</code>方法，因为Spark不支持在同一程序中同时运行的两个上下文。</p>

<h1 id="where-to-go-from-here">从这往哪儿走</h1>

<p>您可以在Spark网站上看到一些<a href="https://spark.apache.org/examples.html">示例Spark程序</a> 。此外，Spark在<code>examples</code>目录（ <a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples">Scala</a> ， <a href="https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples">Java</a> ， <a href="https://github.com/apache/spark/tree/master/examples/src/main/python">Python</a> ， <a href="https://github.com/apache/spark/tree/master/examples/src/main/r">R</a> ）。您可以通过将类名传递给Spark的类来运行Java和Scala示例<code>bin/run-example</code>脚本;例如：</p>

<pre><code>./bin/run-example SparkPi
</code></pre>

<p>对于Python示例，请使用<code>spark-submit</code>代替：</p>

<pre><code>./bin/spark-submit examples/src/main/python/pi.py
</code></pre>

<p>对于R示例，请使用<code>spark-submit</code>代替：</p>

<pre><code>./bin/spark-submit examples/src/main/r/dataframe.R
</code></pre>

<p>为了帮助您优化程序， <a href="configuration.html">配置</a>和<a href="tuning.html">调优</a>指南提供了有关最佳实践的信息。它们对于确保您的数据以有效格式存储在内存中尤其重要。为了获得部署方面的帮助， <a href="cluster-overview.html">群集模式概述</a>介绍了分布式操作和支持的群集管理器中涉及的组件。</p>

<p>最后， <a href="api/scala/#org.apache.spark.package">Scala</a> ， <a href="api/java/">Java</a> ， <a href="api/python/">Python</a>和<a href="api/R/">R中</a>提供了完整的API文档。</p>


                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-3.4.1.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    

</body></html>