<html class="no-js" ><!--<![endif]--><head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>SparkR（R on Spark）-Spark 3.0.0-预览文档</title>
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        
        <!-- Google analytics script -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-32518208-2']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
        </script>
        

    </head>
    <body >
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html"><img src="img/spark-logo-hd.png" style="height:50px"></a> <span class="version">3.0.0预览版</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">总览</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">编程指南<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">快速开始</a></li>
                                <li><a href="rdd-programming-guide.html">RDD，累加器，广播变量</a></li>
                                <li><a href="sql-programming-guide.html">SQL，数据框和数据集</a></li>
                                <li><a href="structured-streaming-programming-guide.html">结构化流</a></li>
                                <li><a href="streaming-programming-guide.html">火花流（DStreams）</a></li>
                                <li><a href="ml-guide.html">MLlib（机器学习）</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX（图形处理）</a></li>
                                <li><a href="sparkr.html">SparkR（Spark上的R）</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API文件<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">斯卡拉</a></li>
                                <li><a href="api/java/index.html">爪哇</a></li>
                                <li><a href="api/python/index.html">蟒蛇</a></li>
                                <li><a href="api/R/index.html">[R</a></li>
                                <li><a href="api/sql/index.html">SQL，内置函数</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">部署中<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">总览</a></li>
                                <li><a href="submitting-applications.html">提交申请</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark独立</a></li>
                                <li><a href="running-on-mesos.html">梅索斯</a></li>
                                <li><a href="running-on-yarn.html">纱</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">更多<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">组态</a></li>
                                <li><a href="monitoring.html">监控方式</a></li>
                                <li><a href="tuning.html">调音指南</a></li>
                                <li><a href="job-scheduling.html">作业调度</a></li>
                                <li><a href="security.html">安全</a></li>
                                <li><a href="hardware-provisioning.html">硬件配置</a></li>
                                <li><a href="migration-guide.html">迁移指南</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">建筑火花</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">为Spark贡献</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">第三方项目</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v3.0.0-preview</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                <div class="content" id="content">
                    
                        <h1 class="title">SparkR（Spark上的R）</h1>
                    

                    <ul id="markdown-toc">
  <li><a href="#overview" id="markdown-toc-overview">总览</a></li>
  <li><a href="#sparkdataframe" id="markdown-toc-sparkdataframe">SparkDataFrame</a>    <ul>
      <li><a href="#starting-up-sparksession" id="markdown-toc-starting-up-sparksession">启动：SparkSession</a></li>
      <li><a href="#starting-up-from-rstudio" id="markdown-toc-starting-up-from-rstudio">从RStudio启动</a></li>
      <li><a href="#creating-sparkdataframes" id="markdown-toc-creating-sparkdataframes">创建SparkDataFrames</a>        <ul>
          <li><a href="#from-local-data-frames" id="markdown-toc-from-local-data-frames">从本地数据帧</a></li>
          <li><a href="#from-data-sources" id="markdown-toc-from-data-sources">来自数据源</a></li>
          <li><a href="#from-hive-tables" id="markdown-toc-from-hive-tables">从蜂巢表</a></li>
        </ul>
      </li>
      <li><a href="#sparkdataframe-operations" id="markdown-toc-sparkdataframe-operations">SparkDataFrame操作</a>        <ul>
          <li><a href="#selecting-rows-columns" id="markdown-toc-selecting-rows-columns">选择行，列</a></li>
          <li><a href="#grouping-aggregation" id="markdown-toc-grouping-aggregation">分组，汇总</a></li>
          <li><a href="#operating-on-columns" id="markdown-toc-operating-on-columns">在列上操作</a></li>
          <li><a href="#applying-user-defined-function" id="markdown-toc-applying-user-defined-function">应用用户定义的功能</a>            <ul>
              <li><a href="#run-a-given-function-on-a-large-dataset-using-dapply-or-dapplycollect" id="markdown-toc-run-a-given-function-on-a-large-dataset-using-dapply-or-dapplycollect">使用大型数据集运行给定函数<code>dapply</code>要么<code>dapplyCollect</code></a>                <ul>
                  <li><a href="#dapply" id="markdown-toc-dapply">dapply</a></li>
                  <li><a href="#dapplycollect" id="markdown-toc-dapplycollect">dapplyCollect</a></li>
                </ul>
              </li>
              <li><a href="#run-a-given-function-on-a-large-dataset-grouping-by-input-columns-and-using-gapply-or-gapplycollect" id="markdown-toc-run-a-given-function-on-a-large-dataset-grouping-by-input-columns-and-using-gapply-or-gapplycollect">在按输入列分组并使用的大型数据集上运行给定函数<code>gapply</code>要么<code>gapplyCollect</code></a>                <ul>
                  <li><a href="#gapply" id="markdown-toc-gapply">缝隙地</a></li>
                  <li><a href="#gapplycollect" id="markdown-toc-gapplycollect">gapply收集</a></li>
                </ul>
              </li>
              <li><a href="#run-local-r-functions-distributed-using-sparklapply" id="markdown-toc-run-local-r-functions-distributed-using-sparklapply">运行使用分发的本地R函数<code>spark.lapply</code></a>                <ul>
                  <li><a href="#sparklapply" id="markdown-toc-sparklapply">火花</a></li>
                </ul>
              </li>
            </ul>
          </li>
          <li><a href="#eager-execution" id="markdown-toc-eager-execution">急于执行</a></li>
        </ul>
      </li>
      <li><a href="#running-sql-queries-from-sparkr" id="markdown-toc-running-sql-queries-from-sparkr">从SparkR运行SQL查询</a></li>
    </ul>
  </li>
  <li><a href="#machine-learning" id="markdown-toc-machine-learning">机器学习</a>    <ul>
      <li><a href="#algorithms" id="markdown-toc-algorithms">演算法</a>        <ul>
          <li><a href="#classification" id="markdown-toc-classification">分类</a></li>
          <li><a href="#regression" id="markdown-toc-regression">回归</a></li>
          <li><a href="#tree" id="markdown-toc-tree">树</a></li>
          <li><a href="#clustering" id="markdown-toc-clustering">聚类</a></li>
          <li><a href="#collaborative-filtering" id="markdown-toc-collaborative-filtering">协同过滤</a></li>
          <li><a href="#frequent-pattern-mining" id="markdown-toc-frequent-pattern-mining">频繁模式挖掘</a></li>
          <li><a href="#statistics" id="markdown-toc-statistics">统计</a></li>
        </ul>
      </li>
      <li><a href="#model-persistence" id="markdown-toc-model-persistence">模型持久性</a></li>
    </ul>
  </li>
  <li><a href="#data-type-mapping-between-r-and-spark" id="markdown-toc-data-type-mapping-between-r-and-spark">R和Spark之间的数据类型映射</a></li>
  <li><a href="#structured-streaming" id="markdown-toc-structured-streaming">结构化流</a></li>
  <li><a href="#apache-arrow-in-sparkr" id="markdown-toc-apache-arrow-in-sparkr">SparkR中的Apache Arrow</a>    <ul>
      <li><a href="#ensure-arrow-installed" id="markdown-toc-ensure-arrow-installed">确保安装了Arrow</a></li>
      <li><a href="#enabling-for-conversion-tofrom-r-dataframe-dapply-and-gapply" id="markdown-toc-enabling-for-conversion-tofrom-r-dataframe-dapply-and-gapply">启用与R DataFrame之间的转换， <code>dapply</code>和<code>gapply</code></a></li>
      <li><a href="#supported-sql-types" id="markdown-toc-supported-sql-types">支持的SQL类型</a></li>
    </ul>
  </li>
  <li><a href="#r-function-name-conflicts" id="markdown-toc-r-function-name-conflicts">R函数名称冲突</a></li>
  <li><a href="#migration-guide" id="markdown-toc-migration-guide">迁移指南</a></li>
</ul>

<h1 id="overview">总览</h1>
<p>SparkR是R软件包，提供轻量级的前端来使用R上的Apache Spark。在Spark 3.0.0-preview中，SparkR提供了分布式数据帧实现，支持诸如选择，过滤，聚合等操作（类似于R数据）。帧， <a href="https://github.com/hadley/dplyr">dplyr</a> ），但适用于大型数据集。SparkR还支持使用MLlib进行分布式机器学习。</p>

<h1 id="sparkdataframe">SparkDataFrame</h1>

<p>SparkDataFrame是组织为命名列的分布式数据集合。从概念上讲，它等效于关系数据库中的表或R中的数据框，但在后台进行了更丰富的优化。SparkDataFrames可以从多种资源中构建，例如：结构化数据文件，Hive中的表，外部数据库或现有的本地R数据帧。</p>

<p>本页上的所有示例均使用R或Spark发行版中包含的示例数据，并且可以使用<code>./bin/sparkR</code>贝壳。</p>

<h2 id="starting-up-sparksession">启动：SparkSession</h2>

<div data-lang="r">
  <p>SparkR的入口点是<code>SparkSession</code>将您的R程序连接到Spark集群。您可以创建一个<code>SparkSession</code>使用<code>sparkR.session</code>并传入诸如应用程序名称，所依赖的任何Spark包等选项。此外，您还可以通过以下方式使用SparkDataFrames <code>SparkSession</code> 。如果您是从<code>sparkR</code>外壳<code>SparkSession</code>应该已经为您创建了，您无需致电<code>sparkR.session</code> 。</p>

  <div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>sparkR.session<span class="p">()</span></code></pre></figure>

  </div>

  <h2 id="starting-up-from-rstudio">从RStudio启动</h2>

  <p>您也可以从RStudio启动SparkR。您可以将R程序从RStudio，R Shell，Rscript或其他R IDE连接到Spark集群。首先，请确保在环境中设置了SPARK_HOME（可以检查<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/Sys.getenv.html">Sys.getenv</a> ），加载SparkR软件包，然后调用<code>sparkR.session</code>如下。它将检查Spark安装，如果找不到，将自动下载并缓存。或者，您也可以运行<code>install.spark</code>手动。</p>

  <p>除了打电话<code>sparkR.session</code> ，您还可以指定某些Spark驱动程序属性。通常，无法以编程方式设置这些<a href="configuration.html#application-properties">应用程序属性</a>和<a href="configuration.html#runtime-environment">运行时环境</a> ，因为将启动驱动程序JVM进程，在这种情况下，SparkR会为您解决这一问题。要设置它们，请像传递其他配置属性一样传递它们<code>sparkConfig</code>争论<code>sparkR.session()</code> 。</p>

  <div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="kr">if</span> <span class="p">(</span><span class="kp">nchar</span><span class="p">(</span><span class="kp">Sys.getenv</span><span class="p">(</span><span class="s">&quot;SPARK_HOME&quot;</span><span class="p">))</span> <span class="o">&lt;</span> <span class="m">1</span><span class="p">)</span> <span class="p">{</span>
  <span class="kp">Sys.setenv</span><span class="p">(</span>SPARK_HOME <span class="o">=</span> <span class="s">&quot;/home/spark&quot;</span><span class="p">)</span>
<span class="p">}</span>
<span class="kn">library</span><span class="p">(</span>SparkR<span class="p">,</span> lib.loc <span class="o">=</span> <span class="kt">c</span><span class="p">(</span><span class="kp">file.path</span><span class="p">(</span><span class="kp">Sys.getenv</span><span class="p">(</span><span class="s">&quot;SPARK_HOME&quot;</span><span class="p">),</span> <span class="s">&quot;R&quot;</span><span class="p">,</span> <span class="s">&quot;lib&quot;</span><span class="p">)))</span>
sparkR.session<span class="p">(</span>master <span class="o">=</span> <span class="s">&quot;local[*]&quot;</span><span class="p">,</span> sparkConfig <span class="o">=</span> <span class="kt">list</span><span class="p">(</span>spark.driver.memory <span class="o">=</span> <span class="s">&quot;2g&quot;</span><span class="p">))</span></code></pre></figure>

  </div>

  <p>可以在以下Spark驱动程序属性中进行设置<code>sparkConfig</code>与<code>sparkR.session</code>从RStudio：</p>

  <table class="table">
  <tbody><tr><th>物业名称</th><th>物业组</th><th><code>spark-submit</code>当量</th></tr>
  <tr>
    <td><code>spark.master</code></td>
    <td>应用属性</td>
    <td><code>--master</code></td>
  </tr>
  <tr>
    <td><code>spark.kerberos.keytab</code></td>
    <td>应用属性</td>
    <td><code>--keytab</code></td>
  </tr>
  <tr>
    <td><code>spark.kerberos.principal</code></td>
    <td>应用属性</td>
    <td><code>--principal</code></td>
  </tr>
  <tr>
    <td><code>spark.driver.memory</code></td>
    <td>应用属性</td>
    <td><code>--driver-memory</code></td>
  </tr>
  <tr>
    <td><code>spark.driver.extraClassPath</code></td>
    <td>运行环境</td>
    <td><code>--driver-class-path</code></td>
  </tr>
  <tr>
    <td><code>spark.driver.extraJavaOptions</code></td>
    <td>运行环境</td>
    <td><code>--driver-java-options</code></td>
  </tr>
  <tr>
    <td><code>spark.driver.extraLibraryPath</code></td>
    <td>运行环境</td>
    <td><code>--driver-library-path</code></td>
  </tr>
</tbody></table>

</div>

<h2 id="creating-sparkdataframes">创建SparkDataFrames</h2>
<p>用<code>SparkSession</code> ，应用程序可以创建<code>SparkDataFrame</code>来自本地R数据帧， <a href="sql-data-sources-hive-tables.html">Hive表</a>或其他<a href="sql-data-sources.html">数据源</a> 。</p>

<h3 id="from-local-data-frames">从本地数据帧</h3>
<p>创建数据帧的最简单方法是将本地R数据帧转换为SparkDataFrame。具体来说，我们可以使用<code>as.DataFrame</code>要么<code>createDataFrame</code>并传入本地R数据帧以创建SparkDataFrame。例如，以下内容创建了一个<code>SparkDataFrame</code>基于使用<code>faithful</code>来自R的数据集。</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>df <span class="o">&lt;-</span> as.DataFrame<span class="p">(</span>faithful<span class="p">)</span>

<span class="c1"># Displays the first part of the SparkDataFrame</span>
<span class="kp">head</span><span class="p">(</span>df<span class="p">)</span>
<span class="c1">##  eruptions waiting</span>
<span class="c1">##1     3.600      79</span>
<span class="c1">##2     1.800      54</span>
<span class="c1">##3     3.333      74</span></code></pre></figure>

</div>

<h3 id="from-data-sources">来自数据源</h3>

<p>SparkR支持通过以下方式对各种数据源进行操作： <code>SparkDataFrame</code>接口。本节介绍使用数据源加载和保存数据的一般方法。您可以查看Spark SQL编程指南，以获取适用于内置数据源的更多<a href="sql-data-sources-load-save-functions.html#manually-specifying-options">特定选项</a> 。</p>

<p>从数据源创建SparkDataFrames的一般方法是<code>read.df</code> 。此方法接受要加载的文件的路径和数据源的类型，并且将自动使用当前活动的SparkSession。SparkR支持本地读取JSON，CSV和Parquet文件，通过<a href="https://spark.apache.org/third-party-projects.html">第三方项目</a>等来源的软件包，您可以找到流行文件格式（如Avro）的数据源连接器。这些软件包可以通过指定来添加<code>--packages</code>与<code>spark-submit</code>要么<code>sparkR</code>命令，或者使用以下命令初始化SparkSession <code>sparkPackages</code>在交互式R Shell中或从RStudio中获取参数。</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>sparkR.session<span class="p">(</span>sparkPackages <span class="o">=</span> <span class="s">&quot;org.apache.spark:spark-avro_2.12:3.0.0-preview&quot;</span><span class="p">)</span></code></pre></figure>

</div>

<p>我们可以使用示例JSON输入文件来查看如何使用数据源。请注意，此处使用的文件<em>不是</em>典型的JSON文件。文件中的每一行都必须包含一个单独的，自包含的有效JSON对象。有关更多信息，请参见<a href="http://jsonlines.org/">JSON Lines文本格式，也称为newline分隔的JSON</a> 。因此，常规的多行JSON文件通常会失败。</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>people <span class="o">&lt;-</span> read.df<span class="p">(</span><span class="s">&quot;./examples/src/main/resources/people.json&quot;</span><span class="p">,</span> <span class="s">&quot;json&quot;</span><span class="p">)</span>
<span class="kp">head</span><span class="p">(</span>people<span class="p">)</span>
<span class="c1">##  age    name</span>
<span class="c1">##1  NA Michael</span>
<span class="c1">##2  30    Andy</span>
<span class="c1">##3  19  Justin</span>

<span class="c1"># SparkR automatically infers the schema from the JSON file</span>
printSchema<span class="p">(</span>people<span class="p">)</span>
<span class="c1"># root</span>
<span class="c1">#  |-- age: long (nullable = true)</span>
<span class="c1">#  |-- name: string (nullable = true)</span>

<span class="c1"># Similarly, multiple files can be read with read.json</span>
people <span class="o">&lt;-</span> read.json<span class="p">(</span><span class="kt">c</span><span class="p">(</span><span class="s">&quot;./examples/src/main/resources/people.json&quot;</span><span class="p">,</span> <span class="s">&quot;./examples/src/main/resources/people2.json&quot;</span><span class="p">))</span></code></pre></figure>

</div>

<p>数据源API本机支持CSV格式的输入文件。有关更多信息，请参阅SparkR <a href="api/R/read.df.html">read.df</a> API文档。</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>df <span class="o">&lt;-</span> read.df<span class="p">(</span>csvPath<span class="p">,</span> <span class="s">&quot;csv&quot;</span><span class="p">,</span> header <span class="o">=</span> <span class="s">&quot;true&quot;</span><span class="p">,</span> inferSchema <span class="o">=</span> <span class="s">&quot;true&quot;</span><span class="p">,</span> na.strings <span class="o">=</span> <span class="s">&quot;NA&quot;</span><span class="p">)</span></code></pre></figure>

</div>

<p>数据源API也可以用于将SparkDataFrame保存为多种文件格式。例如，我们可以使用以下示例将上一个示例中的SparkDataFrame保存到Parquet文件中<code>write.df</code> 。</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>write.df<span class="p">(</span>people<span class="p">,</span> path <span class="o">=</span> <span class="s">&quot;people.parquet&quot;</span><span class="p">,</span> <span class="kn">source</span> <span class="o">=</span> <span class="s">&quot;parquet&quot;</span><span class="p">,</span> mode <span class="o">=</span> <span class="s">&quot;overwrite&quot;</span><span class="p">)</span></code></pre></figure>

</div>

<h3 id="from-hive-tables">从蜂巢表</h3>

<p>您还可以从Hive表创建SparkDataFrames。为此，我们将需要创建一个具有Hive支持的SparkSession，它可以访问Hive MetaStore中的表。请注意，Spark应该已经在<a href="building-spark.html#building-with-hive-and-jdbc-support">Hive支持下</a>构建，并且更多详细信息可以在<a href="sql-getting-started.html#starting-point-sparksession">SQL编程指南中找到</a> 。在SparkR中，默认情况下，它将尝试创建启用了Hive支持的SparkSession（ <code>enableHiveSupport = TRUE</code> ）。</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>sparkR.session<span class="p">()</span>

sql<span class="p">(</span><span class="s">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;</span><span class="p">)</span>
sql<span class="p">(</span><span class="s">&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;</span><span class="p">)</span>

<span class="c1"># Queries can be expressed in HiveQL.</span>
results <span class="o">&lt;-</span> sql<span class="p">(</span><span class="s">&quot;FROM src SELECT key, value&quot;</span><span class="p">)</span>

<span class="c1"># results is now a SparkDataFrame</span>
<span class="kp">head</span><span class="p">(</span>results<span class="p">)</span>
<span class="c1">##  key   value</span>
<span class="c1">## 1 238 val_238</span>
<span class="c1">## 2  86  val_86</span>
<span class="c1">## 3 311 val_311</span></code></pre></figure>

</div>

<h2 id="sparkdataframe-operations">SparkDataFrame操作</h2>

<p>SparkDataFrames支持许多功能来进行结构化数据处理。在这里，我们包括一些基本示例，可以在<a href="api/R/index.html">API</a>文档中找到完整列表：</p>

<h3 id="selecting-rows-columns">选择行，列</h3>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="c1"># Create the SparkDataFrame</span>
df <span class="o">&lt;-</span> as.DataFrame<span class="p">(</span>faithful<span class="p">)</span>

<span class="c1"># Get basic information about the SparkDataFrame</span>
df
<span class="c1">## SparkDataFrame[eruptions:double, waiting:double]</span>

<span class="c1"># Select only the &quot;eruptions&quot; column</span>
<span class="kp">head</span><span class="p">(</span>select<span class="p">(</span>df<span class="p">,</span> df<span class="o">$</span>eruptions<span class="p">))</span>
<span class="c1">##  eruptions</span>
<span class="c1">##1     3.600</span>
<span class="c1">##2     1.800</span>
<span class="c1">##3     3.333</span>

<span class="c1"># You can also pass in column name as strings</span>
<span class="kp">head</span><span class="p">(</span>select<span class="p">(</span>df<span class="p">,</span> <span class="s">&quot;eruptions&quot;</span><span class="p">))</span>

<span class="c1"># Filter the SparkDataFrame to only retain rows with wait times shorter than 50 mins</span>
<span class="kp">head</span><span class="p">(</span>filter<span class="p">(</span>df<span class="p">,</span> df<span class="o">$</span>waiting <span class="o">&lt;</span> <span class="m">50</span><span class="p">))</span>
<span class="c1">##  eruptions waiting</span>
<span class="c1">##1     1.750      47</span>
<span class="c1">##2     1.750      47</span>
<span class="c1">##3     1.867      48</span></code></pre></figure>

</div>

<h3 id="grouping-aggregation">分组，汇总</h3>

<p>SparkR数据帧支持许多常用功能以在分组后聚合数据。例如，我们可以计算出<code>waiting</code>在的时间<code>faithful</code>数据集如下所示</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="c1"># We use the `n` operator to count the number of times each waiting time appears</span>
<span class="kp">head</span><span class="p">(</span>summarize<span class="p">(</span>groupBy<span class="p">(</span>df<span class="p">,</span> df<span class="o">$</span>waiting<span class="p">),</span> count <span class="o">=</span> n<span class="p">(</span>df<span class="o">$</span>waiting<span class="p">)))</span>
<span class="c1">##  waiting count</span>
<span class="c1">##1      70     4</span>
<span class="c1">##2      67     1</span>
<span class="c1">##3      69     2</span>

<span class="c1"># We can also sort the output from the aggregation to get the most common waiting times</span>
waiting_counts <span class="o">&lt;-</span> summarize<span class="p">(</span>groupBy<span class="p">(</span>df<span class="p">,</span> df<span class="o">$</span>waiting<span class="p">),</span> count <span class="o">=</span> n<span class="p">(</span>df<span class="o">$</span>waiting<span class="p">))</span>
<span class="kp">head</span><span class="p">(</span>arrange<span class="p">(</span>waiting_counts<span class="p">,</span> desc<span class="p">(</span>waiting_counts<span class="o">$</span>count<span class="p">)))</span>
<span class="c1">##   waiting count</span>
<span class="c1">##1      78    15</span>
<span class="c1">##2      83    14</span>
<span class="c1">##3      81    13</span></code></pre></figure>

</div>

<p>除标准聚合外，SparkR还支持<a href="https://en.wikipedia.org/wiki/OLAP_cube">OLAP多维数据集</a>运算符<code>cube</code> ：</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="kp">head</span><span class="p">(</span>agg<span class="p">(</span>cube<span class="p">(</span>df<span class="p">,</span> <span class="s">&quot;cyl&quot;</span><span class="p">,</span> <span class="s">&quot;disp&quot;</span><span class="p">,</span> <span class="s">&quot;gear&quot;</span><span class="p">),</span> avg<span class="p">(</span>df<span class="o">$</span>mpg<span class="p">)))</span>
<span class="c1">##  cyl  disp gear avg(mpg)</span>
<span class="c1">##1  NA 140.8    4     22.8</span>
<span class="c1">##2   4  75.7    4     30.4</span>
<span class="c1">##3   8 400.0    3     19.2</span>
<span class="c1">##4   8 318.0    3     15.5</span>
<span class="c1">##5  NA 351.0   NA     15.8</span>
<span class="c1">##6  NA 275.8   NA     16.3</span></code></pre></figure>

</div>

<p>和<code>rollup</code> ：</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="kp">head</span><span class="p">(</span>agg<span class="p">(</span>rollup<span class="p">(</span>df<span class="p">,</span> <span class="s">&quot;cyl&quot;</span><span class="p">,</span> <span class="s">&quot;disp&quot;</span><span class="p">,</span> <span class="s">&quot;gear&quot;</span><span class="p">),</span> avg<span class="p">(</span>df<span class="o">$</span>mpg<span class="p">)))</span>
<span class="c1">##  cyl  disp gear avg(mpg)</span>
<span class="c1">##1   4  75.7    4     30.4</span>
<span class="c1">##2   8 400.0    3     19.2</span>
<span class="c1">##3   8 318.0    3     15.5</span>
<span class="c1">##4   4  78.7   NA     32.4</span>
<span class="c1">##5   8 304.0    3     15.2</span>
<span class="c1">##6   4  79.0   NA     27.3</span></code></pre></figure>

</div>

<h3 id="operating-on-columns">在列上操作</h3>

<p>SparkR还提供了许多功能，这些功能可以直接应用于列以进行数据处理和聚合。以下示例显示了基本算术函数的用法。</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="c1"># Convert waiting time from hours to seconds.</span>
<span class="c1"># Note that we can assign this to a new column in the same SparkDataFrame</span>
df<span class="o">$</span>waiting_secs <span class="o">&lt;-</span> df<span class="o">$</span>waiting <span class="o">*</span> <span class="m">60</span>
<span class="kp">head</span><span class="p">(</span>df<span class="p">)</span>
<span class="c1">##  eruptions waiting waiting_secs</span>
<span class="c1">##1     3.600      79         4740</span>
<span class="c1">##2     1.800      54         3240</span>
<span class="c1">##3     3.333      74         4440</span></code></pre></figure>

</div>

<h3 id="applying-user-defined-function">应用用户定义的功能</h3>
<p>在SparkR中，我们支持多种用户定义的函数：</p>

<h4 id="run-a-given-function-on-a-large-dataset-using-dapply-or-dapplycollect">使用大型数据集运行给定函数<code>dapply</code>要么<code>dapplyCollect</code></h4>

<h5 id="dapply">dapply</h5>
<p>将函数应用于<code>SparkDataFrame</code> 。应用于每个分区的功能<code>SparkDataFrame</code>并且应该只有一个参数， <code>data.frame</code>对应于每个分区将被传递。函数的输出应为<code>data.frame</code> 。模式指定结果行的行格式<code>SparkDataFrame</code> 。它必须与返回值的<a href="#data-type-mapping-between-r-and-spark">数据类型</a>匹配。</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="c1"># Convert waiting time from hours to seconds.</span>
<span class="c1"># Note that we can apply UDF to DataFrame.</span>
schema <span class="o">&lt;-</span> structType<span class="p">(</span>structField<span class="p">(</span><span class="s">&quot;eruptions&quot;</span><span class="p">,</span> <span class="s">&quot;double&quot;</span><span class="p">),</span> structField<span class="p">(</span><span class="s">&quot;waiting&quot;</span><span class="p">,</span> <span class="s">&quot;double&quot;</span><span class="p">),</span>
                     structField<span class="p">(</span><span class="s">&quot;waiting_secs&quot;</span><span class="p">,</span> <span class="s">&quot;double&quot;</span><span class="p">))</span>
df1 <span class="o">&lt;-</span> dapply<span class="p">(</span>df<span class="p">,</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">)</span> <span class="p">{</span> x <span class="o">&lt;-</span> <span class="kp">cbind</span><span class="p">(</span>x<span class="p">,</span> x<span class="o">$</span>waiting <span class="o">*</span> <span class="m">60</span><span class="p">)</span> <span class="p">},</span> schema<span class="p">)</span>
<span class="kp">head</span><span class="p">(</span>collect<span class="p">(</span>df1<span class="p">))</span>
<span class="c1">##  eruptions waiting waiting_secs</span>
<span class="c1">##1     3.600      79         4740</span>
<span class="c1">##2     1.800      54         3240</span>
<span class="c1">##3     3.333      74         4440</span>
<span class="c1">##4     2.283      62         3720</span>
<span class="c1">##5     4.533      85         5100</span>
<span class="c1">##6     2.883      55         3300</span></code></pre></figure>

</div>

<h5 id="dapplycollect">dapplyCollect</h5>
<p>喜欢<code>dapply</code> ，将函数应用于<code>SparkDataFrame</code>并将结果收集回来。函数的输出应为<code>data.frame</code> 。但是，不需要传递架构。注意<code>dapplyCollect</code>如果无法在所有分区上运行的UDF的输出被拉到驱动程序并适合驱动程序内存，则会失败。</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="c1"># Convert waiting time from hours to seconds.</span>
<span class="c1"># Note that we can apply UDF to DataFrame and return a R&#39;s data.frame</span>
ldf <span class="o">&lt;-</span> dapplyCollect<span class="p">(</span>
         df<span class="p">,</span>
         <span class="kr">function</span><span class="p">(</span>x<span class="p">)</span> <span class="p">{</span>
           x <span class="o">&lt;-</span> <span class="kp">cbind</span><span class="p">(</span>x<span class="p">,</span> <span class="s">&quot;waiting_secs&quot;</span> <span class="o">=</span> x<span class="o">$</span>waiting <span class="o">*</span> <span class="m">60</span><span class="p">)</span>
         <span class="p">})</span>
<span class="kp">head</span><span class="p">(</span>ldf<span class="p">,</span> <span class="m">3</span><span class="p">)</span>
<span class="c1">##  eruptions waiting waiting_secs</span>
<span class="c1">##1     3.600      79         4740</span>
<span class="c1">##2     1.800      54         3240</span>
<span class="c1">##3     3.333      74         4440</span></code></pre></figure>

</div>

<h4 id="run-a-given-function-on-a-large-dataset-grouping-by-input-columns-and-using-gapply-or-gapplycollect">在按输入列分组并使用的大型数据集上运行给定函数<code>gapply</code>要么<code>gapplyCollect</code></h4>

<h5 id="gapply">缝隙地</h5>
<p>将功能应用于每个组<code>SparkDataFrame</code> 。该功能适用于每组<code>SparkDataFrame</code>并且应该只有两个参数：分组键和R <code>data.frame</code>对应于该键。这些组选自<code>SparkDataFrame</code>列。函数的输出应为<code>data.frame</code> 。模式指定结果的行格式<code>SparkDataFrame</code> 。它必须基于Spark <a href="#data-type-mapping-between-r-and-spark">数据类型</a>表示R函数的输出模式。返回的列名<code>data.frame</code>由用户设置。</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="c1"># Determine six waiting times with the largest eruption time in minutes.</span>
schema <span class="o">&lt;-</span> structType<span class="p">(</span>structField<span class="p">(</span><span class="s">&quot;waiting&quot;</span><span class="p">,</span> <span class="s">&quot;double&quot;</span><span class="p">),</span> structField<span class="p">(</span><span class="s">&quot;max_eruption&quot;</span><span class="p">,</span> <span class="s">&quot;double&quot;</span><span class="p">))</span>
result <span class="o">&lt;-</span> gapply<span class="p">(</span>
    df<span class="p">,</span>
    <span class="s">&quot;waiting&quot;</span><span class="p">,</span>
    <span class="kr">function</span><span class="p">(</span>key<span class="p">,</span> x<span class="p">)</span> <span class="p">{</span>
        y <span class="o">&lt;-</span> <span class="kt">data.frame</span><span class="p">(</span>key<span class="p">,</span> <span class="kp">max</span><span class="p">(</span>x<span class="o">$</span>eruptions<span class="p">))</span>
    <span class="p">},</span>
    schema<span class="p">)</span>
<span class="kp">head</span><span class="p">(</span>collect<span class="p">(</span>arrange<span class="p">(</span>result<span class="p">,</span> <span class="s">&quot;max_eruption&quot;</span><span class="p">,</span> decreasing <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)))</span>

<span class="c1">##    waiting   max_eruption</span>
<span class="c1">##1      64       5.100</span>
<span class="c1">##2      69       5.067</span>
<span class="c1">##3      71       5.033</span>
<span class="c1">##4      87       5.000</span>
<span class="c1">##5      63       4.933</span>
<span class="c1">##6      89       4.900</span></code></pre></figure>

</div>

<h5 id="gapplycollect">gapply收集</h5>
<p>喜欢<code>gapply</code> ，将函数应用于<code>SparkDataFrame</code>并将结果收集回R data.frame。函数的输出应为<code>data.frame</code> 。但是，不需要传递架构。注意<code>gapplyCollect</code>如果无法在所有分区上运行的UDF的输出被拉到驱动程序并适合驱动程序内存，则会失败。</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="c1"># Determine six waiting times with the largest eruption time in minutes.</span>
result <span class="o">&lt;-</span> gapplyCollect<span class="p">(</span>
    df<span class="p">,</span>
    <span class="s">&quot;waiting&quot;</span><span class="p">,</span>
    <span class="kr">function</span><span class="p">(</span>key<span class="p">,</span> x<span class="p">)</span> <span class="p">{</span>
        y <span class="o">&lt;-</span> <span class="kt">data.frame</span><span class="p">(</span>key<span class="p">,</span> <span class="kp">max</span><span class="p">(</span>x<span class="o">$</span>eruptions<span class="p">))</span>
        <span class="kp">colnames</span><span class="p">(</span>y<span class="p">)</span> <span class="o">&lt;-</span> <span class="kt">c</span><span class="p">(</span><span class="s">&quot;waiting&quot;</span><span class="p">,</span> <span class="s">&quot;max_eruption&quot;</span><span class="p">)</span>
        y
    <span class="p">})</span>
<span class="kp">head</span><span class="p">(</span>result<span class="p">[</span><span class="kp">order</span><span class="p">(</span>result<span class="o">$</span>max_eruption<span class="p">,</span> decreasing <span class="o">=</span> <span class="kc">TRUE</span><span class="p">),</span> <span class="p">])</span>

<span class="c1">##    waiting   max_eruption</span>
<span class="c1">##1      64       5.100</span>
<span class="c1">##2      69       5.067</span>
<span class="c1">##3      71       5.033</span>
<span class="c1">##4      87       5.000</span>
<span class="c1">##5      63       4.933</span>
<span class="c1">##6      89       4.900</span></code></pre></figure>

</div>

<h4 id="run-local-r-functions-distributed-using-sparklapply">运行使用分发的本地R函数<code>spark.lapply</code></h4>

<h5 id="sparklapply">火花</h5>
<p>相似<code>lapply</code>在本地R中， <code>spark.lapply</code>在元素列表上运行一个函数，并使用Spark分发计算。以类似于以下方式应用功能<code>doParallel</code>要么<code>lapply</code>到列表的元素。所有计算的结果都应该适合一台机器。如果不是这种情况，他们可以做类似的事情<code>df <- createDataFrame(list)</code>然后使用<code>dapply</code></p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="c1"># Perform distributed training of multiple models with spark.lapply. Here, we pass</span>
<span class="c1"># a read-only list of arguments which specifies family the generalized linear model should be.</span>
families <span class="o">&lt;-</span> <span class="kt">c</span><span class="p">(</span><span class="s">&quot;gaussian&quot;</span><span class="p">,</span> <span class="s">&quot;poisson&quot;</span><span class="p">)</span>
train <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>family<span class="p">)</span> <span class="p">{</span>
  model <span class="o">&lt;-</span> glm<span class="p">(</span>Sepal.Length <span class="o">~</span> Sepal.Width <span class="o">+</span> Species<span class="p">,</span> iris<span class="p">,</span> family <span class="o">=</span> family<span class="p">)</span>
  <span class="kp">summary</span><span class="p">(</span>model<span class="p">)</span>
<span class="p">}</span>
<span class="c1"># Return a list of model&#39;s summaries</span>
model.summaries <span class="o">&lt;-</span> spark.lapply<span class="p">(</span>families<span class="p">,</span> train<span class="p">)</span>

<span class="c1"># Print the summary of each model</span>
<span class="kp">print</span><span class="p">(</span>model.summaries<span class="p">)</span></code></pre></figure>

</div>

<h3 id="eager-execution">急于执行</h3>

<p>如果启用了急切执行，则数据将在<code>SparkDataFrame</code>被建造。默认情况下，不启用急切执行，可以通过设置配置属性来启用<code>spark.sql.repl.eagerEval.enabled</code>至<code>true</code>当。。。的时候<code>SparkSession</code>启动。</p>

<p>可以通过以下方式控制要显示的最大行数和每列数据的最大字符数<code>spark.sql.repl.eagerEval.maxNumRows</code>和<code>spark.sql.repl.eagerEval.truncate</code>配置属性。这些属性仅在启用急切执行时才有效。如果未明确设置这些属性，则默认情况下，将显示最多20行和每列最多20个字符的数据。</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="c1"># Start up spark session with eager execution enabled</span>
sparkR.session<span class="p">(</span>master <span class="o">=</span> <span class="s">&quot;local[*]&quot;</span><span class="p">,</span>
               sparkConfig <span class="o">=</span> <span class="kt">list</span><span class="p">(</span>spark.sql.repl.eagerEval.enabled <span class="o">=</span> <span class="s">&quot;true&quot;</span><span class="p">,</span>
                                  spark.sql.repl.eagerEval.maxNumRows <span class="o">=</span> <span class="kp">as.integer</span><span class="p">(</span><span class="m">10</span><span class="p">)))</span>

<span class="c1"># Create a grouped and sorted SparkDataFrame</span>
df <span class="o">&lt;-</span> createDataFrame<span class="p">(</span>faithful<span class="p">)</span>
df2 <span class="o">&lt;-</span> arrange<span class="p">(</span>summarize<span class="p">(</span>groupBy<span class="p">(</span>df<span class="p">,</span> df<span class="o">$</span>waiting<span class="p">),</span> count <span class="o">=</span> n<span class="p">(</span>df<span class="o">$</span>waiting<span class="p">)),</span> <span class="s">&quot;waiting&quot;</span><span class="p">)</span>

<span class="c1"># Similar to R data.frame, displays the data returned, instead of SparkDataFrame class string</span>
df2

<span class="c1">##+-------+-----+</span>
<span class="c1">##|waiting|count|</span>
<span class="c1">##+-------+-----+</span>
<span class="c1">##|   43.0|    1|</span>
<span class="c1">##|   45.0|    3|</span>
<span class="c1">##|   46.0|    5|</span>
<span class="c1">##|   47.0|    4|</span>
<span class="c1">##|   48.0|    3|</span>
<span class="c1">##|   49.0|    5|</span>
<span class="c1">##|   50.0|    5|</span>
<span class="c1">##|   51.0|    6|</span>
<span class="c1">##|   52.0|    5|</span>
<span class="c1">##|   53.0|    7|</span>
<span class="c1">##+-------+-----+</span>
<span class="c1">##only showing top 10 rows</span></code></pre></figure>

</div>

<p>请注意，要在<code>sparkR</code>外壳，添加<code>spark.sql.repl.eagerEval.enabled=true</code>配置属性<code>--conf</code>选项。</p>

<h2 id="running-sql-queries-from-sparkr">从SparkR运行SQL查询</h2>
<p>SparkDataFrame也可以在Spark SQL中注册为临时视图，并允许您对其数据运行SQL查询。的<code>sql</code>函数使应用程序能够以编程方式运行SQL查询，并以<code>SparkDataFrame</code> 。</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="c1"># Load a JSON file</span>
people <span class="o">&lt;-</span> read.df<span class="p">(</span><span class="s">&quot;./examples/src/main/resources/people.json&quot;</span><span class="p">,</span> <span class="s">&quot;json&quot;</span><span class="p">)</span>

<span class="c1"># Register this SparkDataFrame as a temporary view.</span>
createOrReplaceTempView<span class="p">(</span>people<span class="p">,</span> <span class="s">&quot;people&quot;</span><span class="p">)</span>

<span class="c1"># SQL statements can be run by using the sql method</span>
teenagers <span class="o">&lt;-</span> sql<span class="p">(</span><span class="s">&quot;SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19&quot;</span><span class="p">)</span>
<span class="kp">head</span><span class="p">(</span>teenagers<span class="p">)</span>
<span class="c1">##    name</span>
<span class="c1">##1 Justin</span></code></pre></figure>

</div>

<h1 id="machine-learning">机器学习</h1>

<h2 id="algorithms">演算法</h2>

<p>SparkR当前支持以下机器学习算法：</p>

<h4 id="classification">分类</h4>

<ul>
  <li><a href="api/R/spark.logit.html"><code>spark.logit</code></a> ：<a href="ml-classification-regression.html#logistic-regression"><code>Logistic Regression</code></a></li>
  <li><a href="api/R/spark.mlp.html"><code>spark.mlp</code></a> ：<a href="ml-classification-regression.html#multilayer-perceptron-classifier"><code>Multilayer Perceptron (MLP)</code></a></li>
  <li><a href="api/R/spark.naiveBayes.html"><code>spark.naiveBayes</code></a> ：<a href="ml-classification-regression.html#naive-bayes"><code>Naive Bayes</code></a></li>
  <li><a href="api/R/spark.svmLinear.html"><code>spark.svmLinear</code></a> ：<a href="ml-classification-regression.html#linear-support-vector-machine"><code>Linear Support Vector Machine</code></a></li>
</ul>

<h4 id="regression">回归</h4>

<ul>
  <li><a href="api/R/spark.survreg.html"><code>spark.survreg</code></a> ：<a href="ml-classification-regression.html#survival-regression"><code>Accelerated Failure Time (AFT) Survival Model</code></a></li>
  <li><a href="api/R/spark.glm.html"><code>spark.glm</code></a>要么<a href="api/R/glm.html"><code>glm</code></a> ：<a href="ml-classification-regression.html#generalized-linear-regression"><code>Generalized Linear Model (GLM)</code></a></li>
  <li><a href="api/R/spark.isoreg.html"><code>spark.isoreg</code></a> ：<a href="ml-classification-regression.html#isotonic-regression"><code>Isotonic Regression</code></a></li>
</ul>

<h4 id="tree">树</h4>

<ul>
  <li><a href="api/R/spark.decisionTree.html"><code>spark.decisionTree</code></a> ： <code>Decision Tree for</code> <a href="ml-classification-regression.html#decision-tree-regression"><code>Regression</code></a> <code>and</code> <a href="ml-classification-regression.html#decision-tree-classifier"><code>Classification</code></a></li>
  <li><a href="api/R/spark.gbt.html"><code>spark.gbt</code></a> ： <code>Gradient Boosted Trees for</code> <a href="ml-classification-regression.html#gradient-boosted-tree-regression"><code>Regression</code></a> <code>and</code> <a href="ml-classification-regression.html#gradient-boosted-tree-classifier"><code>Classification</code></a></li>
  <li><a href="api/R/spark.randomForest.html"><code>spark.randomForest</code></a> ： <code>Random Forest for</code> <a href="ml-classification-regression.html#random-forest-regression"><code>Regression</code></a> <code>and</code> <a href="ml-classification-regression.html#random-forest-classifier"><code>Classification</code></a></li>
</ul>

<h4 id="clustering">聚类</h4>

<ul>
  <li><a href="api/R/spark.bisectingKmeans.html"><code>spark.bisectingKmeans</code></a> ：<a href="ml-clustering.html#bisecting-k-means"><code>Bisecting k-means</code></a></li>
  <li><a href="api/R/spark.gaussianMixture.html"><code>spark.gaussianMixture</code></a> ：<a href="ml-clustering.html#gaussian-mixture-model-gmm"><code>Gaussian Mixture Model (GMM)</code></a></li>
  <li><a href="api/R/spark.kmeans.html"><code>spark.kmeans</code></a> ：<a href="ml-clustering.html#k-means"><code>K-Means</code></a></li>
  <li><a href="api/R/spark.lda.html"><code>spark.lda</code></a> ：<a href="ml-clustering.html#latent-dirichlet-allocation-lda"><code>Latent Dirichlet Allocation (LDA)</code></a></li>
  <li><a href="api/R/spark.powerIterationClustering.html"><code>spark.powerIterationClustering (PIC)</code></a> ：<a href="ml-clustering.html#power-iteration-clustering-pic"><code>Power Iteration Clustering (PIC)</code></a></li>
</ul>

<h4 id="collaborative-filtering">协同过滤</h4>

<ul>
  <li><a href="api/R/spark.als.html"><code>spark.als</code></a> ：<a href="ml-collaborative-filtering.html#collaborative-filtering"><code>Alternating Least Squares (ALS)</code></a></li>
</ul>

<h4 id="frequent-pattern-mining">频繁模式挖掘</h4>

<ul>
  <li><a href="api/R/spark.fpGrowth.html"><code>spark.fpGrowth</code></a> ：<a href="ml-frequent-pattern-mining.html#fp-growth"><code>FP-growth</code></a></li>
  <li><a href="api/R/spark.prefixSpan.html"><code>spark.prefixSpan</code></a> ：<a href="ml-frequent-pattern-mining.html#prefixSpan"><code>PrefixSpan</code></a></li>
</ul>

<h4 id="statistics">统计</h4>

<ul>
  <li><a href="api/R/spark.kstest.html"><code>spark.kstest</code></a> ：<code>Kolmogorov-Smirnov Test</code></li>
</ul>

<p>在后台，SparkR使用MLlib训练模型。请参阅MLlib用户指南的相应部分以获取示例代码。用户可以打电话<code>summary</code>打印的拟合模型的总结， <a href="api/R/predict.html">预测</a>使新数据的预测，并<a href="api/R/write.ml.html">write.ml</a> / <a href="api/R/read.ml.html">read.ml</a>保存/载入拟合模型。SparkR支持可用的R公式运算符的子集来进行模型拟合，包括“〜”，“。”，“：”，“ +”和“-”。</p>

<h2 id="model-persistence">模型持久性</h2>

<p>以下示例显示了如何通过SparkR保存/加载MLlib模型。</p>
<div class="highlight"><pre><span></span>training <span class="o">&lt;-</span> read.df<span class="p">(</span><span class="s">&quot;data/mllib/sample_multiclass_classification_data.txt&quot;</span><span class="p">,</span> <span class="kn">source</span> <span class="o">=</span> <span class="s">&quot;libsvm&quot;</span><span class="p">)</span>
<span class="c1"># Fit a generalized linear model of family &quot;gaussian&quot; with spark.glm</span>
df_list <span class="o">&lt;-</span> randomSplit<span class="p">(</span>training<span class="p">,</span> <span class="kt">c</span><span class="p">(</span><span class="m">7</span><span class="p">,</span><span class="m">3</span><span class="p">),</span> <span class="m">2</span><span class="p">)</span>
gaussianDF <span class="o">&lt;-</span> df_list<span class="p">[[</span><span class="m">1</span><span class="p">]]</span>
gaussianTestDF <span class="o">&lt;-</span> df_list<span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
gaussianGLM <span class="o">&lt;-</span> spark.glm<span class="p">(</span>gaussianDF<span class="p">,</span> label <span class="o">~</span> features<span class="p">,</span> family <span class="o">=</span> <span class="s">&quot;gaussian&quot;</span><span class="p">)</span>

<span class="c1"># Save and then load a fitted MLlib model</span>
modelPath <span class="o">&lt;-</span> <span class="kp">tempfile</span><span class="p">(</span>pattern <span class="o">=</span> <span class="s">&quot;ml&quot;</span><span class="p">,</span> fileext <span class="o">=</span> <span class="s">&quot;.tmp&quot;</span><span class="p">)</span>
write.ml<span class="p">(</span>gaussianGLM<span class="p">,</span> modelPath<span class="p">)</span>
gaussianGLM2 <span class="o">&lt;-</span> read.ml<span class="p">(</span>modelPath<span class="p">)</span>

<span class="c1"># Check model summary</span>
<span class="kp">summary</span><span class="p">(</span>gaussianGLM2<span class="p">)</span>

<span class="c1"># Check model prediction</span>
gaussianPredictions <span class="o">&lt;-</span> predict<span class="p">(</span>gaussianGLM2<span class="p">,</span> gaussianTestDF<span class="p">)</span>
<span class="kp">head</span><span class="p">(</span>gaussianPredictions<span class="p">)</span>

<span class="kp">unlink</span><span class="p">(</span>modelPath<span class="p">)</span>
</pre></div>
<div><small>在“ examples / src / main / r / ml / ml”中找到完整的示例代码。Spark仓库中的R”。</small></div>

<h1 id="data-type-mapping-between-r-and-spark">R和Spark之间的数据类型映射</h1>
<table class="table">
<tbody><tr><th>[R</th><th>火花</th></tr>
<tr>
  <td>字节</td>
  <td>字节</td>
</tr>
<tr>
  <td>整数</td>
  <td>整数</td>
</tr>
<tr>
  <td>浮动</td>
  <td>浮动</td>
</tr>
<tr>
  <td>双</td>
  <td>双</td>
</tr>
<tr>
  <td>数字</td>
  <td>双</td>
</tr>
<tr>
  <td>字符</td>
  <td>串</td>
</tr>
<tr>
  <td>串</td>
  <td>串</td>
</tr>
<tr>
  <td>二元</td>
  <td>二元</td>
</tr>
<tr>
  <td>生的</td>
  <td>二元</td>
</tr>
<tr>
  <td>合乎逻辑的</td>
  <td>布尔值</td>
</tr>
<tr>
  <td><a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/DateTimeClasses.html">POSIXct</a></td>
  <td>时间戳记</td>
</tr>
<tr>
  <td><a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/DateTimeClasses.html">POSIXlt</a></td>
  <td>时间戳记</td>
</tr>
<tr>
  <td><a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/Dates.html">日期</a></td>
  <td>日期</td>
</tr>
<tr>
  <td>数组</td>
  <td>数组</td>
</tr>
<tr>
  <td>清单</td>
  <td>数组</td>
</tr>
<tr>
  <td>环保</td>
  <td>地图</td>
</tr>
</tbody></table>

<h1 id="structured-streaming">结构化流</h1>

<p>SparkR支持结构化流API。结构化流是基于Spark SQL引擎构建的可伸缩且容错的流处理引擎。有关更多信息，请参见《 <a href="structured-streaming-programming-guide.html">结构化流编程指南》中</a>的R API。</p>

<h1 id="apache-arrow-in-sparkr">SparkR中的Apache Arrow</h1>

<p>Apache Arrow是内存中的列式数据格式，在Spark中使用它来在JVM和R进程之间有效地传输数据。另请参阅完成的PySpark优化， <a href="sql-pyspark-pandas-with-arrow.html">带有Apache Arrow的PySpark熊猫使用指南</a> 。本指南旨在通过一些关键点来说明如何在SparkR中使用Arrow优化。</p>

<h2 id="ensure-arrow-installed">确保安装了Arrow</h2>

<p>从<a href="https://issues.apache.org/jira/browse/ARROW-3204">ARROW-3204开始，</a> Arrow R库在CRAN上可用。可以如下安装。</p>

<pre><code class="language-bash">Rscript -e 'install.packages("arrow", repos="https://cloud.r-project.org/")'
</code></pre>

<p>如果您需要安装旧版本，则应直接从Github安装。您可以使用<code>remotes::install_github</code>如下。</p>

<pre><code class="language-bash">Rscript -e 'remotes::install_github("apache/arrow@apache-arrow-0.12.1", subdir = "r")'
</code></pre>

<p><code>apache-arrow-0.12.1</code>是一个版本标记，可以<a href="https://github.com/apache/arrow/releases">在Github的Arrow中</a>检查。您必须确保Arrow R软件包已安装并在所有群集节点上可用。当前支持的最低版本为0.12.1；但是，这可能会在次要发行版之间发生变化，因为SparkR中的Arrow优化是实验性的。</p>

<h2 id="enabling-for-conversion-tofrom-r-dataframe-dapply-and-gapply">启用与R DataFrame之间的转换， <code>dapply</code>和<code>gapply</code></h2>

<p>使用调用将Spark DataFrame转换为R DataFrame时，箭头优化可用<code>collect(spark_df)</code> ，当使用R DataFrame创建Spark DataFrame时<code>createDataFrame(r_df)</code> ，当通过将R本机函数应用于每个分区时<code>dapply(...)</code>当将R native函数应用于通过分组的数据时<code>gapply(...)</code> 。要在执行这些调用时使用Arrow，用户需要首先将Spark配置“ spark.sql.execution.arrow.sparkr.enabled”设置为“ true”。默认情况下禁用此功能。</p>

<p>此外，如果在将Spark DataFrame转换为R DataFrame或从R DataFrame转换为Spark DataFrame之前在Spark中实际计算之前发生错误，则由spark.sql.execution.arrow.sparkr.enabled启用的优化可能会自动回退到非箭头优化实现。</p>

<div data-lang="r">

  <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="c1"># Start up spark session with Arrow optimization enabled</span>
sparkR.session<span class="p">(</span>master <span class="o">=</span> <span class="s">&quot;local[*]&quot;</span><span class="p">,</span>
               sparkConfig <span class="o">=</span> <span class="kt">list</span><span class="p">(</span>spark.sql.execution.arrow.sparkr.enabled <span class="o">=</span> <span class="s">&quot;true&quot;</span><span class="p">))</span>

<span class="c1"># Converts Spark DataFrame from an R DataFrame</span>
spark_df <span class="o">&lt;-</span> createDataFrame<span class="p">(</span>mtcars<span class="p">)</span>

<span class="c1"># Converts Spark DataFrame to an R DataFrame</span>
collect<span class="p">(</span>spark_df<span class="p">)</span>

<span class="c1"># Apply an R native function to each partition.</span>
collect<span class="p">(</span>dapply<span class="p">(</span>spark_df<span class="p">,</span> <span class="kr">function</span><span class="p">(</span>rdf<span class="p">)</span> <span class="p">{</span> <span class="kt">data.frame</span><span class="p">(</span>rdf<span class="o">$</span>gear <span class="o">+</span> <span class="m">1</span><span class="p">)</span> <span class="p">},</span> structType<span class="p">(</span><span class="s">&quot;gear double&quot;</span><span class="p">)))</span>

<span class="c1"># Apply an R native function to grouped data.</span>
collect<span class="p">(</span>gapply<span class="p">(</span>spark_df<span class="p">,</span>
               <span class="s">&quot;gear&quot;</span><span class="p">,</span>
               <span class="kr">function</span><span class="p">(</span>key<span class="p">,</span> group<span class="p">)</span> <span class="p">{</span>
                 <span class="kt">data.frame</span><span class="p">(</span>gear <span class="o">=</span> key<span class="p">[[</span><span class="m">1</span><span class="p">]],</span> disp <span class="o">=</span> <span class="kp">mean</span><span class="p">(</span>group<span class="o">$</span>disp<span class="p">)</span> <span class="o">&gt;</span> group<span class="o">$</span>disp<span class="p">)</span>
               <span class="p">},</span>
               structType<span class="p">(</span><span class="s">&quot;gear double, disp boolean&quot;</span><span class="p">)))</span></code></pre></figure>

</div>

<p>将上述优化与Arrow一起使用将产生与未启用Arrow时相同的结果。请注意，即使使用Arrow， <code>collect(spark_df)</code>导致将DataFrame中的所有记录收集到驱动程序中，并且应该对数据的一小部分进行处理。</p>

<h2 id="supported-sql-types">支持的SQL类型</h2>

<p>当前，基于箭头的转换支持所有Spark SQL数据类型，除了<code>FloatType</code> ， <code>BinaryType</code> ， <code>ArrayType</code> ， <code>StructType</code>和<code>MapType</code> 。</p>

<h1 id="r-function-name-conflicts">R函数名称冲突</h1>

<p>在R中加载和附加新程序包时，可能存在名称<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/library.html">冲突</a> ，其中某个函数掩盖了另一个函数。</p>

<p>SparkR软件包掩盖了以下功能：</p>

<table class="table">
  <tbody><tr><th>屏蔽功能</th><th>如何进入</th></tr>
  <tr>
    <td><code>cov</code>在<code>package:stats</code></td>
    <td><code><pre>stats::cov(x, y = NULL, use = "everything",
           method = c("pearson", "kendall", "spearman"))</pre></code></td>
  </tr>
  <tr>
    <td><code>filter</code>在<code>package:stats</code></td>
    <td><code><pre>stats::filter(x, filter, method = c("convolution", "recursive"),
              sides = 2, circular = FALSE, init)</pre></code></td>
  </tr>
  <tr>
    <td><code>sample</code>在<code>package:base</code></td>
    <td><code>base::sample(x, size, replace = FALSE, prob = NULL)</code></td>
  </tr>
</tbody></table>

<p>由于SparkR的一部分是基于<code>dplyr</code>包中，SparkR中的某些功能与<code>dplyr</code> 。根据两个软件包的加载顺序，先加载的软件包中的某些功能会被后加载的软件包中的功能掩盖。在这种情况下，请使用软件包名称为此类调用添加前缀，例如， <code>SparkR::cume_dist(x)</code>要么<code>dplyr::cume_dist(x)</code> 。</p>

<p>您可以使用以下命令检查R中的搜索路径<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/search.html"><code>search()</code></a></p>

<h1 id="migration-guide">迁移指南</h1>

<p>现在，迁移指南已存档<a href="sparkr-migration-guide.html">在此页面上</a> 。</p>



                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-3.4.1.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    

</body></html>