<html class="no-js" ><!--<![endif]--><head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>蜂巢表-Spark 3.0.0-预览文档</title>
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        
        <!-- Google analytics script -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-32518208-2']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
        </script>
        

    </head>
    <body >
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html"><img src="img/spark-logo-hd.png" style="height:50px"></a> <span class="version">3.0.0预览版</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">总览</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">编程指南<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">快速开始</a></li>
                                <li><a href="rdd-programming-guide.html">RDD，累加器，广播变量</a></li>
                                <li><a href="sql-programming-guide.html">SQL，数据框和数据集</a></li>
                                <li><a href="structured-streaming-programming-guide.html">结构化流</a></li>
                                <li><a href="streaming-programming-guide.html">火花流（DStreams）</a></li>
                                <li><a href="ml-guide.html">MLlib（机器学习）</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX（图形处理）</a></li>
                                <li><a href="sparkr.html">SparkR（Spark上的R）</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API文件<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">斯卡拉</a></li>
                                <li><a href="api/java/index.html">爪哇</a></li>
                                <li><a href="api/python/index.html">蟒蛇</a></li>
                                <li><a href="api/R/index.html">[R</a></li>
                                <li><a href="api/sql/index.html">SQL，内置函数</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">部署中<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">总览</a></li>
                                <li><a href="submitting-applications.html">提交申请</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark独立</a></li>
                                <li><a href="running-on-mesos.html">梅索斯</a></li>
                                <li><a href="running-on-yarn.html">纱</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">更多<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">组态</a></li>
                                <li><a href="monitoring.html">监控方式</a></li>
                                <li><a href="tuning.html">调音指南</a></li>
                                <li><a href="job-scheduling.html">作业调度</a></li>
                                <li><a href="security.html">安全</a></li>
                                <li><a href="hardware-provisioning.html">硬件配置</a></li>
                                <li><a href="migration-guide.html">迁移指南</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">建筑火花</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">为Spark贡献</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">第三方项目</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v3.0.0-preview</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                
                    <div class="left-menu-wrapper">
    <div class="left-menu">
        <h3><a href="sql-programming-guide.html">Spark SQL指南</a></h3>
        
<ul>

    <li>
        <a href="sql-getting-started.html">入门</a>
    </li>
    
    

    <li>
        <a href="sql-data-sources.html">数据源</a>
    </li>
    
    
        
<ul>

    <li>
        <a href="sql-data-sources-load-save-functions.html">通用加载/保存功能</a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-parquet.html">实木复合地板文件</a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-orc.html">ORC文件</a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-json.html">JSON文件</a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-hive-tables.html">
            
                <b>蜂巢表</b>
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-jdbc.html">JDBC到其他数据库</a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-avro.html">Avro文件</a>
    </li>
    
    

    <li>
        <a href="sql-data-sources-troubleshooting.html">故障排除</a>
    </li>
    
    

</ul>

    

    <li>
        <a href="sql-performance-tuning.html">性能调优</a>
    </li>
    
    

    <li>
        <a href="sql-distributed-sql-engine.html">分布式SQL引擎</a>
    </li>
    
    

    <li>
        <a href="sql-pyspark-pandas-with-arrow.html">PySpark使用Apache Arrow的熊猫使用指南</a>
    </li>
    
    

    <li>
        <a href="sql-migration-old.html">迁移指南</a>
    </li>
    
    

    <li>
        <a href="sql-ref.html">SQL参考</a>
    </li>
    
    

</ul>

    </div>
</div>
                
                <input id="nav-trigger" class="nav-trigger" type="checkbox" checked>
                <label for="nav-trigger"></label>
                <div class="content-with-sidebar" id="content">
                    
                        <h1 class="title">蜂巢表</h1>
                    

                    <ul id="markdown-toc">
  <li><a href="#specifying-storage-format-for-hive-tables" id="markdown-toc-specifying-storage-format-for-hive-tables">指定Hive表的存储格式</a></li>
  <li><a href="#interacting-with-different-versions-of-hive-metastore" id="markdown-toc-interacting-with-different-versions-of-hive-metastore">与Hive Metastore的不同版本进行交互</a></li>
</ul>

<p>Spark SQL还支持读写存储在<a href="http://hive.apache.org/">Apache Hive中的</a>数据。但是，由于Hive具有大量依赖关系，因此默认的Spark分发中不包含这些依赖关系。如果可以在类路径上找到Hive依赖项，Spark将自动加载它们。请注意，这些Hive依赖项也必须存在于所有工作节点上，因为它们将需要访问Hive序列化和反序列化库（SerDes）才能访问存储在Hive中的数据。</p>

<p>配置Hive是通过将您的<code>hive-site.xml</code> ， <code>core-site.xml</code> （用于安全配置），以及<code>hdfs-site.xml</code> （用于HDFS配置）文件<code>conf/</code> 。</p>

<p>使用Hive时，必须实例化<code>SparkSession</code> Hive的支持，包括与持久性Hive Metastore的连接，对Hive Serdes的支持以及Hive用户定义的功能。没有现有Hive部署的用户仍可以启用Hive支持。如果没有配置<code>hive-site.xml</code> ，上下文会自动创建<code>metastore_db</code>在当前目录中并创建一个目录，该目录由<code>spark.sql.warehouse.dir</code> ，默认为目录<code>spark-warehouse</code>在启动Spark应用程序的当前目录中。请注意<code>hive.metastore.warehouse.dir</code>财产<code>hive-site.xml</code>自Spark 2.0.0起不推荐使用。相反，使用<code>spark.sql.warehouse.dir</code>指定数据库在仓库中的默认位置。您可能需要向启动Spark应用程序的用户授予写权限。</p>

<div class="codetabs">

<div data-lang="scala">
    <div class="highlight"><pre><span></span><span class="k">import</span> <span class="nn">java.io.File</span>

<span class="k">import</span> <span class="nn">org.apache.spark.sql.</span><span class="o">{</span><span class="nc">Row</span><span class="o">,</span> <span class="nc">SaveMode</span><span class="o">,</span> <span class="nc">SparkSession</span><span class="o">}</span>

<span class="k">case</span> <span class="k">class</span> <span class="nc">Record</span><span class="o">(</span><span class="n">key</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">value</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span>

<span class="c1">// warehouseLocation points to the default location for managed databases and tables</span>
<span class="k">val</span> <span class="n">warehouseLocation</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">File</span><span class="o">(</span><span class="s">&quot;spark-warehouse&quot;</span><span class="o">).</span><span class="n">getAbsolutePath</span>

<span class="k">val</span> <span class="n">spark</span> <span class="k">=</span> <span class="nc">SparkSession</span>
  <span class="o">.</span><span class="n">builder</span><span class="o">()</span>
  <span class="o">.</span><span class="n">appName</span><span class="o">(</span><span class="s">&quot;Spark Hive Example&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">config</span><span class="o">(</span><span class="s">&quot;spark.sql.warehouse.dir&quot;</span><span class="o">,</span> <span class="n">warehouseLocation</span><span class="o">)</span>
  <span class="o">.</span><span class="n">enableHiveSupport</span><span class="o">()</span>
  <span class="o">.</span><span class="n">getOrCreate</span><span class="o">()</span>

<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">spark.sql</span>

<span class="n">sql</span><span class="o">(</span><span class="s">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span><span class="o">)</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;</span><span class="o">)</span>

<span class="c1">// Queries are expressed in HiveQL</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM src&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +---+-------+</span>
<span class="c1">// |key|  value|</span>
<span class="c1">// +---+-------+</span>
<span class="c1">// |238|val_238|</span>
<span class="c1">// | 86| val_86|</span>
<span class="c1">// |311|val_311|</span>
<span class="c1">// ...</span>

<span class="c1">// Aggregation queries are also supported.</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT COUNT(*) FROM src&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +--------+</span>
<span class="c1">// |count(1)|</span>
<span class="c1">// +--------+</span>
<span class="c1">// |    500 |</span>
<span class="c1">// +--------+</span>

<span class="c1">// The results of SQL queries are themselves DataFrames and support all normal functions.</span>
<span class="k">val</span> <span class="n">sqlDF</span> <span class="k">=</span> <span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;</span><span class="o">)</span>

<span class="c1">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span>
<span class="k">val</span> <span class="n">stringsDS</span> <span class="k">=</span> <span class="n">sqlDF</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span>
  <span class="k">case</span> <span class="nc">Row</span><span class="o">(</span><span class="n">key</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">value</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="s">s&quot;Key: </span><span class="si">$key</span><span class="s">, Value: </span><span class="si">$value</span><span class="s">&quot;</span>
<span class="o">}</span>
<span class="n">stringsDS</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +--------------------+</span>
<span class="c1">// |               value|</span>
<span class="c1">// +--------------------+</span>
<span class="c1">// |Key: 0, Value: val_0|</span>
<span class="c1">// |Key: 0, Value: val_0|</span>
<span class="c1">// |Key: 0, Value: val_0|</span>
<span class="c1">// ...</span>

<span class="c1">// You can also use DataFrames to create temporary views within a SparkSession.</span>
<span class="k">val</span> <span class="n">recordsDF</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="o">((</span><span class="mi">1</span> <span class="n">to</span> <span class="mi">100</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">i</span> <span class="k">=&gt;</span> <span class="nc">Record</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="s">s&quot;val_</span><span class="si">$i</span><span class="s">&quot;</span><span class="o">)))</span>
<span class="n">recordsDF</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;records&quot;</span><span class="o">)</span>

<span class="c1">// Queries can then join DataFrame data with data stored in Hive.</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +---+------+---+------+</span>
<span class="c1">// |key| value|key| value|</span>
<span class="c1">// +---+------+---+------+</span>
<span class="c1">// |  2| val_2|  2| val_2|</span>
<span class="c1">// |  4| val_4|  4| val_4|</span>
<span class="c1">// |  5| val_5|  5| val_5|</span>
<span class="c1">// ...</span>

<span class="c1">// Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax</span>
<span class="c1">// `USING hive`</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;CREATE TABLE hive_records(key int, value string) STORED AS PARQUET&quot;</span><span class="o">)</span>
<span class="c1">// Save DataFrame to the Hive managed table</span>
<span class="k">val</span> <span class="n">df</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="o">(</span><span class="s">&quot;src&quot;</span><span class="o">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="o">(</span><span class="nc">SaveMode</span><span class="o">.</span><span class="nc">Overwrite</span><span class="o">).</span><span class="n">saveAsTable</span><span class="o">(</span><span class="s">&quot;hive_records&quot;</span><span class="o">)</span>
<span class="c1">// After insertion, the Hive managed table has data now</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM hive_records&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +---+-------+</span>
<span class="c1">// |key|  value|</span>
<span class="c1">// +---+-------+</span>
<span class="c1">// |238|val_238|</span>
<span class="c1">// | 86| val_86|</span>
<span class="c1">// |311|val_311|</span>
<span class="c1">// ...</span>

<span class="c1">// Prepare a Parquet data directory</span>
<span class="k">val</span> <span class="n">dataDir</span> <span class="k">=</span> <span class="s">&quot;/tmp/parquet_data&quot;</span>
<span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="o">(</span><span class="mi">10</span><span class="o">).</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="o">(</span><span class="n">dataDir</span><span class="o">)</span>
<span class="c1">// Create a Hive external Parquet table</span>
<span class="n">sql</span><span class="o">(</span><span class="s">s&quot;CREATE EXTERNAL TABLE hive_bigints(id bigint) STORED AS PARQUET LOCATION &#39;</span><span class="si">$dataDir</span><span class="s">&#39;&quot;</span><span class="o">)</span>
<span class="c1">// The Hive external table should already have data</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM hive_bigints&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +---+</span>
<span class="c1">// | id|</span>
<span class="c1">// +---+</span>
<span class="c1">// |  0|</span>
<span class="c1">// |  1|</span>
<span class="c1">// |  2|</span>
<span class="c1">// ... Order may vary, as spark processes the partitions in parallel.</span>

<span class="c1">// Turn on flag for Hive Dynamic Partitioning</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sqlContext</span><span class="o">.</span><span class="n">setConf</span><span class="o">(</span><span class="s">&quot;hive.exec.dynamic.partition&quot;</span><span class="o">,</span> <span class="s">&quot;true&quot;</span><span class="o">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sqlContext</span><span class="o">.</span><span class="n">setConf</span><span class="o">(</span><span class="s">&quot;hive.exec.dynamic.partition.mode&quot;</span><span class="o">,</span> <span class="s">&quot;nonstrict&quot;</span><span class="o">)</span>
<span class="c1">// Create a Hive partitioned table using DataFrame API</span>
<span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">partitionBy</span><span class="o">(</span><span class="s">&quot;key&quot;</span><span class="o">).</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;hive&quot;</span><span class="o">).</span><span class="n">saveAsTable</span><span class="o">(</span><span class="s">&quot;hive_part_tbl&quot;</span><span class="o">)</span>
<span class="c1">// Partitioned column `key` will be moved to the end of the schema.</span>
<span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM hive_part_tbl&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>
<span class="c1">// +-------+---+</span>
<span class="c1">// |  value|key|</span>
<span class="c1">// +-------+---+</span>
<span class="c1">// |val_238|238|</span>
<span class="c1">// | val_86| 86|</span>
<span class="c1">// |val_311|311|</span>
<span class="c1">// ...</span>

<span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="o">()</span>
</pre></div>
    <div><small>在Spark存储库中的“ examples / src / main / scala / org / apache / spark / examples / sql / hive / SparkHiveExample.scala”中找到完整的示例代码。</small></div>
  </div>

<div data-lang="java">
    <div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">java.io.File</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.io.Serializable</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.ArrayList</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.List</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.function.MapFunction</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Dataset</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Encoders</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Row</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">Record</span> <span class="kd">implements</span> <span class="n">Serializable</span> <span class="o">{</span>
  <span class="kd">private</span> <span class="kt">int</span> <span class="n">key</span><span class="o">;</span>
  <span class="kd">private</span> <span class="n">String</span> <span class="n">value</span><span class="o">;</span>

  <span class="kd">public</span> <span class="kt">int</span> <span class="nf">getKey</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">key</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">setKey</span><span class="o">(</span><span class="kt">int</span> <span class="n">key</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">.</span><span class="na">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="n">String</span> <span class="nf">getValue</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">value</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">setValue</span><span class="o">(</span><span class="n">String</span> <span class="n">value</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">.</span><span class="na">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="c1">// warehouseLocation points to the default location for managed databases and tables</span>
<span class="n">String</span> <span class="n">warehouseLocation</span> <span class="o">=</span> <span class="k">new</span> <span class="n">File</span><span class="o">(</span><span class="s">&quot;spark-warehouse&quot;</span><span class="o">).</span><span class="na">getAbsolutePath</span><span class="o">();</span>
<span class="n">SparkSession</span> <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span>
  <span class="o">.</span><span class="na">builder</span><span class="o">()</span>
  <span class="o">.</span><span class="na">appName</span><span class="o">(</span><span class="s">&quot;Java Spark Hive Example&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">config</span><span class="o">(</span><span class="s">&quot;spark.sql.warehouse.dir&quot;</span><span class="o">,</span> <span class="n">warehouseLocation</span><span class="o">)</span>
  <span class="o">.</span><span class="na">enableHiveSupport</span><span class="o">()</span>
  <span class="o">.</span><span class="na">getOrCreate</span><span class="o">();</span>

<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span><span class="o">);</span>
<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;</span><span class="o">);</span>

<span class="c1">// Queries are expressed in HiveQL</span>
<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM src&quot;</span><span class="o">).</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +---+-------+</span>
<span class="c1">// |key|  value|</span>
<span class="c1">// +---+-------+</span>
<span class="c1">// |238|val_238|</span>
<span class="c1">// | 86| val_86|</span>
<span class="c1">// |311|val_311|</span>
<span class="c1">// ...</span>

<span class="c1">// Aggregation queries are also supported.</span>
<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT COUNT(*) FROM src&quot;</span><span class="o">).</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +--------+</span>
<span class="c1">// |count(1)|</span>
<span class="c1">// +--------+</span>
<span class="c1">// |    500 |</span>
<span class="c1">// +--------+</span>

<span class="c1">// The results of SQL queries are themselves DataFrames and support all normal functions.</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">sqlDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;</span><span class="o">);</span>

<span class="c1">// The items in DataFrames are of type Row, which lets you to access each column by ordinal.</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">stringsDS</span> <span class="o">=</span> <span class="n">sqlDF</span><span class="o">.</span><span class="na">map</span><span class="o">(</span>
    <span class="o">(</span><span class="n">MapFunction</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;)</span> <span class="n">row</span> <span class="o">-&gt;</span> <span class="s">&quot;Key: &quot;</span> <span class="o">+</span> <span class="n">row</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">+</span> <span class="s">&quot;, Value: &quot;</span> <span class="o">+</span> <span class="n">row</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">1</span><span class="o">),</span>
    <span class="n">Encoders</span><span class="o">.</span><span class="na">STRING</span><span class="o">());</span>
<span class="n">stringsDS</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +--------------------+</span>
<span class="c1">// |               value|</span>
<span class="c1">// +--------------------+</span>
<span class="c1">// |Key: 0, Value: val_0|</span>
<span class="c1">// |Key: 0, Value: val_0|</span>
<span class="c1">// |Key: 0, Value: val_0|</span>
<span class="c1">// ...</span>

<span class="c1">// You can also use DataFrames to create temporary views within a SparkSession.</span>
<span class="n">List</span><span class="o">&lt;</span><span class="n">Record</span><span class="o">&gt;</span> <span class="n">records</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;&gt;();</span>
<span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">key</span> <span class="o">=</span> <span class="mi">1</span><span class="o">;</span> <span class="n">key</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="o">;</span> <span class="n">key</span><span class="o">++)</span> <span class="o">{</span>
  <span class="n">Record</span> <span class="n">record</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Record</span><span class="o">();</span>
  <span class="n">record</span><span class="o">.</span><span class="na">setKey</span><span class="o">(</span><span class="n">key</span><span class="o">);</span>
  <span class="n">record</span><span class="o">.</span><span class="na">setValue</span><span class="o">(</span><span class="s">&quot;val_&quot;</span> <span class="o">+</span> <span class="n">key</span><span class="o">);</span>
  <span class="n">records</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">record</span><span class="o">);</span>
<span class="o">}</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">recordsDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">createDataFrame</span><span class="o">(</span><span class="n">records</span><span class="o">,</span> <span class="n">Record</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">recordsDF</span><span class="o">.</span><span class="na">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;records&quot;</span><span class="o">);</span>

<span class="c1">// Queries can then join DataFrames data with data stored in Hive.</span>
<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;</span><span class="o">).</span><span class="na">show</span><span class="o">();</span>
<span class="c1">// +---+------+---+------+</span>
<span class="c1">// |key| value|key| value|</span>
<span class="c1">// +---+------+---+------+</span>
<span class="c1">// |  2| val_2|  2| val_2|</span>
<span class="c1">// |  2| val_2|  2| val_2|</span>
<span class="c1">// |  4| val_4|  4| val_4|</span>
<span class="c1">// ...</span>
</pre></div>
    <div><small>在Spark存储库中的“ examples / src / main / java / org / apache / spark / examples / sql / hive / JavaSparkHiveExample.java”中找到完整的示例代码。</small></div>
  </div>

<div data-lang="python">
    <div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">os.path</span> <span class="kn">import</span> <span class="n">join</span><span class="p">,</span> <span class="n">abspath</span>

<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>

<span class="c1"># warehouse_location points to the default location for managed databases and tables</span>
<span class="n">warehouse_location</span> <span class="o">=</span> <span class="n">abspath</span><span class="p">(</span><span class="s1">&#39;spark-warehouse&#39;</span><span class="p">)</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span> \
    <span class="o">.</span><span class="n">builder</span> \
    <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;Python Spark SQL Hive integration example&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.sql.warehouse.dir&quot;</span><span class="p">,</span> <span class="n">warehouse_location</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span> \
    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="c1"># spark is an existing SparkSession</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;</span><span class="p">)</span>

<span class="c1"># Queries are expressed in HiveQL</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM src&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +---+-------+</span>
<span class="c1"># |key|  value|</span>
<span class="c1"># +---+-------+</span>
<span class="c1"># |238|val_238|</span>
<span class="c1"># | 86| val_86|</span>
<span class="c1"># |311|val_311|</span>
<span class="c1"># ...</span>

<span class="c1"># Aggregation queries are also supported.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT COUNT(*) FROM src&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +--------+</span>
<span class="c1"># |count(1)|</span>
<span class="c1"># +--------+</span>
<span class="c1"># |    500 |</span>
<span class="c1"># +--------+</span>

<span class="c1"># The results of SQL queries are themselves DataFrames and support all normal functions.</span>
<span class="n">sqlDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;</span><span class="p">)</span>

<span class="c1"># The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span>
<span class="n">stringsDS</span> <span class="o">=</span> <span class="n">sqlDF</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="s2">&quot;Key: </span><span class="si">%d</span><span class="s2">, Value: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">row</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">row</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>
<span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">stringsDS</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
<span class="c1"># Key: 0, Value: val_0</span>
<span class="c1"># Key: 0, Value: val_0</span>
<span class="c1"># Key: 0, Value: val_0</span>
<span class="c1"># ...</span>

<span class="c1"># You can also use DataFrames to create temporary views within a SparkSession.</span>
<span class="n">Record</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">)</span>
<span class="n">recordsDF</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Record</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="s2">&quot;val_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)])</span>
<span class="n">recordsDF</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;records&quot;</span><span class="p">)</span>

<span class="c1"># Queries can then join DataFrame data with data stored in Hive.</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +---+------+---+------+</span>
<span class="c1"># |key| value|key| value|</span>
<span class="c1"># +---+------+---+------+</span>
<span class="c1"># |  2| val_2|  2| val_2|</span>
<span class="c1"># |  4| val_4|  4| val_4|</span>
<span class="c1"># |  5| val_5|  5| val_5|</span>
<span class="c1"># ...</span>
</pre></div>
    <div><small>在Spark存储库中的“ examples / src / main / python / sql / hive.py”中找到完整的示例代码。</small></div>
  </div>

<div data-lang="r">

    <p>使用Hive时，必须实例化<code>SparkSession</code>在Hive的支持下。这增加了对在MetaStore中查找表以及使用HiveQL编写查询的支持。</p>

    <div class="highlight"><pre><span></span><span class="c1"># enableHiveSupport defaults to TRUE</span>
sparkR.session<span class="p">(</span>enableHiveSupport <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
sql<span class="p">(</span><span class="s">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span><span class="p">)</span>
sql<span class="p">(</span><span class="s">&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;</span><span class="p">)</span>

<span class="c1"># Queries can be expressed in HiveQL.</span>
results <span class="o">&lt;-</span> collect<span class="p">(</span>sql<span class="p">(</span><span class="s">&quot;FROM src SELECT key, value&quot;</span><span class="p">))</span>
</pre></div>
    <div><small>在“ examples / src / main / r / RSparkSQLExample”中找到完整的示例代码。Spark仓库中的R”。</small></div>

  </div>
</div>

<h3 id="specifying-storage-format-for-hive-tables">指定Hive表的存储格式</h3>

<p>创建Hive表时，需要定义该表应如何从文件系统读取/写入数据，即“输入格式”和“输出格式”。您还需要定义该表应如何将数据反序列化为行，或将行序列化为数据，即“ serde”。以下选项可用于指定存储格式（“ serde”，“ input format”，“ output format”），例如<code>CREATE TABLE src(id int) USING hive OPTIONS(fileFormat 'parquet')</code> 。默认情况下，我们将以纯文本形式读取表文件。请注意，创建表时尚不支持Hive存储处理程序，您可以在Hive端使用存储处理程序创建表，并使用Spark SQL读取表。</p>

<table class="table">
  <tbody><tr><th>物业名称</th><th>含义</th></tr>
  <tr>
    <td><code>fileFormat</code></td>
    <td>fileFormat是一种存储格式规范的软件包，其中包括“ serde”，“ input format”和“ output format”。目前，我们支持6种文件格式：“ sequencefile”，“ rcfile”，“ orc”，“ parquet”，“ textfile”和“ avro”。
    </td>
  </tr>

  <tr>
    <td><code>inputFormat, outputFormat</code></td>
    <td>这2个选项指定对应的名称<code>InputFormat</code>和<code>OutputFormat</code>类为字符串文字，例如<code>org.apache.hadoop.hive.ql.io.orc.OrcInputFormat</code> 。这两个选项必须成对出现，如果您已经指定了<code>fileFormat</code>选项。
    </td>
  </tr>

  <tr>
    <td><code>serde</code></td>
    <td>此选项指定Serde类的名称。当。。。的时候<code>fileFormat</code>指定选项，如果给定，则不指定此选项<code>fileFormat</code>已经包含Serde的信息。当前，“ sequencefile”，“ textfile”和“ rcfile”不包含SERDE信息，您可以将此选项与这三种文件格式一起使用。
    </td>
  </tr>

  <tr>
    <td><code>fieldDelim, escapeDelim, collectionDelim, mapkeyDelim, lineDelim</code></td>
    <td>这些选项只能与“文本文件” fileFormat一起使用。它们定义了如何将定界文件读取为行。
    </td>
  </tr>
</tbody></table>

<p>使用定义的所有其他属性<code>OPTIONS</code>将被视为Hive serde属性。</p>

<h3 id="interacting-with-different-versions-of-hive-metastore">与Hive Metastore的不同版本进行交互</h3>

<p>与Hive Metastore的交互是Spark SQL对Hive的最重要支持之一，它使Spark SQL能够访问Hive表的元数据。从Spark 1.4.0开始，使用以下描述的配置，可以使用Spark SQL的单个二进制版本来查询Hive元存储库的不同版本。请注意，与用于与metastore进行通信的Hive版本无关，Spark SQL在内部将针对Hive 1.2.1进行编译，并将这些类用于内部执行（serdes，UDF，UDAF等）。</p>

<p>可以使用以下选项来配置用于检索元数据的Hive版本：</p>

<table class="table">
  <tbody><tr><th>物业名称</th><th>默认</th><th>含义</th></tr>
  <tr>
    <td><code>spark.sql.hive.metastore.version</code></td>
    <td><code>1.2.1</code></td>
    <td>Hive Metastore的版本。可用的选项是<code>0.12.0</code>通过<code>2.3.6</code>和<code>3.0.0</code>通过<code>3.1.2</code> 。
    </td>
  </tr>
  <tr>
    <td><code>spark.sql.hive.metastore.jars</code></td>
    <td><code>builtin</code></td>
    <td>用于实例化HiveMetastoreClient的jar的位置。此属性可以是三个选项之一：<ol>
        <li><code>builtin</code></li>使用Hive 1.2.1，它在以下情况下与Spark组件捆绑在一起<code>-Phive</code>已启用。选择此选项后， <code>spark.sql.hive.metastore.version</code>必须是<code>1.2.1</code>或未定义。
        <li><code>maven</code></li>使用从Maven存储库下载的指定版本的Hive jar。通常不建议将此配置用于生产部署。
        <li>JVM的标准格式的类路径。该类路径必须包括所有Hive及其依赖项，包括正确的Hadoop版本。这些罐子只需要存在于驱动程序中，但是如果您以纱线簇模式运行，则必须确保将它们与您的应用程序打包在一起。</li>
      </ol>
    </td>
  </tr>
  <tr>
    <td><code>spark.sql.hive.metastore.sharedPrefixes</code></td>
    <td><code>com.mysql.jdbc,<br>org.postgresql,<br>com.microsoft.sqlserver,<br>oracle.jdbc</code></td>
    <td>
      <p>以逗号分隔的类前缀列表，应使用在Spark SQL和特定版本的Hive之间共享的类加载器加载。应该共享的类的一个示例是与元存储区对话所需的JDBC驱动程序。需要共享的其他类是与已经共享的类进行交互的类。例如，log4j使用的自定义追加程序。
      </p>
    </td>
  </tr>
  <tr>
    <td><code>spark.sql.hive.metastore.barrierPrefixes</code></td>
    <td><code>(empty)</code></td>
    <td>
      <p>以逗号分隔的类前缀列表，应为Spark SQL与之通信的每个Hive版本显式重新加载。例如，在通常会被共享的前缀中声明的Hive UDF（即， <code>org.apache.spark.*</code> ）。
      </p>
    </td>
  </tr>
</tbody></table>


                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-3.4.1.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    

</body></html>