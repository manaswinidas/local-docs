<html ><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">

    <title>Storm SQL示例</title>

    <!-- Bootstrap core CSS -->
    <link href="/assets/css/bootstrap.min.css" rel="stylesheet">
    <!-- Bootstrap theme -->
    <link href="/assets/css/bootstrap-theme.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link rel="stylesheet" href="asset?aid=5">
    <link href="/css/style.css" rel="stylesheet">
    <link href="/assets/css/owl.theme.css" rel="stylesheet">
    <link href="/assets/css/owl.carousel.css" rel="stylesheet">
    <script type="text/javascript" src="/assets/js/jquery.min.js"></script>
    <script type="text/javascript" src="/assets/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="/assets/js/owl.carousel.min.js"></script>
    <script type="text/javascript" src="/assets/js/storm.js"></script>
    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>


  <body >
    <header>
  <div class="container-fluid">
     <div class="row">
          <div class="col-md-5">
            <a href="/index.html"><img src="/images/logo.png" class="logo"></a>
          </div>
          <div class="col-md-5">
            
              <h1>版本：2.1.0</h1>
            
          </div>
          <div class="col-md-2">
            <a href="/downloads.html" class="btn-std btn-block btn-download">下载</a>
          </div>
        </div>
    </div>
</header>
<!--Header End-->
<!--Navigation Begin-->
<div class="navbar" role="banner">
  <div class="container-fluid">
      <div class="navbar-header">
          <button class="navbar-toggle" type="button" data-toggle="collapse" data-target=".bs-navbar-collapse"></button>
        </div>
        <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation">
          <ul class="nav navbar-nav">
              <li><a href="/index.html" id="home">家</a></li>
                <li><a href="/getting-help.html" id="getting-help">获得帮助</a></li>
                <li><a href="/about/integrates.html" id="project-info">项目信息</a></li>
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" id="documentation" data-toggle="dropdown">文献资料<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                      
                        
                          <li><a href="/releases/2.1.0/index.html">2.1.0</a></li>
                        
                      
                        
                          <li><a href="/releases/2.0.0/index.html">2.0.0</a></li>
                        
                      
                        
                          <li><a href="/releases/1.2.3/index.html">1.2.3</a></li>
                        
                      
                    </ul>
                </li>
                <li><a href="/talksAndVideos.html">讲座和幻灯片</a></li>
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" id="contribute" data-toggle="dropdown">社区<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        <li><a href="/contribute/Contributing-to-Storm.html">贡献</a></li>
                        <li><a href="/contribute/People.html">人</a></li>
                        <li><a href="/contribute/BYLAWS.html">按照法律规定</a></li>
                    </ul>
                </li>
                <li><a href="/2019/10/31/storm210-released.html" id="news">新闻</a></li>
            </ul>
        </nav>
    </div>
</div>



    <div class="container-fluid">
    <h1 class="page-title">Storm SQL示例</h1>
          <div class="row">
           	<div class="col-md-12">
	             <!-- Documentation -->

<p class="post-meta"></p>

<div class="documentation-content"><p>此页面通过显示处理Apache日志的示例来显示如何使用Storm SQL。该页面以“操作方法”风格编写，因此您可以按照以下步骤操作，并逐步学习如何使用Storm SQL。</p>

<h2 id="preparation">制备</h2>

<p>该页面假定Apache Zookeeper，Apache Storm和Apache Kafka已本地安装并已正确配置运行。为了方便起见，此页面假定通过以下方式安装了Apache Kafka 0.10.0 <code>brew</code> 。</p>

<p>我们将使用以下工具来准备JSON数据，该数据将被馈送到输入数据源。由于它们是Python项目，因此此页面假定Python 2.7与<code>pip</code> ， <code>virtualenv</code>在本地安装。如果您使用的是Python 3，则可能需要在馈送数据时手动转换某些位置使其与3兼容。</p>

<ul>
<li><a href="https://github.com/kiritbasu/Fake-Apache-Log-Generator">https://github.com/kiritbasu/Fake-Apache-Log-Generator</a></li>
<li><a href="https://github.com/rory/apache-log-parser">https://github.com/rory/apache-log-parser</a></li>
</ul>

<h2 id="creating-topics">创建主题</h2>

<p>在此页面中，我们将使用四个主题， <code>apache-logs</code> ， <code>apache-errorlogs</code> ， <code>apache-slowlogs</code> 。请根据您的环境创建主题。</p>

<p>对于安装了brew的Apache Kafka 0.10.0，</p>
<div class="highlight"><pre><code class="language-" data-lang="">kafka-topics --create --topic apache-logs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-errorlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
kafka-topics --create --topic apache-slowlogs --zookeeper localhost:2181 --replication-factor 1 --partitions 5
</code></pre></div>
<h2 id="feeding-data">进料数据</h2>

<p>让我们将数据输入到输入主题中。在此页面中，我们将生成伪造的Apache日志，并解析为JSON格式，并将JSON馈送给Kafka主题。</p>

<p>让我们创建您的工作目录，因为我们将克隆项目并设置virtualenv。</p>

<p>在您的工作目录中， <code>virtualenv env</code>将virtualenv设置到env目录，然后激活。</p>
<div class="highlight"><pre><code class="language-" data-lang="">$ virtualenv env
$ source env/bin/activate
</code></pre></div>
<p>随意地<code>deactivate</code>完成示例后。</p>

<h3 id="install-and-modify-fake-apache-log-generator">安装和修改Fake-Apache-Log-Generator</h3>

<p><code>Fake-Apache-Log-Generator</code>没有提供给软件包，我们还需要修改脚本。</p>
<div class="highlight"><pre><code class="language-" data-lang="">$ git clone https://github.com/kiritbasu/Fake-Apache-Log-Generator.git
$ cd Fake-Apache-Log-Generator
</code></pre></div>
<p>打开<code>apache-fake-log-gen.py</code>并更换<code>while (flag):</code>声明如下：</p>
<div class="highlight"><pre><code class="language-" data-lang="">        elapsed_us = random.randint(1 * 1000,1000 * 1000) # 1 ms to 1 sec
        seconds=random.randint(30,300)
        increment = datetime.timedelta(seconds=seconds)
        otime += increment

        ip = faker.ipv4()
        dt = otime.strftime('%d/%b/%Y:%H:%M:%S')
        tz = datetime.datetime.now(pytz.timezone('US/Pacific')).strftime('%z')
        vrb = numpy.random.choice(verb,p=[0.6,0.1,0.1,0.2])

        uri = random.choice(resources)
        if uri.find("apps")&gt;0:
                uri += `random.randint(1000,10000)`

        resp = numpy.random.choice(response,p=[0.9,0.04,0.02,0.04])
        byt = int(random.gauss(5000,50))
        referer = faker.uri()
        useragent = numpy.random.choice(ualist,p=[0.5,0.3,0.1,0.05,0.05] )()
        f.write('%s - - [%s %s] %s "%s %s HTTP/1.0" %s %s "%s" "%s"\n' % (ip,dt,tz,elapsed_us,vrb,uri,resp,byt,referer,useragent))

        log_lines = log_lines - 1
        flag = False if log_lines == 0 else True
</code></pre></div>
<p>确保伪造的elapsed_us包含在伪造的日志中。</p>

<p>为了方便起见，您可以跳过克隆项目并从此处下载修改的文件： <a href="https://gist.github.com/HeartSaVioR/79fd4e461604fabecf535ffece47e6c2">apache-fake-log-gen.py（要点）</a></p>

<h3 id="install-apache-log-parser-and-write-parsing-script">安装apache-log-parser并编写解析脚本</h3>

<p><code>apache-log-parser</code>可以通过安装<code>pip</code> 。</p>
<div class="highlight"><pre><code class="language-" data-lang="">$ pip install apache-log-parser
</code></pre></div>
<p>由于apache-log-parser是一个库，为了解析假日志，我们需要编写小的python脚本。创建文件<code>parse-fake-log-gen-to-json-with-incrementing-id.py</code>具有以下内容：</p>
<div class="highlight"><pre><code class="language-" data-lang="">import sys
import apache_log_parser
import json

auto_incr_id = 1
parser_format = '%a - - %t %D "%r" %s %b "%{Referer}i" "%{User-Agent}i"'
line_parser = apache_log_parser.make_parser(parser_format)
while True:
  # we'll use pipe
  line = sys.stdin.readline()
  if not line:
    break
  parsed_dict = line_parser(line)
  parsed_dict['id'] = auto_incr_id
  auto_incr_id += 1

  # works only python 2, but I don't care cause it's just a test module :)
  parsed_dict = {k.upper(): v for k, v in parsed_dict.iteritems() if not k.endswith('datetimeobj')}
  print json.dumps(parsed_dict)
</code></pre></div>
<h3 id="feed-parsed-json-apache-log-to-kafka">将已解析的JSON Apache Log提要到Kafka</h3>

<p>好！我们准备将数据提供给Kafka主题。让我们用<code>kafka-console-producer</code>提供解析的JSON。</p>
<div class="highlight"><pre><code class="language-" data-lang="">$ python apache-fake-log-gen.py -n 0 | python parse-fake-log-gen-to-json-with-incrementing-id.py | kafka-console-producer --broker-list localhost:9092 --topic apache-logs
</code></pre></div>
<p>并在下面执行到另一个终端会话，以确认正在馈送数据。</p>
<div class="highlight"><pre><code class="language-" data-lang="">$ kafka-console-consumer --zookeeper localhost:2181 --topic apache-logs
</code></pre></div>
<p>如果您可以看到如下所示的json，则说明已完成：</p>
<div class="highlight"><pre><code class="language-" data-lang="">{"TIME_US": "757467", "REQUEST_FIRST_LINE": "GET /wp-content HTTP/1.0", "REQUEST_METHOD": "GET", "RESPONSE_BYTES_CLF": "4988", "TIME_RECEIVED_ISOFORMAT": "2021-06-30T22:02:53", "TIME_RECEIVED_TZ_ISOFORMAT": "2021-06-30T22:02:53-07:00", "REQUEST_HTTP_VER": "1.0", "REQUEST_HEADER_USER_AGENT__BROWSER__FAMILY": "Firefox", "REQUEST_HEADER_USER_AGENT__IS_MOBILE": false, "REQUEST_HEADER_USER_AGENT__BROWSER__VERSION_STRING": "3.6.13", "REQUEST_URL_FRAGMENT": "", "REQUEST_HEADER_USER_AGENT": "Mozilla/5.0 (X11; Linux x86_64; rv:1.9.7.20) Gecko/2010-10-13 13:52:34 Firefox/3.6.13", "REQUEST_URL_SCHEME": "", "REQUEST_URL_PATH": "/wp-content", "REQUEST_URL_QUERY_SIMPLE_DICT": {}, "TIME_RECEIVED_UTC_ISOFORMAT": "2021-07-01T05:02:53+00:00", "REQUEST_URL_QUERY_DICT": {}, "STATUS": "200", "REQUEST_URL_NETLOC": "", "REQUEST_URL_QUERY_LIST": [], "REQUEST_URL_QUERY": "", "REQUEST_URL_USERNAME": null, "REQUEST_HEADER_USER_AGENT__OS__VERSION_STRING": "", "REQUEST_URL_HOSTNAME": null, "REQUEST_HEADER_USER_AGENT__OS__FAMILY": "Linux", "REQUEST_URL": "/wp-content", "ID": 904128, "REQUEST_HEADER_REFERER": "http://white.com/terms/", "REQUEST_URL_PORT": null, "REQUEST_URL_PASSWORD": null, "TIME_RECEIVED": "[30/Jun/2021:22:02:53 -0700]", "REMOTE_IP": "88.203.90.62"}
</code></pre></div>
<h2 id="example-filtering-error-logs">示例：过滤错误日志</h2>

<p>在此示例中，我们将从整个日志中过滤错误日志并将其存储到另一个主题。 <code>project</code>和<code>filter</code>功能将被使用。</p>

<p>脚本文件的内容在这里：</p>
<div class="highlight"><pre><code class="language-" data-lang="">CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092'
CREATE EXTERNAL TABLE APACHE_ERROR_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-error-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
INSERT INTO APACHE_ERROR_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, (TIME_US / 1000) AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (CAST(STATUS AS INT) / 100) &gt;= 4
</code></pre></div>
<p>将此文件保存到<code>apache_log_error_filtering.sql</code> 。</p>

<p>让我们看一下脚本。</p>

<p>第一条语句定义表<code>APACHE_LOGS</code>代表输入流。的<code>LOCATION</code>子句指定Kafka主机（ <code>localhost:9092</code> ）和主题（ <code>apache-logs</code> ）。请注意，Kafka数据源需要定义主键。这就是为什么我们为解析的JSON数据放置整数id的原因。</p>

<p>同样，第二条语句指定表<code>APACHE_ERROR_LOGS</code>代表输出流。的<code>TBLPROPERTIES</code>子句指定<a href="http://kafka.apache.org/documentation.html#producerconfigs">KafkaProducer</a>的配置，并且是Kafka接收器表所必需的。</p>

<p>最后一条语句定义拓扑。Storm SQL仅定义拓扑并在DML语句上运行拓扑。DDL语句定义了DML语句将引用的输入数据源，输出数据源和用户定义的函数。</p>

<p>让我们看一下<code>where</code>首先声明。由于我们要过滤错误日志，因此将状态除以100并比较商等于或大于4。 <code>>= 400</code> ）由于JSON中的状态为字符串格式（因此在APACHE_LOGS表中表示为VARCHAR），因此我们在应用除法之前应用CAST（STATUS AS INT）转换为整数类型。现在，我们仅过滤了错误日志。</p>

<p>让我们变换一些列以匹配输出流。在此语句中，我们将CAST（STATUS AS INT）转换为整数类型，然后将TIME_US除以1000以将微秒转换为毫秒。</p>

<p>最后，insert语句将经过过滤和转换的行（元组）存储到输出流。</p>

<p>要运行此示例，用户需要包括数据源（ <code>storm-sql-kafka</code>在这种情况下）及其在类路径中的依赖关系。用户运行时将自动处理Storm SQL核心依赖关系<code>storm sql</code> 。用户可以在提交步骤中包括数据源，如下所示：</p>
<div class="highlight"><pre><code class="language-" data-lang="">$ $STORM_DIR/bin/storm sql apache_log_error_filtering.sql apache_log_error_filtering --artifacts "org.apache.storm:storm-sql-kafka:2.0.0-SNAPSHOT,org.apache.storm:storm-kafka-client:2.0.0-SNAPSHOT,org.apache.kafka:kafka-clients:1.1.0^org.slf4j:slf4j-log4j12"
</code></pre></div>
<p>上面的命令将SQL语句提交给StormSQL。 Storm SQL的命令行语法为<code>storm sql [script file] [topology name]</code> 。如果用户使用不同版本的Storm或Kafka，则用户需要修改每个工件的版本。</p>

<p>如果您的语句通过了验证阶段，则拓扑将显示在Storm UI页面上。</p>

<p>您可以通过控制台查看输出：</p>
<div class="highlight"><pre><code class="language-" data-lang="">$ kafka-console-consumer --zookeeper localhost:2181 --topic apache-error-logs
</code></pre></div>
<p>输出将类似于：</p>
<div class="highlight"><pre><code class="language-" data-lang="">{"ID":854643,"REMOTE_IP":"4.227.214.159","REQUEST_URL":"/wp-content","REQUEST_METHOD":"GET","STATUS":404,"REQUEST_HEADER_USER_AGENT":"Mozilla/5.0 (Windows 98; Win 9x 4.90; it-IT; rv:1.9.2.20) Gecko/2015-06-03 11:20:16 Firefox/3.6.17","TIME_RECEIVED_UTC_ISOFORMAT":"2021-03-28T19:14:44+00:00","TIME_RECEIVED_TIMESTAMP":1616958884000,"TIME_ELAPSED_MS":274.222}
{"ID":854693,"REMOTE_IP":"223.50.249.7","REQUEST_URL":"/apps/cart.jsp?appID=5578","REQUEST_METHOD":"GET","STATUS":404,"REQUEST_HEADER_USER_AGENT":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_6; rv:1.9.2.20) Gecko/2015-11-06 00:20:43 Firefox/3.8","TIME_RECEIVED_UTC_ISOFORMAT":"2021-03-28T21:41:02+00:00","TIME_RECEIVED_TIMESTAMP":1616967662000,"TIME_ELAPSED_MS":716.851}
...
</code></pre></div>
<p>您还可以运行Storm SQL运行程序以通过放置来查看逻辑计划<code>--explain</code>到拓扑名称：</p>
<div class="highlight"><pre><code class="language-" data-lang="">$ $STORM_DIR/bin/storm sql apache_log_error_filtering.sql --explain --artifacts "org.apache.storm:storm-sql-kafka:2.0.0-SNAPSHOT,org.apache.storm:storm-kafka-client:2.0.0-SNAPSHOT,org.apache.kafka:kafka-clients:1.1.0^org.slf4j:slf4j-log4j12"
</code></pre></div>
<p>输出将类似于：</p>
<div class="highlight"><pre><code class="language-" data-lang="">LogicalTableModify(table=[[APACHE_ERROR_LOGS]], operation=[INSERT], updateColumnList=[[]], flattened=[true]), id = 8
  LogicalProject(ID=[$0], REMOTE_IP=[$1], REQUEST_URL=[$2], REQUEST_METHOD=[$3], STATUS=[CAST($4):INTEGER NOT NULL], REQUEST_HEADER_USER_AGENT=[$5], TIME_RECEIVED_UTC_ISOFORMAT=[$6], TIME_ELAPSED_MS=[/($7, 1000)]), id = 7
    LogicalFilter(condition=[&gt;=(/(CAST($4):INTEGER NOT NULL, 100), 4)]), id = 6
      EnumerableTableScan(table=[[APACHE_LOGS]]), id = 5
</code></pre></div>
<p>它可能与您看到的Storm SQL是否应用查询优化有所不同。</p>

<p>我们正在执行第一个Storm SQL拓扑！当您看到足够的输出和日志时，请终止拓扑。</p>

<p>简而言之，我们将跳过对已经看到的内容的解释。</p>

<h2 id="example-filtering-slow-logs">示例：过滤慢日志</h2>

<p>在此示例中，我们将从整个日志中过滤慢速日志并将其存储到另一个主题。 <code>project</code>和<code>filter</code>和<code>User Defined Function (UDF)</code>功能将被使用。这非常类似于<code>filtering error logs</code>但我们将看到如何定义<code>User Defined Function (UDF)</code> 。</p>

<p>脚本文件的内容在这里：</p>
<div class="highlight"><pre><code class="language-" data-lang="">CREATE EXTERNAL TABLE APACHE_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS VARCHAR, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_US DOUBLE) LOCATION 'kafka://apache-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE EXTERNAL TABLE APACHE_SLOW_LOGS (ID INT PRIMARY KEY, REMOTE_IP VARCHAR, REQUEST_URL VARCHAR, REQUEST_METHOD VARCHAR, STATUS INT, REQUEST_HEADER_USER_AGENT VARCHAR, TIME_RECEIVED_UTC_ISOFORMAT VARCHAR, TIME_RECEIVED_TIMESTAMP BIGINT, TIME_ELAPSED_MS INT) LOCATION 'kafka://apache-slow-logs?bootstrap-servers=localhost:9092' TBLPROPERTIES '{"producer":{"acks":"1","key.serializer":"org.apache.storm.kafka.IntSerializer"}}'
CREATE FUNCTION GET_TIME AS 'org.apache.storm.sql.runtime.functions.scalar.datetime.GetTime2'
INSERT INTO APACHE_SLOW_LOGS SELECT ID, REMOTE_IP, REQUEST_URL, REQUEST_METHOD, CAST(STATUS AS INT) AS STATUS_INT, REQUEST_HEADER_USER_AGENT, TIME_RECEIVED_UTC_ISOFORMAT, GET_TIME(TIME_RECEIVED_UTC_ISOFORMAT, 'yyyy-MM-dd''T''HH:mm:ssZZ') AS TIME_RECEIVED_TIMESTAMP, TIME_US / 1000 AS TIME_ELAPSED_MS FROM APACHE_LOGS WHERE (TIME_US / 1000) &gt;= 100
</code></pre></div>
<p>将此文件保存到<code>apache_log_slow_filtering.sql</code> 。</p>

<p>我们可以跳过前两个语句，因为它与上一个示例几乎相同。</p>

<p>第三条陈述定义<code>User defined function</code> 。我们正在定义<code>GET_TIME</code>使用<code>org.apache.storm.sql.runtime.functions.scalar.datetime.GetTime2</code>类。</p>

<p>GetTime2的实现在这里：</p>
<div class="highlight"><pre><code class="language-" data-lang=""><span class="k">package</span> <span class="n">org</span><span class="p">.</span><span class="n">apache</span><span class="p">.</span><span class="n">storm</span><span class="p">.</span><span class="n">sql</span><span class="p">.</span><span class="n">runtime</span><span class="p">.</span><span class="n">functions</span><span class="p">.</span><span class="n">scalar</span><span class="p">.</span><span class="n">datetime</span><span class="p">;</span>

<span class="n">import</span> <span class="n">org</span><span class="p">.</span><span class="n">joda</span><span class="p">.</span><span class="n">time</span><span class="p">.</span><span class="n">format</span><span class="p">.</span><span class="n">DateTimeFormat</span><span class="p">;</span>
<span class="n">import</span> <span class="n">org</span><span class="p">.</span><span class="n">joda</span><span class="p">.</span><span class="n">time</span><span class="p">.</span><span class="n">format</span><span class="p">.</span><span class="n">DateTimeFormatter</span><span class="p">;</span>

<span class="k">public</span> <span class="n">class</span> <span class="n">GetTime2</span> <span class="p">{</span>
    <span class="k">public</span> <span class="n">static</span> <span class="n">Long</span> <span class="n">evaluate</span><span class="p">(</span><span class="k">String</span> <span class="n">dateString</span><span class="p">,</span> <span class="k">String</span> <span class="n">dateFormat</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">try</span> <span class="p">{</span>
            <span class="n">DateTimeFormatter</span> <span class="n">df</span> <span class="p">=</span> <span class="n">DateTimeFormat</span><span class="p">.</span><span class="n">forPattern</span><span class="p">(</span><span class="n">dateFormat</span><span class="p">).</span><span class="n">withZoneUTC</span><span class="p">();</span>
            <span class="n">return</span> <span class="n">df</span><span class="p">.</span><span class="n">parseDateTime</span><span class="p">(</span><span class="n">dateString</span><span class="p">).</span><span class="n">getMillis</span><span class="p">();</span>
        <span class="p">}</span> <span class="n">catch</span> <span class="p">(</span><span class="n">Exception</span> <span class="n">ex</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">throw</span> <span class="n">new</span> <span class="n">RuntimeException</span><span class="p">(</span><span class="n">ex</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>此类可用于UDF，因为它定义了static <code>evaluate</code>方法。参数和返回值的SQL类型由Storm SQL依赖的方解石确定。</p>

<p>请注意，此类应位于类路径中，因此要定义UDF，需要创建包含UDF类的jar文件并运行<code>storm sql</code>与<code>--jar</code>选项。为了简单起见，此页面假定GetTime2位于类路径中。</p>

<p>最后一条语句与过滤错误日志非常相似。唯一的新事物是我们称之为<code>GET_TIME(TIME_RECEIVED_UTC_ISOFORMAT, 'yyyy-MM-dd''T''HH:mm:ssZZ')</code>将字符串时间转换为Unix时间戳（BIGINT）。</p>

<p>让我们执行它。</p>
<div class="highlight"><pre><code class="language-" data-lang="">$ $STORM_DIR/bin/storm sql apache_log_slow_filtering.sql apache_log_slow_filtering --artifacts "org.apache.storm:storm-sql-kafka:2.0.0-SNAPSHOT,org.apache.storm:storm-kafka-client:2.0.0-SNAPSHOT,org.apache.kafka:kafka-clients:1.1.0^org.slf4j:slf4j-log4j12"
</code></pre></div>
<p>您可以通过控制台查看输出：</p>
<div class="highlight"><pre><code class="language-" data-lang="">$ kafka-console-consumer --zookeeper localhost:2181 --topic apache-slow-logs
</code></pre></div>
<p>输出将类似于：</p>
<div class="highlight"><pre><code class="language-" data-lang="">{"ID":890502,"REMOTE_IP":"136.156.159.160","REQUEST_URL":"/list","REQUEST_METHOD":"GET","STATUS":200,"REQUEST_HEADER_USER_AGENT":"Mozilla/5.0 (Windows NT 5.01) AppleWebKit/5311 (KHTML, like Gecko) Chrome/13.0.860.0 Safari/5311","TIME_RECEIVED_UTC_ISOFORMAT":"2021-06-05T03:44:59+00:00","TIME_RECEIVED_TIMESTAMP":1622864699000,"TIME_ELAPSED_MS":638.579}
{"ID":890542,"REMOTE_IP":"105.146.3.190","REQUEST_URL":"/search/tag/list","REQUEST_METHOD":"DELETE","STATUS":200,"REQUEST_HEADER_USER_AGENT":"Mozilla/5.0 (X11; Linux i686) AppleWebKit/5332 (KHTML, like Gecko) Chrome/13.0.891.0 Safari/5332","TIME_RECEIVED_UTC_ISOFORMAT":"2021-06-05T05:54:27+00:00","TIME_RECEIVED_TIMESTAMP":1622872467000,"TIME_ELAPSED_MS":403.957}
...
</code></pre></div>
<p>而已！假设我们有UDF可以通过远程ip查询地理位置，则可以通过地理位置进行过滤，或者将地理位置丰富到转换后的结果。</p>

<h2 id="summary">摘要</h2>

<p>我们研究了Storm SQL的几个简单用例，以学习Storm SQL功能。如果您尚未查看<a href="storm-sql.html">Storm SQL集成</a>和<a href="storm-sql-reference.html">Storm SQL语言</a> ，则需要阅读它才能查看全部受支持的功能。</p>
</div>


	          </div>
	       </div>
	  </div>
<footer>
    <div class="container-fluid">
        <div class="row">
            <div class="col-md-3">
                <div class="footer-widget">
                    <h5>聚会</h5>
                    <ul class="latest-news">
                        
                        <li><a href="http://www.meetup.com/Apache-Storm-Apache-Kafka/">Apache Storm和Apache Kafka</a> <span class="small">（加利福尼亚州桑尼维尔）</span></li>
                        
                        <li><a href="http://www.meetup.com/Apache-Storm-Kafka-Users/">Apache Storm和Kafka用户</a> <span class="small">（华盛顿州西雅图）</span></li>
                        
                        <li><a href="http://www.meetup.com/New-York-City-Storm-User-Group/">NYC Storm用户组</a> <span class="small">（纽约州纽约）</span></li>
                        
                        <li><a href="http://www.meetup.com/Bay-Area-Stream-Processing">湾区流处理</a> <span class="small">（加利福尼亚州埃默里维尔）</span></li>
                        
                        <li><a href="http://www.meetup.com/Boston-Storm-Users/">波士顿实时数据</a> <span class="small">（马萨诸塞州波士顿）</span></li>
                        
                        <li><a href="http://www.meetup.com/storm-london">伦敦风暴用户组</a> <span class="small">（英国伦敦）</span></li>
                        
                        <!-- <li><a href="http://www.meetup.com/Apache-Storm-Kafka-Users/">Seatle, WA</a> <span class="small">(27 Jun 2015)</span></li> -->
                    </ul>
                </div>
            </div>
            <div class="col-md-3">
                <div class="footer-widget">
                    <h5>关于Apache Storm</h5>
                    <p>Apache Storm与任何排队系统和任何数据库系统集成。Apache Storm的喷口抽象使集成新排队系统变得容易。同样，将Apache Storm与数据库系统集成很容易。</p>
               </div>
            </div>
            <div class="col-md-3">
                <div class="footer-widget">
                    <h5>第一眼</h5>
                    <ul class="footer-list">
                        <li><a href="/releases/current/Rationale.html">基本原理</a></li>
                        <li><a href="/releases/current/Tutorial.html">讲解</a></li>
                        <li><a href="/releases/current/Setting-up-development-environment.html">搭建开发环境</a></li>
                        <li><a href="/releases/current/Creating-a-new-Storm-project.html">创建一个新的Apache Storm项目</a></li>
                    </ul>
                </div>
            </div>
            <div class="col-md-3">
                <div class="footer-widget">
                    <h5>文献资料</h5>
                    <ul class="footer-list">
                        <li><a href="/releases/current/index.html">指数</a></li>
                        <li><a href="/releases/current/javadocs/index.html">Java文档</a></li>
                        <li><a href="/releases/current/FAQ.html">常问问题</a></li>
                    </ul>
                </div>
            </div>
        </div>
        <hr>
        <div class="row">   
            <div class="col-md-12">
                <p align="center">版权所有©2019 <a href="http://www.apache.org">Apache Software Foundation</a> 。版权所有。
                    <br>Apache Storm，Apache，Apache Feather徽标和Apache Storm项目徽标是The Apache Software Foundation的商标。
                    <br>提及的所有其他商标可能是其各自所有者的商标或注册商标。</p>
            </div>
        </div>
    </div>
</footer>
<!--Footer End-->
<!-- Scroll to top -->
<span class="totop"><a href="#"><i class="fa fa-angle-up"></i></a></span> 





</body></html>