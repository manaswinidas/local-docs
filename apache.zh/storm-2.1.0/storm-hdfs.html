<html ><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">

    <title>Storm HDFS集成</title>

    <!-- Bootstrap core CSS -->
    <link href="/assets/css/bootstrap.min.css" rel="stylesheet">
    <!-- Bootstrap theme -->
    <link href="/assets/css/bootstrap-theme.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link rel="stylesheet" href="asset?aid=3">
    <link href="/css/style.css" rel="stylesheet">
    <link href="/assets/css/owl.theme.css" rel="stylesheet">
    <link href="/assets/css/owl.carousel.css" rel="stylesheet">
    <script type="text/javascript" src="/assets/js/jquery.min.js"></script>
    <script type="text/javascript" src="/assets/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="/assets/js/owl.carousel.min.js"></script>
    <script type="text/javascript" src="/assets/js/storm.js"></script>
    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>


  <body >
    <header>
  <div class="container-fluid">
     <div class="row">
          <div class="col-md-5">
            <a href="/index.html"><img src="/images/logo.png" class="logo"></a>
          </div>
          <div class="col-md-5">
            
              <h1>版本：2.1.0</h1>
            
          </div>
          <div class="col-md-2">
            <a href="/downloads.html" class="btn-std btn-block btn-download">下载</a>
          </div>
        </div>
    </div>
</header>
<!--Header End-->
<!--Navigation Begin-->
<div class="navbar" role="banner">
  <div class="container-fluid">
      <div class="navbar-header">
          <button class="navbar-toggle" type="button" data-toggle="collapse" data-target=".bs-navbar-collapse"></button>
        </div>
        <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation">
          <ul class="nav navbar-nav">
              <li><a href="/index.html" id="home">家</a></li>
                <li><a href="/getting-help.html" id="getting-help">获得帮助</a></li>
                <li><a href="/about/integrates.html" id="project-info">项目信息</a></li>
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" id="documentation" data-toggle="dropdown">文献资料<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                      
                        
                          <li><a href="/releases/2.1.0/index.html">2.1.0</a></li>
                        
                      
                        
                          <li><a href="/releases/2.0.0/index.html">2.0.0</a></li>
                        
                      
                        
                          <li><a href="/releases/1.2.3/index.html">1.2.3</a></li>
                        
                      
                    </ul>
                </li>
                <li><a href="/talksAndVideos.html">讲座和幻灯片</a></li>
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" id="contribute" data-toggle="dropdown">社区<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        <li><a href="/contribute/Contributing-to-Storm.html">贡献</a></li>
                        <li><a href="/contribute/People.html">人</a></li>
                        <li><a href="/contribute/BYLAWS.html">按照法律规定</a></li>
                    </ul>
                </li>
                <li><a href="/2019/10/31/storm210-released.html" id="news">新闻</a></li>
            </ul>
        </nav>
    </div>
</div>



    <div class="container-fluid">
    <h1 class="page-title">Storm HDFS集成</h1>
          <div class="row">
           	<div class="col-md-12">
	             <!-- Documentation -->

<p class="post-meta"></p>

<div class="documentation-content"><p>用于与HDFS文件系统交互的Storm组件</p>

<h1 id="hdfs-bolt">HDFS螺栓</h1>

<h2 id="usage">用法</h2>

<p>以下示例将用竖线（“ |”）分隔的文件写入HDFS路径hdfs：// localhost：54310 / foo。每1,000个元组之后，它将同步文件系统，使该数据对其他HDFS客户端可见。当文件达到5兆字节时，它将旋转文件。</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="c1">// use "|" instead of "," for field delimiter</span>
<span class="n">RecordFormat</span> <span class="n">format</span> <span class="o">=</span> <span class="k">new</span> <span class="n">DelimitedRecordFormat</span><span class="o">()</span>
        <span class="o">.</span><span class="na">withFieldDelimiter</span><span class="o">(</span><span class="s">"|"</span><span class="o">);</span>

<span class="c1">// sync the filesystem after every 1k tuples</span>
<span class="n">SyncPolicy</span> <span class="n">syncPolicy</span> <span class="o">=</span> <span class="k">new</span> <span class="n">CountSyncPolicy</span><span class="o">(</span><span class="mi">1000</span><span class="o">);</span>

<span class="c1">// rotate files when they reach 5MB</span>
<span class="n">FileRotationPolicy</span> <span class="n">rotationPolicy</span> <span class="o">=</span> <span class="k">new</span> <span class="n">FileSizeRotationPolicy</span><span class="o">(</span><span class="mf">5.0f</span><span class="o">,</span> <span class="n">Units</span><span class="o">.</span><span class="na">MB</span><span class="o">);</span>

<span class="n">FileNameFormat</span> <span class="n">fileNameFormat</span> <span class="o">=</span> <span class="k">new</span> <span class="n">DefaultFileNameFormat</span><span class="o">()</span>
        <span class="o">.</span><span class="na">withPath</span><span class="o">(</span><span class="s">"/foo/"</span><span class="o">);</span>

<span class="n">HdfsBolt</span> <span class="n">bolt</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HdfsBolt</span><span class="o">()</span>
        <span class="o">.</span><span class="na">withFsUrl</span><span class="o">(</span><span class="s">"hdfs://localhost:54310"</span><span class="o">)</span>
        <span class="o">.</span><span class="na">withFileNameFormat</span><span class="o">(</span><span class="n">fileNameFormat</span><span class="o">)</span>
        <span class="o">.</span><span class="na">withRecordFormat</span><span class="o">(</span><span class="n">format</span><span class="o">)</span>
        <span class="o">.</span><span class="na">withRotationPolicy</span><span class="o">(</span><span class="n">rotationPolicy</span><span class="o">)</span>
        <span class="o">.</span><span class="na">withSyncPolicy</span><span class="o">(</span><span class="n">syncPolicy</span><span class="o">);</span>
</code></pre></div>
<h3 id="packaging-a-topology">打包拓扑</h3>

<p>打包拓扑时，请务必使用<a href="">maven-shade-plugin</a>而不是<a href="">maven-assembly-plugin</a> 。</p>

<p>shade插件提供了合并JAR清单条目的工具，hadoop客户端利用这些工具来解析URL方案。</p>

<p>如果遇到以下错误：</p>
<div class="highlight"><pre><code class="language-" data-lang="">java.lang.RuntimeException: Error preparing HdfsBolt: No FileSystem for scheme: hdfs
</code></pre></div>
<p>这表明您的拓扑jar文件未正确打包。</p>

<p>如果您正在使用maven创建拓扑jar，则应使用以下内容<code>maven-shade-plugin</code>配置以创建您的拓扑jar：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;plugin&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>org.apache.maven.plugins<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>maven-shade-plugin<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;version&gt;</span>1.4<span class="nt">&lt;/version&gt;</span>
    <span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;createDependencyReducedPom&gt;</span>true<span class="nt">&lt;/createDependencyReducedPom&gt;</span>
    <span class="nt">&lt;/configuration&gt;</span>
    <span class="nt">&lt;executions&gt;</span>
        <span class="nt">&lt;execution&gt;</span>
            <span class="nt">&lt;phase&gt;</span>package<span class="nt">&lt;/phase&gt;</span>
            <span class="nt">&lt;goals&gt;</span>
                <span class="nt">&lt;goal&gt;</span>shade<span class="nt">&lt;/goal&gt;</span>
            <span class="nt">&lt;/goals&gt;</span>
            <span class="nt">&lt;configuration&gt;</span>
                <span class="nt">&lt;transformers&gt;</span>
                    <span class="nt">&lt;transformer</span>
                            <span class="na">implementation=</span><span class="s">"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"</span><span class="nt">/&gt;</span>
                    <span class="nt">&lt;transformer</span>
                            <span class="na">implementation=</span><span class="s">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span><span class="nt">&gt;</span>
                        <span class="nt">&lt;mainClass&gt;&lt;/mainClass&gt;</span>
                    <span class="nt">&lt;/transformer&gt;</span>
                <span class="nt">&lt;/transformers&gt;</span>
            <span class="nt">&lt;/configuration&gt;</span>
        <span class="nt">&lt;/execution&gt;</span>
    <span class="nt">&lt;/executions&gt;</span>
<span class="nt">&lt;/plugin&gt;</span>

</code></pre></div>
<h3 id="specifying-a-hadoop-version">指定Hadoop版本</h3>

<p>默认情况下，storm-hdfs使用以下Hadoop依赖项：</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>org.apache.hadoop<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>hadoop-client<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;version&gt;</span>2.6.1<span class="nt">&lt;/version&gt;</span>
    <span class="nt">&lt;exclusions&gt;</span>
        <span class="nt">&lt;exclusion&gt;</span>
            <span class="nt">&lt;groupId&gt;</span>org.slf4j<span class="nt">&lt;/groupId&gt;</span>
            <span class="nt">&lt;artifactId&gt;</span>slf4j-log4j12<span class="nt">&lt;/artifactId&gt;</span>
        <span class="nt">&lt;/exclusion&gt;</span>
    <span class="nt">&lt;/exclusions&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>org.apache.hadoop<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>hadoop-hdfs<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;version&gt;</span>2.6.1<span class="nt">&lt;/version&gt;</span>
    <span class="nt">&lt;exclusions&gt;</span>
        <span class="nt">&lt;exclusion&gt;</span>
            <span class="nt">&lt;groupId&gt;</span>org.slf4j<span class="nt">&lt;/groupId&gt;</span>
            <span class="nt">&lt;artifactId&gt;</span>slf4j-log4j12<span class="nt">&lt;/artifactId&gt;</span>
        <span class="nt">&lt;/exclusion&gt;</span>
    <span class="nt">&lt;/exclusions&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div>
<p>如果您使用的是其他版本的Hadoop，则应从storm-hdfs依赖关系中排除Hadoop库，并在pom中添加首选版本的依赖关系。</p>

<p>Hadoop客户端版本不兼容可能表现为以下错误：</p>
<div class="highlight"><pre><code class="language-" data-lang="">com.google.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero)
</code></pre></div>
<h2 id="hdfs-bolt-customization">HDFS螺栓定制</h2>

<h3 id="record-formats">记录格式</h3>

<p>记录格式可以通过提供<code>org.apache.storm.hdfs.format.RecordFormat</code>接口：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">interface</span> <span class="nc">RecordFormat</span> <span class="kd">extends</span> <span class="n">Serializable</span> <span class="o">{</span>
    <span class="kt">byte</span><span class="o">[]</span> <span class="nf">format</span><span class="o">(</span><span class="n">Tuple</span> <span class="n">tuple</span><span class="o">);</span>
<span class="o">}</span>
</code></pre></div>
<p>提供的<code>org.apache.storm.hdfs.format.DelimitedRecordFormat</code>能够产生CSV和制表符分隔文件等格式。</p>

<h3 id="file-naming">文件命名</h3>

<p>可以通过提供以下内容的实现来控制文件命名<code>org.apache.storm.hdfs.format.FileNameFormat</code>接口：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">interface</span> <span class="nc">FileNameFormat</span> <span class="kd">extends</span> <span class="n">Serializable</span> <span class="o">{</span>
    <span class="kt">void</span> <span class="nf">prepare</span><span class="o">(</span><span class="n">Map</span> <span class="n">conf</span><span class="o">,</span> <span class="n">TopologyContext</span> <span class="n">topologyContext</span><span class="o">);</span>
    <span class="n">String</span> <span class="nf">getName</span><span class="o">(</span><span class="kt">long</span> <span class="n">rotation</span><span class="o">,</span> <span class="kt">long</span> <span class="n">timeStamp</span><span class="o">);</span>
    <span class="n">String</span> <span class="nf">getPath</span><span class="o">();</span>
<span class="o">}</span>
</code></pre></div>
<p>提供的<code>org.apache.storm.hdfs.format.DefaultFileNameFormat</code>将使用以下格式创建文件名：</p>
<div class="highlight"><pre><code class="language-" data-lang=""> {prefix}{componentId}-{taskId}-{rotationNum}-{timestamp}{extension}
</code></pre></div>
<p>例如：</p>
<div class="highlight"><pre><code class="language-" data-lang=""> MyBolt-5-7-1390579837830.txt
</code></pre></div>
<p>默认情况下，前缀为空，扩展名为“ .txt”。</p>

<p><strong>新的FileNameFormat：</strong></p>

<p>新提供<code>org.apache.storm.hdfs.format.SimpleFileNameFormat</code>和<code>org.apache.storm.hdfs.trident.format.SimpleFileNameFormat</code>更灵活， <code>withName</code>方法支持参数如下：</p>

<ul>
<li>$ TIME-当前时间。使用<code>withTimeFormat</code>格式化。</li>
<li>$ NUM-轮换号</li>
<li>$ HOST-本地主机名</li>
<li>$ PARTITION-分区索引（ <code>org.apache.storm.hdfs.trident.format.SimpleFileNameFormat</code>只要）</li>
<li>$ COMPONENT-组件ID（ <code>org.apache.storm.hdfs.format.SimpleFileNameFormat</code>只要）</li>
<li>$ TASK-任务ID（ <code>org.apache.storm.hdfs.format.SimpleFileNameFormat</code>只要）</li>
</ul>

<p>例如：<code>seq.$TIME.$HOST.$COMPONENT.$NUM.dat</code></p>

<p>默认文件<code>name</code>是<code>$TIME.$NUM.txt</code> ，以及默认<code>timeFormat</code>是<code>yyyyMMddHHmmss</code> 。</p>

<h3 id="sync-policies">同步政策</h3>

<p>通过同步策略，您可以通过实施<code>org.apache.storm.hdfs.sync.SyncPolicy</code>接口：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">interface</span> <span class="nc">SyncPolicy</span> <span class="kd">extends</span> <span class="n">Serializable</span> <span class="o">{</span>
    <span class="kt">boolean</span> <span class="nf">mark</span><span class="o">(</span><span class="n">Tuple</span> <span class="n">tuple</span><span class="o">,</span> <span class="kt">long</span> <span class="n">offset</span><span class="o">);</span>
    <span class="kt">void</span> <span class="nf">reset</span><span class="o">();</span>
<span class="o">}</span>
</code></pre></div>
<p>的<code>HdfsBolt</code>会打电话给<code>mark()</code>它处理的每个元组的方法。归来<code>true</code>将触发<code>HdfsBolt</code>进行同步/刷新，然后调用<code>reset()</code>方法。</p>

<p>的<code>org.apache.storm.hdfs.sync.CountSyncPolicy</code>在处理了指定数量的元组之后，该类仅触发同步。</p>

<h3 id="file-rotation-policies">文件轮换政策</h3>

<p>与同步策略类似，文件轮换策略可让您通过提供<code>org.apache.storm.hdfs.rotation.FileRotation</code>接口：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">interface</span> <span class="nc">FileRotationPolicy</span> <span class="kd">extends</span> <span class="n">Serializable</span> <span class="o">{</span>
    <span class="kt">boolean</span> <span class="nf">mark</span><span class="o">(</span><span class="n">Tuple</span> <span class="n">tuple</span><span class="o">,</span> <span class="kt">long</span> <span class="n">offset</span><span class="o">);</span>
    <span class="kt">void</span> <span class="nf">reset</span><span class="o">();</span>
    <span class="n">FileRotationPolicy</span> <span class="nf">copy</span><span class="o">();</span>
<span class="o">}</span>
</code></pre></div>
<p>的<code>org.apache.storm.hdfs.rotation.FileSizeRotationPolicy</code>实施允许您在数据文件达到特定文件大小时触发文件旋转：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">FileRotationPolicy</span> <span class="n">rotationPolicy</span> <span class="o">=</span> <span class="k">new</span> <span class="n">FileSizeRotationPolicy</span><span class="o">(</span><span class="mf">5.0f</span><span class="o">,</span> <span class="n">Units</span><span class="o">.</span><span class="na">MB</span><span class="o">);</span>
</code></pre></div>
<h3 id="file-rotation-actions">文件旋转动作</h3>

<p>HDFS螺栓和Trident State实施都允许您注册任意数量的<code>RotationAction</code> s。什么<code>RotationAction</code> s do是提供一个挂钩，使您可以在旋转文件后立即执行某些操作。例如，将文件移动到其他位置或重命名。</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">interface</span> <span class="nc">RotationAction</span> <span class="kd">extends</span> <span class="n">Serializable</span> <span class="o">{</span>
    <span class="kt">void</span> <span class="nf">execute</span><span class="o">(</span><span class="n">FileSystem</span> <span class="n">fileSystem</span><span class="o">,</span> <span class="n">Path</span> <span class="n">filePath</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span><span class="o">;</span>
<span class="o">}</span>
</code></pre></div>
<p>Storm-HDFS包含一个简单的操作，该操作将在旋转后移动文件：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">MoveFileAction</span> <span class="kd">implements</span> <span class="n">RotationAction</span> <span class="o">{</span>
    <span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="n">Logger</span> <span class="n">LOG</span> <span class="o">=</span> <span class="n">LoggerFactory</span><span class="o">.</span><span class="na">getLogger</span><span class="o">(</span><span class="n">MoveFileAction</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

    <span class="kd">private</span> <span class="n">String</span> <span class="n">destination</span><span class="o">;</span>

    <span class="kd">public</span> <span class="n">MoveFileAction</span> <span class="nf">withDestination</span><span class="o">(</span><span class="n">String</span> <span class="n">destDir</span><span class="o">){</span>
        <span class="n">destination</span> <span class="o">=</span> <span class="n">destDir</span><span class="o">;</span>
        <span class="k">return</span> <span class="k">this</span><span class="o">;</span>
    <span class="o">}</span>

    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">execute</span><span class="o">(</span><span class="n">FileSystem</span> <span class="n">fileSystem</span><span class="o">,</span> <span class="n">Path</span> <span class="n">filePath</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span> <span class="o">{</span>
        <span class="n">Path</span> <span class="n">destPath</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Path</span><span class="o">(</span><span class="n">destination</span><span class="o">,</span> <span class="n">filePath</span><span class="o">.</span><span class="na">getName</span><span class="o">());</span>
        <span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">"Moving file {} to {}"</span><span class="o">,</span> <span class="n">filePath</span><span class="o">,</span> <span class="n">destPath</span><span class="o">);</span>
        <span class="kt">boolean</span> <span class="n">success</span> <span class="o">=</span> <span class="n">fileSystem</span><span class="o">.</span><span class="na">rename</span><span class="o">(</span><span class="n">filePath</span><span class="o">,</span> <span class="n">destPath</span><span class="o">);</span>
        <span class="k">return</span><span class="o">;</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div>
<p>如果您使用的是Trident和序列文件，则可以执行以下操作：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java">        <span class="n">HdfsState</span><span class="o">.</span><span class="na">Options</span> <span class="n">seqOpts</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HdfsState</span><span class="o">.</span><span class="na">SequenceFileOptions</span><span class="o">()</span>
                <span class="o">.</span><span class="na">withFileNameFormat</span><span class="o">(</span><span class="n">fileNameFormat</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withSequenceFormat</span><span class="o">(</span><span class="k">new</span> <span class="n">DefaultSequenceFormat</span><span class="o">(</span><span class="s">"key"</span><span class="o">,</span> <span class="s">"data"</span><span class="o">))</span>
                <span class="o">.</span><span class="na">withRotationPolicy</span><span class="o">(</span><span class="n">rotationPolicy</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withFsUrl</span><span class="o">(</span><span class="s">"hdfs://localhost:54310"</span><span class="o">)</span>
                <span class="o">.</span><span class="na">addRotationAction</span><span class="o">(</span><span class="k">new</span> <span class="n">MoveFileAction</span><span class="o">().</span><span class="na">withDestination</span><span class="o">(</span><span class="s">"/dest2/"</span><span class="o">));</span>
</code></pre></div>
<h3 id="data-partitioning">资料分割</h3>

<p>可以根据要处理的元组的特征或纯粹的外部因素（例如系统时间）将数据分区到不同的HDFS目录。要对您的数据进行分区，请编写一个实现<code>Partitioner</code>接口，并将其传递给螺栓的withPartitioner（）方法。getPartitionPath（）方法返回给定元组的分区路径。</p>

<p>这是在特定数据字段上运行的分区程序的示例：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java">
    <span class="n">Partitioner</span> <span class="n">partitoner</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Partitioner</span><span class="o">()</span> <span class="o">{</span>
            <span class="nd">@Override</span>
            <span class="kd">public</span> <span class="n">String</span> <span class="nf">getPartitionPath</span><span class="o">(</span><span class="n">Tuple</span> <span class="n">tuple</span><span class="o">)</span> <span class="o">{</span>
                <span class="k">return</span> <span class="n">Path</span><span class="o">.</span><span class="na">SEPARATOR</span> <span class="o">+</span> <span class="n">tuple</span><span class="o">.</span><span class="na">getStringByField</span><span class="o">(</span><span class="s">"city"</span><span class="o">);</span>
            <span class="o">}</span>
     <span class="o">};</span>
</code></pre></div>
<h2 id="hdfs-bolt-support-for-hdfs-sequence-files">HDFS螺栓对HDFS序列文件的支持</h2>

<p>的<code>org.apache.storm.hdfs.bolt.SequenceFileBolt</code>类可让您将风暴数据写入HDFS序列文件：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java">        <span class="c1">// sync the filesystem after every 1k tuples</span>
        <span class="n">SyncPolicy</span> <span class="n">syncPolicy</span> <span class="o">=</span> <span class="k">new</span> <span class="n">CountSyncPolicy</span><span class="o">(</span><span class="mi">1000</span><span class="o">);</span>

        <span class="c1">// rotate files when they reach 5MB</span>
        <span class="n">FileRotationPolicy</span> <span class="n">rotationPolicy</span> <span class="o">=</span> <span class="k">new</span> <span class="n">FileSizeRotationPolicy</span><span class="o">(</span><span class="mf">5.0f</span><span class="o">,</span> <span class="n">Units</span><span class="o">.</span><span class="na">MB</span><span class="o">);</span>

        <span class="n">FileNameFormat</span> <span class="n">fileNameFormat</span> <span class="o">=</span> <span class="k">new</span> <span class="n">DefaultFileNameFormat</span><span class="o">()</span>
                <span class="o">.</span><span class="na">withExtension</span><span class="o">(</span><span class="s">".seq"</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withPath</span><span class="o">(</span><span class="s">"/data/"</span><span class="o">);</span>

        <span class="c1">// create sequence format instance.</span>
        <span class="n">DefaultSequenceFormat</span> <span class="n">format</span> <span class="o">=</span> <span class="k">new</span> <span class="n">DefaultSequenceFormat</span><span class="o">(</span><span class="s">"timestamp"</span><span class="o">,</span> <span class="s">"sentence"</span><span class="o">);</span>

        <span class="n">SequenceFileBolt</span> <span class="n">bolt</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SequenceFileBolt</span><span class="o">()</span>
                <span class="o">.</span><span class="na">withFsUrl</span><span class="o">(</span><span class="s">"hdfs://localhost:54310"</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withFileNameFormat</span><span class="o">(</span><span class="n">fileNameFormat</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withSequenceFormat</span><span class="o">(</span><span class="n">format</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withRotationPolicy</span><span class="o">(</span><span class="n">rotationPolicy</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withSyncPolicy</span><span class="o">(</span><span class="n">syncPolicy</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withCompressionType</span><span class="o">(</span><span class="n">SequenceFile</span><span class="o">.</span><span class="na">CompressionType</span><span class="o">.</span><span class="na">RECORD</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withCompressionCodec</span><span class="o">(</span><span class="s">"deflate"</span><span class="o">);</span>
</code></pre></div>
<p>的<code>SequenceFileBolt</code>要求您提供一个<code>org.apache.storm.hdfs.bolt.format.SequenceFormat</code>将元组映射到键/值对：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">interface</span> <span class="nc">SequenceFormat</span> <span class="kd">extends</span> <span class="n">Serializable</span> <span class="o">{</span>
    <span class="n">Class</span> <span class="nf">keyClass</span><span class="o">();</span>
    <span class="n">Class</span> <span class="nf">valueClass</span><span class="o">();</span>

    <span class="n">Writable</span> <span class="nf">key</span><span class="o">(</span><span class="n">Tuple</span> <span class="n">tuple</span><span class="o">);</span>
    <span class="n">Writable</span> <span class="nf">value</span><span class="o">(</span><span class="n">Tuple</span> <span class="n">tuple</span><span class="o">);</span>
<span class="o">}</span>
</code></pre></div>
<h2 id="hdfs-bolt-support-for-avro-files">HDFS Bolt对Avro文件的支持</h2>

<p>的<code>org.apache.storm.hdfs.bolt.AvroGenericRecordBolt</code>类允许您将Avro对象直接写入HDFS：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java">        <span class="c1">// sync the filesystem after every 1k tuples</span>
        <span class="n">SyncPolicy</span> <span class="n">syncPolicy</span> <span class="o">=</span> <span class="k">new</span> <span class="n">CountSyncPolicy</span><span class="o">(</span><span class="mi">1000</span><span class="o">);</span>

        <span class="c1">// rotate files when they reach 5MB</span>
        <span class="n">FileRotationPolicy</span> <span class="n">rotationPolicy</span> <span class="o">=</span> <span class="k">new</span> <span class="n">FileSizeRotationPolicy</span><span class="o">(</span><span class="mf">5.0f</span><span class="o">,</span> <span class="n">Units</span><span class="o">.</span><span class="na">MB</span><span class="o">);</span>

        <span class="n">FileNameFormat</span> <span class="n">fileNameFormat</span> <span class="o">=</span> <span class="k">new</span> <span class="n">DefaultFileNameFormat</span><span class="o">()</span>
                <span class="o">.</span><span class="na">withExtension</span><span class="o">(</span><span class="s">".avro"</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withPath</span><span class="o">(</span><span class="s">"/data/"</span><span class="o">);</span>

        <span class="c1">// create sequence format instance.</span>
        <span class="n">DefaultSequenceFormat</span> <span class="n">format</span> <span class="o">=</span> <span class="k">new</span> <span class="n">DefaultSequenceFormat</span><span class="o">(</span><span class="s">"timestamp"</span><span class="o">,</span> <span class="s">"sentence"</span><span class="o">);</span>

        <span class="n">AvroGenericRecordBolt</span> <span class="n">bolt</span> <span class="o">=</span> <span class="k">new</span> <span class="n">AvroGenericRecordBolt</span><span class="o">()</span>
                <span class="o">.</span><span class="na">withFsUrl</span><span class="o">(</span><span class="s">"hdfs://localhost:54310"</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withFileNameFormat</span><span class="o">(</span><span class="n">fileNameFormat</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withRotationPolicy</span><span class="o">(</span><span class="n">rotationPolicy</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withSyncPolicy</span><span class="o">(</span><span class="n">syncPolicy</span><span class="o">);</span>
</code></pre></div>
<p>Avro螺栓将根据正在处理的记录的架构将记录写入单独的文件中。换句话说，如果螺栓接收具有两个不同模式的记录，它将写入两个单独的文件。每个文件将根据指定的轮换策略进行轮换。如果需要大量的Avro模式，则应该为螺栓配置最大数量的打开文件，该数量至少等于预期的模式数量，以防止过多的文件打开/关闭/创建操作。</p>

<p>要使用此螺栓， <strong>必须</strong>在拓扑配置中注册适当的Kryo串行器。为此提供了一种方便的方法：</p>

<p><code>AvroUtils.addAvroKryoSerializations(conf);</code></p>

<p>默认情况下，Storm将使用<code>GenericAvroSerializer</code>处理序列化。这将起作用，但是如果您可以预定义将要使用的架构或使用外部架构注册表，则可以使用更快的选项。提供了使用Confluent Schema Registry的实现，但是可以实现其他实现并将其提供给Storm。有关使用内置选项或创建自己的选项的信息，请参见org.apache.storm.hdfs.avro中的类的javadoc。</p>

<h2 id="hdfs-bolt-support-for-trident-api">HDFS Bolt对Trident API的支持</h2>

<p>Storm-hdfs还包括一个三叉戟<code>state</code>用于将数据写入HDFS的实现，其API与螺栓关系紧密。</p>
<div class="highlight"><pre><code class="language-java" data-lang="java">         <span class="n">Fields</span> <span class="n">hdfsFields</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Fields</span><span class="o">(</span><span class="s">"field1"</span><span class="o">,</span> <span class="s">"field2"</span><span class="o">);</span>

         <span class="n">FileNameFormat</span> <span class="n">fileNameFormat</span> <span class="o">=</span> <span class="k">new</span> <span class="n">DefaultFileNameFormat</span><span class="o">()</span>
                 <span class="o">.</span><span class="na">withPath</span><span class="o">(</span><span class="s">"/trident"</span><span class="o">)</span>
                 <span class="o">.</span><span class="na">withPrefix</span><span class="o">(</span><span class="s">"trident"</span><span class="o">)</span>
                 <span class="o">.</span><span class="na">withExtension</span><span class="o">(</span><span class="s">".txt"</span><span class="o">);</span>

         <span class="n">RecordFormat</span> <span class="n">recordFormat</span> <span class="o">=</span> <span class="k">new</span> <span class="n">DelimitedRecordFormat</span><span class="o">()</span>
                 <span class="o">.</span><span class="na">withFields</span><span class="o">(</span><span class="n">hdfsFields</span><span class="o">);</span>

         <span class="n">FileRotationPolicy</span> <span class="n">rotationPolicy</span> <span class="o">=</span> <span class="k">new</span> <span class="n">FileSizeRotationPolicy</span><span class="o">(</span><span class="mf">5.0f</span><span class="o">,</span> <span class="n">FileSizeRotationPolicy</span><span class="o">.</span><span class="na">Units</span><span class="o">.</span><span class="na">MB</span><span class="o">);</span>

        <span class="n">HdfsState</span><span class="o">.</span><span class="na">Options</span> <span class="n">options</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HdfsState</span><span class="o">.</span><span class="na">HdfsFileOptions</span><span class="o">()</span>
                <span class="o">.</span><span class="na">withFileNameFormat</span><span class="o">(</span><span class="n">fileNameFormat</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withRecordFormat</span><span class="o">(</span><span class="n">recordFormat</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withRotationPolicy</span><span class="o">(</span><span class="n">rotationPolicy</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withFsUrl</span><span class="o">(</span><span class="s">"hdfs://localhost:54310"</span><span class="o">);</span>

         <span class="n">StateFactory</span> <span class="n">factory</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HdfsStateFactory</span><span class="o">().</span><span class="na">withOptions</span><span class="o">(</span><span class="n">options</span><span class="o">);</span>

         <span class="n">TridentState</span> <span class="n">state</span> <span class="o">=</span> <span class="n">stream</span>
                 <span class="o">.</span><span class="na">partitionPersist</span><span class="o">(</span><span class="n">factory</span><span class="o">,</span> <span class="n">hdfsFields</span><span class="o">,</span> <span class="k">new</span> <span class="n">HdfsUpdater</span><span class="o">(),</span> <span class="k">new</span> <span class="n">Fields</span><span class="o">());</span>
</code></pre></div>
<p>使用序列文件<code>State</code>实施，使用<code>HdfsState.SequenceFileOptions</code> ：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java">        <span class="n">HdfsState</span><span class="o">.</span><span class="na">Options</span> <span class="n">seqOpts</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HdfsState</span><span class="o">.</span><span class="na">SequenceFileOptions</span><span class="o">()</span>
                <span class="o">.</span><span class="na">withFileNameFormat</span><span class="o">(</span><span class="n">fileNameFormat</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withSequenceFormat</span><span class="o">(</span><span class="k">new</span> <span class="n">DefaultSequenceFormat</span><span class="o">(</span><span class="s">"key"</span><span class="o">,</span> <span class="s">"data"</span><span class="o">))</span>
                <span class="o">.</span><span class="na">withRotationPolicy</span><span class="o">(</span><span class="n">rotationPolicy</span><span class="o">)</span>
                <span class="o">.</span><span class="na">withFsUrl</span><span class="o">(</span><span class="s">"hdfs://localhost:54310"</span><span class="o">)</span>
                <span class="o">.</span><span class="na">addRotationAction</span><span class="o">(</span><span class="k">new</span> <span class="n">MoveFileAction</span><span class="o">().</span><span class="na">toDestination</span><span class="o">(</span><span class="s">"/dest2/"</span><span class="o">));</span>
</code></pre></div>
<h3 id="note">注意</h3>

<p>每当由于风暴而重播批次时（由于失败），三叉戟状态实现都会通过将直到最后一个事务的数据复制到另一个文件中，自动从当前数据文件中删除重复项。由于此操作涉及大量数据复制，因此请确保以合理的大小旋转数据文件， <code>FileSizeRotationPolicy</code>并在合理的间隔内<code>TimedRotationPolicy</code>这样恢复就可以在topology.message.timeout.secs中完成。</p>

<p>另请注意<code>TimedRotationPolicy</code>即使计时器计时，文件也不会在批处理中间旋转，而仅在批处理完成时才旋转，以便在发生故障时可以有效地恢复完整的批处理。</p>

<h2 id="working-with-secure-hdfs">使用安全HDFS</h2>

<p>如果您的拓扑结构要与安全的HDFS交互，则螺栓/状态需要由NameNode进行身份验证。目前，我们有2个选项可支持此操作：</p>

<h3 id="using-hdfs-delegation-tokens">使用HDFS委托令牌</h3>

<p>您的管理员可以将nimbus配置为代表拓扑提交者用户自动获取委派令牌。nimbus应该以以下配置启动：</p>
<div class="highlight"><pre><code class="language-" data-lang="">nimbus.autocredential.plugins.classes : ["org.apache.storm.hdfs.security.AutoHDFS"]
nimbus.credential.renewers.classes : ["org.apache.storm.hdfs.security.AutoHDFS"]
hdfs.keytab.file: "/path/to/keytab/on/nimbus" (This is the keytab of hdfs super user that can impersonate other users.)
hdfs.kerberos.principal: "superuser@EXAMPLE.com" 
nimbus.credential.renewers.freq.secs : 82800 (23 hours, hdfs tokens needs to be renewed every 24 hours so this value should be less then 24 hours.)
topology.hdfs.uri:"hdfs://host:port" (This is an optional config, by default we will use value of "fs.defaultFS" property specified in hadoop's core-site.xml)
</code></pre></div>
<p>您的拓扑配置应具有：</p>
<div class="highlight"><pre><code class="language-" data-lang="">topology.auto-credentials :["org.apache.storm.hdfs.common.security.AutoHDFS"]
</code></pre></div>
<p>如果nimbus没有上述配置，则需要添加然后重新启动它。确保nimbus的类路径中包含hadoop配置文件（core-site.xml和hdfs-site.xml）和带有所有依赖项的storm-hdfs jar。</p>

<p>作为将配置文件（core-site.xml和hdfs-site.xml）添加到类路径的替代方法，您可以将配置指定为拓扑配置的一部分。例如，在您自定义的storm.yaml中（或在提交拓扑时使用-c选项），</p>
<div class="highlight"><pre><code class="language-" data-lang="">hdfsCredentialsConfigKeys : ["cluster1", "cluster2"] (the hdfs clusters you want to fetch the tokens from)
"cluster1": {"config1": "value1", "config2": "value2", ... } (A map of config key-values specific to cluster1)
"cluster2": {"config1": "value1", "hdfs.keytab.file": "/path/to/keytab/for/cluster2/on/nimubs", "hdfs.kerberos.principal": "cluster2user@EXAMPLE.com"} (here along with other configs, we have custom keytab and principal for "cluster2" which will override the keytab/principal specified at topology level)
</code></pre></div>
<p>除了指定键值，您还可以直接指定资源文件，例如，</p>
<div class="highlight"><pre><code class="language-" data-lang="">"cluster1": {"resources": ["/path/to/core-site1.xml", "/path/to/hdfs-site1.xml"]}
"cluster2": {"resources": ["/path/to/core-site2.xml", "/path/to/hdfs-site2.xml"]}
</code></pre></div>
<p>Storm将为每个群集分别下载令牌，并将其填充到主题中，并定期更新令牌。这样，将有可能运行多个螺栓，以连接到同一拓扑中的单独的HDFS群集。</p>

<p>Nimbus将使用配置中指定的密钥表和主体与Namenode进行身份验证。从那时起，对于每个拓扑提交，nimbus都会模拟拓扑提交者用户，并代表拓扑提交者用户获取委派令牌。如果拓扑是在topology.auto-credentials设置为AutoHDFS的情况下启动的，则nimbus会将委托令牌推送给拓扑的所有工作程序，并且hdfs螺栓/状态将使用这些令牌向namenode进行身份验证。</p>

<p>由于nimbus冒充拓扑提交者用户，因此您需要确保hdfs.kerberos.principal中指定的用户具有代表其他用户获取令牌的权限。为此，您需要遵循此链接上列出的配置说明<a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html</a></p>

<p>您可以在此处阅读有关设置安全HDFS的信息： <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html">http</a> : <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html">//hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html</a> 。</p>

<h3 id="using-keytabs-on-all-worker-hosts">在所有辅助主机上使用密钥表</h3>

<p>如果您已在所有潜在的辅助主机上分发了hdfs用户的密钥表文件，则可以使用此方法。您应该使用HdfsBolt / State.withconfigKey（“ somekey”）方法指定hdfs配置密钥，并且该密钥的值映射应具有以下2个属性：</p>

<p>hdfs.keytab.file：“ / path / to / keytab /” hdfs.kerberos.principal：“ <a href="mailto:user@EXAMPLE.com">user@EXAMPLE.com</a> ”</p>

<p>在工作主机上，螺栓/三叉戟状态代码将使用配置中提供的具有主体的keytab文件对Namenode进行身份验证。此方法几乎没有危险，因为您需要确保所有工作线程都将keytab文件放在同一位置，并且在群集中启动新主机时需要记住这一点。</p>

<hr>

<h1 id="hdfs-spout">HDFS喷口</h1>

<p>HDFS喷口旨在允许将数据从HDFS目录馈送到Storm中。它将主动监视目录以使用目录中出现的所有新文件。HDFS喷口当前不支持Trident。</p>

<p><strong>Impt</strong> ：Hdfs spout假定未主动写入在监视目录中对其可见的文件。只有在完全写入文件后，才能使文件对喷口可见。这可以通过以下方式实现：将文件写出到另一个目录中，一旦完全写完，将其移动到受监视的目录中。或者，可以在受监视的目录中使用“ .ignore”后缀来创建文件，并在数据完全写入后，对其进行重命名而不使用后缀。喷口将忽略带有'.ignore'后缀的文件名。</p>

<p>当喷口正在积极使用文件时，它将使用“ .inprogress”后缀重命名该文件。在使用完文件中的所有内容之后，文件将被移至可配置的<em>完成</em>目录，并且后缀'.inprogress'将被删除。</p>

<p><strong>并发</strong>如果在拓扑中使用多个喷口实例，则每个实例将使用一个不同的文件。使用在受监视目录下的（默认）“。lock”子目录中创建的锁定文件来完成spout实例之间的同步。在lock目录中创建与正在使用的文件同名的文件（不带进行中的后缀）。文件完全消耗完后，将删除相应的锁定文件。</p>

<p><strong>从故障中恢复</strong>喷口还会定期记录有关锁定文件中已消耗了多少文件的进度信息。如果喷口实例崩溃（或强行杀死拓扑），则另一个喷口可以接管文件并从锁定文件中记录的位置恢复。</p>

<p>某些错误情况（例如喷口崩溃）可能会留下锁文件而不删除它们。这种过时的锁定文件还指示相应的输入文件也没有被完全处理。一旦检测到，这些过时的锁定文件的所有权将被转移到另一个喷嘴。<br>配置“ hdfsspout.lock.timeout.sec”用于指定不活动的持续时间，在此之后应将锁定文件视为过期。为使锁文件所有权转移成功，文件（从上一个锁所有者处）的HDFS租约应已到期。在选择下一个要使用的文件之前，喷口会扫描过时的锁定文件。</p>

<p><strong>锁定<em>.lock</em>目录</strong> Hdfs spout实例<strong>在<em>.lock</em>目录中</strong>创建一个<em>DIRLOCK</em>文件，以协调对.lock目录本身的某些访问。当需要访问.lock目录时，spout将尝试创建它，然后在完成后将其删除。在拓扑崩溃，强制终止或喷口过早死亡等错误情况下，此文件可能不会被删除。一旦由于hdfsspout.lock.timeout.sec秒处于不活动状态而使DIRLOCK文件失效，则将来运行的spout实例最终将恢复该状态。</p>

<h2 id="usage">用法</h2>

<p>以下示例创建一个HDFS喷口，该喷口从HDFS路径hdfs：// localhost：54310 / source读取文本文件。</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="c1">// Instantiate spout to read text files</span>
<span class="n">HdfsSpout</span> <span class="n">textReaderSpout</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HdfsSpout</span><span class="o">().</span><span class="na">setReaderType</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
                                           <span class="o">.</span><span class="na">withOutputFields</span><span class="o">(</span><span class="n">TextFileReader</span><span class="o">.</span><span class="na">defaultFields</span><span class="o">)</span>                                      
                                           <span class="o">.</span><span class="na">setHdfsUri</span><span class="o">(</span><span class="s">"hdfs://localhost:54310"</span><span class="o">)</span>  <span class="c1">// required</span>
                                           <span class="o">.</span><span class="na">setSourceDir</span><span class="o">(</span><span class="s">"/data/in"</span><span class="o">)</span>              <span class="c1">// required                                      </span>
                                           <span class="o">.</span><span class="na">setArchiveDir</span><span class="o">(</span><span class="s">"/data/done"</span><span class="o">)</span>           <span class="c1">// required</span>
                                           <span class="o">.</span><span class="na">setBadFilesDir</span><span class="o">(</span><span class="s">"/data/badfiles"</span><span class="o">);</span>     <span class="c1">// required                                      </span>
<span class="c1">// If using Kerberos</span>
<span class="n">HashMap</span> <span class="n">hdfsSettings</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HashMap</span><span class="o">();</span>
<span class="n">hdfsSettings</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">"hdfs.keytab.file"</span><span class="o">,</span> <span class="s">"/path/to/keytab"</span><span class="o">);</span>
<span class="n">hdfsSettings</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">"hdfs.kerberos.principal"</span><span class="o">,</span><span class="s">"user@EXAMPLE.com"</span><span class="o">);</span>

<span class="n">textReaderSpout</span><span class="o">.</span><span class="na">setHdfsClientSettings</span><span class="o">(</span><span class="n">hdfsSettings</span><span class="o">);</span>

<span class="c1">// Create topology</span>
<span class="n">TopologyBuilder</span> <span class="n">builder</span> <span class="o">=</span> <span class="k">new</span> <span class="n">TopologyBuilder</span><span class="o">();</span>
<span class="n">builder</span><span class="o">.</span><span class="na">setSpout</span><span class="o">(</span><span class="s">"hdfsspout"</span><span class="o">,</span> <span class="n">textReaderSpout</span><span class="o">,</span> <span class="n">SPOUT_NUM</span><span class="o">);</span>

<span class="c1">// Setup bolts and wire up topology</span>
     <span class="o">..</span><span class="na">snip</span><span class="o">..</span>

<span class="c1">// Submit topology with config</span>
<span class="n">Config</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Config</span><span class="o">();</span>
<span class="n">StormSubmitter</span><span class="o">.</span><span class="na">submitTopologyWithProgressBar</span><span class="o">(</span><span class="s">"topologyName"</span><span class="o">,</span> <span class="n">conf</span><span class="o">,</span> <span class="n">builder</span><span class="o">.</span><span class="na">createTopology</span><span class="o">());</span>
</code></pre></div>
<p>风暴启动器模块中提供了示例拓扑HdfsSpoutTopology。</p>

<h2 id="configuration-settings">配置设定</h2>

<p>以下是用于配置的HdfsSpout成员函数的列表。也可以通过在提交拓扑期间传递的Config对象来实现等效的config。但是，不建议使用后一种机制，因为它不允许多个Hdfs喷口使用不同的设置。 ：</p>

<p>仅需要<strong>粗体</strong>提及的方法。</p>

<table><thead>
<tr>
<th>方法</th>
<th>备用配置名称（不建议使用）</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead><tbody>
<tr>
<td><strong>.setReaderType（）</strong></td>
<td>~~ hdfsspout.reader.type ~~</td>
<td></td>
<td>确定要使用的文件阅读器。设置为“ seq”（用于读取序列文件）或“文本”（用于文本文件）。如果使用自定义文件读取器类（该类实现接口org.apache.storm.hdfs.spout，请设置为标准类名）。FileReader）</td>
</tr>
<tr>
<td><strong>.withOutputFields（）</strong></td>
<td></td>
<td></td>
<td>设置喷口输出字段的名称。字段数取决于所使用的阅读器。为了方便起见，内置的读取器类型公开了一个静态成员，称为<code>defaultFields</code>可以用来设置它。</td>
</tr>
<tr>
<td><strong>.setHdfsUri（）</strong></td>
<td>~~ hdfsspout.hdfs ~~</td>
<td></td>
<td>hdfs名称节点的HDFS URI。示例：hdfs：// namenodehost：8020</td>
</tr>
<tr>
<td><strong>.setSourceDir（）</strong></td>
<td>~~ hdfsspout.source.dir ~~</td>
<td></td>
<td>从中读取文件的HDFS目录。例如/ data / inputdir</td>
</tr>
<tr>
<td><strong>.setArchiveDir（）</strong></td>
<td>~~ hdfsspout.archive.dir ~~</td>
<td></td>
<td>文件完全处理后，将被移到该HDFS目录。如果此目录不存在，将创建它。例如/ data / done</td>
</tr>
<tr>
<td><strong>.setBadFilesDir（）</strong></td>
<td>~~ hdfsspout.badfiles.dir ~~</td>
<td></td>
<td>如果解析文件内容时出错，则文件将移至该位置。如果此目录不存在，将创建它。例如/ data / badfiles</td>
</tr>
<tr>
<td>.setLockDir（）</td>
<td>~~ hdfsspout.lock.dir ~~</td>
<td>hdfsspout.source.dir下的'.lock'子目录</td>
<td>将在其中创建锁定文件的目录。并发HDFS出口实例使用<em>锁定</em>文件进行同步。在处理文件之前，spout实例在此目录中创建一个与输入文件同名的锁定文件，并在处理该文件后删除该锁定文件。Spout还会定期在锁定文件中记录其进度（读输入文件的wrt），这样，如果由于某种原因而死，另一个Spout实例可以在同一文件上恢复进度。</td>
</tr>
<tr>
<td>.setIgnoreSuffix（）</td>
<td>~~ hdfsspout.ignore.suffix ~~</td>
<td>。忽视</td>
<td>hdfsspout.source.dir位置中带有此后缀的文件名将不被处理</td>
</tr>
<tr>
<td>.setCommitFrequencyCount（）</td>
<td>~~ hdfsspout.commit.count ~~</td>
<td>20000</td>
<td>处理完许多记录后，在锁定文件中记录进度。如果设置为0，将不使用此条件。</td>
</tr>
<tr>
<td>.setCommitFrequencySec（）</td>
<td>~~ hdfsspout.commit.sec ~~</td>
<td>10</td>
<td>这几秒钟后，在锁定文件中记录进度。必须大于0</td>
</tr>
<tr>
<td>.setMaxOutstanding（）</td>
<td>~~ hdfsspout.max.outstanding ~~</td>
<td>10000</td>
<td>通过暂停元组生成来限制未确认元组的数量（如果在拓扑中使用了ACKer）</td>
</tr>
<tr>
<td>.setLockTimeoutSec（）</td>
<td>~~ hdfsspout.lock.timeout.sec ~~</td>
<td>5分钟</td>
<td>不活动的持续时间，在此持续时间内，锁定文件被认为已放弃并准备好让另一个壶嘴拥有所有权</td>
</tr>
<tr>
<td>.setClocksInSync（）</td>
<td>~~ hdfsspout.clocks.insync ~~</td>
<td>真正</td>
<td>指示风暴计算机上的时钟是否同步（使用NTP等服务）。用于检测过时的锁定。</td>
</tr>
<tr>
<td>.withConfigKey（）</td>
<td></td>
<td></td>
<td>可选设置。覆盖用于指定HDFS客户端配置的默认密钥名称（“ hdfs.config”，请参见下文）。</td>
</tr>
<tr>
<td>.setHdfsClientSettings（）</td>
<td>~~ hdfs.config ~~（除非通过withConfigKey进行更改）</td>
<td></td>
<td>将其设置为表示要使用的HDFS设置的键/值对映射。例如，可以使用此设置keytab和principal。请参阅下面的“ HDFS螺栓”下的<strong>在所有辅助主机上使用密钥表</strong>一节。</td>
</tr>
<tr>
<td>.withOutputStream（）</td>
<td></td>
<td></td>
<td>输出流的名称。如果设置，则将元组发送到指定的流。其他元组将被发送到默认输出流</td>
</tr>
</tbody></table>

<hr>
</div>


	          </div>
	       </div>
	  </div>
<footer>
    <div class="container-fluid">
        <div class="row">
            <div class="col-md-3">
                <div class="footer-widget">
                    <h5>聚会</h5>
                    <ul class="latest-news">
                        
                        <li><a href="http://www.meetup.com/Apache-Storm-Apache-Kafka/">Apache Storm和Apache Kafka</a> <span class="small">（加利福尼亚州桑尼维尔）</span></li>
                        
                        <li><a href="http://www.meetup.com/Apache-Storm-Kafka-Users/">Apache Storm和Kafka用户</a> <span class="small">（华盛顿州西雅图）</span></li>
                        
                        <li><a href="http://www.meetup.com/New-York-City-Storm-User-Group/">NYC Storm用户组</a> <span class="small">（纽约州纽约）</span></li>
                        
                        <li><a href="http://www.meetup.com/Bay-Area-Stream-Processing">湾区流处理</a> <span class="small">（加利福尼亚州埃默里维尔）</span></li>
                        
                        <li><a href="http://www.meetup.com/Boston-Storm-Users/">波士顿实时数据</a> <span class="small">（马萨诸塞州波士顿）</span></li>
                        
                        <li><a href="http://www.meetup.com/storm-london">伦敦风暴用户组</a> <span class="small">（英国伦敦）</span></li>
                        
                        <!-- <li><a href="http://www.meetup.com/Apache-Storm-Kafka-Users/">Seatle, WA</a> <span class="small">(27 Jun 2015)</span></li> -->
                    </ul>
                </div>
            </div>
            <div class="col-md-3">
                <div class="footer-widget">
                    <h5>关于Apache Storm</h5>
                    <p>Apache Storm与任何排队系统和任何数据库系统集成。Apache Storm的喷口抽象使集成新排队系统变得容易。同样，将Apache Storm与数据库系统集成很容易。</p>
               </div>
            </div>
            <div class="col-md-3">
                <div class="footer-widget">
                    <h5>第一眼</h5>
                    <ul class="footer-list">
                        <li><a href="/releases/current/Rationale.html">基本原理</a></li>
                        <li><a href="/releases/current/Tutorial.html">讲解</a></li>
                        <li><a href="/releases/current/Setting-up-development-environment.html">搭建开发环境</a></li>
                        <li><a href="/releases/current/Creating-a-new-Storm-project.html">创建一个新的Apache Storm项目</a></li>
                    </ul>
                </div>
            </div>
            <div class="col-md-3">
                <div class="footer-widget">
                    <h5>文献资料</h5>
                    <ul class="footer-list">
                        <li><a href="/releases/current/index.html">指数</a></li>
                        <li><a href="/releases/current/javadocs/index.html">Java文档</a></li>
                        <li><a href="/releases/current/FAQ.html">常问问题</a></li>
                    </ul>
                </div>
            </div>
        </div>
        <hr>
        <div class="row">   
            <div class="col-md-12">
                <p align="center">版权所有©2019 <a href="http://www.apache.org">Apache Software Foundation</a> 。版权所有。
                    <br>Apache Storm，Apache，Apache Feather徽标和Apache Storm项目徽标是The Apache Software Foundation的商标。
                    <br>提及的所有其他商标可能是其各自所有者的商标或注册商标。</p>
            </div>
        </div>
    </div>
</footer>
<!--Footer End-->
<!-- Scroll to top -->
<span class="totop"><a href="#"><i class="fa fa-angle-up"></i></a></span> 





</body></html>