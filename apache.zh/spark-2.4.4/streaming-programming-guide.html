<html class="no-js" ><head></head><body >﻿<!--<![endif]-->
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Spark流-Spark 2.4.4文档</title>
        
          <meta name="description" content="Spark Streaming programming guide and tutorial for Spark 2.4.4">
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        
        <!-- Google analytics script -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-32518208-2']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
        </script>
        

    
    
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html"><img src="img/spark-logo-hd.png" style="height:50px"></a> <span class="version">2.4.4</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">总览</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">编程指南<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">快速开始</a></li>
                                <li><a href="rdd-programming-guide.html">RDD，累加器，广播变量</a></li>
                                <li><a href="sql-programming-guide.html">SQL，数据框和数据集</a></li>
                                <li><a href="structured-streaming-programming-guide.html">结构化流</a></li>
                                <li><a href="streaming-programming-guide.html">火花流（DStreams）</a></li>
                                <li><a href="ml-guide.html">MLlib（机器学习）</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX（图形处理）</a></li>
                                <li><a href="sparkr.html">SparkR（Spark上的R）</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API文件<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">斯卡拉</a></li>
                                <li><a href="api/java/index.html">爪哇</a></li>
                                <li><a href="api/python/index.html">蟒蛇</a></li>
                                <li><a href="api/R/index.html">[R</a></li>
                                <li><a href="api/sql/index.html">SQL，内置函数</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">部署中<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">总览</a></li>
                                <li><a href="submitting-applications.html">提交申请</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark独立</a></li>
                                <li><a href="running-on-mesos.html">梅索斯</a></li>
                                <li><a href="running-on-yarn.html">纱</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">更多<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">组态</a></li>
                                <li><a href="monitoring.html">监控方式</a></li>
                                <li><a href="tuning.html">调音指南</a></li>
                                <li><a href="job-scheduling.html">作业调度</a></li>
                                <li><a href="security.html">安全</a></li>
                                <li><a href="hardware-provisioning.html">硬件配置</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">建筑火花</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">为Spark贡献</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">第三方项目</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v2.4.4</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                <div class="content" id="content">
                    
                        <h1 class="title">Spark Streaming编程指南</h1>
                    

                    <ul id="markdown-toc">
  <li><a href="#overview" id="markdown-toc-overview">总览</a></li>
  <li><a href="#a-quick-example" id="markdown-toc-a-quick-example">一个简单的例子</a></li>
  <li><a href="#basic-concepts" id="markdown-toc-basic-concepts">基本概念</a>    <ul>
      <li><a href="#linking" id="markdown-toc-linking">连结中</a></li>
      <li><a href="#initializing-streamingcontext" id="markdown-toc-initializing-streamingcontext">初始化StreamingContext</a></li>
      <li><a href="#discretized-streams-dstreams" id="markdown-toc-discretized-streams-dstreams">离散流（DStreams）</a></li>
      <li><a href="#input-dstreams-and-receivers" id="markdown-toc-input-dstreams-and-receivers">输入DStreams和接收器</a></li>
      <li><a href="#transformations-on-dstreams" id="markdown-toc-transformations-on-dstreams">DStreams上的转换</a></li>
      <li><a href="#output-operations-on-dstreams" id="markdown-toc-output-operations-on-dstreams">DStreams上的输出操作</a></li>
      <li><a href="#dataframe-and-sql-operations" id="markdown-toc-dataframe-and-sql-operations">DataFrame和SQL操作</a></li>
      <li><a href="#mllib-operations" id="markdown-toc-mllib-operations">MLlib操作</a></li>
      <li><a href="#caching--persistence" id="markdown-toc-caching--persistence">缓存/持久化</a></li>
      <li><a href="#checkpointing" id="markdown-toc-checkpointing">检查点</a></li>
      <li><a href="#accumulators-broadcast-variables-and-checkpoints" id="markdown-toc-accumulators-broadcast-variables-and-checkpoints">累加器，广播变量和检查点</a></li>
      <li><a href="#deploying-applications" id="markdown-toc-deploying-applications">部署应用</a></li>
      <li><a href="#monitoring-applications" id="markdown-toc-monitoring-applications">监控应用</a></li>
    </ul>
  </li>
  <li><a href="#performance-tuning" id="markdown-toc-performance-tuning">性能调优</a>    <ul>
      <li><a href="#reducing-the-batch-processing-times" id="markdown-toc-reducing-the-batch-processing-times">减少批处理时间</a></li>
      <li><a href="#setting-the-right-batch-interval" id="markdown-toc-setting-the-right-batch-interval">设置正确的批次间隔</a></li>
      <li><a href="#memory-tuning" id="markdown-toc-memory-tuning">内存调优</a></li>
    </ul>
  </li>
  <li><a href="#fault-tolerance-semantics" id="markdown-toc-fault-tolerance-semantics">容错语义</a></li>
  <li><a href="#where-to-go-from-here" id="markdown-toc-where-to-go-from-here">从这往哪儿走</a></li>
</ul>

<h1 id="overview">总览</h1>
<p>Spark Streaming是核心Spark API的扩展，可实现实时数据流的可伸缩，高吞吐量，容错流处理。可以从许多来源（例如Kafka，Flume，Kinesis或TCP套接字）中提取数据，并可以使用以高级功能（例如， <code>map</code> ， <code>reduce</code> ， <code>join</code>和<code>window</code> 。最后，可以将处理后的数据推送到文件系统，数据库和实时仪表板。实际上，您可以在数据流上应用Spark的<a href="ml-guide.html">机器学习</a>和<a href="graphx-programming-guide.html">图形处理</a>算法。</p>

<p style="text-align:center">
  <img src="img/streaming-arch.png" title="Spark Streaming架构" alt="火花流" width="70%">
</p>

<p>在内部，它的工作方式如下。Spark Streaming接收实时输入数据流，并将数据分成批次，然后由Spark引擎进行处理，以成批生成最终结果流。</p>

<p style="text-align:center">
  <img src="img/streaming-flow.png" title="Spark Streaming数据流" alt="火花流" width="70%">
</p>

<p>Spark Streaming提供了称为<em>离散流</em>或<em>DStream</em>的高级抽象，它表示连续的数据流。可以根据来自Kafka，Flume和Kinesis等来源的输入数据流来创建DStream，也可以对其他DStream应用高级操作。在内部，DStream表示为<a href="api/scala/index.html#org.apache.spark.rdd.RDD">RDD</a>序列。</p>

<p>本指南向您展示如何开始使用DStreams编写Spark Streaming程序。您可以使用Scala，Java或Python（Spark 1.2中引入）编写Spark Streaming程序，本指南中介绍了所有这些程序。您将在本指南中找到一些标签，可以在不同语言的代码段之间进行选择。</p>

<p><strong>注意：</strong>有一些API可能不同，或者在Python中不可用。在本指南中，您会发现<span class="badge" style="background-color:grey">Python API</span>标签突出了这些差异。</p>

<hr>

<h1 id="a-quick-example">一个简单的例子</h1>
<p>在详细介绍如何编写自己的Spark Streaming程序之前，让我们快速看一下简单的Spark Streaming程序的外观。假设我们要计算从侦听TCP套接字的数据服务器接收到的文本数据中的单词数。您需要做的如下。</p>

<div class="codetabs">
<div data-lang="scala">
    <p>首先，我们将Spark Streaming类的名称以及从StreamingContext进行的一些隐式转换导入到我们的环境中，以便向我们需要的其他类（如DStream）添加有用的方法。<a href="api/scala/index.html#org.apache.spark.streaming.StreamingContext">StreamingContext</a>是所有流功能的主要入口点。我们创建具有两个执行线程和1秒批处理间隔的本地StreamingContext。</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">org.apache.spark._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.streaming._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.streaming.StreamingContext._</span> <span class="c1">// not necessary since Spark 1.3</span>

<span class="c1">// Create a local StreamingContext with two working thread and batch interval of 1 second.</span>
<span class="c1">// The master requires 2 cores to prevent a starvation scenario.</span>

<span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">().</span><span class="n">setMaster</span><span class="o">(</span><span class="s">&quot;local[2]&quot;</span><span class="o">).</span><span class="n">setAppName</span><span class="o">(</span><span class="s">&quot;NetworkWordCount&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">ssc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StreamingContext</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span> <span class="nc">Seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span></code></pre></figure>

    <p>使用此上下文，我们可以创建一个DStream，该DStream表示来自TCP源的流数据，指定为主机名（例如<code>localhost</code> ）和端口（例如<code>9999</code> ）。</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// Create a DStream that will connect to hostname:port, like localhost:9999</span>
<span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">ssc</span><span class="o">.</span><span class="n">socketTextStream</span><span class="o">(</span><span class="s">&quot;localhost&quot;</span><span class="o">,</span> <span class="mi">9999</span><span class="o">)</span></code></pre></figure>

    <p>这个<code>lines</code> DStream表示将从数据服务器接收的数据流。此DStream中的每个记录都是一行文本。接下来，我们要用空格将行分割成单词。</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// Split each line into words</span>
<span class="k">val</span> <span class="n">words</span> <span class="k">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">))</span></code></pre></figure>

    <p><code>flatMap</code>是一对多DStream操作，它通过从源DStream中的每个记录生成多个新记录来创建新的DStream。在这种情况下，每行将分为多个单词，单词流表示为<code>words</code> DStream。接下来，我们要计算这些单词。</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">org.apache.spark.streaming.StreamingContext._</span> <span class="c1">// not necessary since Spark 1.3</span>
<span class="c1">// Count each word in each batch</span>
<span class="k">val</span> <span class="n">pairs</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">word</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">word</span><span class="o">,</span> <span class="mi">1</span><span class="o">))</span>
<span class="k">val</span> <span class="n">wordCounts</span> <span class="k">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKey</span><span class="o">(</span><span class="k">_</span> <span class="o">+</span> <span class="k">_</span><span class="o">)</span>

<span class="c1">// Print the first ten elements of each RDD generated in this DStream to the console</span>
<span class="n">wordCounts</span><span class="o">.</span><span class="n">print</span><span class="o">()</span></code></pre></figure>

    <p>的<code>words</code> DStream被进一步映射（一对一转换）到<code>(word, 1)</code>对，然后减少以获取每批数据中单词的频率。最后， <code>wordCounts.print()</code>将打印每秒产生的一些计数。</p>

    <p>请注意，执行这些行时，Spark Streaming仅设置启动时将执行的计算，并且尚未开始任何实际处理。在完成所有转换后，要开始处理，我们最终调用</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">ssc</span><span class="o">.</span><span class="n">start</span><span class="o">()</span>             <span class="c1">// Start the computation</span>
<span class="n">ssc</span><span class="o">.</span><span class="n">awaitTermination</span><span class="o">()</span>  <span class="c1">// Wait for the computation to terminate</span></code></pre></figure>

    <p>完整的代码可以在Spark Streaming示例<a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala">NetworkWordCount中找到</a> 。
<br></p>

  </div>
<div data-lang="java">

    <p>首先，我们创建一个<a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html">JavaStreamingContext</a>对象，它是所有流功能的主要入口点。我们创建具有两个执行线程和1秒批处理间隔的本地StreamingContext。</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.function.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.streaming.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.streaming.api.java.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">scala.Tuple2</span><span class="o">;</span>

<span class="c1">// Create a local StreamingContext with two working thread and batch interval of 1 second</span>
<span class="n">SparkConf</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SparkConf</span><span class="o">().</span><span class="na">setMaster</span><span class="o">(</span><span class="s">&quot;local[2]&quot;</span><span class="o">).</span><span class="na">setAppName</span><span class="o">(</span><span class="s">&quot;NetworkWordCount&quot;</span><span class="o">);</span>
<span class="n">JavaStreamingContext</span> <span class="n">jssc</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaStreamingContext</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span> <span class="n">Durations</span><span class="o">.</span><span class="na">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">));</span></code></pre></figure>

    <p>使用此上下文，我们可以创建一个DStream，该DStream表示来自TCP源的流数据，指定为主机名（例如<code>localhost</code> ）和端口（例如<code>9999</code> ）。</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// Create a DStream that will connect to hostname:port, like localhost:9999</span>
<span class="n">JavaReceiverInputDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">lines</span> <span class="o">=</span> <span class="n">jssc</span><span class="o">.</span><span class="na">socketTextStream</span><span class="o">(</span><span class="s">&quot;localhost&quot;</span><span class="o">,</span> <span class="mi">9999</span><span class="o">);</span></code></pre></figure>

    <p>这个<code>lines</code> DStream表示将从数据服务器接收的数据流。此流中的每个记录都是一行文本。然后，我们想将行按空间分成单词。</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// Split each line into words</span>
<span class="n">JavaDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">words</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="n">x</span> <span class="o">-&gt;</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">)).</span><span class="na">iterator</span><span class="o">());</span></code></pre></figure>

    <p><code>flatMap</code>是DStream操作，它通过从源DStream中的每个记录生成多个新记录来创建新的DStream。在这种情况下，每行将分为多个单词，单词流表示为<code>words</code> DStream。请注意，我们使用<a href="api/scala/index.html#org.apache.spark.api.java.function.FlatMapFunction">FlatMapFunction</a>对象定义了转换。正如我们将在此过程中发现的那样，Java API中有许多此类便利类可帮助定义DStream转换。</p>

    <p>接下来，我们要计算这些单词。</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// Count each word in each batch</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="na">mapToPair</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;&gt;(</span><span class="n">s</span><span class="o">,</span> <span class="mi">1</span><span class="o">));</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">wordCounts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="na">reduceByKey</span><span class="o">((</span><span class="n">i1</span><span class="o">,</span> <span class="n">i2</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">i1</span> <span class="o">+</span> <span class="n">i2</span><span class="o">);</span>

<span class="c1">// Print the first ten elements of each RDD generated in this DStream to the console</span>
<span class="n">wordCounts</span><span class="o">.</span><span class="na">print</span><span class="o">();</span></code></pre></figure>

    <p>的<code>words</code> DStream被进一步映射（一对一转换）到<code>(word, 1)</code>配对，使用<a href="api/scala/index.html#org.apache.spark.api.java.function.PairFunction">PairFunction</a>对象。然后，使用<a href="api/scala/index.html#org.apache.spark.api.java.function.Function2">Function2</a>对象将其减少以获取每批数据中单词的频率。最后， <code>wordCounts.print()</code>将打印每秒产生的一些计数。</p>

    <p>请注意，执行这些行时，Spark Streaming仅设置启动后将执行的计算，并且尚未开始任何实际处理。在完成所有转换后，要开始处理，我们最终调用<code>start</code>方法。</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">jssc</span><span class="o">.</span><span class="na">start</span><span class="o">();</span>              <span class="c1">// Start the computation</span>
<span class="n">jssc</span><span class="o">.</span><span class="na">awaitTermination</span><span class="o">();</span>   <span class="c1">// Wait for the computation to terminate</span></code></pre></figure>

    <p>完整的代码可以在Spark Streaming示例<a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java">JavaNetworkWordCount中找到</a> 。
<br></p>

  </div>
<div data-lang="python">
    <p>首先，我们导入<a href="api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext">StreamingContext</a> ，这是所有流功能的主要入口点。我们创建具有两个执行线程和1秒批处理间隔的本地StreamingContext。</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>
<span class="kn">from</span> <span class="nn">pyspark.streaming</span> <span class="kn">import</span> <span class="n">StreamingContext</span>

<span class="c1"># Create a local StreamingContext with two working thread and batch interval of 1 second</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="s2">&quot;local[2]&quot;</span><span class="p">,</span> <span class="s2">&quot;NetworkWordCount&quot;</span><span class="p">)</span>
<span class="n">ssc</span> <span class="o">=</span> <span class="n">StreamingContext</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></code></pre></figure>

    <p>使用此上下文，我们可以创建一个DStream，该DStream表示来自TCP源的流数据，指定为主机名（例如<code>localhost</code> ）和端口（例如<code>9999</code> ）。</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># Create a DStream that will connect to hostname:port, like localhost:9999</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">ssc</span><span class="o">.</span><span class="n">socketTextStream</span><span class="p">(</span><span class="s2">&quot;localhost&quot;</span><span class="p">,</span> <span class="mi">9999</span><span class="p">)</span></code></pre></figure>

    <p>这个<code>lines</code> DStream表示将从数据服务器接收的数据流。此DStream中的每个记录都是一行文本。接下来，我们要按行将行分成单词。</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># Split each line into words</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">))</span></code></pre></figure>

    <p><code>flatMap</code>是一对多DStream操作，它通过从源DStream中的每个记录生成多个新记录来创建新的DStream。在这种情况下，每行将分为多个单词，单词流表示为<code>words</code> DStream。接下来，我们要计算这些单词。</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># Count each word in each batch</span>
<span class="n">pairs</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">wordCounts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Print the first ten elements of each RDD generated in this DStream to the console</span>
<span class="n">wordCounts</span><span class="o">.</span><span class="n">pprint</span><span class="p">()</span></code></pre></figure>

    <p>的<code>words</code> DStream被进一步映射（一对一转换）到<code>(word, 1)</code>对，然后减少以获取每批数据中单词的频率。最后， <code>wordCounts.pprint()</code>将打印每秒产生的一些计数。</p>

    <p>请注意，执行这些行时，Spark Streaming仅设置启动时将执行的计算，并且尚未开始任何实际处理。在完成所有转换后，要开始处理，我们最终调用</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">ssc</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>             <span class="c1"># Start the computation</span>
<span class="n">ssc</span><span class="o">.</span><span class="n">awaitTermination</span><span class="p">()</span>  <span class="c1"># Wait for the computation to terminate</span></code></pre></figure>

    <p>完整的代码可以在Spark Streaming示例<a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/python/streaming/network_wordcount.py">NetworkWordCount中找到</a> 。
<br></p>

  </div>
</div>

<p>如果您已经<a href="index.html#downloading">下载</a>并<a href="index.html#building">构建了</a> Spark，则可以按以下方式运行此示例。您首先需要通过使用以下命令将Netcat（在大多数类Unix系统中找到的一个小实用程序）作为数据服务器运行</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ nc -lk <span class="m">9999</span></code></pre></figure>

<p>然后，在另一个终端中，您可以通过使用</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/run-example streaming.NetworkWordCount localhost <span class="m">9999</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/run-example streaming.JavaNetworkWordCount localhost <span class="m">9999</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost <span class="m">9999</span></code></pre></figure>

  </div>
</div>

<p>然后，将对运行netcat服务器的终端中键入的任何行进行计数并每秒打印一次。它将类似于以下内容。</p>

<table width="100%">
    <tbody><tr><td>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="c1"># TERMINAL 1:</span>
<span class="c1"># Running Netcat</span>

$ nc -lk <span class="m">9999</span>

hello world



...</code></pre></figure>

    </td>
    <td width="2%"></td>
    <td>
<div class="codetabs">

<div data-lang="scala">

        <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="c1"># TERMINAL 2: RUNNING NetworkWordCount</span>

$ ./bin/run-example streaming.NetworkWordCount localhost <span class="m">9999</span>
...
-------------------------------------------
Time: <span class="m">1357008430000</span> ms
-------------------------------------------
<span class="o">(</span>hello,1<span class="o">)</span>
<span class="o">(</span>world,1<span class="o">)</span>
...</code></pre></figure>

      </div>

<div data-lang="java">

        <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="c1"># TERMINAL 2: RUNNING JavaNetworkWordCount</span>

$ ./bin/run-example streaming.JavaNetworkWordCount localhost <span class="m">9999</span>
...
-------------------------------------------
Time: <span class="m">1357008430000</span> ms
-------------------------------------------
<span class="o">(</span>hello,1<span class="o">)</span>
<span class="o">(</span>world,1<span class="o">)</span>
...</code></pre></figure>

      </div>
<div data-lang="python">

        <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="c1"># TERMINAL 2: RUNNING network_wordcount.py</span>

$ ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost <span class="m">9999</span>
...
-------------------------------------------
Time: <span class="m">2014</span>-10-14 <span class="m">15</span>:25:21
-------------------------------------------
<span class="o">(</span>hello,1<span class="o">)</span>
<span class="o">(</span>world,1<span class="o">)</span>
...</code></pre></figure>

      </div>
</div>
    </td>
</tr></tbody></table>

<hr>
<hr>

<h1 id="basic-concepts">基本概念</h1>

<p>接下来，我们将超越简单的示例，并详细介绍Spark Streaming的基础知识。</p>

<h2 id="linking">连结中</h2>

<p>与Spark相似，可以通过Maven Central使用Spark Streaming。要编写自己的Spark Streaming程序，您必须将以下依赖项添加到SBT或Maven项目中。</p>

<div class="codetabs">
<div data-lang="Maven">

    <pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-streaming_2.12&lt;/artifactId&gt;
    &lt;version&gt;2.4.4&lt;/version&gt;
    &lt;scope&gt;provided&lt;/scope&gt;
&lt;/dependency&gt;
</code></pre>
  </div>
<div data-lang="SBT">

    <pre><code>libraryDependencies += "org.apache.spark" % "spark-streaming_2.12" % "2.4.4" % "provided"
</code></pre>
  </div>
</div>

<p>要从Spark Streaming核心API中从Kafka，Flume和Kinesis等来源获取数据，则必须添加相应的工件<code>spark-streaming-xyz_2.12</code>到依赖项。例如，一些常见的如下。</p>

<table class="table">
<tbody><tr><th>资源</th><th>神器</th></tr>
<tr><td>卡夫卡</td><td>spark-streaming-kafka-0-10_2.12</td></tr>
<tr><td>水槽</td><td>spark-streaming-flume_2.12</td></tr>
<tr><td>运动学<br></td><td>spark-streaming-kinesis-asl_2.12 [Amazon软件许可]</td></tr>
<tr><td></td><td></td></tr>
</tbody></table>

<p>有关最新列表，请参阅<a href="https://search.maven.org/#search|ga|1|g%3A"org.apache.spark" AND v%3A"2.4.4"">Maven存储库</a> ，以获取受支持的源和工件的完整列表。</p>

<hr>

<h2 id="initializing-streamingcontext">初始化StreamingContext</h2>

<p>要初始化Spark Streaming程序，必须创建<strong>StreamingContext</strong>对象，该对象是所有Spark Streaming功能的主要入口点。</p>

<div class="codetabs">
<div data-lang="scala">

    <p>可以从<a href="api/scala/index.html#org.apache.spark.SparkConf">SparkConf</a>对象创建<a href="api/scala/index.html#org.apache.spark.streaming.StreamingContext">StreamingContext</a>对象。</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">org.apache.spark._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.streaming._</span>

<span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">().</span><span class="n">setAppName</span><span class="o">(</span><span class="n">appName</span><span class="o">).</span><span class="n">setMaster</span><span class="o">(</span><span class="n">master</span><span class="o">)</span>
<span class="k">val</span> <span class="n">ssc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StreamingContext</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span> <span class="nc">Seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span></code></pre></figure>

    <p>的<code>appName</code>参数是您的应用程序在集群UI上显示的名称。 <code>master</code>是<a href="submitting-applications.html#master-urls">Spark，Mesos，Kubernetes或YARN群集URL</a>或特殊的<strong>“ local [*]”</strong>字符串，以在本地模式下运行。实际上，在群集上运行时，您将不希望硬编码<code>master</code>在程序中，而是<a href="submitting-applications.html">使用<code>spark-submit</code></a>并在那里收到。但是，对于本地测试和单元测试，您可以传递“ local [*]”以在内部运行Spark Streaming（检测本地系统中的内核数）。请注意，这会在内部创建一个<a href="api/scala/index.html#org.apache.spark.SparkContext">SparkContext</a> （所有Spark功能的起点），可以通过以下方式访问<code>ssc.sparkContext</code> 。</p>

    <p>必须根据应用程序的延迟要求和可用的群集资源来设置批处理间隔。有关更多详细信息，请参见<a href="#setting-the-right-batch-interval">性能调整</a>部分。</p>

    <p>一种<code>StreamingContext</code>也可以从现有对象创建对象<code>SparkContext</code>宾语。</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">org.apache.spark.streaming._</span>

<span class="k">val</span> <span class="n">sc</span> <span class="k">=</span> <span class="o">...</span>                <span class="c1">// existing SparkContext</span>
<span class="k">val</span> <span class="n">ssc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StreamingContext</span><span class="o">(</span><span class="n">sc</span><span class="o">,</span> <span class="nc">Seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span></code></pre></figure>

  </div>
<div data-lang="java">

    <p>甲<a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html">JavaStreamingContext</a>对象可以从被创建<a href="api/java/index.html?org/apache/spark/SparkConf.html">SparkConf</a>对象。</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.streaming.api.java.*</span><span class="o">;</span>

<span class="n">SparkConf</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SparkConf</span><span class="o">().</span><span class="na">setAppName</span><span class="o">(</span><span class="n">appName</span><span class="o">).</span><span class="na">setMaster</span><span class="o">(</span><span class="n">master</span><span class="o">);</span>
<span class="n">JavaStreamingContext</span> <span class="n">ssc</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaStreamingContext</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span> <span class="k">new</span> <span class="n">Duration</span><span class="o">(</span><span class="mi">1000</span><span class="o">));</span></code></pre></figure>

    <p>的<code>appName</code>参数是您的应用程序在集群UI上显示的名称。 <code>master</code>是<a href="submitting-applications.html#master-urls">Spark，Mesos或YARN群集URL</a>或特殊的<strong>“ local [*]”</strong>字符串，以在本地模式下运行。实际上，在群集上运行时，您将不希望硬编码<code>master</code>在程序中，而是<a href="submitting-applications.html">使用<code>spark-submit</code></a>并在那里收到。但是，对于本地测试和单元测试，您可以传递“ local [*]”以运行Spark Streaming。请注意，这会在内部创建一个<a href="api/java/index.html?org/apache/spark/api/java/JavaSparkContext.html">JavaSparkContext</a> （所有Spark功能的起点），可以通过以下方式访问<code>ssc.sparkContext</code> 。</p>

    <p>必须根据应用程序的延迟要求和可用的群集资源来设置批处理间隔。有关更多详细信息，请参见<a href="#setting-the-right-batch-interval">性能调整</a>部分。</p>

    <p>一种<code>JavaStreamingContext</code>也可以从现有对象创建对象<code>JavaSparkContext</code> 。</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.streaming.api.java.*</span><span class="o">;</span>

<span class="n">JavaSparkContext</span> <span class="n">sc</span> <span class="o">=</span> <span class="o">...</span>   <span class="c1">//existing JavaSparkContext</span>
<span class="n">JavaStreamingContext</span> <span class="n">ssc</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaStreamingContext</span><span class="o">(</span><span class="n">sc</span><span class="o">,</span> <span class="n">Durations</span><span class="o">.</span><span class="na">seconds</span><span class="o">(</span><span class="mi">1</span><span class="o">));</span></code></pre></figure>

  </div>
<div data-lang="python">

    <p>可以从<a href="api/python/pyspark.html#pyspark.SparkContext">SparkContext</a>对象创建<a href="api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext">StreamingContext</a>对象。</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>
<span class="kn">from</span> <span class="nn">pyspark.streaming</span> <span class="kn">import</span> <span class="n">StreamingContext</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">master</span><span class="p">,</span> <span class="n">appName</span><span class="p">)</span>
<span class="n">ssc</span> <span class="o">=</span> <span class="n">StreamingContext</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></code></pre></figure>

    <p>的<code>appName</code>参数是您的应用程序在集群UI上显示的名称。 <code>master</code>是<a href="submitting-applications.html#master-urls">Spark，Mesos或YARN群集URL</a>或特殊的<strong>“ local [*]”</strong>字符串，以在本地模式下运行。实际上，在群集上运行时，您将不希望硬编码<code>master</code>在程序中，而是<a href="submitting-applications.html">使用<code>spark-submit</code></a>并在那里收到。但是，对于本地测试和单元测试，您可以传递“ local [*]”以在内部运行Spark Streaming（检测本地系统中的内核数）。</p>

    <p>必须根据应用程序的延迟要求和可用的群集资源来设置批处理间隔。有关更多详细信息，请参见<a href="#setting-the-right-batch-interval">性能调整</a>部分。</p>
  </div>
</div>

<p>定义上下文后，必须执行以下操作。</p>

<ol>
  <li>通过创建输入DStream定义输入源。</li>
  <li>通过将转换和输出操作应用于DStream来定义流计算。</li>
  <li>开始接收数据并使用进行处理<code>streamingContext.start()</code> 。</li>
  <li>等待使用以下命令停止处理（手动或由于任何错误） <code>streamingContext.awaitTermination()</code> 。</li>
  <li>可以使用手动停止处理<code>streamingContext.stop()</code> 。</li>
</ol>

<h5 class="no_toc" id="points-to-remember">要记住的要点：</h5>
<ul>
  <li>一旦启动上下文，就无法设置新的流计算或将其添加到该流计算中。</li>
  <li>上下文一旦停止，就无法重新启动。</li>
  <li>JVM中只能同时激活一个StreamingContext。</li>
  <li>StreamingContext上的stop（）也会停止SparkContext。要仅停止StreamingContext，请设置的可选参数<code>stop()</code>叫<code>stopSparkContext</code>虚假。</li>
  <li>只要在创建下一个StreamingContext之前停止了上一个StreamingContext（不停止SparkContext），就可以将SparkContext重用于创建多个StreamingContext。</li>
</ul>

<hr>

<h2 id="discretized-streams-dstreams">离散流（DStreams）</h2>
<p><strong>离散流</strong>或<strong>DStream</strong>是Spark Streaming提供的基本抽象。它代表连续的数据流，可以是从源接收的输入数据流，也可以是通过转换输入流生成的已处理数据流。在内部，DStream由一系列连续的RDD表示，这是Spark对不可变的分布式数据集的抽象（有关更多详细信息，请参见<a href="rdd-programming-guide.html#resilient-distributed-datasets-rdds">Spark编程指南</a> ）。DStream中的每个RDD都包含来自特定间隔的数据，如下图所示。</p>

<p style="text-align:center">
  <img src="img/streaming-dstream.png" title="Spark Streaming数据流" alt="火花流" width="70%">
</p>

<p>在DStream上执行的任何操作都转换为对基础RDD的操作。例如，在<a href="#a-quick-example">前面</a>的将行流转换为单词的<a href="#a-quick-example">示例</a>中， <code>flatMap</code>操作将应用于<code>lines</code> DStream生成的RDD <code>words</code> DStream。如下图所示。</p>

<p style="text-align:center">
  <img src="img/streaming-dstream-ops.png" title="Spark Streaming数据流" alt="火花流" width="70%">
</p>

<p>这些基础的RDD转换由Spark引擎计算。DStream操作隐藏了大多数这些细节，并为开发人员提供了更高级别的API，以方便使用。这些操作将在后面的部分中详细讨论。</p>

<hr>

<h2 id="input-dstreams-and-receivers">输入DStreams和接收器</h2>
<p>输入DStream是表示从流源接收的输入数据流的DStream。在<a href="#a-quick-example">快速示例中</a> ， <code>lines</code>是输入DStream，因为它表示从netcat服务器接收的数据流。每个输入DStream（文件流除外，本节稍后将讨论）都与一个<strong>Receiver对象</strong> （ <a href="api/scala/index.html#org.apache.spark.streaming.receiver.Receiver">Scala doc</a> ， <a href="api/java/org/apache/spark/streaming/receiver/Receiver.html">Java doc</a> ）关联，该对象从源接收数据并将其存储在Spark的内存中以进行处理。</p>

<p>Spark Streaming提供了两类内置的流媒体源。</p>

<ul>
  <li><em>基本来源</em> ：可直接在StreamingContext API中获得的来源。示例：文件系统和套接字连接。</li>
  <li><em>高级资源</em> ：可以通过其他实用程序类获得诸如Kafka，Flume，Kinesis等资源。如<a href="#linking">链接</a>部分所述，它们需要针对额外的依赖项进行<a href="#linking">链接</a> 。</li>
</ul>

<p>我们将在本节后面的每个类别中讨论一些资源。</p>

<p>请注意，如果要在流应用程序中并行接收多个数据流，则可以创建多个输入DStream（在“ <a href="#level-of-parallelism-in-data-receiving">性能调整”</a>部分中进一步讨论）。这将创建多个接收器，这些接收器将同时接收多个数据流。但是请注意，Spark工作程序/执行程序是一项长期运行的任务，因此它占用了分配给Spark Streaming应用程序的核心之一。因此，重要的是要记住，必须为Spark Streaming应用程序分配足够的核心（或线程，如果在本地运行），以处理接收到的数据以及运行接收器。</p>

<h5 class="no_toc" id="points-to-remember-1">要记住的要点</h5>

<ul>
  <li>
    <p>在本地运行Spark Streaming程序时，请勿使用“ local”或“ local [1]”作为主URL。这两种方式均意味着仅一个线程将用于本地运行任务。如果您使用基于接收方的输入DStream（例如套接字，Kafka，Flume等），则将使用单个线程来运行接收方，而不会留下任何线程来处理接收到的数据。因此，在本地运行时，请始终使用“ local [ <em>n</em> ]”作为主URL，其中<em>n</em> >要运行的接收器数（有关如何设置主服务器的信息，请参见<a href="configuration.html#spark-properties">Spark属性</a> ）。</p>
  </li>
  <li>
    <p>为了将逻辑扩展到在集群上运行，分配给Spark Streaming应用程序的内核数必须大于接收器数。否则，系统将接收数据，但无法处理它。</p>
  </li>
</ul>

<h3 class="no_toc" id="basic-sources">基本资料</h3>

<p>我们已经看过<code>ssc.socketTextStream(...)</code>在<a href="#a-quick-example">快速示例中</a> ，该示例根据通过TCP套接字连接接收的文本数据创建DStream。除了套接字外，StreamingContext API还提供了从文件作为输入源创建DStream的方法。</p>

<h4 class="no_toc" id="file-streams">文件流</h4>

<p>要从与HDFS API兼容的任何文件系统（即HDFS，S3，NFS等）上的文件中读取数据，可以通过以下方式创建DStream： <code>StreamingContext.fileStream[KeyClass, ValueClass, InputFormatClass]</code> 。</p>

<p>文件流不需要运行接收器，因此不需要分配任何内核来接收文件数据。</p>

<p>对于简单的文本文件，最简单的方法是<code>StreamingContext.textFileStream(dataDirectory)</code> 。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">streamingContext</span><span class="o">.</span><span class="n">fileStream</span><span class="o">[</span><span class="kt">KeyClass</span>, <span class="kt">ValueClass</span>, <span class="kt">InputFormatClass</span><span class="o">](</span><span class="n">dataDirectory</span><span class="o">)</span></code></pre></figure>

    <p>对于文本文件</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">streamingContext</span><span class="o">.</span><span class="n">textFileStream</span><span class="o">(</span><span class="n">dataDirectory</span><span class="o">)</span></code></pre></figure>

  </div>

<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">streamingContext</span><span class="o">.</span><span class="na">fileStream</span><span class="o">&lt;</span><span class="n">KeyClass</span><span class="o">,</span> <span class="n">ValueClass</span><span class="o">,</span> <span class="n">InputFormatClass</span><span class="o">&gt;(</span><span class="n">dataDirectory</span><span class="o">);</span></code></pre></figure>

    <p>对于文本文件</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">streamingContext</span><span class="o">.</span><span class="na">textFileStream</span><span class="o">(</span><span class="n">dataDirectory</span><span class="o">);</span></code></pre></figure>

  </div>

<div data-lang="python">
    <p><code>fileStream</code>在Python API中不可用；只要<code>textFileStream</code>可用。</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">streamingContext</span><span class="o">.</span><span class="n">textFileStream</span><span class="p">(</span><span class="n">dataDirectory</span><span class="p">)</span></code></pre></figure>

  </div>

</div>

<h5 class="no_toc" id="how-directories-are-monitored">如何监控目录</h5>

<p>Spark Streaming将监视目录<code>dataDirectory</code>并处理在该目录中创建的所有文件。</p>

<ul>
  <li>可以监视一个简单的目录，例如<code>"hdfs://namenode:8040/logs/"</code> 。发现后，将直接处理该路径下的所有文件。</li>
  <li>可以提供<a href="http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_13_02">POSIX球形图案</a> ，例如<code>"hdfs://namenode:8040/logs/2017/*"</code> 。在此，DStream将包含与模式匹配的目录中的所有文件。也就是说：这是目录的模式，而不是目录中的文件。</li>
  <li>所有文件必须使用相同的数据格式。</li>
  <li>根据文件的修改时间而非创建时间，将其视为时间段的一部分。</li>
  <li>处理后，在当前窗口中对文件的更改将不会导致重新读取该文件。也就是说： <em>忽略更新</em> 。</li>
  <li>目录下的文件越多，扫描更改所需的时间就越长-即使未修改任何文件。</li>
  <li>如果使用通配符来标识目录，例如<code>"hdfs://namenode:8040/logs/2016-*"</code> ，重命名整个目录以匹配路径会将目录添加到受监视目录列表中。流中仅包含目录中修改时间在当前窗口内的文件。</li>
  <li>呼唤<a href="https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html#setTimes-org.apache.hadoop.fs.Path-long-long-"><code>FileSystem.setTimes()</code></a>修复时间戳是一种在以后的窗口中拾取文件的方法，即使其内容没有更改。</li>
</ul>

<h5 class="no_toc" id="using-object-stores-as-a-source-of-data">使用对象存储作为数据源</h5>

<p>HDFS之类的“完整”文件系统往往会在创建输出流后立即对其文件设置修改时间。当打开文件时，即使在数据完全写入之前，它也可能包含在<code>DStream</code> -之后，将忽略对同一窗口中文件的更新。也就是说：更改可能会丢失，流中会省略数据。</p>

<p>为了确保在窗口中能够接收到更改，请将文件写入一个不受监视的目录，然后在关闭输出流后立即将其重命名为目标目录。如果重命名的文件在创建窗口期间显示在扫描的目标目录中，则将提取新数据。</p>

<p>相反，由于实际复制了数据，因此诸如Amazon S3和Azure存储之类的对象存储通常具有较慢的重命名操作。此外，重命名的对象可能具有<code>rename()</code>操作作为其修改时间，因此可能不被视为原始创建时间所暗示的窗口的一部分。</p>

<p>需要对目标对象存储进行仔细的测试，以验证存储的时间戳行为与Spark Streaming期望的一致。直接写入目标目录可能是通过所选对象存储流传输数据的适当策略。</p>

<p>有关此主题的更多详细信息，请参阅<a href="https://hadoop.apache.org/docs/stable2/hadoop-project-dist/hadoop-common/filesystem/introduction.html">Hadoop Filesystem Specification</a> 。</p>

<h4 class="no_toc" id="streams-based-on-custom-receivers">基于自定义接收器的流</h4>

<p>可以使用通过自定义接收器接收的数据流来创建DStream。有关更多详细信息，请参见《 <a href="streaming-custom-receivers.html">定制接收器指南》</a> 。</p>

<h4 class="no_toc" id="queue-of-rdds-as-a-stream">RDD排队作为流</h4>

<p>为了使用测试数据测试Spark Streaming应用程序，还可以使用以下命令基于RDD队列创建DStream： <code>streamingContext.queueStream(queueOfRDDs)</code> 。推送到队列中的每个RDD将被视为DStream中的一批数据，并像流一样进行处理。</p>

<p>有关套接字和文件流的更多详细信息，请参见<a href="api/scala/index.html#org.apache.spark.streaming.StreamingContext">StreamingContext</a> for Scala， <a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html">JavaStreamingContext</a> for Java和<a href="api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext">StreamingContext</a> for Python中相关功能的API文档。</p>

<h3 class="no_toc" id="advanced-sources">进阶资源</h3>

<p><span class="badge" style="background-color:grey">Python API</span>从Spark 2.4.4开始， <span class="badge" style="background-color:grey">Python API</span>中提供了上述来源中的Kafka，Kinesis和Flume。</p>

<p>这类来源需要与外部非Spark库进行接口，其中一些库具有复杂的依存关系（例如，Kafka和Flume）。因此，为了最大程度地减少与依赖项版本冲突有关的问题，从这些源创建DStream的功能已移至单独的库，可以在必要时显式<a href="#linking">链接</a>到这些库。</p>

<p>请注意，Spark Shell中没有这些高级源，因此无法在Shell中测试基于这些高级源的应用程序。如果您真的想在Spark shell中使用它们，则必须下载相应的Maven工件的JAR及其依赖项，并将其添加到类路径中。</p>

<p>其中一些高级资源如下。</p>

<ul>
  <li>
    <p><strong>Kafka：</strong> Spark Streaming 2.4.4与0.8.2.1或更高版本的Kafka代理兼容。有关更多详细信息，请参见《 <a href="streaming-kafka-integration.html">Kafka集成指南》</a> 。</p>
  </li>
  <li>
    <p><strong>Flume：</strong> Spark Streaming 2.4.4与Flume 1.6.0兼容。有关更多详细信息，请参见《 <a href="streaming-flume-integration.html">Flume集成指南》</a> 。</p>
  </li>
  <li>
    <p><strong>Kinesis：</strong> Spark Streaming 2.4.4与Kinesis Client Library 1.2.1兼容。有关更多详细信息，请参见《 <a href="streaming-kinesis-integration.html">Kinesis集成指南》</a> 。</p>
  </li>
</ul>

<h3 class="no_toc" id="custom-sources">自订来源</h3>

<p><span class="badge" style="background-color:grey">Python API Python</span>尚不支持此功能。</p>

<p>输入DStreams也可以从自定义数据源中创建。您所需要做的就是实现一个用户定义的<strong>接收器</strong> （请参阅下一节以了解其含义），该接收器可以接收来自自定义源的数据并将其推送到Spark中。有关详细信息，请参见《 <a href="streaming-custom-receivers.html">定制接收器指南</a> 》。</p>

<h3 class="no_toc" id="receiver-reliability">接收器可靠性</h3>

<p>根据数据<em>可靠性，</em>可以有两种数据源。源（例如Kafka和Flume）允许确认传输的数据。如果从这些<em>可靠</em>来源接收数据的系统正确地确认了接收到的数据，则可以确保不会由于任何类型的故障而丢失任何数据。这导致两种接收器：</p>

<ol>
  <li><em>可靠的接收器</em> - <em>可靠的接收器</em>在接收到数据并通过复制将其存储在Spark中后，可以正确地将确认发送到可靠的源。</li>
  <li><em>不可靠的接收器</em> -一个<em>不可靠的接收器</em> <em>不</em>发送确认的资源等。它可以用于不支持确认的来源，甚至可以用于不希望或不需要进入确认复杂性的可靠来源。</li>
</ol>

<p>《 <a href="streaming-custom-receivers.html">定制接收器指南》</a>中讨论了如何编写可靠的接收器的详细信息。</p>

<hr>

<h2 id="transformations-on-dstreams">DStreams上的转换</h2>
<p>与RDD相似，转换允许修改来自输入DStream的数据。DStream支持普通Spark RDD上可用的许多转换。一些常见的方法如下。</p>

<table class="table">
<tbody><tr><th style="width:25%">转型</th><th>含义</th></tr>
<tr>
  <td> <b>地图</b> （ <i>func</i> ）</td>
  <td>通过将源DStream的每个元素传递给函数<i>func来</i>返回新的DStream。</td>
</tr>
<tr>
  <td> <b>flatMap</b> （ <i>func</i> ）</td>
  <td>与map相似，但是每个输入项可以映射到0个或多个输出项。</td>
</tr>
<tr>
  <td> <b>过滤器</b> （ <i>func</i> ）</td>
  <td>通过仅选择<i>func</i>返回true的源DStream的记录来返回新的DStream。</td>
</tr>
<tr>
  <td> <b>重新分区</b> （ <i>numPartitions</i> ）</td>
  <td>通过创建更多或更少的分区来更改此DStream中的并行度。</td>
</tr>
<tr>
  <td> <b>联合</b> （ <i>otherStream</i> ）</td>
  <td>返回一个新的DStream，其中包含源DStream和<i>otherDStream</i>中的元素的并<i>集</i> 。</td>
</tr>
<tr>
  <td> <b>数</b> （）</td>
  <td>通过计算源DStream的每个RDD中的元素数，返回一个新的单元素RDD DStream。</td>
</tr>
<tr>
  <td> <b>减少</b> （ <i>func</i> ）</td>
  <td>通过使用函数<i>func</i> （带有两个参数并返回一个）来聚合源DStream的每个RDD中的元素，从而返回一个单元素RDD的新DStream。该函数应具有关联性和可交换性，以便可以并行计算。</td>
</tr>
<tr>
  <td> <b>countByValue</b> （）</td>
  <td>在类型为K的元素的DStream上调用时，返回一个新的（K，Long）对的DStream，其中每个键的值是其在源DStream的每个RDD中的频率。</td>
</tr>
<tr>
  <td> <b>reduceByKey</b> （ <i>func</i> ，[ <i>numTasks</i> ]）</td>
  <td>在（K，V）对的DStream上调用时，返回一个新的（K，V）对的DStream，其中使用给定的reduce函数汇总每个键的值。<b>注意：</b>默认情况下，这使用Spark的并行任务的默认数量（本地模式为2，而在集群模式下，数量由config属性确定<code>spark.default.parallelism</code> ）进行分组。您可以传递可选<code>numTasks</code>参数来设置不同数量的任务。</td>
</tr>
<tr>
  <td> <b>加入</b> （ <i>otherStream</i> ，[ <i>numTasks</i> ]）</td>
  <td>在（K，V）和（K，W）对的两个DStream上调用时，返回一个新的（K，（V，W））对的DStream，每个键都有所有元素对。</td>
</tr>
<tr>
  <td> <b>协同组</b> <i><i>（otherStream，[numTasks]）</i></i></td>
  <td>在（K，V）和（K，W）对的DStream上调用时，返回一个新的（K，Seq [V]，Seq [W]）元组的DStream。</td>
</tr>
<tr>
  <td> <b>转换</b> （ <i>func</i> ）</td>
  <td>通过对源DStream的每个RDD应用RDD-to-RDD函数来返回新的DStream。这可用于在DStream上执行任意RDD操作。</td>
</tr>
<tr>
  <td> <b>updateStateByKey</b> （ <i>func</i> ）</td>
  <td>返回一个新的“状态” DStream，在该DStream中，通过在键的先前状态和键的新值上应用给定函数来更新每个键的状态。这可用于维护每个键的任意状态数据。</td>
</tr>
<tr><td></td><td></td></tr>
</tbody></table>

<p>其中一些转换值得详细讨论。</p>

<h4 class="no_toc" id="updatestatebykey-operation">UpdateStateByKey操作</h4>
<p>的<code>updateStateByKey</code>操作使您可以保持任意状态，同时不断用新信息更新它。要使用此功能，您将必须执行两个步骤。</p>

<ol>
  <li>定义状态-状态可以是任意数据类型。</li>
  <li>定义状态更新功能-使用功能指定如何使用输入流中的先前状态和新值来更新状态。</li>
</ol>

<p>在每个批次中，Spark都会对所有现有密钥应用状态更新功能，无论它们是否在批次中具有新数据。如果更新功能返回<code>None</code>那么键值对将被消除。</p>

<p>让我们用一个例子来说明。假设您要保持在文本数据流中看到的每个单词的连续计数。此处，运行计数是状态，它是整数。我们将更新函数定义为：</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">def</span> <span class="n">updateFunction</span><span class="o">(</span><span class="n">newValues</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">Int</span><span class="o">],</span> <span class="n">runningCount</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">newCount</span> <span class="k">=</span> <span class="o">...</span>  <span class="c1">// add the new values with the previous running count to get the new count</span>
    <span class="nc">Some</span><span class="o">(</span><span class="n">newCount</span><span class="o">)</span>
<span class="o">}</span></code></pre></figure>

    <p>这适用于包含单词的DStream（例如， <code>pairs</code> DStream包含<code>(word, 1)</code>对（在<a href="#a-quick-example">前面的示例中</a> ）。</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">runningCounts</span> <span class="k">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">updateStateByKey</span><span class="o">[</span><span class="kt">Int</span><span class="o">](</span><span class="n">updateFunction</span> <span class="k">_</span><span class="o">)</span></code></pre></figure>

    <p>将为每个单词调用更新功能，其中<code>newValues</code>具有1的序列（从<code>(word, 1)</code>对）和<code>runningCount</code>具有先前的计数。</p>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">Function2</span><span class="o">&lt;</span><span class="n">List</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;,</span> <span class="n">Optional</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;,</span> <span class="n">Optional</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;&gt;</span> <span class="n">updateFunction</span> <span class="o">=</span>
  <span class="o">(</span><span class="n">values</span><span class="o">,</span> <span class="n">state</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="n">Integer</span> <span class="n">newSum</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1">// add the new values with the previous running count to get the new count</span>
    <span class="k">return</span> <span class="n">Optional</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="n">newSum</span><span class="o">);</span>
  <span class="o">};</span></code></pre></figure>

    <p>这适用于包含单词的DStream（例如， <code>pairs</code> DStream包含<code>(word, 1)</code>在<a href="#a-quick-example">快速示例中</a> ）。</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">runningCounts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="na">updateStateByKey</span><span class="o">(</span><span class="n">updateFunction</span><span class="o">);</span></code></pre></figure>

    <p>将为每个单词调用更新功能，其中<code>newValues</code>具有1的序列（从<code>(word, 1)</code>对）和<code>runningCount</code>具有先前的计数。有关完整的Java代码，请查看示例<a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java">JavaStatefulNetworkWordCount.java</a> 。</p>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">def</span> <span class="nf">updateFunction</span><span class="p">(</span><span class="n">newValues</span><span class="p">,</span> <span class="n">runningCount</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">runningCount</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">runningCount</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">newValues</span><span class="p">,</span> <span class="n">runningCount</span><span class="p">)</span>  <span class="c1"># add the new values with the previous running count to get the new count</span></code></pre></figure>

    <p>这适用于包含单词的DStream（例如， <code>pairs</code> DStream包含<code>(word, 1)</code>对（在<a href="#a-quick-example">前面的示例中</a> ）。</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">runningCounts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">updateStateByKey</span><span class="p">(</span><span class="n">updateFunction</span><span class="p">)</span></code></pre></figure>

    <p>将为每个单词调用更新功能，其中<code>newValues</code>具有1的序列（从<code>(word, 1)</code>对）和<code>runningCount</code>具有先前的计数。有关完整的Python代码，请查看示例<a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/python/streaming/stateful_network_wordcount.py">stateful_network_wordcount.py</a> 。</p>

  </div>
</div>

<p>注意使用<code>updateStateByKey</code>需要配置检查点目录，这将在<a href="#checkpointing">检查点</a>部分中详细讨论。</p>

<h4 class="no_toc" id="transform-operation">转型运营</h4>
<p>的<code>transform</code>操作（及其类似的变化<code>transformWith</code> ）允许将任意RDD-to-RDD函数应用于DStream。它可用于应用DStream API中未公开的任何RDD操作。例如，将数据流中的每个批次与另一个数据集连接在一起的功能未直接在DStream API中公开。但是，您可以轻松使用<code>transform</code>去做这个。这实现了非常强大的可能性。例如，可以通过将输入数据流与预先计算的垃圾邮件信息（也可能由Spark生成）结合在一起，然后基于该信息进行过滤来进行实时数据清除。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">spamInfoRDD</span> <span class="k">=</span> <span class="n">ssc</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">newAPIHadoopRDD</span><span class="o">(...)</span> <span class="c1">// RDD containing spam information</span>

<span class="k">val</span> <span class="n">cleanedDStream</span> <span class="k">=</span> <span class="n">wordCounts</span><span class="o">.</span><span class="n">transform</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>
  <span class="n">rdd</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">spamInfoRDD</span><span class="o">).</span><span class="n">filter</span><span class="o">(...)</span> <span class="c1">// join data stream with spam information to do data cleaning</span>
  <span class="o">...</span>
<span class="o">}</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.streaming.api.java.*</span><span class="o">;</span>
<span class="c1">// RDD containing spam information</span>
<span class="n">JavaPairRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Double</span><span class="o">&gt;</span> <span class="n">spamInfoRDD</span> <span class="o">=</span> <span class="n">jssc</span><span class="o">.</span><span class="na">sparkContext</span><span class="o">().</span><span class="na">newAPIHadoopRDD</span><span class="o">(...);</span>

<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">cleanedDStream</span> <span class="o">=</span> <span class="n">wordCounts</span><span class="o">.</span><span class="na">transform</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="n">rdd</span><span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">spamInfoRDD</span><span class="o">).</span><span class="na">filter</span><span class="o">(...);</span> <span class="c1">// join data stream with spam information to do data cleaning</span>
  <span class="o">...</span>
<span class="o">});</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">spamInfoRDD</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">pickleFile</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># RDD containing spam information</span>

<span class="c1"># join data stream with spam information to do data cleaning</span>
<span class="n">cleanedDStream</span> <span class="o">=</span> <span class="n">wordCounts</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="k">lambda</span> <span class="n">rdd</span><span class="p">:</span> <span class="n">rdd</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">spamInfoRDD</span><span class="p">)</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="o">...</span><span class="p">))</span></code></pre></figure>

  </div>
</div>

<p>请注意，在每个批处理间隔中都会调用提供的函数。这使您可以执行随时间变化的RDD操作，即可以在批之间更改RDD操作，分区数，广播变量等。</p>

<h4 class="no_toc" id="window-operations">窗口操作</h4>
<p>Spark Streaming还提供了<em>窗口计算</em> ，可让您在数据的滑动窗口上应用转换。下图说明了此滑动窗口。</p>

<p style="text-align:center">
  <img src="img/streaming-dstream-window.png" title="Spark Streaming数据流" alt="火花流" width="60%">
</p>

<p>如该图所示，每当通过源DSTREAM窗口<em>滑动</em> ，落入窗口内的源RDDS被组合及操作以产生RDDS的窗DSTREAM。在这种特定情况下，该操作将应用于数据的最后3个时间单位，并以2个时间单位滑动。这表明任何窗口操作都需要指定两个参数。</p>

<ul>
  <li><i>窗口长度</i> - <i>窗口</i>的持续时间（图中3）。</li>
  <li><i>滑动间隔</i> -进行窗口操作的间隔（图中为2）。</li>
</ul>

<p>这两个参数必须是源DStream的批处理间隔的倍数（图中为1）。</p>

<p>让我们用一个例子来说明窗口操作。假设您想扩展<a href="#a-quick-example">前面的示例</a> ，方法是每10秒在数据的最后30秒生成一次字数统计。为此，我们必须应用<code>reduceByKey</code>操作上<code>pairs</code> DStream of <code>(word, 1)</code>在最近30秒的数据中配对。使用操作完成<code>reduceByKeyAndWindow</code> 。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// Reduce last 30 seconds of data, every 10 seconds</span>
<span class="k">val</span> <span class="n">windowedWordCounts</span> <span class="k">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKeyAndWindow</span><span class="o">((</span><span class="n">a</span><span class="k">:</span><span class="kt">Int</span><span class="o">,</span><span class="n">b</span><span class="k">:</span><span class="kt">Int</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">),</span> <span class="nc">Seconds</span><span class="o">(</span><span class="mi">30</span><span class="o">),</span> <span class="nc">Seconds</span><span class="o">(</span><span class="mi">10</span><span class="o">))</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// Reduce last 30 seconds of data, every 10 seconds</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">windowedWordCounts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="na">reduceByKeyAndWindow</span><span class="o">((</span><span class="n">i1</span><span class="o">,</span> <span class="n">i2</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">i1</span> <span class="o">+</span> <span class="n">i2</span><span class="o">,</span> <span class="n">Durations</span><span class="o">.</span><span class="na">seconds</span><span class="o">(</span><span class="mi">30</span><span class="o">),</span> <span class="n">Durations</span><span class="o">.</span><span class="na">seconds</span><span class="o">(</span><span class="mi">10</span><span class="o">));</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># Reduce last 30 seconds of data, every 10 seconds</span>
<span class="n">windowedWordCounts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKeyAndWindow</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span></code></pre></figure>

  </div>
</div>

<p>一些常见的窗口操作如下。所有这些操作都采用上述两个参数<i>-windowLength</i>和<i>slideInterval</i> 。</p>

<table class="table">
<tbody><tr><th style="width:25%">转型</th><th>含义</th></tr>
<tr>
  <td> <b>窗口</b> （ <i>windowLength</i> ， <i>slideInterval</i> ）</td>
  <td>返回基于源DStream的窗口批处理计算的新DStream。
  </td>
</tr>
<tr>
  <td> <b>countByWindow</b> （ <i>windowLength</i> ， <i>slideInterval</i> ）</td>
  <td>返回流中元素的滑动窗口计数。
  </td>
</tr>
<tr>
  <td> <b>reduceByWindow</b> （ <i>func</i> ， <i>windowLength</i> ， <i>slideInterval</i> ）</td>
  <td>返回一个新的单元素流，该流是通过使用<i>func</i>在滑动间隔内聚合流中的元素而创建的。该函数应该是关联的和可交换的，以便可以并行正确地计算它。
  </td>
</tr>
<tr>
  <td> <b>reduceByKeyAndWindow</b> （ <i>func</i> ， <i>windowLength</i> ， <i>slideInterval</i> ，[ <i>numTasks</i> ]）</td>
  <td>在（K，V）对的DStream上调用时，返回新的（K，V）对的DStream，其中使用给定的reduce函数<i>func</i>在滑动窗口中的批处理上聚合每个键的值。<b>注意：</b>默认情况下，这使用Spark的并行任务的默认数量（本地模式为2，而在集群模式下，数量由config属性确定<code>spark.default.parallelism</code> ）进行分组。您可以传递可选<code>numTasks</code>参数来设置不同数量的任务。
  </td>
</tr>
<tr>
  <td> <b>reduceByKeyAndWindow</b> （ <i>func</i> ， <i>invFunc</i> ， <i>windowLength</i> ， <i>slideInterval</i> ，[ <i>numTasks</i> ]）</td>
  <td>
      <p>上面的更有效的版本<code>reduceByKeyAndWindow()</code>其中，使用前一个窗口的减少值递增计算每个窗口的减少值。这是通过减少进入滑动窗口的新数据并“逆向减少”离开窗口的旧数据来完成的。一个示例是在窗口滑动时“增加”和“减少”键的计数。但是，它仅适用于“可逆归约函数”，即具有相应的“逆归约”函数（作为参数<i>invFunc</i> ）的归约函数。像<code>reduceByKeyAndWindow</code> ，reduce任务的数量可以通过可选参数配置。请注意，必须启用<a href="#checkpointing">检查点</a>才能使用此操作。</p>
    </td>
</tr>
<tr>
  <td> <b>countByValueAndWindow</b> （ <i>windowLength</i> ， <i>slideInterval</i> ，[ <i>numTasks</i> ]）</td>
  <td>在（K，V）对的DStream上调用时，返回新的（K，Long）对的DStream，其中每个键的值是其在滑动窗口内的频率。像<code>reduceByKeyAndWindow</code> ，reduce任务的数量可以通过可选参数配置。
</td>
</tr>
<tr><td></td><td></td></tr>
</tbody></table>

<h4 class="no_toc" id="join-operations">加盟运营</h4>
<p>最后，值得一提的是，您可以轻松地在Spark Streaming中执行各种类型的联接。</p>

<h5 class="no_toc" id="stream-stream-joins">流流连接</h5>
<p>流可以很容易地与其他流合并。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">stream1</span><span class="k">:</span> <span class="kt">DStream</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">stream2</span><span class="k">:</span> <span class="kt">DStream</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">joinedStream</span> <span class="k">=</span> <span class="n">stream1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">stream2</span><span class="o">)</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">stream1</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">stream2</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">joinedStream</span> <span class="o">=</span> <span class="n">stream1</span><span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">stream2</span><span class="o">);</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">stream1</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">stream2</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">joinedStream</span> <span class="o">=</span> <span class="n">stream1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">stream2</span><span class="p">)</span></code></pre></figure>

  </div>
</div>
<p>在此，在每个批次间隔中， <code>stream1</code>将与由<code>stream2</code> 。你也可以<code>leftOuterJoin</code> ， <code>rightOuterJoin</code> ， <code>fullOuterJoin</code> 。此外，在流的窗口上进行连接通常非常有用。这也很容易。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">windowedStream1</span> <span class="k">=</span> <span class="n">stream1</span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Seconds</span><span class="o">(</span><span class="mi">20</span><span class="o">))</span>
<span class="k">val</span> <span class="n">windowedStream2</span> <span class="k">=</span> <span class="n">stream2</span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Minutes</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
<span class="k">val</span> <span class="n">joinedStream</span> <span class="k">=</span> <span class="n">windowedStream1</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">windowedStream2</span><span class="o">)</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">windowedStream1</span> <span class="o">=</span> <span class="n">stream1</span><span class="o">.</span><span class="na">window</span><span class="o">(</span><span class="n">Durations</span><span class="o">.</span><span class="na">seconds</span><span class="o">(</span><span class="mi">20</span><span class="o">));</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">windowedStream2</span> <span class="o">=</span> <span class="n">stream2</span><span class="o">.</span><span class="na">window</span><span class="o">(</span><span class="n">Durations</span><span class="o">.</span><span class="na">minutes</span><span class="o">(</span><span class="mi">1</span><span class="o">));</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">joinedStream</span> <span class="o">=</span> <span class="n">windowedStream1</span><span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">windowedStream2</span><span class="o">);</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">windowedStream1</span> <span class="o">=</span> <span class="n">stream1</span><span class="o">.</span><span class="n">window</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">windowedStream2</span> <span class="o">=</span> <span class="n">stream2</span><span class="o">.</span><span class="n">window</span><span class="p">(</span><span class="mi">60</span><span class="p">)</span>
<span class="n">joinedStream</span> <span class="o">=</span> <span class="n">windowedStream1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">windowedStream2</span><span class="p">)</span></code></pre></figure>

  </div>
</div>

<h5 class="no_toc" id="stream-dataset-joins">流数据集联接</h5>
<p>这已经在前面解释时显示了<code>DStream.transform</code>操作。这是将窗口流与数据集结合在一起的另一个示例。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">dataset</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">windowedStream</span> <span class="k">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">window</span><span class="o">(</span><span class="nc">Seconds</span><span class="o">(</span><span class="mi">20</span><span class="o">))...</span>
<span class="k">val</span> <span class="n">joinedStream</span> <span class="k">=</span> <span class="n">windowedStream</span><span class="o">.</span><span class="n">transform</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span> <span class="n">rdd</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">dataset</span><span class="o">)</span> <span class="o">}</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">JavaPairRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">dataset</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">windowedStream</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="na">window</span><span class="o">(</span><span class="n">Durations</span><span class="o">.</span><span class="na">seconds</span><span class="o">(</span><span class="mi">20</span><span class="o">));</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">joinedStream</span> <span class="o">=</span> <span class="n">windowedStream</span><span class="o">.</span><span class="na">transform</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="n">rdd</span><span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">dataset</span><span class="o">));</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># some RDD</span>
<span class="n">windowedStream</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">window</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">joinedStream</span> <span class="o">=</span> <span class="n">windowedStream</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="k">lambda</span> <span class="n">rdd</span><span class="p">:</span> <span class="n">rdd</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span></code></pre></figure>

  </div>
</div>

<p>实际上，您还可以动态更改要加入的数据集。提供给的功能<code>transform</code>在每个批次间隔进行评估，因此将使用当前数据集<code>dataset</code>参考指向。</p>

<p>API文档中提供了DStream转换的完整列表。有关Scala API，请参见<a href="api/scala/index.html#org.apache.spark.streaming.dstream.DStream">DStream</a>和<a href="api/scala/index.html#org.apache.spark.streaming.dstream.PairDStreamFunctions">PairDStreamFunctions</a> 。有关Java API，请参见<a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaDStream.html">JavaDStream</a>和<a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaPairDStream.html">JavaPairDStream</a> 。有关Python API，请参见<a href="api/python/pyspark.streaming.html#pyspark.streaming.DStream">DStream</a> 。</p>

<hr>

<h2 id="output-operations-on-dstreams">DStreams上的输出操作</h2>
<p>输出操作允许将DStream的数据推出到外部系统，例如数据库或文件系统。由于输出操作实际上允许转换后的数据被外部系统使用，因此它们会触发所有DStream转换的实际执行（类似于RDD的操作）。当前，定义了以下输出操作：</p>

<table class="table">
<tbody><tr><th style="width:30%">输出操作</th><th>含义</th></tr>
<tr>
  <td> <b>列印</b> （）</td>
  <td>在运行流应用程序的驱动程序节点上，打印DStream中每批数据的前十个元素。这对于开发和调试很有用。
  <br>
  <span class="badge" style="background-color:grey">Python API</span>在Python API中称为<b>pprint（）</b> 。</td>
</tr>
<tr>
  <td> <b>saveAsTextFiles</b> （ <i>前缀</i> ，[ <i>后缀</i> ]）</td>
  <td>将此DStream的内容另存为文本文件。每个批处理间隔的文件名都是基于<i>前缀</i>和<i>后缀</i> <i>“ prefix-TIME_IN_MS [.suffix]”生成的</i> 。</td>
</tr>
<tr>
  <td> <b>saveAsObjectFiles</b> （ <i>前缀</i> ，[ <i>后缀</i> ]）</td>
  <td>将此DStream的内容另存为<code>SequenceFiles</code>序列化的Java对象。每个批处理间隔的文件名都是基于<i>前缀</i>和<i>后缀</i> <i>“ prefix-TIME_IN_MS [.suffix]”生成的</i> 。
  <br>
  <span class="badge" style="background-color:grey">Python API</span>这在Python API中不可用。</td>
</tr>
<tr>
  <td> <b>saveAsHadoopFiles</b> （ <i>前缀</i> ，[ <i>后缀</i> ]）</td>
  <td>将此DStream的内容另存为Hadoop文件。每个批处理间隔的文件名都是基于<i>前缀</i>和<i>后缀</i> <i>“ prefix-TIME_IN_MS [.suffix]”生成的</i> 。
  <br>
  <span class="badge" style="background-color:grey">Python API</span>这在Python API中不可用。</td>
</tr>
<tr>
  <td> <b>foreachRDD</b> （ <i>func</i> ）</td>
  <td>最通用的输出运算符，将函数<i>func</i>应用于从流生成的每个RDD。此功能应将每个RDD中的数据推送到外部系统，例如将RDD保存到文件或通过网络将其写入数据库。请注意，函数<i>func</i>在运行流应用程序的驱动程序进程中执行，并且通常在其中具有RDD操作，这将强制计算流RDD。</td>
</tr>
<tr><td></td><td></td></tr>
</tbody></table>

<h3 class="no_toc" id="design-patterns-for-using-foreachrdd">使用foreachRDD的设计模式</h3>
<p><code>dstream.foreachRDD</code>是一个强大的原语，可以将数据发送到外部系统。但是，重要的是要了解如何正确有效地使用此原语。应避免的一些常见错误如下。</p>

<p>通常，将数据写入外部系统需要创建一个连接对象（例如，到远程服务器的TCP连接），并使用该对象将数据发送到远程系统。为此，开发人员可能会无意间尝试在Spark驱动程序中创建连接对象，然后尝试在Spark worker中使用该对象以将记录保存在RDD中。例如（在Scala中），</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>
  <span class="k">val</span> <span class="n">connection</span> <span class="k">=</span> <span class="n">createNewConnection</span><span class="o">()</span>  <span class="c1">// executed at the driver</span>
  <span class="n">rdd</span><span class="o">.</span><span class="n">foreach</span> <span class="o">{</span> <span class="n">record</span> <span class="k">=&gt;</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="o">(</span><span class="n">record</span><span class="o">)</span> <span class="c1">// executed at the worker</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">dstream</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="n">Connection</span> <span class="n">connection</span> <span class="o">=</span> <span class="n">createNewConnection</span><span class="o">();</span> <span class="c1">// executed at the driver</span>
  <span class="n">rdd</span><span class="o">.</span><span class="na">foreach</span><span class="o">(</span><span class="n">record</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="n">connection</span><span class="o">.</span><span class="na">send</span><span class="o">(</span><span class="n">record</span><span class="o">);</span> <span class="c1">// executed at the worker</span>
  <span class="o">});</span>
<span class="o">});</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">def</span> <span class="nf">sendRecord</span><span class="p">(</span><span class="n">rdd</span><span class="p">):</span>
    <span class="n">connection</span> <span class="o">=</span> <span class="n">createNewConnection</span><span class="p">()</span>  <span class="c1"># executed at the driver</span>
    <span class="n">rdd</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="k">lambda</span> <span class="n">record</span><span class="p">:</span> <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">record</span><span class="p">))</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span><span class="p">(</span><span class="n">sendRecord</span><span class="p">)</span></code></pre></figure>

  </div>
</div>

<p>这是不正确的，因为这要求将连接对象序列化并从驱动程序发送给工作程序。这样的连接对象很少能在机器之间转移。此错误可能表现为序列化错误（连接对象不可序列化），初始化错误（连接对象需要在工作程序中初始化）等。正确的解决方案是在工作程序中创建连接对象。</p>

<p>但是，这可能会导致另一个常见错误-为每个记录创建一个新的连接。例如，</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>
  <span class="n">rdd</span><span class="o">.</span><span class="n">foreach</span> <span class="o">{</span> <span class="n">record</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">connection</span> <span class="k">=</span> <span class="n">createNewConnection</span><span class="o">()</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="o">(</span><span class="n">record</span><span class="o">)</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">close</span><span class="o">()</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">dstream</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="n">rdd</span><span class="o">.</span><span class="na">foreach</span><span class="o">(</span><span class="n">record</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="n">Connection</span> <span class="n">connection</span> <span class="o">=</span> <span class="n">createNewConnection</span><span class="o">();</span>
    <span class="n">connection</span><span class="o">.</span><span class="na">send</span><span class="o">(</span><span class="n">record</span><span class="o">);</span>
    <span class="n">connection</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
  <span class="o">});</span>
<span class="o">});</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">def</span> <span class="nf">sendRecord</span><span class="p">(</span><span class="n">record</span><span class="p">):</span>
    <span class="n">connection</span> <span class="o">=</span> <span class="n">createNewConnection</span><span class="p">()</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span><span class="p">(</span><span class="k">lambda</span> <span class="n">rdd</span><span class="p">:</span> <span class="n">rdd</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">sendRecord</span><span class="p">))</span></code></pre></figure>

  </div>
</div>

<p>通常，创建连接对象会浪费时间和资源。因此，为每个记录创建和销毁连接对象会导致不必要的高开销，并且会大大降低系统的整体吞吐量。更好的解决方案是使用<code>rdd.foreachPartition</code> -创建一个连接对象，并使用该连接在RDD分区中发送所有记录。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>
  <span class="n">rdd</span><span class="o">.</span><span class="n">foreachPartition</span> <span class="o">{</span> <span class="n">partitionOfRecords</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">connection</span> <span class="k">=</span> <span class="n">createNewConnection</span><span class="o">()</span>
    <span class="n">partitionOfRecords</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">record</span> <span class="k">=&gt;</span> <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="o">(</span><span class="n">record</span><span class="o">))</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">close</span><span class="o">()</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">dstream</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="n">rdd</span><span class="o">.</span><span class="na">foreachPartition</span><span class="o">(</span><span class="n">partitionOfRecords</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="n">Connection</span> <span class="n">connection</span> <span class="o">=</span> <span class="n">createNewConnection</span><span class="o">();</span>
    <span class="k">while</span> <span class="o">(</span><span class="n">partitionOfRecords</span><span class="o">.</span><span class="na">hasNext</span><span class="o">())</span> <span class="o">{</span>
      <span class="n">connection</span><span class="o">.</span><span class="na">send</span><span class="o">(</span><span class="n">partitionOfRecords</span><span class="o">.</span><span class="na">next</span><span class="o">());</span>
    <span class="o">}</span>
    <span class="n">connection</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
  <span class="o">});</span>
<span class="o">});</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">def</span> <span class="nf">sendPartition</span><span class="p">(</span><span class="nb">iter</span><span class="p">):</span>
    <span class="n">connection</span> <span class="o">=</span> <span class="n">createNewConnection</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="nb">iter</span><span class="p">:</span>
        <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
    <span class="n">connection</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span><span class="p">(</span><span class="k">lambda</span> <span class="n">rdd</span><span class="p">:</span> <span class="n">rdd</span><span class="o">.</span><span class="n">foreachPartition</span><span class="p">(</span><span class="n">sendPartition</span><span class="p">))</span></code></pre></figure>

  </div>
</div>

<p>这将分摊许多记录上的连接创建开销。</p>

<p>最后，可以通过在多个RDD /批次之间重用连接对象来进一步优化。与将多个批次的RDD推送到外部系统时可以重用的连接对象相比，它可以维护一个静态的连接对象池，从而进一步减少了开销。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>
  <span class="n">rdd</span><span class="o">.</span><span class="n">foreachPartition</span> <span class="o">{</span> <span class="n">partitionOfRecords</span> <span class="k">=&gt;</span>
    <span class="c1">// ConnectionPool is a static, lazily initialized pool of connections</span>
    <span class="k">val</span> <span class="n">connection</span> <span class="k">=</span> <span class="nc">ConnectionPool</span><span class="o">.</span><span class="n">getConnection</span><span class="o">()</span>
    <span class="n">partitionOfRecords</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">record</span> <span class="k">=&gt;</span> <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="o">(</span><span class="n">record</span><span class="o">))</span>
    <span class="nc">ConnectionPool</span><span class="o">.</span><span class="n">returnConnection</span><span class="o">(</span><span class="n">connection</span><span class="o">)</span>  <span class="c1">// return to the pool for future reuse</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

  </div>

<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">dstream</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="n">rdd</span><span class="o">.</span><span class="na">foreachPartition</span><span class="o">(</span><span class="n">partitionOfRecords</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="c1">// ConnectionPool is a static, lazily initialized pool of connections</span>
    <span class="n">Connection</span> <span class="n">connection</span> <span class="o">=</span> <span class="n">ConnectionPool</span><span class="o">.</span><span class="na">getConnection</span><span class="o">();</span>
    <span class="k">while</span> <span class="o">(</span><span class="n">partitionOfRecords</span><span class="o">.</span><span class="na">hasNext</span><span class="o">())</span> <span class="o">{</span>
      <span class="n">connection</span><span class="o">.</span><span class="na">send</span><span class="o">(</span><span class="n">partitionOfRecords</span><span class="o">.</span><span class="na">next</span><span class="o">());</span>
    <span class="o">}</span>
    <span class="n">ConnectionPool</span><span class="o">.</span><span class="na">returnConnection</span><span class="o">(</span><span class="n">connection</span><span class="o">);</span> <span class="c1">// return to the pool for future reuse</span>
  <span class="o">});</span>
<span class="o">});</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">def</span> <span class="nf">sendPartition</span><span class="p">(</span><span class="nb">iter</span><span class="p">):</span>
    <span class="c1"># ConnectionPool is a static, lazily initialized pool of connections</span>
    <span class="n">connection</span> <span class="o">=</span> <span class="n">ConnectionPool</span><span class="o">.</span><span class="n">getConnection</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="nb">iter</span><span class="p">:</span>
        <span class="n">connection</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
    <span class="c1"># return to the pool for future reuse</span>
    <span class="n">ConnectionPool</span><span class="o">.</span><span class="n">returnConnection</span><span class="p">(</span><span class="n">connection</span><span class="p">)</span>

<span class="n">dstream</span><span class="o">.</span><span class="n">foreachRDD</span><span class="p">(</span><span class="k">lambda</span> <span class="n">rdd</span><span class="p">:</span> <span class="n">rdd</span><span class="o">.</span><span class="n">foreachPartition</span><span class="p">(</span><span class="n">sendPartition</span><span class="p">))</span></code></pre></figure>

  </div>
</div>

<p>请注意，应按需延迟创建池中的连接，如果一段时间不使用，则超时。这样可以最有效地将数据发送到外部系统。</p>

<h5 class="no_toc" id="other-points-to-remember">其他要记住的要点：</h5>
<ul>
  <li>
    <p>DStream由输出操作延迟执行，就像RDD由RDD操作延迟执行一样。具体来说，DStream输出操作内部的RDD动作会强制处理接收到的数据。因此，如果您的应用程序没有任何输出操作，或者具有类似<code>dstream.foreachRDD()</code>如果它们内部没有任何RDD操作，则将不会执行任何操作。系统将仅接收数据并将其丢弃。</p>
  </li>
  <li>
    <p>默认情况下，输出操作一次执行一次。它们按照在应用程序中定义的顺序执行。</p>
  </li>
</ul>

<hr>

<h2 id="dataframe-and-sql-operations">DataFrame和SQL操作</h2>
<p>您可以轻松地对流数据使用<a href="sql-programming-guide.html">DataFrames和SQL</a>操作。您必须使用StreamingContext使用的SparkContext创建一个SparkSession。此外，必须这样做，以便可以在驱动程序故障时重新启动它。这是通过创建SparkSession的延迟实例化单例实例来完成的。在下面的示例中显示。它修改了前面的<a href="#a-quick-example">单词计数示例，</a>以使用DataFrames和SQL生成单词计数。每个RDD都转换为一个DataFrame，注册为临时表，然后使用SQL查询。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="cm">/** DataFrame operations inside your streaming program */</span>

<span class="k">val</span> <span class="n">words</span><span class="k">:</span> <span class="kt">DStream</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">...</span>

<span class="n">words</span><span class="o">.</span><span class="n">foreachRDD</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>

  <span class="c1">// Get the singleton instance of SparkSession</span>
  <span class="k">val</span> <span class="n">spark</span> <span class="k">=</span> <span class="nc">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">config</span><span class="o">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">getConf</span><span class="o">).</span><span class="n">getOrCreate</span><span class="o">()</span>
  <span class="k">import</span> <span class="nn">spark.implicits._</span>

  <span class="c1">// Convert RDD[String] to DataFrame</span>
  <span class="k">val</span> <span class="n">wordsDataFrame</span> <span class="k">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">toDF</span><span class="o">(</span><span class="s">&quot;word&quot;</span><span class="o">)</span>

  <span class="c1">// Create a temporary view</span>
  <span class="n">wordsDataFrame</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;words&quot;</span><span class="o">)</span>

  <span class="c1">// Do word count on DataFrame using SQL and print it</span>
  <span class="k">val</span> <span class="n">wordCountsDataFrame</span> <span class="k">=</span> 
    <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;select word, count(*) as total from words group by word&quot;</span><span class="o">)</span>
  <span class="n">wordCountsDataFrame</span><span class="o">.</span><span class="n">show</span><span class="o">()</span>
<span class="o">}</span></code></pre></figure>

    <p>请参阅完整的<a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala">源代码</a> 。</p>
  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="cm">/** Java Bean class for converting RDD to DataFrame */</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">JavaRow</span> <span class="kd">implements</span> <span class="n">java</span><span class="o">.</span><span class="na">io</span><span class="o">.</span><span class="na">Serializable</span> <span class="o">{</span>
  <span class="kd">private</span> <span class="n">String</span> <span class="n">word</span><span class="o">;</span>

  <span class="kd">public</span> <span class="n">String</span> <span class="nf">getWord</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">word</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">setWord</span><span class="o">(</span><span class="n">String</span> <span class="n">word</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">.</span><span class="na">word</span> <span class="o">=</span> <span class="n">word</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="o">...</span>

<span class="cm">/** DataFrame operations inside your streaming program */</span>

<span class="n">JavaDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">words</span> <span class="o">=</span> <span class="o">...</span> 

<span class="n">words</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">((</span><span class="n">rdd</span><span class="o">,</span> <span class="n">time</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="c1">// Get the singleton instance of SparkSession</span>
  <span class="n">SparkSession</span> <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="na">builder</span><span class="o">().</span><span class="na">config</span><span class="o">(</span><span class="n">rdd</span><span class="o">.</span><span class="na">sparkContext</span><span class="o">().</span><span class="na">getConf</span><span class="o">()).</span><span class="na">getOrCreate</span><span class="o">();</span>

  <span class="c1">// Convert RDD[String] to RDD[case class] to DataFrame</span>
  <span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">JavaRow</span><span class="o">&gt;</span> <span class="n">rowRDD</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="n">JavaRow</span> <span class="n">record</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaRow</span><span class="o">();</span>
    <span class="n">record</span><span class="o">.</span><span class="na">setWord</span><span class="o">(</span><span class="n">word</span><span class="o">);</span>
    <span class="k">return</span> <span class="n">record</span><span class="o">;</span>
  <span class="o">});</span>
  <span class="n">DataFrame</span> <span class="n">wordsDataFrame</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">createDataFrame</span><span class="o">(</span><span class="n">rowRDD</span><span class="o">,</span> <span class="n">JavaRow</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

  <span class="c1">// Creates a temporary view using the DataFrame</span>
  <span class="n">wordsDataFrame</span><span class="o">.</span><span class="na">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;words&quot;</span><span class="o">);</span>

  <span class="c1">// Do word count on table using SQL and print it</span>
  <span class="n">DataFrame</span> <span class="n">wordCountsDataFrame</span> <span class="o">=</span>
    <span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;select word, count(*) as total from words group by word&quot;</span><span class="o">);</span>
  <span class="n">wordCountsDataFrame</span><span class="o">.</span><span class="na">show</span><span class="o">();</span>
<span class="o">});</span></code></pre></figure>

    <p>请参阅完整的<a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java">源代码</a> 。</p>
  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># Lazily instantiated global instance of SparkSession</span>
<span class="k">def</span> <span class="nf">getSparkSessionInstance</span><span class="p">(</span><span class="n">sparkConf</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;sparkSessionSingletonInstance&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">()):</span>
        <span class="nb">globals</span><span class="p">()[</span><span class="s2">&quot;sparkSessionSingletonInstance&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">SparkSession</span> \
            <span class="o">.</span><span class="n">builder</span> \
            <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">sparkConf</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">globals</span><span class="p">()[</span><span class="s2">&quot;sparkSessionSingletonInstance&quot;</span><span class="p">]</span>

<span class="o">...</span>

<span class="c1"># DataFrame operations inside your streaming program</span>

<span class="n">words</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># DStream of strings</span>

<span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;========= </span><span class="si">%s</span><span class="s2"> =========&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">time</span><span class="p">))</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Get the singleton instance of SparkSession</span>
        <span class="n">spark</span> <span class="o">=</span> <span class="n">getSparkSessionInstance</span><span class="p">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">getConf</span><span class="p">())</span>

        <span class="c1"># Convert RDD[String] to RDD[Row] to DataFrame</span>
        <span class="n">rowRdd</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="n">word</span><span class="o">=</span><span class="n">w</span><span class="p">))</span>
        <span class="n">wordsDataFrame</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rowRdd</span><span class="p">)</span>

        <span class="c1"># Creates a temporary view using the DataFrame</span>
        <span class="n">wordsDataFrame</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;words&quot;</span><span class="p">)</span>

        <span class="c1"># Do word count on table using SQL and print it</span>
        <span class="n">wordCountsDataFrame</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;select word, count(*) as total from words group by word&quot;</span><span class="p">)</span>
        <span class="n">wordCountsDataFrame</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">pass</span>

<span class="n">words</span><span class="o">.</span><span class="n">foreachRDD</span><span class="p">(</span><span class="n">process</span><span class="p">)</span></code></pre></figure>

    <p>请参阅完整的<a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/python/streaming/sql_network_wordcount.py">源代码</a> 。</p>

  </div>
</div>

<p>您还可以在来自不同线程的流数据定义的表上运行SQL查询（即与正在运行的StreamingContext异步）。只需确保将StreamingContext设置为记住足够的流数据即可运行查询。否则，不知道任何异步SQL查询的StreamingContext将在查询完成之前删除旧的流数据。例如，如果您要查询最后一批，但是查询可能需要5分钟才能运行，请致电<code>streamingContext.remember(Minutes(5))</code> （使用Scala或其他语言的等效语言）。</p>

<p>请参阅<a href="sql-programming-guide.html">DataFrames和SQL</a>指南以了解有关DataFrames的更多信息。</p>

<hr>

<h2 id="mllib-operations">MLlib操作</h2>
<p>您还可以轻松使用<a href="ml-guide.html">MLlib</a>提供的机器学习算法。首先，存在流机器学习算法（例如， <a href="mllib-linear-methods.html#streaming-linear-regression">流线性回归</a> ， <a href="mllib-clustering.html#streaming-k-means">流KMeans</a>等），它们可以同时从流数据中学习并将模型应用于流数据。除此之外，对于更大种类的机器学习算法，您可以离线学习学习模型（即使用历史数据），然后在线将模型应用于流数据。有关更多详细信息，请参见<a href="ml-guide.html">MLlib</a>指南。</p>

<hr>

<h2 id="caching--persistence">缓存/持久化</h2>
<p>与RDD相似，DStreams还允许开发人员将流的数据持久存储在内存中。也就是说，使用<code>persist()</code> DStream上的方法将自动将该DStream的每个RDD持久存储在内存中。如果DStream中的数据将被多次计算（例如，对同一数据进行多次操作），这将很有用。对于基于窗口的操作，例如<code>reduceByWindow</code>和<code>reduceByKeyAndWindow</code>和基于状态的操作，例如<code>updateStateByKey</code> ，这确实是正确的。因此，由基于窗口的操作生成的DStream会自动保存在内存中，而无需开发人员调用<code>persist()</code> 。</p>

<p>对于通过网络接收数据的输入流（例如Kafka，Flume，套接字等），默认的持久性级别设置为将数据复制到两个节点以实现容错。</p>

<p>请注意，与RDD不同，DStream的默认持久性级别将数据序列化在内存中。<a href="#memory-tuning">性能调整</a>部分将对此进行进一步讨论。有关不同持久性级别的更多信息，请参见《 <a href="rdd-programming-guide.html#rdd-persistence">Spark编程指南》</a> 。</p>

<hr>

<h2 id="checkpointing">检查点</h2>
<p>流应用程序必须24/7全天候运行，因此必须对与应用程序逻辑无关的故障（例如系统故障，JVM崩溃等）具有弹性。为此，Spark Streaming需要为容错存储系统<em>检查</em>足够的信息，以便它可以从故障中恢复。检查点有两种类型的数据。</p>

<ul>
  <li><em>元数据检查点</em> -将定义流计算的信息保存到HDFS等容错存储中。这用于从运行流应用程序的驱动程序的节点的故障中恢复（稍后详细讨论）。元数据包括：<ul>
      <li><em>配置</em> -用于创建流应用程序的配置。</li>
      <li><em>DStream操作</em> -定义流应用程序的DStream操作集。</li>
      <li><em>不完整的批次</em> -作业排队但尚未完成的批次。</li>
    </ul>
  </li>
  <li><em>数据检查点</em> -将生成的RDD保存到可靠的存储中。在某些<em>状态</em>转换中，这是必须的，这些转换将跨多个批次的数据进行合并。在此类转换中，生成的RDD依赖于先前批次的RDD，这导致依赖项链的长度随时间不断增加。为了避免恢复时间的这种无限制的增加（与依存关系链成比例），有状态转换的中间RDD定期<em>检查点</em>到可靠的存储（例如HDFS）以切断依存关系链。</li>
</ul>

<p>综上所述，从驱动程序故障中恢复时，主要需要元数据检查点，而如果使用有状态转换，即使是基本功能，也需要数据或RDD检查点。</p>

<h4 class="no_toc" id="when-to-enable-checkpointing">何时启用检查点</h4>

<p>必须为具有以下任何要求的应用程序启用检查点：</p>

<ul>
  <li><em>有状态转换的用法</em> -如果有<code>updateStateByKey</code>要么<code>reduceByKeyAndWindow</code> （具有反函数）（在应用程序中），则必须提供检查点目录以允许定期进行RDD检查点。</li>
  <li><em>从运行应用程序的驱动程序故障中恢复</em> -元数据检查点用于恢复进度信息。</li>
</ul>

<p>请注意，没有前述状态转换的简单流应用程序可以在不启用检查点的情况下运行。在这种情况下，从驱动程序故障中恢复也将是部分的（某些丢失但未处理的数据可能会丢失）。这通常是可以接受的，并且许多都以这种方式运行Spark Streaming应用程序。预计将来会改善对非Hadoop环境的支持。</p>

<h4 class="no_toc" id="how-to-configure-checkpointing">如何配置检查点</h4>

<p>可以通过在容错，可靠的文件系统（例如，HDFS，S3等）中设置目录来启用检查点，将检查点信息保存到该目录中。这是通过使用<code>streamingContext.checkpoint(checkpointDirectory)</code> 。这将允许您使用前面提到的有状态转换。此外，如果要使应用程序从驱动程序故障中恢复，则应重写流应用程序以具有以下行为。</p>

<ul>
  <li>程序首次启动时，它将创建一个新的StreamingContext，设置所有流，然后调用start（）。</li>
  <li>失败后重新启动程序时，它将根据检查点目录中的检查点数据重新创建StreamingContext。</li>
</ul>

<div class="codetabs">
<div data-lang="scala">

    <p>通过使用以下行为使此行为变得简单<code>StreamingContext.getOrCreate</code> 。如下使用。</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// Function to create and setup a new StreamingContext</span>
<span class="k">def</span> <span class="n">functionToCreateContext</span><span class="o">()</span><span class="k">:</span> <span class="kt">StreamingContext</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">ssc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StreamingContext</span><span class="o">(...)</span>   <span class="c1">// new context</span>
  <span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">ssc</span><span class="o">.</span><span class="n">socketTextStream</span><span class="o">(...)</span> <span class="c1">// create DStreams</span>
  <span class="o">...</span>
  <span class="n">ssc</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">(</span><span class="n">checkpointDirectory</span><span class="o">)</span>   <span class="c1">// set checkpoint directory</span>
  <span class="n">ssc</span>
<span class="o">}</span>

<span class="c1">// Get StreamingContext from checkpoint data or create a new one</span>
<span class="k">val</span> <span class="n">context</span> <span class="k">=</span> <span class="nc">StreamingContext</span><span class="o">.</span><span class="n">getOrCreate</span><span class="o">(</span><span class="n">checkpointDirectory</span><span class="o">,</span> <span class="n">functionToCreateContext</span> <span class="k">_</span><span class="o">)</span>

<span class="c1">// Do additional setup on context that needs to be done,</span>
<span class="c1">// irrespective of whether it is being started or restarted</span>
<span class="n">context</span><span class="o">.</span> <span class="o">...</span>

<span class="c1">// Start the context</span>
<span class="n">context</span><span class="o">.</span><span class="n">start</span><span class="o">()</span>
<span class="n">context</span><span class="o">.</span><span class="n">awaitTermination</span><span class="o">()</span></code></pre></figure>

    <p>如果<code>checkpointDirectory</code>存在，然后将根据检查点数据重新创建上下文。如果该目录不存在（即首次运行），则该函数<code>functionToCreateContext</code>将被调用以创建新的上下文并设置DStreams。请参阅Scala示例<a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala">RecoverableNetworkWordCount</a> 。本示例将网络数据的字数附加到文件中。</p>

  </div>
<div data-lang="java">

    <p>通过使用以下行为使此行为变得简单<code>JavaStreamingContext.getOrCreate</code> 。如下使用。</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// Create a factory object that can create and setup a new JavaStreamingContext</span>
<span class="n">JavaStreamingContextFactory</span> <span class="n">contextFactory</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaStreamingContextFactory</span><span class="o">()</span> <span class="o">{</span>
  <span class="nd">@Override</span> <span class="kd">public</span> <span class="n">JavaStreamingContext</span> <span class="nf">create</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">JavaStreamingContext</span> <span class="n">jssc</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaStreamingContext</span><span class="o">(...);</span>  <span class="c1">// new context</span>
    <span class="n">JavaDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">lines</span> <span class="o">=</span> <span class="n">jssc</span><span class="o">.</span><span class="na">socketTextStream</span><span class="o">(...);</span>     <span class="c1">// create DStreams</span>
    <span class="o">...</span>
    <span class="n">jssc</span><span class="o">.</span><span class="na">checkpoint</span><span class="o">(</span><span class="n">checkpointDirectory</span><span class="o">);</span>                       <span class="c1">// set checkpoint directory</span>
    <span class="k">return</span> <span class="n">jssc</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">};</span>

<span class="c1">// Get JavaStreamingContext from checkpoint data or create a new one</span>
<span class="n">JavaStreamingContext</span> <span class="n">context</span> <span class="o">=</span> <span class="n">JavaStreamingContext</span><span class="o">.</span><span class="na">getOrCreate</span><span class="o">(</span><span class="n">checkpointDirectory</span><span class="o">,</span> <span class="n">contextFactory</span><span class="o">);</span>

<span class="c1">// Do additional setup on context that needs to be done,</span>
<span class="c1">// irrespective of whether it is being started or restarted</span>
<span class="n">context</span><span class="o">.</span> <span class="o">...</span>

<span class="c1">// Start the context</span>
<span class="n">context</span><span class="o">.</span><span class="na">start</span><span class="o">();</span>
<span class="n">context</span><span class="o">.</span><span class="na">awaitTermination</span><span class="o">();</span></code></pre></figure>

    <p>如果<code>checkpointDirectory</code>存在，然后将根据检查点数据重新创建上下文。如果该目录不存在（即首次运行），则该函数<code>contextFactory</code>将被调用以创建新的上下文并设置DStreams。请参阅Java示例<a href="https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java">JavaRecoverableNetworkWordCount</a> 。本示例将网络数据的字数附加到文件中。</p>

  </div>
<div data-lang="python">

    <p>通过使用以下行为使此行为变得简单<code>StreamingContext.getOrCreate</code> 。如下使用。</p>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># Function to create and setup a new StreamingContext</span>
<span class="k">def</span> <span class="nf">functionToCreateContext</span><span class="p">():</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># new context</span>
    <span class="n">ssc</span> <span class="o">=</span> <span class="n">StreamingContext</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="n">ssc</span><span class="o">.</span><span class="n">socketTextStream</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># create DStreams</span>
    <span class="o">...</span>
    <span class="n">ssc</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span><span class="n">checkpointDirectory</span><span class="p">)</span>  <span class="c1"># set checkpoint directory</span>
    <span class="k">return</span> <span class="n">ssc</span>

<span class="c1"># Get StreamingContext from checkpoint data or create a new one</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">StreamingContext</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">(</span><span class="n">checkpointDirectory</span><span class="p">,</span> <span class="n">functionToCreateContext</span><span class="p">)</span>

<span class="c1"># Do additional setup on context that needs to be done,</span>
<span class="c1"># irrespective of whether it is being started or restarted</span>
<span class="n">context</span><span class="o">.</span> <span class="o">...</span>

<span class="c1"># Start the context</span>
<span class="n">context</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="n">context</span><span class="o">.</span><span class="n">awaitTermination</span><span class="p">()</span></code></pre></figure>

    <p>如果<code>checkpointDirectory</code>存在，然后将根据检查点数据重新创建上下文。如果该目录不存在（即首次运行），则该函数<code>functionToCreateContext</code>将被调用以创建新的上下文并设置DStreams。请参阅Python示例<a href="https://github.com/apache/spark/tree/master/examples/src/main/python/streaming/recoverable_network_wordcount.py">recoveryable_network_wordcount.py</a> 。本示例将网络数据的字数附加到文件中。</p>

    <p>您还可以显式创建一个<code>StreamingContext</code>从检查点数据开始，并通过使用<code>StreamingContext.getOrCreate(checkpointDirectory, None)</code> 。</p>

  </div>
</div>

<p>除了使用<code>getOrCreate</code>还需要确保驱动程序进程在发生故障时自动重新启动。这只能通过用于运行应用程序的部署基础结构来完成。这将在“ <a href="#deploying-applications">部署”</a>部分中进一步讨论。</p>

<p>请注意，RDD的检查点会导致保存到可靠存储的成本。这可能会导致RDD被检查点的那些批次的处理时间增加。因此，需要仔细设置检查点的间隔。在小批量（例如1秒）时，每批检查点可能会大大降低操作吞吐量。相反，检查点太少会导致沿袭和任务规模增加，这可能会产生不利影响。对于需要RDD检查点的有状态转换，默认间隔为批处理间隔的倍数，至少应为10秒。可以使用设置<code>dstream.checkpoint(checkpointInterval)</code> 。通常，DStream的5-10个滑动间隔的检查点间隔是一个很好的尝试设置。</p>

<hr>

<h2 id="accumulators-broadcast-variables-and-checkpoints">累加器，广播变量和检查点</h2>

<p>无法从Spark Streaming中的检查点恢复<a href="rdd-programming-guide.html#accumulators">累加器</a>和<a href="rdd-programming-guide.html#broadcast-variables">广播变量</a> 。如果启用检查点并同时使用“ <a href="rdd-programming-guide.html#accumulators">累加器”</a>或“ <a href="rdd-programming-guide.html#broadcast-variables">广播”变量</a> ，则必须为“ <a href="rdd-programming-guide.html#accumulators">累加器”</a>和“ <a href="rdd-programming-guide.html#broadcast-variables">广播”变量</a>创建延迟实例化的单例实例，以便在驱动程序发生故障重新启动后可以重新实例化它们。在下面的示例中显示。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">object</span> <span class="nc">WordBlacklist</span> <span class="o">{</span>

  <span class="nd">@volatile</span> <span class="k">private</span> <span class="k">var</span> <span class="n">instance</span><span class="k">:</span> <span class="kt">Broadcast</span><span class="o">[</span><span class="kt">Seq</span><span class="o">[</span><span class="kt">String</span><span class="o">]]</span> <span class="k">=</span> <span class="kc">null</span>

  <span class="k">def</span> <span class="n">getInstance</span><span class="o">(</span><span class="n">sc</span><span class="k">:</span> <span class="kt">SparkContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">Broadcast</span><span class="o">[</span><span class="kt">Seq</span><span class="o">[</span><span class="kt">String</span><span class="o">]]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">synchronized</span> <span class="o">{</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
          <span class="k">val</span> <span class="n">wordBlacklist</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">&quot;a&quot;</span><span class="o">,</span> <span class="s">&quot;b&quot;</span><span class="o">,</span> <span class="s">&quot;c&quot;</span><span class="o">)</span>
          <span class="n">instance</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">broadcast</span><span class="o">(</span><span class="n">wordBlacklist</span><span class="o">)</span>
        <span class="o">}</span>
      <span class="o">}</span>
    <span class="o">}</span>
    <span class="n">instance</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="k">object</span> <span class="nc">DroppedWordsCounter</span> <span class="o">{</span>

  <span class="nd">@volatile</span> <span class="k">private</span> <span class="k">var</span> <span class="n">instance</span><span class="k">:</span> <span class="kt">LongAccumulator</span> <span class="o">=</span> <span class="kc">null</span>

  <span class="k">def</span> <span class="n">getInstance</span><span class="o">(</span><span class="n">sc</span><span class="k">:</span> <span class="kt">SparkContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">LongAccumulator</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">synchronized</span> <span class="o">{</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
          <span class="n">instance</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">longAccumulator</span><span class="o">(</span><span class="s">&quot;WordsInBlacklistCounter&quot;</span><span class="o">)</span>
        <span class="o">}</span>
      <span class="o">}</span>
    <span class="o">}</span>
    <span class="n">instance</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="n">wordCounts</span><span class="o">.</span><span class="n">foreachRDD</span> <span class="o">{</span> <span class="o">(</span><span class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[(</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">)],</span> <span class="n">time</span><span class="k">:</span> <span class="kt">Time</span><span class="o">)</span> <span class="k">=&gt;</span>
  <span class="c1">// Get or register the blacklist Broadcast</span>
  <span class="k">val</span> <span class="n">blacklist</span> <span class="k">=</span> <span class="nc">WordBlacklist</span><span class="o">.</span><span class="n">getInstance</span><span class="o">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">)</span>
  <span class="c1">// Get or register the droppedWordsCounter Accumulator</span>
  <span class="k">val</span> <span class="n">droppedWordsCounter</span> <span class="k">=</span> <span class="nc">DroppedWordsCounter</span><span class="o">.</span><span class="n">getInstance</span><span class="o">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">)</span>
  <span class="c1">// Use blacklist to drop words and use droppedWordsCounter to count them</span>
  <span class="k">val</span> <span class="n">counts</span> <span class="k">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">filter</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">word</span><span class="o">,</span> <span class="n">count</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">blacklist</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">word</span><span class="o">))</span> <span class="o">{</span>
      <span class="n">droppedWordsCounter</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">count</span><span class="o">)</span>
      <span class="kc">false</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="kc">true</span>
    <span class="o">}</span>
  <span class="o">}.</span><span class="n">collect</span><span class="o">().</span><span class="n">mkString</span><span class="o">(</span><span class="s">&quot;[&quot;</span><span class="o">,</span> <span class="s">&quot;, &quot;</span><span class="o">,</span> <span class="s">&quot;]&quot;</span><span class="o">)</span>
  <span class="k">val</span> <span class="n">output</span> <span class="k">=</span> <span class="s">&quot;Counts at time &quot;</span> <span class="o">+</span> <span class="n">time</span> <span class="o">+</span> <span class="s">&quot; &quot;</span> <span class="o">+</span> <span class="n">counts</span>
<span class="o">})</span></code></pre></figure>

    <p>请参阅完整的<a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala">源代码</a> 。</p>
  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kd">class</span> <span class="nc">JavaWordBlacklist</span> <span class="o">{</span>

  <span class="kd">private</span> <span class="kd">static</span> <span class="kd">volatile</span> <span class="n">Broadcast</span><span class="o">&lt;</span><span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">instance</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>

  <span class="kd">public</span> <span class="kd">static</span> <span class="n">Broadcast</span><span class="o">&lt;</span><span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;&gt;</span> <span class="nf">getInstance</span><span class="o">(</span><span class="n">JavaSparkContext</span> <span class="n">jsc</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="kd">synchronized</span> <span class="o">(</span><span class="n">JavaWordBlacklist</span><span class="o">.</span><span class="na">class</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
          <span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">wordBlacklist</span> <span class="o">=</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="s">&quot;a&quot;</span><span class="o">,</span> <span class="s">&quot;b&quot;</span><span class="o">,</span> <span class="s">&quot;c&quot;</span><span class="o">);</span>
          <span class="n">instance</span> <span class="o">=</span> <span class="n">jsc</span><span class="o">.</span><span class="na">broadcast</span><span class="o">(</span><span class="n">wordBlacklist</span><span class="o">);</span>
        <span class="o">}</span>
      <span class="o">}</span>
    <span class="o">}</span>
    <span class="k">return</span> <span class="n">instance</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="kd">class</span> <span class="nc">JavaDroppedWordsCounter</span> <span class="o">{</span>

  <span class="kd">private</span> <span class="kd">static</span> <span class="kd">volatile</span> <span class="n">LongAccumulator</span> <span class="n">instance</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>

  <span class="kd">public</span> <span class="kd">static</span> <span class="n">LongAccumulator</span> <span class="nf">getInstance</span><span class="o">(</span><span class="n">JavaSparkContext</span> <span class="n">jsc</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="kd">synchronized</span> <span class="o">(</span><span class="n">JavaDroppedWordsCounter</span><span class="o">.</span><span class="na">class</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">instance</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
          <span class="n">instance</span> <span class="o">=</span> <span class="n">jsc</span><span class="o">.</span><span class="na">sc</span><span class="o">().</span><span class="na">longAccumulator</span><span class="o">(</span><span class="s">&quot;WordsInBlacklistCounter&quot;</span><span class="o">);</span>
        <span class="o">}</span>
      <span class="o">}</span>
    <span class="o">}</span>
    <span class="k">return</span> <span class="n">instance</span><span class="o">;</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="n">wordCounts</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">((</span><span class="n">rdd</span><span class="o">,</span> <span class="n">time</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="c1">// Get or register the blacklist Broadcast</span>
  <span class="n">Broadcast</span><span class="o">&lt;</span><span class="n">List</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">blacklist</span> <span class="o">=</span> <span class="n">JavaWordBlacklist</span><span class="o">.</span><span class="na">getInstance</span><span class="o">(</span><span class="k">new</span> <span class="n">JavaSparkContext</span><span class="o">(</span><span class="n">rdd</span><span class="o">.</span><span class="na">context</span><span class="o">()));</span>
  <span class="c1">// Get or register the droppedWordsCounter Accumulator</span>
  <span class="n">LongAccumulator</span> <span class="n">droppedWordsCounter</span> <span class="o">=</span> <span class="n">JavaDroppedWordsCounter</span><span class="o">.</span><span class="na">getInstance</span><span class="o">(</span><span class="k">new</span> <span class="n">JavaSparkContext</span><span class="o">(</span><span class="n">rdd</span><span class="o">.</span><span class="na">context</span><span class="o">()));</span>
  <span class="c1">// Use blacklist to drop words and use droppedWordsCounter to count them</span>
  <span class="n">String</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="na">filter</span><span class="o">(</span><span class="n">wordCount</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">blacklist</span><span class="o">.</span><span class="na">value</span><span class="o">().</span><span class="na">contains</span><span class="o">(</span><span class="n">wordCount</span><span class="o">.</span><span class="na">_1</span><span class="o">()))</span> <span class="o">{</span>
      <span class="n">droppedWordsCounter</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">wordCount</span><span class="o">.</span><span class="na">_2</span><span class="o">());</span>
      <span class="k">return</span> <span class="kc">false</span><span class="o">;</span>
    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
      <span class="k">return</span> <span class="kc">true</span><span class="o">;</span>
    <span class="o">}</span>
  <span class="o">}).</span><span class="na">collect</span><span class="o">().</span><span class="na">toString</span><span class="o">();</span>
  <span class="n">String</span> <span class="n">output</span> <span class="o">=</span> <span class="s">&quot;Counts at time &quot;</span> <span class="o">+</span> <span class="n">time</span> <span class="o">+</span> <span class="s">&quot; &quot;</span> <span class="o">+</span> <span class="n">counts</span><span class="o">;</span>
<span class="o">}</span></code></pre></figure>

    <p>请参阅完整的<a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java">源代码</a> 。</p>
  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">def</span> <span class="nf">getWordBlacklist</span><span class="p">(</span><span class="n">sparkContext</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;wordBlacklist&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">()):</span>
        <span class="nb">globals</span><span class="p">()[</span><span class="s2">&quot;wordBlacklist&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sparkContext</span><span class="o">.</span><span class="n">broadcast</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">])</span>
    <span class="k">return</span> <span class="nb">globals</span><span class="p">()[</span><span class="s2">&quot;wordBlacklist&quot;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">getDroppedWordsCounter</span><span class="p">(</span><span class="n">sparkContext</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;droppedWordsCounter&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">()):</span>
        <span class="nb">globals</span><span class="p">()[</span><span class="s2">&quot;droppedWordsCounter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sparkContext</span><span class="o">.</span><span class="n">accumulator</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">globals</span><span class="p">()[</span><span class="s2">&quot;droppedWordsCounter&quot;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">echo</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
    <span class="c1"># Get or register the blacklist Broadcast</span>
    <span class="n">blacklist</span> <span class="o">=</span> <span class="n">getWordBlacklist</span><span class="p">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">context</span><span class="p">)</span>
    <span class="c1"># Get or register the droppedWordsCounter Accumulator</span>
    <span class="n">droppedWordsCounter</span> <span class="o">=</span> <span class="n">getDroppedWordsCounter</span><span class="p">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">context</span><span class="p">)</span>

    <span class="c1"># Use blacklist to drop words and use droppedWordsCounter to count them</span>
    <span class="k">def</span> <span class="nf">filterFunc</span><span class="p">(</span><span class="n">wordCount</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">wordCount</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">blacklist</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
            <span class="n">droppedWordsCounter</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">wordCount</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="bp">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">True</span>

    <span class="n">counts</span> <span class="o">=</span> <span class="s2">&quot;Counts at time </span><span class="si">%s</span><span class="s2"> </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">rdd</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">filterFunc</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>

<span class="n">wordCounts</span><span class="o">.</span><span class="n">foreachRDD</span><span class="p">(</span><span class="n">echo</span><span class="p">)</span></code></pre></figure>

    <p>请参阅完整的<a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/python/streaming/recoverable_network_wordcount.py">源代码</a> 。</p>

  </div>
</div>

<hr>

<h2 id="deploying-applications">部署应用</h2>
<p>本部分讨论了部署Spark Streaming应用程序的步骤。</p>

<h3 class="no_toc" id="requirements">要求</h3>

<p>要运行Spark Streaming应用程序，您需要具备以下条件。</p>

<ul>
  <li>
    <p><em>使用集群管理器进行集群</em> -这是任何Spark应用程序的一般要求，并且在<a href="cluster-overview.html">部署指南</a>中进行了详细讨论。</p>
  </li>
  <li>
    <p><em>将应用程序JAR打包</em> -您必须将流式应用程序编译为JAR。如果您正在使用<a href="submitting-applications.html"><code>spark-submit</code></a>要启动该应用程序，则无需在JAR中提供Spark和Spark Streaming。但是，如果您的应用程序使用<a href="#advanced-sources">高级资源</a> （例如Kafka，Flume），则必须将它们链接到的额外工件及其依赖项打包在用于部署应用程序的JAR中。例如，使用<code>KafkaUtils</code>将必须包括<code>spark-streaming-kafka-0-10_2.12</code>及其在应用程序JAR中的所有传递依赖项。</p>
  </li>
  <li>
    <p><em>为执行者配置足够的内存</em> -由于必须将接收到的数据存储在内存中，因此必须为执行者配置足够的内存来保存接收到的数据。请注意，如果您要执行10分钟的窗口操作，则系统必须在内存中至少保留最后10分钟的数据。因此，应用程序的内存要求取决于应用程序中使用的操作。</p>
  </li>
  <li>
    <p><em>配置检查点</em> -如果流应用程序需要它，则必须将与Hadoop API兼容的容错存储中的目录（例如HDFS，S3等）配置为检查点目录，并且以这样的方式编写流应用程序：用于故障恢复。有关更多详细信息，请参见<a href="#checkpointing">检查点</a>部分。</p>
  </li>
  <li><em>配置应用程序驱动程序的自动重启</em> -要从驱动程序故障中自动恢复，用于运行流式应用程序的部署基础结构必须监视驱动程序进程，并在驱动程序失败时重新启动。不同的<a href="cluster-overview.html#cluster-manager-types">集群管理器</a>具有不同的工具来实现这一目标。
    <ul>
      <li><em>Spark Standalone-</em>可提交Spark应用程序驱动程序以在Spark Standalone集群内运行（请参阅<a href="spark-standalone.html#launching-spark-applications">集群部署模式</a> ），即，应用程序驱动程序本身在工作程序节点之一上运行。此外，可以指示独立群集管理器<em>监督</em>驱动程序，并在驱动程序由于非零退出代码或由于运行驱动程序的节点故障而失败时重新启动它。有关更多详细信息，请参见<a href="spark-standalone.html">Spark Standalone指南</a>中的<em>集群模式</em>和<em>监督</em> 。</li>
      <li><em>YARN</em> -Yarn支持自动重启应用程序的类似机制。请参阅YARN文档以获取更多详细信息。</li>
      <li><em>Mesos-</em> <a href="https://github.com/mesosphere/marathon">马拉松</a>已经被<em>Mesos</em>用来实现这一目标。</li>
    </ul>
  </li>
  <li>
    <p><em>配置预写日志</em> -自Spark 1.2起，我们引入了<em>预写日志</em>以实现强大的容错保证。如果启用，则将从接收器接收的所有数据写入配置检查点目录中的预写日志。这样可以防止驱动程序恢复时丢失数据，从而确保零数据丢失（在“ <a href="#fault-tolerance-semantics">容错语义”</a>部分中进行了详细讨论）。可以通过设置<a href="configuration.html#spark-streaming">配置参数</a>来启用<code>spark.streaming.receiver.writeAheadLog.enable</code>至<code>true</code> 。但是，这些更强的语义可能会以单个接收器的接收吞吐量为代价。可以通过<a href="#level-of-parallelism-in-data-receiving">并行</a>运行<a href="#level-of-parallelism-in-data-receiving">更多接收器</a>以提高总吞吐量来纠正此问题。另外，由于启用了预写日志，因此建议禁用Spark中接收数据的复制，因为该日志已经存储在复制的存储系统中。这可以通过将输入流的存储级别设置为<code>StorageLevel.MEMORY_AND_DISK_SER</code> 。在将S3（或任何不支持刷新的文件系统）用于<em>预写日志时</em> ，请记住启用<code>spark.streaming.driver.writeAheadLog.closeFileAfterWrite</code>和<code>spark.streaming.receiver.writeAheadLog.closeFileAfterWrite</code> 。有关更多详细信息，请参见<a href="configuration.html#spark-streaming">Spark Streaming配置</a> 。请注意，启用I / O加密后，Spark不会加密写入预写日志的数据。如果需要对预写日志数据进行加密，则应将其存储在本机支持加密的文件系统中。</p>
  </li>
  <li><em>设置最大接收速率</em> -如果群集资源不够大，无法使流应用程序以<em>最快的速度</em>处理数据，则可以通过设置记录/秒的最大速率限制来限制接收器的速率。查看<a href="configuration.html#spark-streaming">配置参数</a> <code>spark.streaming.receiver.maxRate</code>对于接收器和<code>spark.streaming.kafka.maxRatePerPartition</code>直接卡夫卡方法。在Spark 1.5中，我们引入了一个称为<em>背压</em>的功能，该功能消除了设置此速率限制的需要，因为Spark Streaming会自动找出速率限制，并在处理条件发生变化时动态调整它们。可以通过设置<a href="configuration.html#spark-streaming">配置参数</a>来启用此背压<code>spark.streaming.backpressure.enabled</code>至<code>true</code> 。</li>
</ul>

<h3 class="no_toc" id="upgrading-application-code">升级应用程序代码</h3>

<p>如果需要使用新的应用程序代码升级正在运行的Spark Streaming应用程序，则有两种可能的机制。</p>

<ul>
  <li>
    <p>升级后的Spark Streaming应用程序将启动，并与现有应用程序并行运行。一旦新的（接收与旧的数据相同）的数据被预热并准备好进行黄金时段，就可以关闭旧的数据。请注意，对于支持将数据发送到两个目标的数据源（即，较早和升级的应用程序），可以这样做。</p>
  </li>
  <li>
    <p>现有应用程序正常关闭（请参阅<a href="api/scala/index.html#org.apache.spark.streaming.StreamingContext"><code>StreamingContext.stop(...)</code></a>要么<a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html"><code>JavaStreamingContext.stop(...)</code></a> （用于正常关闭选项），以确保在关闭之前已完全处理已接收的数据。然后可以启动升级的应用程序，它将从较早的应用程序停止的同一点开始处理。请注意，只能使用支持源端缓冲的输入源（例如Kafka和Flume）来完成此操作，因为在上一个应用程序关闭且升级后的应用程序尚未启动时，需要缓冲数据。并且无法完成从升级前代码的较早检查点信息重新启动的操作。检查点信息本质上包含序列化的Scala / Java / Python对象，尝试使用经过修改的新类反序列化对象可能会导致错误。在这种情况下，请使用其他检查点目录启动升级的应用程序，或者删除先前的检查点目录。</p>
  </li>
</ul>

<hr>

<h2 id="monitoring-applications">监控应用</h2>
<p>除了Spark的<a href="monitoring.html">监视功能</a>外，Spark Streaming还具有其他特定功能。使用StreamingContext时， <a href="monitoring.html#web-interfaces">Spark Web UI会</a>显示一个附加<code>Streaming</code>该选项卡显示有关正在运行的接收器的统计信息（接收器是否处于活动状态，接收到的记录数，接收器错误等）和完成的批次（批次处理时间，排队延迟等）。这可用于监视流应用程序的进度。</p>

<p>Web UI中的以下两个指标特别重要：</p>

<ul>
  <li><em>处理时间</em> -处理每批数据的时间。</li>
  <li><em>调度延迟</em> -批处理在队列中等待先前批处理完成的时间。</li>
</ul>

<p>如果批处理时间始终大于批处理时间间隔和/或排队延迟持续增加，则表明系统无法像生成批处理一样快处理批处理，并且落后于此。在这种情况下，请考虑<a href="#reducing-the-batch-processing-times">减少</a>批处理时间。</p>

<p>还可以使用<a href="api/scala/index.html#org.apache.spark.streaming.scheduler.StreamingListener">StreamingListener</a>界面监视Spark Streaming程序的进度，该界面可让您获取接收器状态和处理时间。请注意，这是一个开发人员API，将来可能会得到改进（即，报告了更多信息）。</p>

<hr>
<hr>

<h1 id="performance-tuning">性能调优</h1>
<p>要在集群上的Spark Streaming应用程序中获得最佳性能，需要进行一些调整。本节说明了可以调整以提高应用程序性能的许多参数和配置。在较高级别上，您需要考虑两件事：</p>

<ol>
  <li>
    <p>通过有效使用群集资源减少每批数据的处理时间。</p>
  </li>
  <li>
    <p>设置正确的批处理大小，以便可以在接收到批处理数据后尽快对其进行处理（也就是说，数据处理与数据摄取保持同步）。</p>
  </li>
</ol>

<h2 id="reducing-the-batch-processing-times">减少批处理时间</h2>
<p>Spark可以进行许多优化，以最大程度地减少每批的处理时间。这些已在《 <a href="tuning.html">调优指南》</a>中详细讨论。本节重点介绍一些最重要的内容。</p>

<h3 class="no_toc" id="level-of-parallelism-in-data-receiving">数据接收中的并行度</h3>
<p>通过网络（例如Kafka，Flume，套接字等）接收数据需要对数据进行反序列化并将其存储在Spark中。如果数据接收成为系统的瓶颈，请考虑并行化数据接收。请注意，每个输入DStream都会创建一个接收器（在工作计算机上运行），该接收器接收单个数据流。因此，可以通过创建多个输入DStream并将其配置为从源接收数据流的不同分区来实现接收多个数据流。例如，可以将接收两个主题数据的单个Kafka输入DStream拆分为两个Kafka输入流，每个输入流仅接收一个主题。这将运行两个接收器，从而允许并行接收数据，从而提高了总体吞吐量。这些多个DStream可以结合在一起以创建单个DStream。然后，可以将应用于单个输入DStream的转换应用于统一流。这样做如下。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">numStreams</span> <span class="k">=</span> <span class="mi">5</span>
<span class="k">val</span> <span class="n">kafkaStreams</span> <span class="k">=</span> <span class="o">(</span><span class="mi">1</span> <span class="n">to</span> <span class="n">numStreams</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="n">i</span> <span class="k">=&gt;</span> <span class="nc">KafkaUtils</span><span class="o">.</span><span class="n">createStream</span><span class="o">(...)</span> <span class="o">}</span>
<span class="k">val</span> <span class="n">unifiedStream</span> <span class="k">=</span> <span class="n">streamingContext</span><span class="o">.</span><span class="n">union</span><span class="o">(</span><span class="n">kafkaStreams</span><span class="o">)</span>
<span class="n">unifiedStream</span><span class="o">.</span><span class="n">print</span><span class="o">()</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kt">int</span> <span class="n">numStreams</span> <span class="o">=</span> <span class="mi">5</span><span class="o">;</span>
<span class="n">List</span><span class="o">&lt;</span><span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">kafkaStreams</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ArrayList</span><span class="o">&lt;&gt;(</span><span class="n">numStreams</span><span class="o">);</span>
<span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numStreams</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
  <span class="n">kafkaStreams</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">KafkaUtils</span><span class="o">.</span><span class="na">createStream</span><span class="o">(...));</span>
<span class="o">}</span>
<span class="n">JavaPairDStream</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">unifiedStream</span> <span class="o">=</span> <span class="n">streamingContext</span><span class="o">.</span><span class="na">union</span><span class="o">(</span><span class="n">kafkaStreams</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="n">kafkaStreams</span><span class="o">.</span><span class="na">subList</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="n">kafkaStreams</span><span class="o">.</span><span class="na">size</span><span class="o">()));</span>
<span class="n">unifiedStream</span><span class="o">.</span><span class="na">print</span><span class="o">();</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">numStreams</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">kafkaStreams</span> <span class="o">=</span> <span class="p">[</span><span class="n">KafkaUtils</span><span class="o">.</span><span class="n">createStream</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">numStreams</span><span class="p">)]</span>
<span class="n">unifiedStream</span> <span class="o">=</span> <span class="n">streamingContext</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="o">*</span><span class="n">kafkaStreams</span><span class="p">)</span>
<span class="n">unifiedStream</span><span class="o">.</span><span class="n">pprint</span><span class="p">()</span></code></pre></figure>

  </div>
</div>

<p>应该考虑的另一个参数是接收机的块间隔，该间隔由<a href="configuration.html#spark-streaming">配置参数</a>确定<code>spark.streaming.blockInterval</code> 。对于大多数接收器，接收到的数据在存储在Spark内存中之前会合并为数据块。每批中的块数确定了将在类似地图的转换中用于处理接收到的数据的任务数。每批接收器中每个接收器的任务数大约为（批处理间隔/块间隔）。例如，200 ms的块间隔将每2秒批处理创建10个任务。如果任务数太少（即少于每台计算机的核心数），那么它将效率低下，因为将不使用所有可用的核心来处理数据。要增加给定批处理间隔的任务数，请减小阻止间隔。但是，建议的块间隔最小值约为50毫秒，在此之下，任务启动开销可能是个问题。</p>

<p>使用多个输入流/接收器接收数据的另一种方法是显式重新划分输入数据流（使用<code>inputStream.repartition(<number of partitions>)</code> ）。在进一步处理之前，这会将接收到的数据批分布在群集中指定数量的计算机上。</p>

<p>对于直接流，请参阅<a href="streaming-kafka-integration.html">Spark Streaming + Kafka集成指南</a></p>

<h3 class="no_toc" id="level-of-parallelism-in-data-processing">数据处理中的并行度</h3>
<p>如果在计算的任何阶段使用的并行任务数量不够高，则群集资源可能无法得到充分利用。例如，对于分布式归约操作，例如<code>reduceByKey</code>和<code>reduceByKeyAndWindow</code> ，并行任务的默认数量由<code>spark.default.parallelism</code> <a href="configuration.html#spark-properties">配置属性</a> 。您可以将并行性级别作为参数传递（请参见<a href="api/scala/index.html#org.apache.spark.streaming.dstream.PairDStreamFunctions"><code>PairDStreamFunctions</code></a>文档），或设置<code>spark.default.parallelism</code> <a href="configuration.html#spark-properties">配置属性</a>以更改默认值。</p>

<h3 class="no_toc" id="data-serialization">数据序列化</h3>
<p>可以通过调整序列化格式来减少数据序列化的开销。在流传输的情况下，有两种类型的数据正在序列化。</p>

<ul>
  <li>
    <p><strong>输入数据</strong> ：默认情况下，通过Receiver接收的输入数据通过<a href="api/scala/index.html#org.apache.spark.storage.StorageLevel$">StorageLevel存储在执行程序的内存中</a><a href="api/scala/index.html#org.apache.spark.storage.StorageLevel$">。MEMORY_AND_DISK_SER_2</a> 。也就是说，数据被序列化为字节以减少GC开销，并被复制以容忍执行器故障。同样，数据首先保存在内存中，并且仅在内存不足以容纳流计算所需的所有输入数据时才溢出到磁盘。显然，这种序列化会产生开销–接收器必须对接收到的数据进行反序列化，然后使用Spark的序列化格式对其进行重新序列化。</p>
  </li>
  <li>
    <p><strong>流操作生成的持久RDD</strong> ：流计算生成的RDD可以保留在内存中。例如，窗口操作会将数据保留在内存中，因为它们将被多次处理。但是，与Spark Core默认的<a href="api/scala/index.html#org.apache.spark.storage.StorageLevel$">StorageLevel不同。MEMORY_ONLY</a> ，由流计算生成的RDD持久存储在<a href="api/scala/index.html#org.apache.spark.storage.StorageLevel$">StorageLevel中。默认情况下， MEMORY_ONLY_SER</a> （即序列化）可最大程度地减少GC开销。</p>
  </li>
</ul>

<p>在这两种情况下，使用Kryo序列化都可以减少CPU和内存的开销。有关更多详细信息，请参见《 <a href="tuning.html#data-serialization">Spark Tuning Guide》</a> 。对于Kryo，请考虑注册自定义类，并禁用对象引用跟踪（请参阅《 <a href="configuration.html#compression-and-serialization">配置指南》中</a>与Kryo相关的<a href="configuration.html#compression-and-serialization">配置</a> ）。</p>

<p>在流应用程序需要保留的数据量不大的特定情况下，将数据（两种类型）持久化为反序列化对象是可行的，而不会产生过多的GC开销。例如，如果您使用的是几秒钟的批处理间隔并且没有窗口操作，那么您可以尝试通过显式设置存储级别来禁用持久化数据中的序列化。这将减少由于序列化导致的CPU开销，从而可能在没有太多GC开销的情况下提高性能。</p>

<h3 class="no_toc" id="task-launching-overheads">任务启动开销</h3>
<p>如果每秒启动的任务数量很高（例如，每秒50个或更多），则向从服务器发送任务的开销可能会很大，并且将难以实现亚秒级的延迟。可以通过以下更改来减少开销：</p>

<ul>
  <li><strong>执行模式</strong> ：在独立模式或粗粒度的Mesos模式<strong>下</strong>运行Spark可以比细粒度的Mesos模式更好地执行任务启动时间。有关更多详细信息，请参阅<a href="running-on-mesos.html">“在Mesos</a>上<a href="running-on-mesos.html">运行”指南</a> 。</li>
</ul>

<p>这些更改可以将批处理时间减少100毫秒，从而使亚秒级的批处理大小可行。</p>

<hr>

<h2 id="setting-the-right-batch-interval">设置正确的批次间隔</h2>
<p>为了使在群集上运行的Spark Streaming应用程序稳定，系统应能够尽快处理接收到的数据。换句话说，应尽快处理一批数据。可以通过<a href="#monitoring-applications">监视</a>流式Web UI中的处理时间来找到对于应用程序是否正确的方法，其中批处理时间应小于批处理间隔。</p>

<p>根据流计算的性质，所使用的批处理间隔可能会对数据速率产生重大影响，而数据速率可以由应用程序在一组固定的群集资源上维持。例如，让我们考虑前面的WordCountNetwork示例。对于特定的数据速率，系统可能能够跟上每2秒（即2秒的批处理间隔）而不是每500毫秒报告字数的情况。因此，需要设置批次间隔，以便可以维持生产中的预期数据速率。</p>

<p>找出适合您的应用程序的正确批处理大小的一种好方法是使用保守的批处理间隔（例如5-10秒）和低数据速率进行测试。要验证系统是否能够保持数据速率，您可以检查每个已处理批次经历的端到端延迟的值（在Spark驱动程序log4j日志中查找“ Total delay”（总延迟），或使用<a href="api/scala/index.html#org.apache.spark.streaming.scheduler.StreamingListener">StreamingListener</a>接口）。如果延迟保持与批次大小相当，则系统是稳定的。否则，如果延迟持续增加，则意味着系统无法跟上，因此不稳定。一旦有了稳定配置的想法，就可以尝试提高数据速率和/或减小批处理大小。注意，由于暂时的数据速率增加而引起的延迟的瞬时增加可能是好的，只要延迟减小回到较低的值（即小于批大小）即可。</p>

<hr>

<h2 id="memory-tuning">内存调优</h2>
<p>在“ <a href="tuning.html#memory-tuning">调整指南”中</a>已详细讨论了<a href="tuning.html#memory-tuning">调整</a> Spark应用程序的内存使用情况和GC行为。强烈建议您阅读。在本节中，我们将专门在Spark Streaming应用程序的上下文中讨论一些调整参数。</p>

<p>Spark Streaming应用程序所需的群集内存量在很大程度上取决于所使用的转换类型。例如，如果要对最后10分钟的数据使用窗口操作，则群集应具有足够的内存以将10分钟的数据保留在内存中。或者如果您想使用<code>updateStateByKey</code>如果使用大量的键，则所需的内存将很高。相反，如果您想执行一个简单的map-filter-store操作，则所需的内存将很少。</p>

<p>通常，由于通过接收器接收的数据存储在StorageLevel中。MEMORY_AND_DISK_SER_2，内存中无法容纳的数据将溢出到磁盘上。这可能会降低流应用程序的性能，因此建议您提供流应用程序所需的足够内存。最好尝试以小规模查看内存使用情况并据此进行估计。</p>

<p>内存调整的另一个方面是垃圾回收。对于需要低延迟的流应用程序，不希望有由JVM垃圾回收引起的大停顿。</p>

<p>有一些参数可以帮助您调整内存使用和GC开销：</p>

<ul>
  <li>
    <p><strong>DStream的持久性级别</strong> ：如前面的“ <a href="#data-serialization">数据序列化”</a>部分所述，默认情况下，输入数据和RDD被持久化为序列化字节。与反序列化的持久性相比，这减少了内存使用和GC开销。启用Kryo序列化可进一步减少序列化的大小和内存使用量。通过压缩可以进一步减少内存使用量（请参见Spark配置） <code>spark.rdd.compress</code> ），但会占用CPU时间。</p>
  </li>
  <li>
    <p><strong>清除旧数据</strong> ：默认情况下，将自动清除DStream转换生成的所有输入数据和持久性RDD。Spark Streaming根据使用的转换来决定何时清除数据。例如，如果您使用10分钟的窗口操作，那么Spark Streaming将保留最后10分钟的数据，并主动丢弃较旧的数据。通过设置，可以将数据保留更长的时间（例如，以交互方式查询较早的数据） <code>streamingContext.remember</code> 。</p>
  </li>
  <li>
    <p><strong>CMS垃圾收集器</strong> ：强烈建议使用并发标记扫掠GC，以使与GC相关的暂停时间始终保持较低。尽管已知并发GC会降低系统的整体处理吞吐量，但仍建议使用它来实现更一致的批处理时间。确保在两个驱动程序上都设置了CMS GC（使用<code>--driver-java-options</code>在<code>spark-submit</code> ）和执行程序（使用<a href="configuration.html#runtime-environment">Spark配置</a> <code>spark.executor.extraJavaOptions</code> ）。</p>
  </li>
  <li>
    <p><strong>其他提示</strong> ：为了进一步减少GC开销，请尝试以下更多提示。</p>
    <ul>
      <li>坚持使用<code>OFF_HEAP</code>存储级别。请参阅《 <a href="rdd-programming-guide.html#rdd-persistence">Spark编程指南》</a>中的更多详细信息。</li>
      <li>使用更多具有较小堆大小的执行程序。这将减少每个JVM堆中的GC压力。</li>
    </ul>
  </li>
</ul>

<hr>

<h5 class="no_toc" id="important-points-to-remember">要记住的要点：</h5>
<ul>
  <li>
    <p>DStream与单个接收器关联。为了获得读取并行性，需要创建多个接收器，即多个DStream。接收器在执行器中运行。它占据了一个核心。预订接收器插槽后，请确保有足够的内核用于处理，即<code>spark.cores.max</code>应该考虑接收器插槽。接收者以循环方式分配给执行者。</p>
  </li>
  <li>
    <p>当从流源接收数据时，接收器会创建数据块。每blockInterval毫秒生成一个新的数据块。在batchInterval期间创建了N个数据块，其中N = batchInterval / blockInterval。这些块由当前执行器的BlockManager分发给其他执行器的块管理器。之后，驱动程序上运行的网络输入跟踪器将被告知有关块的位置，以进行进一步处理。</p>
  </li>
  <li>
    <p>在驱动程序上为在batchInterval期间创建的块创建了RDD。在batchInterval期间生成的块是RDD的分区。每个分区都是一个任务。 blockInterval == batchinterval意味着将创建一个分区，并且可能在本地对其进行处理。</p>
  </li>
  <li>
    <p>块上的映射任务在具有块的执行器（一个接收块，另一个接收块的执行器）中处理，而不管块间隔如何，除非启动非本地调度。更大的blockinterval意味着更大的block。很高的价值<code>spark.locality.wait</code>增加了在本地节点上处理块的机会。需要在这两个参数之间找到平衡，以确保较大的块在本地处理。</p>
  </li>
  <li>
    <p>可以不依赖于batchInterval和blockInterval，而可以通过调用来定义分区数<code>inputDstream.repartition(n)</code> 。这会随机重新随机排列RDD中的数据以创建n个分区。是的，以获得更大的并行度。虽然以洗牌为代价。RDD的处理由驾驶员的Jobscheduler安排为工作。在给定的时间点，只有一项作业处于活动状态。因此，如果一个作业正在执行，则其他作业将排队。</p>
  </li>
  <li>
    <p>如果您有两个dstream，则将形成两个RDD，并且将创建两个作业，这些作业将一个接一个地调度。为避免这种情况，可以合并两个dstream。这将确保为dstream的两个RDD形成单个unionRDD。然后将此unionRDD视为一项工作。但是，RDD的分区不受影响。</p>
  </li>
  <li>
    <p>如果批处理时间超过batchinterval，那么显然接收方的内存将开始填满，并最终引发抛出异常（很可能是BlockNotFoundException）。当前，无法暂停接收器。使用SparkConf配置<code>spark.streaming.receiver.maxRate</code> ，接收器的速率可能会受到限制。</p>
  </li>
</ul>

<hr>
<hr>

<h1 id="fault-tolerance-semantics">容错语义</h1>
<p>在本节中，我们将讨论发生故障时Spark Streaming应用程序的行为。</p>

<h2 class="no_toc" id="background">背景</h2>
<p>要了解Spark Streaming提供的语义，让我们记住Spark的RDD的基本容错语义。</p>

<ol>
  <li>RDD是一个不变的，确定性可重新计算的分布式数据集。每个RDD都会记住在容错输入数据集上用于创建它的确定性操作的沿袭。</li>
  <li>如果由于工作节点故障而导致RDD的任何分区丢失，则可以使用操作沿袭从原始容错数据集中重新计算该分区。</li>
  <li>假设所有RDD转换都是确定性的，则最终转换后的RDD中的数据将始终相同，而与Spark集群中的故障无关。</li>
</ol>

<p>Spark在容错文件系统（例如HDFS或S3）中的数据上运行。因此，从容错数据生成的所有RDD也是容错的。但是，Spark Streaming并非如此，因为大多数情况下是通过网络接收数据的（除非<code>fileStream</code>用来）。为了对所有生成的RDD实现相同的容错属性，将接收到的数据复制到集群中工作节点中的多个Spark执行程序中（默认复制因子为2）。这导致系统中发生故障时需要恢复的两种数据：</p>

<ol>
  <li><em>接收和复制的</em>数据-由于该数据的副本存在于其他节点之一上，因此该数据在单个工作节点发生故障时仍可幸免。</li>
  <li><em>已接收但已缓冲数据以进行复制</em> -由于不进行复制，因此恢复此数据的唯一方法是再次从源中获取数据。</li>
</ol>

<p>此外，我们应该关注两种故障：</p>

<ol>
  <li><em>工作节点的故障</em> -运行执行程序的任何工作节点都可能发生故障，并且这些节点上的所有内存中数据都将丢失。如果任何接收器在故障节点上运行，则其缓冲的数据将丢失。</li>
  <li><em>驱动程序节点发生故障</em> -如果运行Spark Streaming应用程序的驱动程序节点发生故障，则很显然SparkContext将丢失，并且所有执行程序及其内存中数据也会丢失。</li>
</ol>

<p>有了这些基础知识，让我们了解Spark Streaming的容错语义。</p>

<h2 class="no_toc" id="definitions">定义</h2>
<p>流系统的语义通常是根据系统可以处理每个记录多少次来捕获的。系统在所有可能的操作条件下（尽管有故障等）可以提供三种保证。</p>

<ol>
  <li><em>最多一次</em> ：每个记录将被处理一次或根本不被处理。</li>
  <li><em>至少一次</em> ：每个记录将被处理一次或多次。它比<em>最多一次</em>强<em>，</em>因为它确保不会丢失任何数据。但是可能有重复项。</li>
  <li><em>恰好一次</em> ：每个记录将被恰好处理一次-不会丢失任何数据，也不会多次处理任何数据。这显然是三者中最强有力的保证。</li>
</ol>

<h2 class="no_toc" id="basic-semantics">基本语义</h2>
<p>概括地说，在任何流处理系统中，处理数据都需要三个步骤。</p>

<ol>
  <li>
    <p><em>接收数据</em> ：使用接收器或其他方式从源接收数据。</p>
  </li>
  <li>
    <p><em>转换数据</em> ：使用DStream和RDD转换对接收到的数据进行转换。</p>
  </li>
  <li>
    <p><em>推送数据</em> ：将最终转换后的数据推送到外部系统，例如文件系统，数据库，仪表板等。</p>
  </li>
</ol>

<p>如果流应用程序必须获得端到端的精确一次保证，那么每个步骤都必须提供精确一次保证。也就是说，每条记录必须被接收一次，被转换一次，并被推送到下游系统一次。让我们在Spark Streaming的上下文中了解这些步骤的语义。</p>

<ol>
  <li>
    <p><em>接收数据</em> ：不同的输入源提供不同的保证。下一部分将对此进行详细讨论。</p>
  </li>
  <li>
    <p><em>转换数据</em> ：由于RDD提供的保证，所有接收到的数据将只处理<em>一次</em> 。即使出现故障，只要可以访问接收到的输入数据，最终转换后的RDD将始终具有相同的内容。</p>
  </li>
  <li>
    <p><em>推送数据</em> ：默认情况下，输出操作确保<em>至少一次</em>语义，因为它取决于输出操作的类型（是否为幂等）和下游系统的语义（是否支持事务）。但是用户可以实现自己的事务处理机制来实现<em>一次</em>语义。本节稍后将对此进行详细讨论。</p>
  </li>
</ol>

<h2 class="no_toc" id="semantics-of-received-data">接收数据的语义</h2>
<p>不同的输入源提供不同的保证，范围从<em>至少一次</em>到<em>恰好一次</em> 。阅读更多详细信息。</p>

<h3 class="no_toc" id="with-files">带文件</h3>
<p>如果所有输入数据已经存在于诸如HDFS之类的容错文件系统中，则Spark Streaming始终可以从任何故障中恢复并处理所有数据。这提供<em>了一次精确的</em>语义，这意味着无论发生什么故障，所有数据都会被精确处理一次。</p>

<h3 class="no_toc" id="with-receiver-based-sources">使用基于接收器的源</h3>
<p>对于基于接收方的输入源，容错语义取决于故障情况和接收方的类型。正如我们所讨论的<a href="#receiver-reliability">前面</a> ，有两种类型的接收器：</p>

<ol>
  <li><em>可靠的接收器</em> -这些接收器仅在确保已复制接收到的数据后才确认可靠的来源。如果这样的接收器发生故障，则源将不会收到对缓冲（未复制）数据的确认。因此，如果重新启动接收器，则源将重新发送数据，并且不会由于失败而丢失任何数据。</li>
  <li><em>不可靠的接收器</em> -这种接收器<em>不</em>发送确认，因此当由于工作程序或驱动程序故障而失败时， <em>可能</em>会丢失数据。</li>
</ol>

<p>根据所使用的接收器类型，我们可以实现以下语义。如果工作节点发生故障，那么可靠的接收器不会造成数据丢失。如果接收器不可靠，则接收到但未复制的数据可能会丢失。如果驱动程序节点发生故障，则除了这些丢失之外，所有已接收并复制到内存中的过去数据都将丢失。这将影响有状态转换的结果。</p>

<p>为了避免丢失过去收到的数据，Spark 1.2引入了预<em>写日志</em> ，该<em>日志</em>将收到的数据保存到容错存储中。通过<a href="#deploying-applications">启用预写日志</a>和可靠的接收器，数据丢失为零。就语义而言，它至少提供了一次保证。</p>

<p>下表总结了失败时的语义：</p>

<table class="table">
  <tbody><tr>
    <th style="width:30%">部署方案</th>
    <th>工人失败</th>
    <th>驱动故障</th>
  </tr>
  <tr>
    <td>
      <i>Spark 1.1或更早版本，</i>或<br>
      <i>没有预写日志的Spark 1.2或更高版本</i>
    </td>
    <td>接收器不可靠导致缓冲数据丢失<br>可靠的接收器实现零数据丢失<br>至少一次语义</td>
    <td>接收器不可靠导致缓冲数据丢失<br>所有接收者丢失的过去数据<br>未定义的语义</td>
  </tr>
  <tr>
    <td><i>Spark 1.2或更高版本具有预写日志</i></td>
    <td>可靠的接收器实现零数据丢失<br>至少一次语义</td>
    <td>可靠的接收器和文件可实现零数据丢失<br>至少一次语义</td>
  </tr>
  <tr>
    <td></td>
    <td></td>
    <td></td>
  </tr>
</tbody></table>

<h3 class="no_toc" id="with-kafka-direct-api">使用Kafka Direct API</h3>
<p>在Spark 1.3中，我们引入了新的Kafka Direct API，它可以确保Spark Streaming一次接收所有Kafka数据。同时，如果实施一次精确的输出操作，则可以实现端到端的一次精确保证。《 <a href="streaming-kafka-integration.html">Kafka集成指南》中</a>进一步讨论了这种方法。</p>

<h2 class="no_toc" id="semantics-of-output-operations">输出操作的语义</h2>
<p>输出操作（如<code>foreachRDD</code> ） <em>至少具有一次</em>语义，也就是说，在工作程序失败的情况下，转换后的数据可能多次写入外部实体。虽然可以使用以下命令将其保存到文件系统中<code>saveAs***Files</code>操作（因为文件只会被相同的数据覆盖），因此可能需要付出额外的努力才能实现一次语义。有两种方法。</p>

<ul>
  <li>
    <p><em>幂等更新</em> ：多次尝试总是写入相同的数据。例如， <code>saveAs***Files</code>总是将相同的数据写入生成的文件。</p>
  </li>
  <li>
    <p><em>事务性更新</em> ：所有更新都是以事务方式进行的，因此原子更新仅进行一次。一种做到这一点的方法如下。</p>

    <ul>
      <li>使用批处理时间（在<code>foreachRDD</code> ）和RDD的分区索引来创建标识符。该标识符唯一地标识流应用程序中的Blob数据。</li>
      <li>
        <p>使用标识符以事务方式（即，原子地一次）更新与此Blob的外部系统。也就是说，如果尚未提交标识符，则自动提交分区数据和标识符。否则，如果已经提交，则跳过更新。</p>

        <pre><code>dstream.foreachRDD { (rdd, time) =&gt;
  rdd.foreachPartition { partitionIterator =&gt;
    val partitionId = TaskContext.get.partitionId()
    val uniqueId = generateUniqueId(time.milliseconds, partitionId)
    // use this uniqueId to transactionally commit the data in partitionIterator
  }
}
</code></pre>
      </li>
    </ul>
  </li>
</ul>

<hr>
<hr>

<h1 id="where-to-go-from-here">从这往哪儿走</h1>
<ul>
  <li>其他指南<ul>
      <li><a href="streaming-kafka-integration.html">Kafka集成指南</a></li>
      <li><a href="streaming-kinesis-integration.html">Kinesis集成指南</a></li>
      <li><a href="streaming-custom-receivers.html">自定义接收器指南</a></li>
    </ul>
  </li>
  <li>第三方DStream数据源可以在<a href="https://spark.apache.org/third-party-projects.html">第三方项目中</a>找到</li>
  <li>API文档<ul>
      <li>Scala文档<ul>
          <li><a href="api/scala/index.html#org.apache.spark.streaming.StreamingContext">StreamingContext</a>和<a href="api/scala/index.html#org.apache.spark.streaming.dstream.DStream">DStream</a></li>
          <li><a href="api/scala/index.html#org.apache.spark.streaming.kafka.KafkaUtils$">KafkaUtils</a> ， <a href="api/scala/index.html#org.apache.spark.streaming.flume.FlumeUtils$">FlumeUtils</a> ， <a href="api/scala/index.html#org.apache.spark.streaming.kinesis.KinesisUtils$">KinesisUtils</a> ，</li>
        </ul>
      </li>
      <li>Java文档<ul>
          <li><a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html">JavaStreamingContext</a> ， <a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaDStream.html">JavaDStream</a>和<a href="api/java/index.html?org/apache/spark/streaming/api/java/JavaPairDStream.html">JavaPairDStream</a></li>
          <li><a href="api/java/index.html?org/apache/spark/streaming/kafka/KafkaUtils.html">KafkaUtils</a> ， <a href="api/java/index.html?org/apache/spark/streaming/flume/FlumeUtils.html">FlumeUtils</a> ， <a href="api/java/index.html?org/apache/spark/streaming/kinesis/KinesisUtils.html">KinesisUtils</a></li>
        </ul>
      </li>
      <li>Python文档<ul>
          <li><a href="api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext">StreamingContext</a>和<a href="api/python/pyspark.streaming.html#pyspark.streaming.DStream">DStream</a></li>
          <li><a href="api/python/pyspark.streaming.html#pyspark.streaming.kafka.KafkaUtils">KafkaUtils</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/streaming">Scala</a>和<a href="https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples/streaming">Java</a>和<a href="https://github.com/apache/spark/tree/master/examples/src/main/python/streaming">Python中的</a>更多示例</li>
  <li>描述Spark Streaming的<a href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-259.pdf">论文</a>和<a href="http://youtu.be/g171ndOHgJ0">视频</a> 。</li>
</ul>


                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-1.12.4.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    

</body></html>