<html class="no-js" ><!--<![endif]--><head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Spark Streaming + Kafka集成指南（Kafka代理版本0.10.0或更高版本）-Spark 2.4.4文档</title>
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        
        <!-- Google analytics script -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-32518208-2']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
        </script>
        

    </head>
    <body >
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html"><img src="img/spark-logo-hd.png" style="height:50px"></a> <span class="version">2.4.4</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">总览</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">编程指南<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">快速开始</a></li>
                                <li><a href="rdd-programming-guide.html">RDD，累加器，广播变量</a></li>
                                <li><a href="sql-programming-guide.html">SQL，数据框和数据集</a></li>
                                <li><a href="structured-streaming-programming-guide.html">结构化流</a></li>
                                <li><a href="streaming-programming-guide.html">火花流（DStreams）</a></li>
                                <li><a href="ml-guide.html">MLlib（机器学习）</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX（图形处理）</a></li>
                                <li><a href="sparkr.html">SparkR（Spark上的R）</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API文件<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">斯卡拉</a></li>
                                <li><a href="api/java/index.html">爪哇</a></li>
                                <li><a href="api/python/index.html">蟒蛇</a></li>
                                <li><a href="api/R/index.html">[R</a></li>
                                <li><a href="api/sql/index.html">SQL，内置函数</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">部署中<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">总览</a></li>
                                <li><a href="submitting-applications.html">提交申请</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark独立</a></li>
                                <li><a href="running-on-mesos.html">梅索斯</a></li>
                                <li><a href="running-on-yarn.html">纱</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">更多<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">组态</a></li>
                                <li><a href="monitoring.html">监控方式</a></li>
                                <li><a href="tuning.html">调音指南</a></li>
                                <li><a href="job-scheduling.html">作业调度</a></li>
                                <li><a href="security.html">安全</a></li>
                                <li><a href="hardware-provisioning.html">硬件配置</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">建筑火花</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">为Spark贡献</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">第三方项目</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v2.4.4</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                <div class="content" id="content">
                    
                        <h1 class="title">Spark Streaming + Kafka集成指南（Kafka代理版本0.10.0或更高版本）</h1>
                    

                    <p>Kafka 0.10的Spark Streaming集成在设计上类似于0.8 <a href="streaming-kafka-0-8-integration.html#approach-2-direct-approach-no-receivers">Direct Stream方法</a> 。它提供简单的并行性，Kafka分区和Spark分区之间的1：1对应关系以及对偏移量和元数据的访问。但是，由于较新的集成使用了<a href="http://kafka.apache.org/documentation.html#newconsumerapi">新的Kafka使用者API</a>而不是简单的API，因此用法上存在显着差异。集成的此版本标记为实验性的，因此API可能会发生更改。</p>

<h3 id="linking">连结中</h3>
<p>对于使用SBT / Maven项目定义的Scala / Java应用程序，将流应用程序与以下工件<a href="streaming-programming-guide.html#linking">链接</a> （有关更多信息，请参见主编程指南中的“ <a href="streaming-programming-guide.html#linking">链接”部分</a> ）。</p>

<pre><code>groupId = org.apache.spark
artifactId = spark-streaming-kafka-0-10_2.12
version = 2.4.4
</code></pre>

<p><strong>不要</strong>手动添加依赖项<code>org.apache.kafka</code>文物（例如<code>kafka-clients</code> ）。的<code>spark-streaming-kafka-0-10</code>工件已经具有适当的传递依赖关系，并且不同的版本可能难以诊断，因此不兼容。</p>

<h3 id="creating-a-direct-stream">创建直接流</h3>
<p>请注意，导入的名称空间包括版本org.apache.spark.streaming.kafka010</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">org.apache.kafka.clients.consumer.ConsumerRecord</span>
<span class="k">import</span> <span class="nn">org.apache.kafka.common.serialization.StringDeserializer</span>
<span class="k">import</span> <span class="nn">org.apache.spark.streaming.kafka010._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent</span>
<span class="k">import</span> <span class="nn">org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe</span>

<span class="k">val</span> <span class="n">kafkaParams</span> <span class="k">=</span> <span class="nc">Map</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Object</span><span class="o">](</span>
  <span class="s">&quot;bootstrap.servers&quot;</span> <span class="o">-&gt;</span> <span class="s">&quot;localhost:9092,anotherhost:9092&quot;</span><span class="o">,</span>
  <span class="s">&quot;key.deserializer&quot;</span> <span class="o">-&gt;</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">StringDeserializer</span><span class="o">],</span>
  <span class="s">&quot;value.deserializer&quot;</span> <span class="o">-&gt;</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">StringDeserializer</span><span class="o">],</span>
  <span class="s">&quot;group.id&quot;</span> <span class="o">-&gt;</span> <span class="s">&quot;use_a_separate_group_id_for_each_stream&quot;</span><span class="o">,</span>
  <span class="s">&quot;auto.offset.reset&quot;</span> <span class="o">-&gt;</span> <span class="s">&quot;latest&quot;</span><span class="o">,</span>
  <span class="s">&quot;enable.auto.commit&quot;</span> <span class="o">-&gt;</span> <span class="o">(</span><span class="kc">false</span><span class="k">:</span> <span class="kt">java.lang.Boolean</span><span class="o">)</span>
<span class="o">)</span>

<span class="k">val</span> <span class="n">topics</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="s">&quot;topicA&quot;</span><span class="o">,</span> <span class="s">&quot;topicB&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">stream</span> <span class="k">=</span> <span class="nc">KafkaUtils</span><span class="o">.</span><span class="n">createDirectStream</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">](</span>
  <span class="n">streamingContext</span><span class="o">,</span>
  <span class="nc">PreferConsistent</span><span class="o">,</span>
  <span class="nc">Subscribe</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">](</span><span class="n">topics</span><span class="o">,</span> <span class="n">kafkaParams</span><span class="o">)</span>
<span class="o">)</span>

<span class="n">stream</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">record</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">record</span><span class="o">.</span><span class="n">key</span><span class="o">,</span> <span class="n">record</span><span class="o">.</span><span class="n">value</span><span class="o">))</span></code></pre></figure>

    <p>流中的每个项目都是一个<a href="http://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/ConsumerRecord.html">ConsumerRecord</a></p>
  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kn">import</span> <span class="nn">java.util.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.SparkConf</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.TaskContext</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.function.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.streaming.api.java.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.streaming.kafka010.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.kafka.clients.consumer.ConsumerRecord</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.kafka.common.TopicPartition</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.kafka.common.serialization.StringDeserializer</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">scala.Tuple2</span><span class="o">;</span>

<span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Object</span><span class="o">&gt;</span> <span class="n">kafkaParams</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HashMap</span><span class="o">&lt;&gt;();</span>
<span class="n">kafkaParams</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;bootstrap.servers&quot;</span><span class="o">,</span> <span class="s">&quot;localhost:9092,anotherhost:9092&quot;</span><span class="o">);</span>
<span class="n">kafkaParams</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;key.deserializer&quot;</span><span class="o">,</span> <span class="n">StringDeserializer</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">kafkaParams</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;value.deserializer&quot;</span><span class="o">,</span> <span class="n">StringDeserializer</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">kafkaParams</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;group.id&quot;</span><span class="o">,</span> <span class="s">&quot;use_a_separate_group_id_for_each_stream&quot;</span><span class="o">);</span>
<span class="n">kafkaParams</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;auto.offset.reset&quot;</span><span class="o">,</span> <span class="s">&quot;latest&quot;</span><span class="o">);</span>
<span class="n">kafkaParams</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;enable.auto.commit&quot;</span><span class="o">,</span> <span class="kc">false</span><span class="o">);</span>

<span class="n">Collection</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">topics</span> <span class="o">=</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="s">&quot;topicA&quot;</span><span class="o">,</span> <span class="s">&quot;topicB&quot;</span><span class="o">);</span>

<span class="n">JavaInputDStream</span><span class="o">&lt;</span><span class="n">ConsumerRecord</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">stream</span> <span class="o">=</span>
  <span class="n">KafkaUtils</span><span class="o">.</span><span class="na">createDirectStream</span><span class="o">(</span>
    <span class="n">streamingContext</span><span class="o">,</span>
    <span class="n">LocationStrategies</span><span class="o">.</span><span class="na">PreferConsistent</span><span class="o">(),</span>
    <span class="n">ConsumerStrategies</span><span class="o">.&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span><span class="n">Subscribe</span><span class="o">(</span><span class="n">topics</span><span class="o">,</span> <span class="n">kafkaParams</span><span class="o">)</span>
  <span class="o">);</span>

<span class="n">stream</span><span class="o">.</span><span class="na">mapToPair</span><span class="o">(</span><span class="n">record</span> <span class="o">-&gt;</span> <span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;&gt;(</span><span class="n">record</span><span class="o">.</span><span class="na">key</span><span class="o">(),</span> <span class="n">record</span><span class="o">.</span><span class="na">value</span><span class="o">()));</span></code></pre></figure>

  </div>
</div>

<p>有关可能的kafkaParams的信息，请参阅<a href="http://kafka.apache.org/documentation.html#newconsumerconfigs">Kafka Consumer config docs</a> 。如果您的Spark批处理持续时间大于默认的Kafka心跳会话超时（30秒），请适当增加heartbeat.interval.ms和session.timeout.ms。对于大于5分钟的批处理，这将需要更改代理上的group.max.session.timeout.ms。请注意，示例将enable.auto.commit设置为false，有关讨论，请参见下面的<a href="streaming-kafka-0-10-integration.html#storing-offsets">存储偏移</a> 。</p>

<h3 id="locationstrategies">区位策略</h3>
<p>新的Kafka使用者API将消息预取到缓冲区中。因此，出于性能原因，Spark集成将缓存的使用者保留在执行程序上（而不是为每个批次重新创建它们），并且更喜欢在具有适当使用者的主机位置上计划分区，这一点很重要。</p>

<p>在大多数情况下，您应该使用<code>LocationStrategies.PreferConsistent</code>如上图所示。这将在可用执行程序之间平均分配分区。如果您的执行人与您的Kafka经纪人位于同一主机上，请使用<code>PreferBrokers</code> ，它将更喜欢在Kafka领导者上为该分区安排分区。最后，如果分区之间的负载有明显的偏差，请使用<code>PreferFixed</code> 。这使您可以指定分区到主机的显式映射（任何未指定的分区将使用一致的位置）。</p>

<p>使用者的缓存的默认最大大小为64。如果您希望处理的卡夫卡分区数量超过（64 *执行程序数）个，则可以通过以下方式更改此设置： <code>spark.streaming.kafka.consumer.cache.maxCapacity</code> 。</p>

<p>如果您想为Kafka用户禁用缓存，可以设置<code>spark.streaming.kafka.consumer.cache.enabled</code>至<code>false</code> 。</p>

<p>缓存由topicpartition和group.id设置密钥，因此请<strong>单独</strong>使用<code>group.id</code>每次致电<code>createDirectStream</code> 。</p>

<h3 id="consumerstrategies">消费者策略</h3>
<p>新的Kafka消费者API具有多种不同的方式来指定主题，其中一些方式需要大量的后对象实例化设置。 <code>ConsumerStrategies</code>提供了一种抽象，即使从检查点重新启动后，Spark仍可以获取正确配置的使用者。</p>

<p><code>ConsumerStrategies.Subscribe</code> ，如上所示，允许您订阅固定的主题集合。 <code>SubscribePattern</code>允许您使用正则表达式指定感兴趣的主题。请注意，与0.8集成不同，使用<code>Subscribe</code>要么<code>SubscribePattern</code>应该在运行流期间响应添加分区。最后， <code>Assign</code>允许您指定固定的分区集合。这三种策略都具有重载的构造函数，这些构造函数使您可以为特定分区指定起始偏移量。</p>

<p>如果您有上述选项无法满足的特定消费者设置需求， <code>ConsumerStrategy</code>是可以扩展的公共课程。</p>

<h3 id="creating-an-rdd">创建一个RDD</h3>
<p>如果您有一个更适合批处理的用例，则可以为定义的偏移量范围创建一个RDD。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// Import dependencies and create kafka params as in Create Direct Stream above</span>

<span class="k">val</span> <span class="n">offsetRanges</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span>
  <span class="c1">// topic, partition, inclusive starting offset, exclusive ending offset</span>
  <span class="nc">OffsetRange</span><span class="o">(</span><span class="s">&quot;test&quot;</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="mi">100</span><span class="o">),</span>
  <span class="nc">OffsetRange</span><span class="o">(</span><span class="s">&quot;test&quot;</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="mi">100</span><span class="o">)</span>
<span class="o">)</span>

<span class="k">val</span> <span class="n">rdd</span> <span class="k">=</span> <span class="nc">KafkaUtils</span><span class="o">.</span><span class="n">createRDD</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">](</span><span class="n">sparkContext</span><span class="o">,</span> <span class="n">kafkaParams</span><span class="o">,</span> <span class="n">offsetRanges</span><span class="o">,</span> <span class="nc">PreferConsistent</span><span class="o">)</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// Import dependencies and create kafka params as in Create Direct Stream above</span>

<span class="n">OffsetRange</span><span class="o">[]</span> <span class="n">offsetRanges</span> <span class="o">=</span> <span class="o">{</span>
  <span class="c1">// topic, partition, inclusive starting offset, exclusive ending offset</span>
  <span class="n">OffsetRange</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="s">&quot;test&quot;</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="mi">100</span><span class="o">),</span>
  <span class="n">OffsetRange</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="s">&quot;test&quot;</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="mi">100</span><span class="o">)</span>
<span class="o">};</span>

<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">ConsumerRecord</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">rdd</span> <span class="o">=</span> <span class="n">KafkaUtils</span><span class="o">.</span><span class="na">createRDD</span><span class="o">(</span>
  <span class="n">sparkContext</span><span class="o">,</span>
  <span class="n">kafkaParams</span><span class="o">,</span>
  <span class="n">offsetRanges</span><span class="o">,</span>
  <span class="n">LocationStrategies</span><span class="o">.</span><span class="na">PreferConsistent</span><span class="o">()</span>
<span class="o">);</span></code></pre></figure>

  </div>
</div>

<p>请注意，您不能使用<code>PreferBrokers</code> ，因为没有流，没有驱动程序端使用者可以自动为您查找代理元数据。使用<code>PreferFixed</code>必要时使用您自己的元数据查找。</p>

<h3 id="obtaining-offsets">获取偏移</h3>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">stream</span><span class="o">.</span><span class="n">foreachRDD</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>
  <span class="k">val</span> <span class="n">offsetRanges</span> <span class="k">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">HasOffsetRanges</span><span class="o">].</span><span class="n">offsetRanges</span>
  <span class="n">rdd</span><span class="o">.</span><span class="n">foreachPartition</span> <span class="o">{</span> <span class="n">iter</span> <span class="k">=&gt;</span>
    <span class="k">val</span> <span class="n">o</span><span class="k">:</span> <span class="kt">OffsetRange</span> <span class="o">=</span> <span class="n">offsetRanges</span><span class="o">(</span><span class="nc">TaskContext</span><span class="o">.</span><span class="n">get</span><span class="o">.</span><span class="n">partitionId</span><span class="o">)</span>
    <span class="n">println</span><span class="o">(</span><span class="s">s&quot;</span><span class="si">${</span><span class="n">o</span><span class="o">.</span><span class="n">topic</span><span class="si">}</span><span class="s"> </span><span class="si">${</span><span class="n">o</span><span class="o">.</span><span class="n">partition</span><span class="si">}</span><span class="s"> </span><span class="si">${</span><span class="n">o</span><span class="o">.</span><span class="n">fromOffset</span><span class="si">}</span><span class="s"> </span><span class="si">${</span><span class="n">o</span><span class="o">.</span><span class="n">untilOffset</span><span class="si">}</span><span class="s">&quot;</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">stream</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="n">OffsetRange</span><span class="o">[]</span> <span class="n">offsetRanges</span> <span class="o">=</span> <span class="o">((</span><span class="n">HasOffsetRanges</span><span class="o">)</span> <span class="n">rdd</span><span class="o">.</span><span class="na">rdd</span><span class="o">()).</span><span class="na">offsetRanges</span><span class="o">();</span>
  <span class="n">rdd</span><span class="o">.</span><span class="na">foreachPartition</span><span class="o">(</span><span class="n">consumerRecords</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="n">OffsetRange</span> <span class="n">o</span> <span class="o">=</span> <span class="n">offsetRanges</span><span class="o">[</span><span class="n">TaskContext</span><span class="o">.</span><span class="na">get</span><span class="o">().</span><span class="na">partitionId</span><span class="o">()];</span>
    <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span>
      <span class="n">o</span><span class="o">.</span><span class="na">topic</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot; &quot;</span> <span class="o">+</span> <span class="n">o</span><span class="o">.</span><span class="na">partition</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot; &quot;</span> <span class="o">+</span> <span class="n">o</span><span class="o">.</span><span class="na">fromOffset</span><span class="o">()</span> <span class="o">+</span> <span class="s">&quot; &quot;</span> <span class="o">+</span> <span class="n">o</span><span class="o">.</span><span class="na">untilOffset</span><span class="o">());</span>
  <span class="o">});</span>
<span class="o">});</span></code></pre></figure>

  </div>
</div>

<p>请注意， <code>HasOffsetRanges</code>只有在对的结果进行调用的第一个方法中完成后，才会成功<code>createDirectStream</code> ，此后不再介绍一系列方法。请注意，RDD分区和Kafka分区之间的一对一映射在经过任何改组或重新分区的方法后都不会保留，例如reduceByKey（）或window（）。</p>

<h3 id="storing-offsets">存储偏移</h3>
<p>发生故障时的Kafka交付语义取决于存储偏移量的方式和时间。Spark输出操作<a href="streaming-programming-guide.html#semantics-of-output-operations">至少一次</a> 。因此，如果您希望等效于一次语义，则必须在幂等输出之后存储偏移量，或者在输出中将偏移量存储在原子事务中。通过这种集成，您可以按照增加可靠性（和代码复杂度）的顺序使用3个选项来存储偏移量。</p>

<h4 id="checkpoints">检查点</h4>
<p>如果启用Spark <a href="streaming-programming-guide.html#checkpointing">checkpointing</a> ，则偏移量将存储在检查点中。这很容易实现，但是有缺点。您的输出操作必须是幂等的，因为您将获得重复的输出。交易不是一种选择。此外，如果您的应用程序代码已更改，则无法从检查点恢复。对于计划的升级，您可以通过与旧代码同时运行新代码来减轻这种情况（因为输出无论如何都需要等幂，因此它们不应冲突）。但是对于需要代码更改的计划外故障，除非有另一种方法来识别已知的良好起始偏移量，否则您将丢失数据。</p>

<h4 id="kafka-itself">卡夫卡本身</h4>
<p>Kafka具有偏移提交API，该API在特殊的Kafka主题中存储偏移量。默认情况下，新使用者将定期自动提交偏移量。这几乎肯定不是您想要的，因为由使用者成功轮询的消息可能尚未导致Spark输出操作，从而导致语义未定义。这就是为什么上面的流示例将“ enable.auto.commit”设置为false的原因。但是，您可以在知道已存储输出之后，使用以下命令将偏移量提交给Kafka。 <code>commitAsync</code> API。与检查点相比，它的好处是，无论您对应用程序代码进行的更改如何，Kafka都是持久存储。但是，Kafka不是事务性的，因此您的输出必须仍然是幂等的。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">stream</span><span class="o">.</span><span class="n">foreachRDD</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>
  <span class="k">val</span> <span class="n">offsetRanges</span> <span class="k">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">HasOffsetRanges</span><span class="o">].</span><span class="n">offsetRanges</span>

  <span class="c1">// some time later, after outputs have completed</span>
  <span class="n">stream</span><span class="o">.</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">CanCommitOffsets</span><span class="o">].</span><span class="n">commitAsync</span><span class="o">(</span><span class="n">offsetRanges</span><span class="o">)</span>
<span class="o">}</span></code></pre></figure>

    <p>与HasOffsetRanges一样，仅在createDirectStream的结果上调用时，对CanCommitOffsets的强制转换才能成功，而不是在转换后进行。commitAsync调用是线程安全的，但如果需要有意义的语义，则必须在输出之后发生。</p>
  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">stream</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="n">OffsetRange</span><span class="o">[]</span> <span class="n">offsetRanges</span> <span class="o">=</span> <span class="o">((</span><span class="n">HasOffsetRanges</span><span class="o">)</span> <span class="n">rdd</span><span class="o">.</span><span class="na">rdd</span><span class="o">()).</span><span class="na">offsetRanges</span><span class="o">();</span>

  <span class="c1">// some time later, after outputs have completed</span>
  <span class="o">((</span><span class="n">CanCommitOffsets</span><span class="o">)</span> <span class="n">stream</span><span class="o">.</span><span class="na">inputDStream</span><span class="o">()).</span><span class="na">commitAsync</span><span class="o">(</span><span class="n">offsetRanges</span><span class="o">);</span>
<span class="o">});</span></code></pre></figure>

  </div>
</div>

<h4 id="your-own-data-store">您自己的数据存储</h4>
<p>对于支持事务的数据存储，即使在失败情况下，将偏移与结果保存在同一事务中也可以使两者保持同步。如果您在检测重复或跳过的偏移量范围时很谨慎，则回滚事务可防止重复或丢失的消息影响结果。这相当于一次语义。即使是由于聚合而产生的输出，通常也很难使等幂，也可以使用这种策略。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// The details depend on your data store, but the general idea looks like this</span>

<span class="c1">// begin from the the offsets committed to the database</span>
<span class="k">val</span> <span class="n">fromOffsets</span> <span class="k">=</span> <span class="n">selectOffsetsFromYourDatabase</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">resultSet</span> <span class="k">=&gt;</span>
  <span class="k">new</span> <span class="nc">TopicPartition</span><span class="o">(</span><span class="n">resultSet</span><span class="o">.</span><span class="n">string</span><span class="o">(</span><span class="s">&quot;topic&quot;</span><span class="o">),</span> <span class="n">resultSet</span><span class="o">.</span><span class="n">int</span><span class="o">(</span><span class="s">&quot;partition&quot;</span><span class="o">))</span> <span class="o">-&gt;</span> <span class="n">resultSet</span><span class="o">.</span><span class="n">long</span><span class="o">(</span><span class="s">&quot;offset&quot;</span><span class="o">)</span>
<span class="o">}.</span><span class="n">toMap</span>

<span class="k">val</span> <span class="n">stream</span> <span class="k">=</span> <span class="nc">KafkaUtils</span><span class="o">.</span><span class="n">createDirectStream</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">](</span>
  <span class="n">streamingContext</span><span class="o">,</span>
  <span class="nc">PreferConsistent</span><span class="o">,</span>
  <span class="nc">Assign</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">](</span><span class="n">fromOffsets</span><span class="o">.</span><span class="n">keys</span><span class="o">.</span><span class="n">toList</span><span class="o">,</span> <span class="n">kafkaParams</span><span class="o">,</span> <span class="n">fromOffsets</span><span class="o">)</span>
<span class="o">)</span>

<span class="n">stream</span><span class="o">.</span><span class="n">foreachRDD</span> <span class="o">{</span> <span class="n">rdd</span> <span class="k">=&gt;</span>
  <span class="k">val</span> <span class="n">offsetRanges</span> <span class="k">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">HasOffsetRanges</span><span class="o">].</span><span class="n">offsetRanges</span>

  <span class="k">val</span> <span class="n">results</span> <span class="k">=</span> <span class="n">yourCalculation</span><span class="o">(</span><span class="n">rdd</span><span class="o">)</span>

  <span class="c1">// begin your transaction</span>

  <span class="c1">// update results</span>
  <span class="c1">// update offsets where the end of existing offsets matches the beginning of this batch of offsets</span>
  <span class="c1">// assert that offsets were updated correctly</span>

  <span class="c1">// end your transaction</span>
<span class="o">}</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// The details depend on your data store, but the general idea looks like this</span>

<span class="c1">// begin from the the offsets committed to the database</span>
<span class="n">Map</span><span class="o">&lt;</span><span class="n">TopicPartition</span><span class="o">,</span> <span class="n">Long</span><span class="o">&gt;</span> <span class="n">fromOffsets</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HashMap</span><span class="o">&lt;&gt;();</span>
<span class="k">for</span> <span class="o">(</span><span class="n">resultSet</span> <span class="o">:</span> <span class="n">selectOffsetsFromYourDatabase</span><span class="o">)</span>
  <span class="n">fromOffsets</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="k">new</span> <span class="n">TopicPartition</span><span class="o">(</span><span class="n">resultSet</span><span class="o">.</span><span class="na">string</span><span class="o">(</span><span class="s">&quot;topic&quot;</span><span class="o">),</span> <span class="n">resultSet</span><span class="o">.</span><span class="na">int</span><span class="o">(</span><span class="s">&quot;partition&quot;</span><span class="o">)),</span> <span class="n">resultSet</span><span class="o">.</span><span class="na">long</span><span class="o">(</span><span class="s">&quot;offset&quot;</span><span class="o">));</span>
<span class="o">}</span>

<span class="n">JavaInputDStream</span><span class="o">&lt;</span><span class="n">ConsumerRecord</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">stream</span> <span class="o">=</span> <span class="n">KafkaUtils</span><span class="o">.</span><span class="na">createDirectStream</span><span class="o">(</span>
  <span class="n">streamingContext</span><span class="o">,</span>
  <span class="n">LocationStrategies</span><span class="o">.</span><span class="na">PreferConsistent</span><span class="o">(),</span>
  <span class="n">ConsumerStrategies</span><span class="o">.&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span><span class="n">Assign</span><span class="o">(</span><span class="n">fromOffsets</span><span class="o">.</span><span class="na">keySet</span><span class="o">(),</span> <span class="n">kafkaParams</span><span class="o">,</span> <span class="n">fromOffsets</span><span class="o">)</span>
<span class="o">);</span>

<span class="n">stream</span><span class="o">.</span><span class="na">foreachRDD</span><span class="o">(</span><span class="n">rdd</span> <span class="o">-&gt;</span> <span class="o">{</span>
  <span class="n">OffsetRange</span><span class="o">[]</span> <span class="n">offsetRanges</span> <span class="o">=</span> <span class="o">((</span><span class="n">HasOffsetRanges</span><span class="o">)</span> <span class="n">rdd</span><span class="o">.</span><span class="na">rdd</span><span class="o">()).</span><span class="na">offsetRanges</span><span class="o">();</span>
  
  <span class="n">Object</span> <span class="n">results</span> <span class="o">=</span> <span class="n">yourCalculation</span><span class="o">(</span><span class="n">rdd</span><span class="o">);</span>

  <span class="c1">// begin your transaction</span>

  <span class="c1">// update results</span>
  <span class="c1">// update offsets where the end of existing offsets matches the beginning of this batch of offsets</span>
  <span class="c1">// assert that offsets were updated correctly</span>

  <span class="c1">// end your transaction</span>
<span class="o">});</span></code></pre></figure>

  </div>
</div>

<h3 id="ssl--tls">SSL / TLS</h3>
<p>新的Kafka使用者<a href="http://kafka.apache.org/documentation.html#security_ssl">支持SSL</a> 。要启用它，请在传递给之前适当地设置kafkaParams <code>createDirectStream</code> / <code>createRDD</code> 。请注意，这仅适用于Spark和Kafka经纪人之间的通信。您仍需负责分别<a href="security.html">保护</a> Spark节点间通信的安全。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">kafkaParams</span> <span class="k">=</span> <span class="nc">Map</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Object</span><span class="o">](</span>
  <span class="c1">// the usual params, make sure to change the port in bootstrap.servers if 9092 is not TLS</span>
  <span class="s">&quot;security.protocol&quot;</span> <span class="o">-&gt;</span> <span class="s">&quot;SSL&quot;</span><span class="o">,</span>
  <span class="s">&quot;ssl.truststore.location&quot;</span> <span class="o">-&gt;</span> <span class="s">&quot;/some-directory/kafka.client.truststore.jks&quot;</span><span class="o">,</span>
  <span class="s">&quot;ssl.truststore.password&quot;</span> <span class="o">-&gt;</span> <span class="s">&quot;test1234&quot;</span><span class="o">,</span>
  <span class="s">&quot;ssl.keystore.location&quot;</span> <span class="o">-&gt;</span> <span class="s">&quot;/some-directory/kafka.client.keystore.jks&quot;</span><span class="o">,</span>
  <span class="s">&quot;ssl.keystore.password&quot;</span> <span class="o">-&gt;</span> <span class="s">&quot;test1234&quot;</span><span class="o">,</span>
  <span class="s">&quot;ssl.key.password&quot;</span> <span class="o">-&gt;</span> <span class="s">&quot;test1234&quot;</span>
<span class="o">)</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Object</span><span class="o">&gt;</span> <span class="n">kafkaParams</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HashMap</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Object</span><span class="o">&gt;();</span>
<span class="c1">// the usual params, make sure to change the port in bootstrap.servers if 9092 is not TLS</span>
<span class="n">kafkaParams</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;security.protocol&quot;</span><span class="o">,</span> <span class="s">&quot;SSL&quot;</span><span class="o">);</span>
<span class="n">kafkaParams</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;ssl.truststore.location&quot;</span><span class="o">,</span> <span class="s">&quot;/some-directory/kafka.client.truststore.jks&quot;</span><span class="o">);</span>
<span class="n">kafkaParams</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;ssl.truststore.password&quot;</span><span class="o">,</span> <span class="s">&quot;test1234&quot;</span><span class="o">);</span>
<span class="n">kafkaParams</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;ssl.keystore.location&quot;</span><span class="o">,</span> <span class="s">&quot;/some-directory/kafka.client.keystore.jks&quot;</span><span class="o">);</span>
<span class="n">kafkaParams</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;ssl.keystore.password&quot;</span><span class="o">,</span> <span class="s">&quot;test1234&quot;</span><span class="o">);</span>
<span class="n">kafkaParams</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">&quot;ssl.key.password&quot;</span><span class="o">,</span> <span class="s">&quot;test1234&quot;</span><span class="o">);</span></code></pre></figure>

  </div>
</div>

<h3 id="deploying">部署中</h3>

<p>与任何Spark应用程序一样， <code>spark-submit</code>用于启动您的应用程序。</p>

<p>对于Scala和Java应用程序，如果您使用SBT或Maven进行项目管理，则打包<code>spark-streaming-kafka-0-10_2.12</code>及其依赖关系到应用程序JAR中。确保<code>spark-core_2.12</code>和<code>spark-streaming_2.12</code>被标记为<code>provided</code>依赖关系，如Spark安装中已经存在的依赖关系。然后使用<code>spark-submit</code>以启动您的应用程序（请参阅主要编程指南中的“ <a href="streaming-programming-guide.html#deploying-applications">部署”部分</a> ）。</p>



                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-1.12.4.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    

</body></html>