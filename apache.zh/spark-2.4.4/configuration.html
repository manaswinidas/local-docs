<html class="no-js" ><head></head><body >﻿<!--<![endif]-->
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>配置-Spark 2.4.4文档</title>
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        
        <!-- Google analytics script -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-32518208-2']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
        </script>
        

    
    
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html"><img src="img/spark-logo-hd.png" style="height:50px"></a> <span class="version">2.4.4</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">总览</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">编程指南<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">快速开始</a></li>
                                <li><a href="rdd-programming-guide.html">RDD，累加器，广播变量</a></li>
                                <li><a href="sql-programming-guide.html">SQL，数据框和数据集</a></li>
                                <li><a href="structured-streaming-programming-guide.html">结构化流</a></li>
                                <li><a href="streaming-programming-guide.html">火花流（DStreams）</a></li>
                                <li><a href="ml-guide.html">MLlib（机器学习）</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX（图形处理）</a></li>
                                <li><a href="sparkr.html">SparkR（Spark上的R）</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API文件<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">斯卡拉</a></li>
                                <li><a href="api/java/index.html">爪哇</a></li>
                                <li><a href="api/python/index.html">蟒蛇</a></li>
                                <li><a href="api/R/index.html">[R</a></li>
                                <li><a href="api/sql/index.html">SQL，内置函数</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">部署中<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">总览</a></li>
                                <li><a href="submitting-applications.html">提交申请</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark独立</a></li>
                                <li><a href="running-on-mesos.html">梅索斯</a></li>
                                <li><a href="running-on-yarn.html">纱</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">更多<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">组态</a></li>
                                <li><a href="monitoring.html">监控方式</a></li>
                                <li><a href="tuning.html">调音指南</a></li>
                                <li><a href="job-scheduling.html">作业调度</a></li>
                                <li><a href="security.html">安全</a></li>
                                <li><a href="hardware-provisioning.html">硬件配置</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">建筑火花</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">为Spark贡献</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">第三方项目</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v2.4.4</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                <div class="content" id="content">
                    
                        <h1 class="title">火花配置</h1>
                    

                    <ul id="markdown-toc">
  <li><a href="#spark-properties" id="markdown-toc-spark-properties">火花特性</a>    <ul>
      <li><a href="#dynamically-loading-spark-properties" id="markdown-toc-dynamically-loading-spark-properties">动态加载Spark属性</a></li>
      <li><a href="#viewing-spark-properties" id="markdown-toc-viewing-spark-properties">查看Spark属性</a></li>
      <li><a href="#available-properties" id="markdown-toc-available-properties">可用属性</a>        <ul>
          <li><a href="#application-properties" id="markdown-toc-application-properties">应用属性</a></li>
          <li><a href="#runtime-environment" id="markdown-toc-runtime-environment">运行环境</a></li>
          <li><a href="#shuffle-behavior" id="markdown-toc-shuffle-behavior">随机播放行为</a></li>
          <li><a href="#spark-ui" id="markdown-toc-spark-ui">Spark用户界面</a></li>
          <li><a href="#compression-and-serialization" id="markdown-toc-compression-and-serialization">压缩和序列化</a></li>
          <li><a href="#memory-management" id="markdown-toc-memory-management">内存管理</a></li>
          <li><a href="#execution-behavior" id="markdown-toc-execution-behavior">执行行为</a></li>
          <li><a href="#networking" id="markdown-toc-networking">联网</a></li>
          <li><a href="#scheduling" id="markdown-toc-scheduling">排程</a></li>
          <li><a href="#dynamic-allocation" id="markdown-toc-dynamic-allocation">动态分配</a></li>
          <li><a href="#security" id="markdown-toc-security">安全</a></li>
          <li><a href="#spark-sql" id="markdown-toc-spark-sql">Spark SQL</a></li>
          <li><a href="#spark-streaming" id="markdown-toc-spark-streaming">火花流</a></li>
          <li><a href="#sparkr" id="markdown-toc-sparkr">星火</a></li>
          <li><a href="#graphx" id="markdown-toc-graphx">GraphX</a></li>
          <li><a href="#deploy" id="markdown-toc-deploy">部署</a></li>
          <li><a href="#cluster-managers" id="markdown-toc-cluster-managers">集群管理者</a>            <ul>
              <li><a href="#yarn" id="markdown-toc-yarn">纱</a></li>
              <li><a href="#mesos" id="markdown-toc-mesos">梅索斯</a></li>
              <li><a href="#kubernetes" id="markdown-toc-kubernetes">Kubernetes</a></li>
              <li><a href="#standalone-mode" id="markdown-toc-standalone-mode">独立模式</a></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#environment-variables" id="markdown-toc-environment-variables">环境变量</a></li>
  <li><a href="#configuring-logging" id="markdown-toc-configuring-logging">配置日志</a></li>
  <li><a href="#overriding-configuration-directory" id="markdown-toc-overriding-configuration-directory">覆盖配置目录</a></li>
  <li><a href="#inheriting-hadoop-cluster-configuration" id="markdown-toc-inheriting-hadoop-cluster-configuration">继承Hadoop集群配置</a></li>
  <li><a href="#custom-hadoophive-configuration" id="markdown-toc-custom-hadoophive-configuration">自定义Hadoop / Hive配置</a></li>
</ul>

<p>Spark提供了三个位置来配置系统：</p>

<ul>
  <li><a href="#spark-properties">Spark属性</a>控制大多数应用程序参数，可以使用<a href="api/scala/index.html#org.apache.spark.SparkConf">SparkConf</a>对象或Java系统属性来设置。</li>
  <li><a href="#environment-variables">环境变量</a>可用于通过以下方式设置每台计算机的设置，例如IP地址。 <code>conf/spark-env.sh</code>每个节点上的脚本。</li>
  <li><a href="#configuring-logging">日志</a>可以通过配置<code>log4j.properties</code> 。</li>
</ul>

<h1 id="spark-properties">火花特性</h1>

<p>Spark属性控制大多数应用程序设置，并分别为每个应用程序配置。这些属性可以在传递给您的<a href="api/scala/index.html#org.apache.spark.SparkConf">SparkConf</a>上直接设置<code>SparkContext</code> 。 <code>SparkConf</code>允许您配置一些常用属性（例如主URL和应用程序名称），以及通过<code>set()</code>方法。例如，我们可以使用两个线程来初始化应用程序，如下所示：</p>

<p>请注意，我们使用local [2]运行，这意味着两个线程-表示“最小”并行度，这可以帮助检测仅当我们在分布式上下文中运行时才存在的错误。</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">()</span>
             <span class="o">.</span><span class="n">setMaster</span><span class="o">(</span><span class="s">&quot;local[2]&quot;</span><span class="o">)</span>
             <span class="o">.</span><span class="n">setAppName</span><span class="o">(</span><span class="s">&quot;CountingSheep&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">sc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">)</span></code></pre></figure>

<p>请注意，在本地模式下我们可以有多个线程，在类似Spark Streaming的情况下，我们实际上可能需要多个线程来防止任何类型的饥饿问题。</p>

<p>指定一定持续时间的属性应配置一个时间单位。接受以下格式：</p>

<pre><code>25ms (milliseconds)
5s (seconds)
10m or 10min (minutes)
3h (hours)
5d (days)
1y (years)
</code></pre>

<p>指定字节大小的属性应配置为大小单位。接受以下格式：</p>

<pre><code>1b (bytes)
1k or 1kb (kibibytes = 1024 bytes)
1m or 1mb (mebibytes = 1024 kibibytes)
1g or 1gb (gibibytes = 1024 mebibytes)
1t or 1tb (tebibytes = 1024 gibibytes)
1p or 1pb (pebibytes = 1024 tebibytes)
</code></pre>

<p>通常将不带单位的数字解释为字节，而将少数解释为KiB或MiB。请参阅各个配置属性的文档。在可能的情况下，希望指定单位。</p>

<h2 id="dynamically-loading-spark-properties">动态加载Spark属性</h2>

<p>在某些情况下，您可能要避免对某些配置进行硬编码<code>SparkConf</code> 。例如，如果您想使用不同的主机或不同的内存量运行同一应用程序。Spark允许您简单地创建一个空的conf：</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">sc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkContext</span><span class="o">(</span><span class="k">new</span> <span class="nc">SparkConf</span><span class="o">())</span></code></pre></figure>

<p>然后，您可以在运行时提供配置值：</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>./bin/spark-submit --name <span class="s2">&quot;My app&quot;</span> --master local<span class="o">[</span><span class="m">4</span><span class="o">]</span> --conf spark.eventLog.enabled<span class="o">=</span><span class="nb">false</span>
  --conf <span class="s2">&quot;spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps&quot;</span> myApp.jar</code></pre></figure>

<p>Spark外壳和<a href="submitting-applications.html"><code>spark-submit</code></a>该工具支持两种动态加载配置的方式。第一个是命令行选项，例如<code>--master</code> ，如上所示。 <code>spark-submit</code>可以使用接受任何Spark属性<code>--conf</code>标志，但对在启动Spark应用程序中起作用的属性使用特殊标志。跑步<code>./bin/spark-submit --help</code>将显示这些选项的完整列表。</p>

<p><code>bin/spark-submit</code>还将从中读取配置选项<code>conf/spark-defaults.conf</code> ，其中每行包含一个键和一个由空格分隔的值。例如：</p>

<pre><code>spark.master            spark://5.6.7.8:7077
spark.executor.memory   4g
spark.eventLog.enabled  true
spark.serializer        org.apache.spark.serializer.KryoSerializer
</code></pre>

<p>指定为标志或属性文件中的任何值都将传递到应用程序，并与通过SparkConf指定的那些值合并。直接在SparkConf上设置的属性具有最高优先级，然后将标志传递给<code>spark-submit</code>要么<code>spark-shell</code> ，然后选择<code>spark-defaults.conf</code>文件。自早期版本的Spark起，一些配置密钥已经重命名；在这种情况下，较旧的密钥名称仍会被接受，但其优先级低于较新密钥的任何实例。</p>

<p>Spark属性主要可以分为两种：一种与部署相关，例如“ spark.driver.memory”，“ spark.executor.instances”，当通过编程方式进行设置时，此类属性可能不会受到影响。 <code>SparkConf</code>在运行时，或者行为取决于您选择的集群管理器和部署模式，因此建议您通过配置文件或<code>spark-submit</code>命令行选项；另一个主要与Spark运行时控件有关，例如“ spark.task.maxFailures”，可以用任何一种方式设置这种属性。</p>

<h2 id="viewing-spark-properties">查看Spark属性</h2>

<p>应用程序Web UI位于<code>http://<driver>:4040</code>在“环境”选项卡中列出Spark属性。这是检查以确保正确设置属性的有用位置。请注意，只有通过以下方式明确指定的值<code>spark-defaults.conf</code> ， <code>SparkConf</code> ，否则将显示命令行。对于所有其他配置属性，您可以假定使用默认值。</p>

<h2 id="available-properties">可用属性</h2>

<p>控制内部设置的大多数属性具有合理的默认值。一些最常见的设置是：</p>

<h3 id="application-properties">应用属性</h3>

<table class="table">
<tbody><tr><th>物业名称</th><th>默认</th><th>含义</th></tr>
<tr>
  <td><code>spark.app.name</code></td>
  <td>（没有）</td>
  <td>您的应用程序的名称。这将出现在UI和日志数据中。
  </td>
</tr>
<tr>
  <td><code>spark.driver.cores</code></td>
  <td>1个</td>
  <td>仅在群集模式下，用于驱动程序进程的核心数。
  </td>
</tr>
<tr>
  <td><code>spark.driver.maxResultSize</code></td>
  <td>1克</td>
  <td>每个Spark操作（例如收集）的所有分区的序列化结果的总大小的限制（以字节为单位）。至少应为1M，否则应为0（无限制）。如果总大小超过此限制，作业将中止。具有较高的限制可能会导致驱动程序出现内存不足错误（取决于spark.driver.memory和JVM中对象的内存开销）。设置适当的限制可以保护驱动程序免受内存不足错误的影响。
  </td>
</tr>
<tr>
  <td><code>spark.driver.memory</code></td>
  <td>1克</td>
  <td>用于驱动程序进程的内存量，即用于初始化SparkContext的内存量，其格式与带有大小单位后缀（“ k”，“ m”，“ g”或“ t”）的JVM内存字符串相同（例如<code>512m</code> ， <code>2g</code> ）。
    <br>
    <em>注意：</em>在客户端模式下，不得通过<code>SparkConf</code>直接在您的应用程序中，因为驱动程序JVM已在此时启动。相反，请通过<code>--driver-memory</code>命令行选项或您的默认属性文件中。
  </td>
</tr>
<tr>
  <td><code>spark.driver.memoryOverhead</code></td>
  <td>driverMemory * 0.10，最小为384</td>
  <td>除非另有说明，否则在集群模式下（MiB）以群集模式为每个驱动程序分配的堆外内存量。这是内存，用于解决VM开销，内部字符串，其他本机开销等问题。随着容器大小的增加（通常为6％至10％），该内存通常会增加。YARN和Kubernetes当前支持此选项。
  </td>
</tr>
<tr>
  <td><code>spark.executor.memory</code></td>
  <td>1克</td>
  <td>每个执行程序进程要使用的内存量，格式与带有后缀大小单位（“ k”，“ m”，“ g”或“ t”）的JVM内存字符串相同（例如<code>512m</code> ， <code>2g</code> ）。
  </td>
</tr>
<tr>
 <td><code>spark.executor.pyspark.memory</code></td>
  <td>没有设置</td>
  <td>除非另有说明，否则在MiB中要分配给每个执行程序中PySpark的内存量。如果设置，执行者的PySpark内存将被限制为该数量。如果未设置，Spark将不会限制Python的内存使用，并且取决于应用程序来避免超出与其他非JVM进程共享的开销内存空间。当PySpark在YARN或Kubernetes中运行时，此内存将添加到执行程序资源请求中。注意：在不支持资源限制的平台（例如Windows）上，可能不会限制Python内存的使用。
  </td>
</tr>
<tr>
 <td><code>spark.executor.memoryOverhead</code></td>
  <td>executorMemory * 0.10，最小为384</td>
  <td>除非另有说明，否则每个执行器要分配的堆外内存量（以MiB为单位）。这是内存，用于解决VM开销，内部字符串，其他本机开销等问题。随着执行程序大小的增加（通常为6％到10％），内存通常会增加。YARN和Kubernetes当前支持此选项。
  </td>
</tr>
<tr>
  <td><code>spark.extraListeners</code></td>
  <td>（没有）</td>
  <td>以逗号分隔的实现的类列表<code>SparkListener</code> ;初始化SparkContext时，将创建这些类的实例并在Spark的侦听器总线中注册。如果类具有接受SparkConf的单参数构造函数，则该构造函数将被调用；否则，将调用零参数构造函数。如果找不到有效的构造函数，则SparkContext创建将失败，并出现异常。
  </td>
</tr>
<tr>
  <td><code>spark.local.dir</code></td>
  <td>/ tmp</td>
  <td>用于Spark中“临时”空间的目录，包括映射输出文件和存储在磁盘上的RDD。它应该在系统中的快速本地磁盘上。它也可以是不同磁盘上多个目录的逗号分隔列表。注意：在Spark 1.0和更高版本中，它将由集群管理器设置的SPARK_LOCAL_DIRS（独立），MESOS_SANDBOX（Mesos）或LOCAL_DIRS（YARN）环境变量覆盖。
  </td>
</tr>
<tr>
  <td><code>spark.logConf</code></td>
  <td>假</td>
  <td>启动SparkContext时，将有效的SparkConf记录为INFO。
  </td>
</tr>
<tr>
  <td><code>spark.master</code></td>
  <td>（没有）</td>
  <td>要连接的集群管理器。请参阅<a href="submitting-applications.html#master-urls">允许的主URL的</a>列表。
  </td>
</tr>
<tr>
  <td><code>spark.submit.deployMode</code></td>
  <td>（没有）</td>
  <td>Spark驱动程序的部署模式，可以是“客户端”或“集群”，这意味着可以在集群内部的一个节点上本地（“客户端”）或远程（“集群”）启动驱动程序。
  </td>
</tr>
<tr>
  <td><code>spark.log.callerContext</code></td>
  <td>（没有）</td>
  <td>在Yarn / HDFS上运行时将写入Yarn RM日志/ HDFS审核日志中的应用程序信息。它的长度取决于Hadoop配置<code>hadoop.caller.context.max.size</code> 。它应该简洁明了，通常最多可以包含50个字符。
  </td>
</tr>
<tr>
  <td><code>spark.driver.supervise</code></td>
  <td>假</td>
  <td>如果为true，则如果驱动程序以非零退出状态失败，则会自动重新启动驱动程序。仅在Spark独立模式或Mesos集群部署模式下有效。
  </td>
</tr>
</tbody></table>

<p>除此之外，以下属性也可用，并且在某些情况下可能有用：</p>

<h3 id="runtime-environment">运行环境</h3>

<table class="table">
<tbody><tr><th>物业名称</th><th>默认</th><th>含义</th></tr>
<tr>
  <td><code>spark.driver.extraClassPath</code></td>
  <td>（没有）</td>
  <td>要附加在驱动程序的类路径前面的其他类路径条目。

    <br><em>注意：</em>在客户端模式下，不得通过<code>SparkConf</code>直接在您的应用程序中，因为驱动程序JVM已在此时启动。相反，请通过<code>--driver-class-path</code>命令行选项或您的默认属性文件中。
  </td>
</tr>
<tr>
  <td><code>spark.driver.extraJavaOptions</code></td>
  <td>（没有）</td>
  <td>传递给驱动程序的一串额外的JVM选项。例如，GC设置或其他日志记录。请注意，使用此选项设置最大堆大小（-Xmx）设置是非法的。最大堆大小设置可以使用<code>spark.driver.memory</code>在群集模式下并通过<code>--driver-memory</code>客户端模式下的命令行选项。

    <br><em>注意：</em>在客户端模式下，不得通过<code>SparkConf</code>直接在您的应用程序中，因为驱动程序JVM已在此时启动。相反，请通过<code>--driver-java-options</code>命令行选项或您的默认属性文件中。
  </td>
</tr>
<tr>
  <td><code>spark.driver.extraLibraryPath</code></td>
  <td>（没有）</td>
  <td>设置启动驱动程序JVM时要使用的特殊库路径。<br><em>注意：</em>在客户端模式下，不得通过<code>SparkConf</code>直接在您的应用程序中，因为驱动程序JVM已在此时启动。相反，请通过<code>--driver-library-path</code>命令行选项或您的默认属性文件中。
  </td>
</tr>
<tr>
  <td><code>spark.driver.userClassPathFirst</code></td>
  <td>假</td>
  <td>（实验性的）在驱动程序中加载类时，是否使用户添加的jar优先于Spark自己的jar。此功能可用于缓解Spark依赖项和用户依赖项之间的冲突。目前，这是一项实验功能。仅在群集模式下使用。
  </td>
</tr>
<tr>
  <td><code>spark.executor.extraClassPath</code></td>
  <td>（没有）</td>
  <td>额外的类路径条目，以附加到执行者的类路径。主要是为了与旧版本的Spark向后兼容。用户通常不需要设置此选项。
  </td>
</tr>
<tr>
  <td><code>spark.executor.extraJavaOptions</code></td>
  <td>（没有）</td>
  <td>一串额外的JVM选项传递给执行者。例如，GC设置或其他日志记录。请注意，使用此选项设置Spark属性或最大堆大小（-Xmx）设置是非法的。应该使用SparkConf对象或与spark-submit脚本一起使用的spark-defaults.conf文件设置Spark属性。可以通过spark.executor.memory设置最大堆大小设置。下列符号（如果存在）将被内插：将被应用程序ID替换，并将被执行者ID替换。例如，要在/ tmp中启用详细gc日志记录到以应用程序的执行者ID命名的文件，请传递以下值： <code>-verbose:gc -Xloggc:/tmp/-.gc</code>
  </td>
</tr>
<tr>
  <td><code>spark.executor.extraLibraryPath</code></td>
  <td>（没有）</td>
  <td>设置启动执行程序JVM时要使用的特殊库路径。
  </td>
</tr>
<tr>
  <td><code>spark.executor.logs.rolling.maxRetainedFiles</code></td>
  <td>（没有）</td>
  <td>设置系统将保留的最新滚动日志文件的数量。较旧的日志文件将被删除。默认禁用。
  </td>
</tr>
<tr>
  <td><code>spark.executor.logs.rolling.enableCompression</code></td>
  <td>假</td>
  <td>启用执行程序日志压缩。如果启用，则滚动的执行程序日志将被压缩。默认禁用。
  </td>
</tr>
<tr>
  <td><code>spark.executor.logs.rolling.maxSize</code></td>
  <td>（没有）</td>
  <td>设置文件的最大大小（以字节为单位），执行程序日志将通过该大小翻转。默认情况下禁用滚动。看到<code>spark.executor.logs.rolling.maxRetainedFiles</code>用于自动清洁旧日志。
  </td>
</tr>
<tr>
  <td><code>spark.executor.logs.rolling.strategy</code></td>
  <td>（没有）</td>
  <td>设置滚动执行程序日志的策略。默认情况下，它是禁用的。可以将其设置为“时间”（基于时间的滚动）或“尺寸”（基于尺寸的滚动）。对于“时间”，请使用<code>spark.executor.logs.rolling.time.interval</code>设置滚动间隔。对于“大小”，请使用<code>spark.executor.logs.rolling.maxSize</code>设置滚动的最大文件大小。
  </td>
</tr>
<tr>
  <td><code>spark.executor.logs.rolling.time.interval</code></td>
  <td>日常</td>
  <td>设置执行程序日志将被翻转的时间间隔。默认情况下禁用滚动。有效值为<code>daily</code> ， <code>hourly</code> ， <code>minutely</code>或以秒为单位的任何间隔。看到<code>spark.executor.logs.rolling.maxRetainedFiles</code>用于自动清洁旧日志。
  </td>
</tr>
<tr>
  <td><code>spark.executor.userClassPathFirst</code></td>
  <td>假</td>
  <td>（实验性）与以下功能相同<code>spark.driver.userClassPathFirst</code> ，但适用于执行程序实例。
  </td>
</tr>
<tr>
  <td><code>spark.executorEnv.[EnvironmentVariableName]</code></td>
  <td>（没有）</td>
  <td>添加环境变量指定<code>EnvironmentVariableName</code>进入执行程序流程。用户可以指定多个以设置多个环境变量。
  </td>
</tr>
<tr>
  <td><code>spark.redaction.regex</code></td>
  <td>（？i）秘密|密码</td>
  <td>正则表达式决定驱动程序和执行程序环境中的哪些Spark配置属性和环境变量包含敏感信息。当此正则表达式与属性键或值匹配时，将从环境UI和各种日志（如YARN和事件日志）中删除该值。
  </td>
</tr>
<tr>
  <td><code>spark.python.profile</code></td>
  <td>假</td>
  <td>在Python worker中启用分析，配置文件结果将显示为<code>sc.show_profiles()</code> ，否则它将在驱动程序退出之前显示。也可以通过以下方式将其转储到磁盘中<code>sc.dump_profiles(path)</code> 。如果某些配置文件结果是手动显示的，则在退出驱动程序之前不会自动显示。默认情况下<code>pyspark.profiler.BasicProfiler</code>将被使用，但是可以通过将探查器类作为参数传递给<code>SparkContext</code>构造函数。
  </td>
</tr>
<tr>
  <td><code>spark.python.profile.dump</code></td>
  <td>（没有）</td>
  <td>在驱动程序退出之前，用于转储概要文件结果的目录。结果将作为每个RDD的单独文件转储。他们可以被加载<code>pstats.Stats()</code> 。如果指定此选项，则配置文件结果将不会自动显示。
  </td>
</tr>
<tr>
  <td><code>spark.python.worker.memory</code></td>
  <td>512m</td>
  <td>聚合期间每个python worker进程要使用的内存量，格式与带有后缀大小单位（“ k”，“ m”，“ g”或“ t”）的JVM内存字符串的格式相同（例如<code>512m</code> ， <code>2g</code> ）。如果聚合过程中使用的内存超过该数量，则会将数据溢出到磁盘中。
  </td>
</tr>
<tr>
  <td><code>spark.python.worker.reuse</code></td>
  <td>真正</td>
  <td>是否重复使用Python worker。如果是，它将使用固定数量的Python工作者，不需要为每个任务fork（）一个Python进程。如果存在大量广播，这将非常有用，那么对于每个任务，都不需要将广播从JVM传输到Python worker。
  </td>
</tr>
<tr>
  <td><code>spark.files</code></td>
  <td></td>
  <td>以逗号分隔的文件列表，将其放置在每个执行程序的工作目录中。允许使用小球。
  </td>
</tr>
<tr>
  <td><code>spark.submit.pyFiles</code></td>
  <td></td>
  <td>以逗号分隔的.zip，.egg或.py文件列表，用于Python应用程序的PYTHONPATH上。允许使用小球。
  </td>
</tr>
<tr>
  <td><code>spark.jars</code></td>
  <td></td>
  <td>以逗号分隔的jar列表，包括在驱动程序和执行程序的类路径中。允许使用小球。
  </td>
</tr>
<tr>
  <td><code>spark.jars.packages</code></td>
  <td></td>
  <td>以逗号分隔的jar的Maven坐标列表，包括在驱动程序和执行程序的类路径中。坐标应为groupId：artifactId：version。如果<code>spark.jars.ivySettings</code>给定的工件将根据文件中的配置进行解析，否则将在本地Maven存储库中搜索工件，然后在Maven Central中搜索，最后在命令行选项中指定任何其他远程存储库<code>--repositories</code> 。有关更多详细信息，请参见<a href="submitting-applications.html#advanced-dependency-management">高级依赖关系管理</a> 。
  </td>
</tr>
<tr>
  <td><code>spark.jars.excludes</code></td>
  <td></td>
  <td>以逗号分隔的groupId：artifactId列表，在解析提供的依赖项时要排除<code>spark.jars.packages</code>避免依赖冲突。
  </td>
</tr>
<tr>
  <td><code>spark.jars.ivy</code></td>
  <td></td>
  <td>指定常春藤用户目录的路径，用于本地常春藤缓存和来自的软件包文件<code>spark.jars.packages</code> 。这将覆盖常春藤属性<code>ivy.default.ivy.user.dir</code>默认为〜/ .ivy2。
  </td>
</tr>
<tr>
  <td><code>spark.jars.ivySettings</code></td>
  <td></td>
  <td>常春藤设置文件的路径，以自定义使用以下命令指定的罐子的分辨率<code>spark.jars.packages</code>而不是内置默认值，例如maven central。命令行选项提供的其他存储库<code>--repositories</code>要么<code>spark.jars.repositories</code>也将包括在内。对于允许Spark从防火墙后面解析伪像很有用，例如，通过内部伪像服务器（如Artifactory）。有关设置文件格式的详细信息，请参见<a href="http://ant.apache.org/ivy/history/latest-milestone/settings.html">设置文件。</a>
  </td>
</tr>
 <tr>
  <td><code>spark.jars.repositories</code></td>
  <td></td>
  <td>以逗号分隔的其他远程存储库列表，用于搜索用给出的Maven坐标<code>--packages</code>要么<code>spark.jars.packages</code> 。
  </td>
</tr>
<tr>
  <td><code>spark.pyspark.driver.python</code></td>
  <td></td>
  <td>在驱动程序中用于PySpark的Python二进制可执行文件。（默认为<code>spark.pyspark.python</code> ）</td>
</tr>
<tr>
  <td><code>spark.pyspark.python</code></td>
  <td></td>
  <td>在驱动程序和执行程序中都可用于PySpark的Python二进制可执行文件。
  </td>
</tr>
</tbody></table>

<h3 id="shuffle-behavior">随机播放行为</h3>

<table class="table">
<tbody><tr><th>物业名称</th><th>默认</th><th>含义</th></tr>
<tr>
  <td><code>spark.reducer.maxSizeInFlight</code></td>
  <td>48m</td>
  <td>除非另外指定，否则从MiB中同时从每个reduce任务获取的映射输出的最大大小。由于每个输出都需要我们创建一个缓冲区来接收它，因此这代表每个reduce任务的固定内存开销，因此，除非您有大量内存，否则请使其保持较小。
  </td>
</tr>
<tr>
  <td><code>spark.reducer.maxReqsInFlight</code></td>
  <td>整数最大值</td>
  <td>此配置限制了在任意给定点获取块的远程请求的数量。当群集中的主机数量增加时，可能导致到一个或多个节点的大量入站连接，从而导致工作负载在负载下失败。通过允许它限制获取请求的数量，可以缓解这种情况。
  </td>
</tr>
<tr>
  <td><code>spark.reducer.maxBlocksInFlightPerAddress</code></td>
  <td>整数最大值</td>
  <td>此配置限制了每个缩减任务从给定主机端口获取的远程块的数量。当在单个提取中或同时从给定地址请求大量块时，这可能会使服务执行程序或节点管理器崩溃。启用外部随机播放时，这对于减少节点管理器上的负载特别有用。您可以通过将其设置为较低的值来缓解此问题。
  </td>
</tr>
<tr>
  <td><code>spark.maxRemoteBlockSizeFetchToMem</code></td>
  <td>最大最大值-512</td>
  <td>当块的大小超过此阈值（以字节为单位）时，远程块将被提取到磁盘。这是为了避免占用太多内存的巨大请求。默认情况下，仅对大于2GB的块启用此功能，因为无论可用的资源如何，都无法将其直接提取到内存中。但是可以将其调低到更低的值（例如200m），以避免在较小的块上也使用过多的内存。请注意，此配置将影响混洗获取和块管理器远程块获取。对于启用了外部随机播放服务的用户，仅当外部随机播放服务比Spark 2.2更高时才能使用此功能。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.compress</code></td>
  <td>真正</td>
  <td>是否压缩地图输出文件。通常是个好主意。压缩将使用<code>spark.io.compression.codec</code> 。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.file.buffer</code></td>
  <td>32k</td>
  <td>除非另有说明，否则每个随机播放文件输出流的内存缓冲区大小（以KiB为单位）。这些缓冲区减少了在创建中间混排文件时进行的磁盘查找和系统调用的数量。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.io.maxRetries</code></td>
  <td>3</td>
  <td>（仅限Netty）如果将其设置为非零值，则将自动重试由于与IO相关的异常而失败的访存。这种重试逻辑有助于在长时间的GC暂停或瞬态网络连接问题时稳定大型混洗。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.io.numConnectionsPerPeer</code></td>
  <td>1个</td>
  <td>（仅限Netty）主机之间的连接被重用，以减少大型群集的连接建立。对于具有许多硬盘和少量主机的群集，这可能导致并发不足以使所有磁盘饱和，因此用户可以考虑增加此值。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.io.preferDirectBufs</code></td>
  <td>真正</td>
  <td>（仅限Netty）堆外缓冲区用于减少随机播放和高速缓存块传输期间的垃圾收集。对于堆外内存受到严格限制的环境，用户可能希望将其关闭以强制将Netty的所有分配都堆上。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.io.retryWait</code></td>
  <td>5秒</td>
  <td>（仅适用于Netty）两次重试之间等待的时间。默认情况下，重试导致的最大延迟为15秒，计算公式为<code>maxRetries * retryWait</code> 。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.io.backLog</code></td>
  <td>64</td>
  <td>随机播放服务的接受队列的长度。对于大型应用程序，可能需要增加该值，以便在服务无法在短时间内到达大量连接的情况下不断开传入连接。无论洗牌服务本身在哪里运行，都需要对其进行配置（可能在应用程序外部）（请参阅<code>spark.shuffle.service.enabled</code>下面的选项）。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.service.enabled</code></td>
  <td>假</td>
  <td>启用外部随机播放服务。此服务保留执行者编写的随机播放文件，以便可以安全地删除执行者。如果必须启用<code>spark.dynamicAllocation.enabled</code>是真的”。必须设置外部随机播放服务才能启用它。有关更多信息，请参阅<a href="job-scheduling.html#configuration-and-setup">动态分配配置和设置文档</a> 。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.service.port</code></td>
  <td>7337</td>
  <td>外部随机播放服务将在其上运行的端口。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.service.index.cache.size</code></td>
  <td>100m</td>
  <td>高速缓存条目限制为指定的内存占用空间（以字节为单位）。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.maxChunksBeingTransferred</code></td>
  <td>长。MAX_VALUE</td>
  <td>随机播放服务上允许同时传输的最大块数。请注意，当达到最大数量时，新的传入连接将关闭。客户端将根据随机重试配置重试（请参阅<code>spark.shuffle.io.maxRetries</code>和<code>spark.shuffle.io.retryWait</code> ），如果达到了这些限制，则任务将失败，并导致抓取失败。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.sort.bypassMergeThreshold</code></td>
  <td>200</td>
  <td>（高级）在基于排序的混洗管理器中，如果没有地图端聚合并且最多有很多缩减分区，请避免合并排序数据。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.spill.compress</code></td>
  <td>真正</td>
  <td>是否压缩随机播放中溢出的数据。压缩将使用<code>spark.io.compression.codec</code> 。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.accurateBlockThreshold</code></td>
  <td>100 * 1024 * 1024</td>
  <td>阈值，以字节为单位，高于该阈值时，HighlyCompressedMapStatus中的混洗块的大小将被准确记录。通过避免在提取混洗块时低估混洗块的大小，有助于防止OOM。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.registration.timeout</code></td>
  <td>5000</td>
  <td>注册到外部随机播放服务的超时时间（以毫秒为单位）。
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.registration.maxAttempts</code></td>
  <td>3</td>
  <td>当我们无法注册到外部随机播放服务时，我们将重试maxAttempts次。
  </td>
</tr>
</tbody></table>

<h3 id="spark-ui">Spark用户界面</h3>

<table class="table">
<tbody><tr><th>物业名称</th><th>默认</th><th>含义</th></tr>
<tr>
  <td><code>spark.eventLog.logBlockUpdates.enabled</code></td>
  <td>假</td>
  <td>是否记录每次块更新的事件，如果<code>spark.eventLog.enabled</code>是真的。*警告*：这将大大增加事件日志的大小。
  </td>
</tr>
<tr>
  <td><code>spark.eventLog.longForm.enabled</code></td>
  <td>假</td>
  <td>如果为true，请在事件日志中使用长格式的呼叫站点。否则，请使用简写形式。
  </td>
</tr>
<tr>
  <td><code>spark.eventLog.compress</code></td>
  <td>假</td>
  <td>是否压缩记录的事件，如果<code>spark.eventLog.enabled</code>是真的。压缩将使用<code>spark.io.compression.codec</code> 。
  </td>
</tr>
<tr>
  <td><code>spark.eventLog.dir</code></td>
  <td>文件：/// tmp / spark-events</td>
  <td>记录Spark事件的基本目录（如果有） <code>spark.eventLog.enabled</code>是真的。在此基本目录中，Spark为每个应用程序创建一个子目录，并将特定于该应用程序的事件记录在此目录中。用户可能希望将其设置为统一位置，例如HDFS目录，以便历史记录服务器可以读取历史记录文件。
  </td>
</tr>
<tr>
  <td><code>spark.eventLog.enabled</code></td>
  <td>假</td>
  <td>是否记录Spark事件，这对于在应用程序完成后重建Web UI很有用。
  </td>
</tr>
<tr>
  <td><code>spark.eventLog.overwrite</code></td>
  <td>假</td>
  <td>是否覆盖任何现有文件。
  </td>
</tr>
<tr>
  <td><code>spark.eventLog.buffer.kb</code></td>
  <td>100k</td>
  <td>除非另有说明，否则在写入输出流时要使用的缓冲区大小，以KiB为单位。
  </td>
</tr>
<tr>
  <td><code>spark.ui.dagGraph.retainedRootRDDs</code></td>
  <td>整数最大值</td>
  <td>垃圾回收之前，Spark UI和状态API记住多少个DAG图形节点。
  </td>
</tr>
<tr>
  <td><code>spark.ui.enabled</code></td>
  <td>真正</td>
  <td>是否为Spark应用程序运行Web UI。
  </td>
</tr>
<tr>
  <td><code>spark.ui.killEnabled</code></td>
  <td>真正</td>
  <td>允许从Web UI中取消作业和阶段。</td>
</tr>
<tr>
  <td><code>spark.ui.liveUpdate.period</code></td>
  <td>100毫秒</td>
  <td>多久更新一次活动实体。 -1表示重播应用程序时“从不更新”，这意味着只会发生最后一次写入。对于实时应用程序，这避免了我们在快速处理传入任务事件时无需进行的一些操作。
  </td>
</tr>
<tr>
  <td><code>spark.ui.liveUpdate.minFlushPeriod</code></td>
  <td>1秒</td>
  <td>刷新过时的UI数据之前经过的最短时间。当传入的任务事件不经常触发时，这可以避免UI失效。
  </td>
</tr>
<tr>
  <td><code>spark.ui.port</code></td>
  <td>4040</td>
  <td>应用程序仪表板的端口，其中显示内存和工作负载数据。
  </td>
</tr>
<tr>
  <td><code>spark.ui.retainedJobs</code></td>
  <td>1000</td>
  <td>垃圾收集之前，Spark UI和状态API会记住多少个作业。这是目标最大值，在某些情况下可以保留较少的元素。
  </td>
</tr>
<tr>
  <td><code>spark.ui.retainedStages</code></td>
  <td>1000</td>
  <td>垃圾收集之前，Spark UI和状态API会记住多少阶段。这是目标最大值，在某些情况下可以保留较少的元素。
  </td>
</tr>
<tr>
  <td><code>spark.ui.retainedTasks</code></td>
  <td>100000</td>
  <td>垃圾回收之前，Spark UI和状态API会记住多少个任务。这是目标最大值，在某些情况下可以保留较少的元素。
  </td>
</tr>
<tr>
  <td><code>spark.ui.reverseProxy</code></td>
  <td>假</td>
  <td>启用运行Spark Master作为工作人员和应用程序UI的反向代理。在此模式下，Spark主站将反向代理工作人员和应用程序用户界面以启用访问权限，而无需直接访问其主机。请谨慎使用它，因为将无法直接访问工作人员和应用程序用户界面，因此您只能通过spark master / proxy公共URL访问它们。此设置影响集群中运行的所有工作程序和应用程序UI，并且必须在所有工作程序，驱动程序和主服务器上进行设置。
  </td>
</tr>
<tr>
  <td><code>spark.ui.reverseProxyUrl</code></td>
  <td></td>
  <td>这是代理运行所在的URL。该URL用于运行在Spark Master前面的代理。当运行身份验证代理（例如OAuth代理）时，这很有用。确保这是一个完整的URL，包括方案（http / https）和访问您的代理的端口。
  </td>
</tr>
<tr>
  <td><code>spark.ui.showConsoleProgress</code></td>
  <td>假</td>
  <td>在控制台中显示进度栏。进度条显示了运行时间超过500毫秒的阶段的进度。如果多个阶段同时运行，则多个进度条将显示在同一行上。
    <br>
    <em>注意：</em>在Shell环境中，spark.ui.showConsoleProgress的默认值为true。
  </td>
</tr>
<tr>
  <td><code>spark.worker.ui.retainedExecutors</code></td>
  <td>1000</td>
  <td>垃圾回收之前，Spark UI和状态API会记住多少完成的执行程序。
  </td>
</tr>
<tr>
  <td><code>spark.worker.ui.retainedDrivers</code></td>
  <td>1000</td>
  <td>垃圾回收之前，Spark UI和状态API会记住多少完成的驱动程序。
  </td>
</tr>
<tr>
  <td><code>spark.sql.ui.retainedExecutions</code></td>
  <td>1000</td>
  <td>垃圾回收之前，Spark UI和状态API会记住多少完成的执行。
  </td>
</tr>
<tr>
  <td><code>spark.streaming.ui.retainedBatches</code></td>
  <td>1000</td>
  <td>垃圾收集之前，Spark UI和状态API会记住多少批成品。
  </td>
</tr>
<tr>
  <td><code>spark.ui.retainedDeadExecutors</code></td>
  <td>100</td>
  <td>垃圾回收之前，Spark UI和状态API会记住多少个死执行器。
  </td>
</tr>
<tr>
  <td><code>spark.ui.filters</code></td>
  <td>没有</td>
  <td>以逗号分隔的过滤器类名称列表，以应用于Spark Web UI。该过滤器应该是标准的<a href="http://docs.oracle.com/javaee/6/api/javax/servlet/Filter.html">javax servlet Filter</a> 。

    <br>过滤器参数也可以在配置中通过设置表单的配置条目来指定<code>spark.<class name of filter>.param.<param name>=<value></code>

    <br>例如：<br><code>spark.ui.filters=com.test.filter1</code>
    <br><code>spark.com.test.filter1.param.name1=foo</code>
    <br><code>spark.com.test.filter1.param.name2=bar</code>
  </td>
</tr>
<tr>
  <td><code>spark.ui.requestHeaderSize</code></td>
  <td>8k</td>
  <td>除非另外指定，否则HTTP请求标头的最大允许大小（以字节为单位）。此设置也适用于Spark History Server。
  </td>
</tr>
</tbody></table>

<h3 id="compression-and-serialization">压缩和序列化</h3>

<table class="table">
<tbody><tr><th>物业名称</th><th>默认</th><th>含义</th></tr>
<tr>
  <td><code>spark.broadcast.compress</code></td>
  <td>真正</td>
  <td>发送前是否压缩广播变量。通常是个好主意。压缩将使用<code>spark.io.compression.codec</code> 。
  </td>
</tr>
<tr>
  <td><code>spark.checkpoint.compress</code></td>
  <td>假</td>
  <td>是否压缩RDD检查点。通常是个好主意。压缩将使用<code>spark.io.compression.codec</code> 。
   </td>
</tr>
<tr>
  <td><code>spark.io.compression.codec</code></td>
  <td>lz4</td>
  <td>用于压缩内部数据的编解码器，例如RDD分区，事件日志，广播变量和随机播放输出。默认情况下，Spark提供四种编解码器： <code>lz4</code> ， <code>lzf</code> ， <code>snappy</code>和<code>zstd</code> 。您也可以使用完全限定的类名来指定编解码器，例如<code>org.apache.spark.io.LZ4CompressionCodec</code> ， <code>org.apache.spark.io.LZFCompressionCodec</code> ， <code>org.apache.spark.io.SnappyCompressionCodec</code>和<code>org.apache.spark.io.ZStdCompressionCodec</code> 。
  </td>
</tr>
<tr>
  <td><code>spark.io.compression.lz4.blockSize</code></td>
  <td>32k</td>
  <td>在使用LZ4压缩编解码器的情况下，LZ4压缩中使用的块大小（以字节为单位）。当使用LZ4时，减小此块的大小也会降低随机播放存储器的使用率。
  </td>
</tr>
<tr>
  <td><code>spark.io.compression.snappy.blockSize</code></td>
  <td>32k</td>
  <td>在使用Snappy压缩编解码器的情况下，在Snappy压缩中使用的块大小（以字节为单位）。使用Snappy时，减小此块的大小也会降低随机播放存储器的使用率。
  </td>
</tr>
<tr>
  <td><code>spark.io.compression.zstd.level</code></td>
  <td>1个</td>
  <td>Zstd压缩编解码器的压缩级别。增加压缩级别将导致更好的压缩，但会占用更多的CPU和内存。
  </td>
</tr>
<tr>
  <td><code>spark.io.compression.zstd.bufferSize</code></td>
  <td>32k</td>
  <td>在使用Zstd压缩编解码器的情况下，Zstd压缩中使用的缓冲区大小（以字节为单位）。减小此大小将降低使用Zstd时的混洗内存使用量，但由于过多的JNI调用开销，可能会增加压缩成本。
  </td>
</tr>
<tr>
  <td><code>spark.kryo.classesToRegister</code></td>
  <td>（没有）</td>
  <td>如果您使用Kryo序列化，请提供用逗号分隔的自定义类名称列表，以向Kryo注册。有关更多详细信息，请参见<a href="tuning.html#data-serialization">调整指南</a> 。
  </td>
</tr>
<tr>
  <td><code>spark.kryo.referenceTracking</code></td>
  <td>真正</td>
  <td>使用Kryo序列化数据时是否跟踪对同一对象的引用，如果对象图具有循环，则这是必要的；如果它们包含相同对象的多个副本，则对于提高效率很有用。如果您不是这种情况，可以禁用它以提高性能。
  </td>
</tr>
<tr>
  <td><code>spark.kryo.registrationRequired</code></td>
  <td>假</td>
  <td>是否要求向Kryo注册。如果设置为“ true”，则如果未注册的类被序列化，Kryo将引发异常。如果设置为false（默认值），Kryo将与每个对象一起写入未注册的类名称。编写类名称可能会导致很大的性能开销，因此启用此选项可以严格执行以下操作：用户没有从注册中省略类。
  </td>
</tr>
<tr>
  <td><code>spark.kryo.registrator</code></td>
  <td>（没有）</td>
  <td>如果您使用Kryo序列化，请提供一个用逗号分隔的类列表，这些类将向Kryo注册您的自定义类。如果您需要以自定义方式注册类（例如，指定自定义字段序列化程序），则此属性很有用。除此以外<code>spark.kryo.classesToRegister</code>更简单。应该将其设置为扩展的类<a href="api/scala/index.html#org.apache.spark.serializer.KryoRegistrator"><code>KryoRegistrator</code></a> 。有关更多详细信息，请参见<a href="tuning.html#data-serialization">调整指南</a> 。
  </td>
</tr>
<tr>
  <td><code>spark.kryo.unsafe</code></td>
  <td>假</td>
  <td>是否使用不安全的Kryo串行器。使用基于不安全的IO可以大大提高速度。</td>
</tr>
<tr>
  <td><code>spark.kryoserializer.buffer.max</code></td>
  <td>64m</td>
  <td>除非另有说明，否则Kryo序列化缓冲区的最大允许大小（以MiB为单位）。它必须大于您尝试序列化的任何对象，并且必须小于2048m。如果在Kryo中收到“超出缓冲区限制”异常，请增加此值。
  </td>
</tr>
<tr>
  <td><code>spark.kryoserializer.buffer</code></td>
  <td>64k</td>
  <td>除非另有说明，否则Kryo序列化缓冲区的初始大小（以KiB为单位）。请注意，每个工作线程上每个<i>内核</i>将有一个缓冲区。该缓冲区将增长到<code>spark.kryoserializer.buffer.max</code>如果需要的话。
  </td>
</tr>
<tr>
  <td><code>spark.rdd.compress</code></td>
  <td>假</td>
  <td>是否压缩序列化的RDD分区（例如用于<code>StorageLevel.MEMORY_ONLY_SER</code>在Java和Scala中，或<code>StorageLevel.MEMORY_ONLY</code>在Python中）。可以节省大量空间，但要花一些额外的CPU时间。压缩将使用<code>spark.io.compression.codec</code> 。
  </td>
</tr>
<tr>
  <td><code>spark.serializer</code></td>
  <td>org.apache.spark.serializer。<br>JavaSerializer</td>
  <td>用于序列化将通过网络发送或需要以序列化形式缓存的对象的类。Java序列化的默认设置可与任何Serializable Java对象一起使用，但是速度很慢，因此我们建议<a href="tuning.html">使用<code>org.apache.spark.serializer.KryoSerializer</code>并</a>在需要速度时<a href="tuning.html">配置Kryo序列化</a> 。可以是以下的任何子类<a href="api/scala/index.html#org.apache.spark.serializer.Serializer"><code>org.apache.spark.Serializer</code></a> 。
  </td>
</tr>
<tr>
  <td><code>spark.serializer.objectStreamReset</code></td>
  <td>100</td>
  <td>使用org.apache.spark.serializer进行序列化时。JavaSerializer，序列化程序缓存对象以防止写入冗余数据，但是这会停止垃圾回收这些对象。通过调用“重置”，您可以从序列化程序中清除该信息，并允许收集旧对象。要关闭此定期重置，请将其设置为-1。默认情况下，它将每100个对象重置序列化程序。
  </td>
</tr>
</tbody></table>

<h3 id="memory-management">内存管理</h3>

<table class="table">
<tbody><tr><th>物业名称</th><th>默认</th><th>含义</th></tr>
<tr>
  <td><code>spark.memory.fraction</code></td>
  <td>0.6</td>
  <td>用于执行和存储的部分（堆空间-300MB）。数值越低，溢出和缓存数据逐出的频率就越高。此配置的目的是为稀疏的，异常大的记录留出用于内部元数据，用户数据结构以及大小估计不精确的内存。建议将其保留为默认值。有关更多详细信息，包括有关增加此值时正确调整JVM垃圾回收的重要信息，请参阅<a href="tuning.html#memory-management-overview">此描述</a> 。
  </td>
</tr>
<tr>
  <td><code>spark.memory.storageFraction</code></td>
  <td>0.5</td>
  <td>不受驱逐的存储内存量，表示为被预留的区域大小的一部分<code>spark.memory.fraction</code> 。这越高，可用于执行的工作内存就越少，任务可能会更频繁地溢出到磁盘上。建议将其保留为默认值。有关更多详细信息，请参见此<a href="tuning.html#memory-management-overview">描述</a> 。
  </td>
</tr>
<tr>
  <td><code>spark.memory.offHeap.enabled</code></td>
  <td>假</td>
  <td>如果为true，Spark将尝试将堆外内存用于某些操作。如果启用了堆外内存使用，则<code>spark.memory.offHeap.size</code>必须是积极的。
  </td>
</tr>
<tr>
  <td><code>spark.memory.offHeap.size</code></td>
  <td>0</td>
  <td>可用于堆外分配的绝对内存量（以字节为单位）。此设置对堆内存使用没有影响，因此，如果执行者的总内存消耗必须在某个硬限制内，那么请确保相应地缩小JVM堆大小。在以下情况下必须将此值设置为正值<code>spark.memory.offHeap.enabled=true</code> 。
  </td>
</tr>
<tr>
  <td><code>spark.memory.useLegacyMode</code></td>
  <td>假</td>
  <td>是否启用Spark 1.5及之前版本的旧版内存管理模式。传统模式将堆空间严格划分为固定大小的区域，如果未调整应用程序，则可能导致过多的溢出。除非已启用，否则不会读取以下不推荐使用的内存分数配置：<code>spark.shuffle.memoryFraction</code><br>
    <code>spark.storage.memoryFraction</code><br>
    <code>spark.storage.unrollFraction</code>
  </td>
</tr>
<tr>
  <td><code>spark.shuffle.memoryFraction</code></td>
  <td>0.2</td>
  <td>（不建议使用）仅当以下情况时才读<code>spark.memory.useLegacyMode</code>已启用。在洗牌期间用于聚合和共组的Java堆的分数。在任何给定时间，用于随机播放的所有内存映射的集合大小都受到此限制的限制，超出此限制，内容将开始溢出到磁盘。如果经常发生泄漏，请考虑增加此值，但要以<code>spark.storage.memoryFraction</code> 。
  </td>
</tr>
<tr>
  <td><code>spark.storage.memoryFraction</code></td>
  <td>0.6</td>
  <td>（不建议使用）仅当以下情况时才读<code>spark.memory.useLegacyMode</code>已启用。用于Spark的内存缓存的Java堆的分数。该值不应大于JVM中对象的“旧”代，默认情况下，该对象的堆大小为0.6，但是如果您配置自己的旧代大小，则可以增加它。
  </td>
</tr>
<tr>
  <td><code>spark.storage.unrollFraction</code></td>
  <td>0.2</td>
  <td>（不建议使用）仅当以下情况时才读<code>spark.memory.useLegacyMode</code>已启用。的分数<code>spark.storage.memoryFraction</code>用于展开内存中的块。当没有足够的可用存储空间来完全展开新块时，通过删除现有块来动态分配该块。
  </td>
</tr>
<tr>
  <td><code>spark.storage.replication.proactive</code></td>
  <td>假</td>
  <td>为RDD块启用主动块复制。如果存在任何现有可用副本，则会补充由于执行程序故障而丢失的缓存RDD块副本。这将尝试使块的复制级别达到初始编号。
  </td>
</tr>
<tr>
  <td><code>spark.cleaner.periodicGC.interval</code></td>
  <td>30分钟</td>
  <td>控制触发垃圾回收的频率。<br><br>仅当弱引用被垃圾回收时，此上下文清除器才触发清除。在具有大型驱动程序JVM的长时间运行的应用程序中，驱动程序上的内存压力很小，这可能很少发生或根本没有发生。根本不清理可能会导致执行程序过一会儿耗尽磁盘空间。
  </td>
</tr>
<tr>
  <td><code>spark.cleaner.referenceTracking</code></td>
  <td>真正</td>
  <td>启用或禁用上下文清理。
  </td>
</tr>
<tr>
  <td><code>spark.cleaner.referenceTracking.blocking</code></td>
  <td>真正</td>
  <td>控制清洁线程是否应阻止执行清除任务（除了随机播放以外，其由<code>spark.cleaner.referenceTracking.blocking.shuffle</code> Spark属性）。
  </td>
</tr>
<tr>
  <td><code>spark.cleaner.referenceTracking.blocking.shuffle</code></td>
  <td>假</td>
  <td>控制清洗线程是否应在随机清洗任务上阻塞。
  </td>
</tr>
<tr>
  <td><code>spark.cleaner.referenceTracking.cleanCheckpoints</code></td>
  <td>假</td>
  <td>控制如果引用超出范围，是否清理检查点文件。
  </td>
</tr>
</tbody></table>

<h3 id="execution-behavior">执行行为</h3>

<table class="table">
<tbody><tr><th>物业名称</th><th>默认</th><th>含义</th></tr>
<tr>
  <td><code>spark.broadcast.blockSize</code></td>
  <td>4m</td>
  <td>每块的大小<code>TorrentBroadcastFactory</code> ，除非另有说明，否则在KiB中。太大的值会降低广播期间的并行度（使其变慢）；但是，如果太小， <code>BlockManager</code>可能会影响性能。
  </td>
</tr>
<tr>
  <td><code>spark.broadcast.checksum</code></td>
  <td>真正</td>
  <td>是否启用广播校验和。如果启用，广播将包含校验和，以计算和发送更多数据为代价，可以帮助检测损坏的块。如果网络具有其他机制来保证广播期间数据不会被破坏，则可以禁用它。
  </td>
</tr>
<tr>
  <td><code>spark.executor.cores</code></td>
  <td>在YARN模式下为1，在独立模式和Mesos粗粒度模式下工作器上的所有可用内核。
  </td>
  <td>每个执行程序上要使用的内核数。在独立模式和Mesos粗粒度模式下，有关更多详细信息，请参阅<a href="spark-standalone.html#Executors Scheduling">此描述</a> 。
  </td>
</tr>
<tr>
  <td><code>spark.default.parallelism</code></td>
  <td>对于分布式洗牌操作，例如<code>reduceByKey</code>和<code>join</code> ，是父RDD中最多的分区数。对于像<code>parallelize</code>没有父RDD，则取决于集群管理器：<ul>
      <li>本地模式：本地计算机上的内核数</li>
      <li>Mesos细颗粒模式：8</li>
      <li>其他：所有执行程序节点上的核心总数或2，以较大者为准</li>
    </ul>
  </td>
  <td>像这样的转换返回的RDD中的默认分区数<code>join</code> ， <code>reduceByKey</code>和<code>parallelize</code>未由用户设置时。
  </td>
</tr>
<tr>
    <td><code>spark.executor.heartbeatInterval</code></td>
    <td>10秒</td>
    <td>每个执行者对驾驶员的心跳之间的间隔。心跳使驾驶员知道执行器仍在运行，并使用正在进行的任务的度量标准对其进行更新。 spark.executor.heartbeatInterval应该大大小于spark.network.timeout</td>
</tr>
<tr>
  <td><code>spark.files.fetchTimeout</code></td>
  <td>60年代</td>
  <td>从驱动程序获取通过SparkContext.addFile（）添加的文件时使用的通信超时。
  </td>
</tr>
<tr>
  <td><code>spark.files.useFetchCache</code></td>
  <td>真正</td>
  <td>如果设置为true（默认值），则文件提取将使用由属于同一应用程序的执行程序共享的本地缓存，当在同一主机上运行许多执行程序时，可以提高任务启动性能。如果设置为false，则将禁用这些缓存优化，并且所有执行程序都将获取其自己的文件副本。为了使用驻留在NFS文件系统上的Spark本地目录，可以禁用此优化（有关更多详细信息，请参阅<a href="https://issues.apache.org/jira/browse/SPARK-6313">SPARK-6313</a> ）。
  </td>
</tr>
<tr>
  <td><code>spark.files.overwrite</code></td>
  <td>假</td>
  <td>当目标文件存在且其内容与源文件不匹配时，是否覆盖通过SparkContext.addFile（）添加的文件。
  </td>
</tr>
<tr>
  <td><code>spark.files.maxPartitionBytes</code></td>
  <td>134217728（128 MB）</td>
  <td>读取文件时打包到单个分区中的最大字节数。
  </td>
</tr>
<tr>
  <td><code>spark.files.openCostInBytes</code></td>
  <td>4194304（4 MB）</td>
  <td>可以同时扫描以字节数衡量的打开文件的估计成本。将多个文件放入分区时使用。最好高估一下，然后文件较小的分区将比文件较大的分区更快。
  </td>
</tr>
<tr>
    <td><code>spark.hadoop.cloneConf</code></td>
    <td>假</td>
    <td>如果设置为true，则克隆一个新的Hadoop <code>Configuration</code>每个任务的对象。应该启用此选项以解决<code>Configuration</code>线程安全问题（有关更多详细信息，请参阅<a href="https://issues.apache.org/jira/browse/SPARK-2546">SPARK-2546</a> ）。默认情况下禁用此功能，以避免不受这些问题影响的作业出现意外的性能下降。</td>
</tr>
<tr>
    <td><code>spark.hadoop.validateOutputSpecs</code></td>
    <td>真正</td>
    <td>如果设置为true，则验证saveAsHadoopFile和其他变体中使用的输出规范（例如，检查输出目录是否已存在）。可以禁用此选项以使由于预先存在的输出目录而导致的异常静音。我们建议用户不要禁用此功能，除非试图与以前版本的Spark兼容。只需使用Hadoop的FileSystem API即可手动删除输出目录。对于通过Spark Streaming的StreamingContext生成的作业，此设置将被忽略，因为在检查点恢复期间可能需要将数据重写到预先存在的输出目录中。</td>
</tr>
<tr>
  <td><code>spark.storage.memoryMapThreshold</code></td>
  <td>2m</td>
  <td>从磁盘读取块时，Spark内存在其上映射的块大小（以字节为单位）。这可以防止Spark从内存映射很小的块。通常，对于接近或小于操作系统页面大小的块，内存映射具有高开销。
  </td>
</tr>
<tr>
  <td><code>spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version</code></td>
  <td>1个</td>
  <td>文件输出提交者算法版本，有效算法版本号：1或2。根据<a href="https://issues.apache.org/jira/browse/MAPREDUCE-4815">MAPREDUCE-4815</a> ，版本2可能具有更好的性能，但是版本1在某些情况下可以更好地处理故障。
  </td>
</tr>
</tbody></table>

<h3 id="networking">联网</h3>

<table class="table">
<tbody><tr><th>物业名称</th><th>默认</th><th>含义</th></tr>
<tr>
  <td><code>spark.rpc.message.maxSize</code></td>
  <td>128</td>
  <td>“控制平面”通信中允许的最大消息大小（以MB为单位）；通常仅适用于在执行程序和驱动程序之间发送的地图输出大小信息。如果您正在运行具有数千个map的作业，请增加此数量，并减少任务并查看有关RPC消息大小的消息。
  </td>
</tr>
<tr>
  <td><code>spark.blockManager.port</code></td>
  <td>（随机）</td>
  <td>所有块管理器都可以监听的端口。这些在驱动程序和执行程序上都存在。
  </td>
</tr>
<tr>
  <td><code>spark.driver.blockManager.port</code></td>
  <td>（spark.blockManager.port的值）</td>
  <td>特定于驱动程序的端口，块管理器可以侦听，以防其无法使用与执行程序相同的配置。
  </td>
</tr>
<tr>
  <td><code>spark.driver.bindAddress</code></td>
  <td>（spark.driver.host的值）</td>
  <td>绑定侦听套接字的主机名或IP地址。此配置将覆盖SPARK_LOCAL_IP环境变量（请参见下文）。

    <br>它还允许将与本地地址不同的地址发布给执行者或外部系统。例如，在通过桥接网络运行容器时，这很有用。为了使其正常工作，需要从容器的主机转发驱动程序使用的不同端口（RPC，块管理器和UI）。
  </td>
</tr>
<tr>
  <td><code>spark.driver.host</code></td>
  <td>（本地主机名）</td>
  <td>驱动程序的主机名或IP地址。这用于与执行者和独立的Master通信。
  </td>
</tr>
<tr>
  <td><code>spark.driver.port</code></td>
  <td>（随机）</td>
  <td>供驾驶员监听的端口。这用于与执行者和独立的Master通信。
  </td>
</tr>
<tr>
  <td><code>spark.rpc.io.backLog</code></td>
  <td>64</td>
  <td>RPC服务器的接受队列的长度。对于大型应用程序，可能需要增加该值，以便在短时间内有大量连接到达时不会丢失传入连接。
  </td>
</tr>
<tr>
  <td><code>spark.network.timeout</code></td>
  <td>120秒</td>
  <td>所有网络交互的默认超时。此配置将代替<code>spark.core.connection.ack.wait.timeout</code> ， <code>spark.storage.blockManagerSlaveTimeoutMs</code> ， <code>spark.shuffle.io.connectionTimeout</code> ， <code>spark.rpc.askTimeout</code>要么<code>spark.rpc.lookupTimeout</code>如果未配置它们。
  </td>
</tr>
<tr>
  <td><code>spark.port.maxRetries</code></td>
  <td>16</td>
  <td>绑定到端口之前放弃的最大重试次数。如果为端口指定了特定值（非0），则每次后续重试都会在重试之前将上一次尝试中使用的端口加1。从本质上讲，这使它可以尝试从指定的起始端口到port + maxRetries的一系列端口。
  </td>
</tr>
<tr>
  <td><code>spark.rpc.numRetries</code></td>
  <td>3</td>
  <td>放弃RPC任务之前要重试的次数。RPC任务将在此次数的大多数时间运行。
  </td>
</tr>
<tr>
  <td><code>spark.rpc.retry.wait</code></td>
  <td>3秒</td>
  <td>重试之前RPC询问操作等待的持续时间。
  </td>
</tr>
<tr>
  <td><code>spark.rpc.askTimeout</code></td>
  <td><code>spark.network.timeout</code></td>
  <td>RPC询问操作等待超时之前的持续时间。
  </td>
</tr>
<tr>
  <td><code>spark.rpc.lookupTimeout</code></td>
  <td>120秒</td>
  <td>RPC远程端点查找操作在超时之前要等待的持续时间。
  </td>
</tr>
<tr>
  <td><code>spark.core.connection.ack.wait.timeout</code></td>
  <td><code>spark.network.timeout</code></td>
  <td>在超时并放弃之前，连接等待确认发生多长时间。为避免长时间停顿（如GC）引起的不希望的超时，可以设置较大的值。
  </td>
</tr>
</tbody></table>

<h3 id="scheduling">排程</h3>

<table class="table">
<tbody><tr><th>物业名称</th><th>默认</th><th>含义</th></tr>
<tr>
  <td><code>spark.cores.max</code></td>
  <td>（没有设置）</td>
  <td><a href="running-on-mesos.html#mesos-run-modes">在“粗粒度”共享模式下在</a> <a href="spark-standalone.html">独立部署群集</a>或<a href="running-on-mesos.html#mesos-run-modes">Mesos群集上运行时</a> ，跨群集（而不是从每台计算机）请求应用程序的最大CPU内核数量。如果未设置，则默认值为<code>spark.deploy.defaultCores</code>在Spark的独立集群管理器上运行，或者在Mesos上运行无限（所有可用内核）。
  </td>
</tr>
<tr>
  <td><code>spark.locality.wait</code></td>
  <td>3秒</td>
  <td>在放弃并在不太本地的节点上启动它之前，要等待多长时间才能启动本地数据任务。相同的等待将用于逐步遍历多个位置级别（过程本地，节点本地，机架本地，然后任意）。还可以通过设置自定义每个级别的等待时间<code>spark.locality.wait.node</code>等等。如果您的任务很长并且位置不佳，则应增加此设置，但是默认设置通常效果很好。
  </td>
</tr>
<tr>
  <td><code>spark.locality.wait.node</code></td>
  <td>spark.locality.wait</td>
  <td>定制位置等待节点位置。例如，您可以将其设置为0以跳过节点的位置并立即搜索机架的位置（如果您的集群具有机架信息）。
  </td>
</tr>
<tr>
  <td><code>spark.locality.wait.process</code></td>
  <td>spark.locality.wait</td>
  <td>自定义位置，等待过程位置。这会影响尝试访问特定执行程序进程中的缓存数据的任务。
  </td>
</tr>
<tr>
  <td><code>spark.locality.wait.rack</code></td>
  <td>spark.locality.wait</td>
  <td>定制位置，等待机架位置。
  </td>
</tr>
<tr>
  <td><code>spark.scheduler.maxRegisteredResourcesWaitingTime</code></td>
  <td>30秒</td>
  <td>在调度开始之前等待资源注册的最长时间。
  </td>
</tr>
<tr>
  <td><code>spark.scheduler.minRegisteredResourcesRatio</code></td>
  <td>KUBERNETES模式为0.8; YARN模式为0.8; 0.0（对于独立模式和Mesos粗粒度模式）</td>
  <td>注册资源的最小比例（注册资源/总预期资源）（资源是yarn模式和Kubernetes模式的执行者，独立模式和Mesos粗粒度模式的CPU内核['spark.cores.max'值是计划开始之前要等待的Mesos粗粒度模式]）。指定为0.0到1.0之间的两倍。无论是否已达到最小资源比例，调度开始之前等待的最大时间由config控制<code>spark.scheduler.maxRegisteredResourcesWaitingTime</code> 。
  </td>
</tr>
<tr>
  <td><code>spark.scheduler.mode</code></td>
  <td>先进先出</td>
  <td>提交到相同SparkContext的作业之间的<a href="job-scheduling.html#scheduling-within-an-application">调度模式</a> 。可以设置为<code>FAIR</code>使用公平共享而不是一个接一个地排队。对于多用户服务很有用。
  </td>
</tr>
<tr>
  <td><code>spark.scheduler.revive.interval</code></td>
  <td>1秒</td>
  <td>调度程序恢复工作资源所提供的间隔长度以运行任务。
  </td>
</tr>
<tr>
  <td><code>spark.scheduler.listenerbus.eventqueue.capacity</code></td>
  <td>10000</td>
  <td>Spark侦听器总线中事件队列的容量必须大于0。如果侦听器事件被丢弃，则考虑增加值（例如20000）。增加该值可能会导致驱动程序使用更多的内存。
  </td>
</tr>
<tr>
  <td><code>spark.scheduler.blacklist.unschedulableTaskSetTimeout</code></td>
  <td>120秒</td>
  <td>等待中止新任务的执行者等待超时的时间（以秒为单位），因为该任务集由于被完全列入黑名单而无法计划，因此中止该任务集。
  </td>
</tr>
<tr>
  <td><code>spark.blacklist.enabled</code></td>
  <td>假</td>
  <td>如果设置为“ true”，则阻止Spark在由于任务失败过多而被列入黑名单的执行器上调度任务。可以通过其他“ spark.blacklist”配置选项进一步控制黑名单算法。
  </td>
</tr>
<tr>
  <td><code>spark.blacklist.timeout</code></td>
  <td>1小时</td>
  <td>（实验性的）节点或执行程序在整个应用程序中被列入黑名单的时间，然后无条件地将其从黑名单中删除以尝试运行新任务。
  </td>
</tr>
<tr>
  <td><code>spark.blacklist.task.maxTaskAttemptsPerExecutor</code></td>
  <td>1个</td>
  <td>（实验性的）对于给定的任务，在将一个执行者列入该任务的黑名单之前，可以对该执行者重试多少次。
  </td>
</tr>
<tr>
  <td><code>spark.blacklist.task.maxTaskAttemptsPerNode</code></td>
  <td>2</td>
  <td>（实验性的）对于给定的任务，在将整个节点列入该任务的黑名单之前，可以在一个节点上重试该节点多少次。
  </td>
</tr>
<tr>
  <td><code>spark.blacklist.stage.maxFailedTasksPerExecutor</code></td>
  <td>2</td>
  <td>（实验性的）在一个执行者将某个执行者列入该阶段的黑名单之前的一个阶段中，有多少个不同的任务必须失败。
  </td>
</tr>
<tr>
  <td><code>spark.blacklist.stage.maxFailedExecutorsPerNode</code></td>
  <td>2</td>
  <td>（实验）在给定阶段将整个节点标记为失败之前，给定阶段有多少个不同的执行程序被标记为黑名单。
  </td>
</tr>
<tr>
  <td><code>spark.blacklist.application.maxFailedTasksPerExecutor</code></td>
  <td>2</td>
  <td>（实验）在成功的任务集中，一个执行器必须失败多少个不同的任务，然后该执行器才被列入整个应用程序的黑名单。指定的超时后，列入黑名单的执行者将自动添加回可用资源池中<code>spark.blacklist.timeout</code> 。但是请注意，使用动态分配时，执行程序可能会被标记为空闲，并被集群管理器收回。
  </td>
</tr>
<tr>
  <td><code>spark.blacklist.application.maxFailedExecutorsPerNode</code></td>
  <td>2</td>
  <td>（实验性的）在将节点列入整个应用程序的黑名单之前，必须为整个应用程序将几个不同的执行者列入黑名单。指定的超时后，列入黑名单的节点将自动添加回可用资源池中<code>spark.blacklist.timeout</code> 。但是请注意，使用动态分配时，节点上的执行程序可能会被标记为空闲，并被集群管理器收回。
  </td>
</tr>
<tr>
  <td><code>spark.blacklist.killBlacklistedExecutors</code></td>
  <td>假</td>
  <td>（实验性的）如果设置为“ true”，则允许Spark在执行失败时将其列入黑名单或在整个应用程序中将其列入黑名单（由spark.blacklist.application控制），以自动杀死执行者。*。请注意，将整个节点添加到黑名单后，该节点上的所有执行程序将被杀死。
  </td>
</tr>
<tr>
  <td><code>spark.blacklist.application.fetchFailure.enabled</code></td>
  <td>假</td>
  <td>（实验性的）如果设置为“ true”，则在发生获取失败时，Spark将立即将执行程序列入黑名单。如果启用了外部随机播放服务，则整个节点将被列入黑名单。
  </td>
</tr>
<tr>
  <td><code>spark.speculation</code></td>
  <td>假</td>
  <td>如果设置为“ true”，则执行任务的推测执行。这意味着如果一个或多个任务在一个阶段中运行缓慢，它们将被重新启动。
  </td>
</tr>
<tr>
  <td><code>spark.speculation.interval</code></td>
  <td>100毫秒</td>
  <td>Spark多久检查一次要推测的任务。
  </td>
</tr>
<tr>
  <td><code>spark.speculation.multiplier</code></td>
  <td>1.5</td>
  <td>任务要比要考虑的中位数慢多少倍。
  </td>
</tr>
<tr>
  <td><code>spark.speculation.quantile</code></td>
  <td>0.75</td>
  <td>在特定阶段启用推测之前必须完成的部分任务。
  </td>
</tr>
<tr>
  <td><code>spark.task.cpus</code></td>
  <td>1个</td>
  <td>为每个任务分配的核心数。
  </td>
</tr>
<tr>
  <td><code>spark.task.maxFailures</code></td>
  <td>4</td>
  <td>在放弃工作之前，任何特定任务的失败次数。分布在不同任务上的故障总数不会导致作业失败。特定任务必须使此尝试次数失败。应该大于或等于1。重试次数=此值-1。
  </td>
</tr>
<tr>
  <td><code>spark.task.reaper.enabled</code></td>
  <td>假</td>
  <td>启用监视已终止/中断的任务。当设置为true时，执行器将监视所有被杀死的任务，直到该任务实际完成执行为止。见其他<code>spark.task.reaper.*</code>有关如何控制此监视的确切行为的详细信息的配置。当设置为false（默认值）时，任务杀死将使用缺少此类监视的较旧代码路径。
  </td>
</tr>
<tr>
  <td><code>spark.task.reaper.pollingInterval</code></td>
  <td>10秒</td>
  <td>什么时候<code>spark.task.reaper.enabled = true</code> ，此设置控制执行程序轮询被终止任务状态的频率。如果在查询时被杀死的任务仍在运行，则将记录警告，并且默认情况下将记录任务的线程转储（可以通过以下方式禁用该线程转储： <code>spark.task.reaper.threadDump</code>设置，如下所示）。
  </td>
</tr>
<tr>
  <td><code>spark.task.reaper.threadDump</code></td>
  <td>真正</td>
  <td>什么时候<code>spark.task.reaper.enabled = true</code> ，此设置控制在定期轮询被终止的任务期间是否记录任务线程转储。将此设置为false可禁用线程转储的收集。
  </td>
</tr>
<tr>
  <td><code>spark.task.reaper.killTimeout</code></td>
  <td>-1</td>
  <td>什么时候<code>spark.task.reaper.enabled = true</code> ，此设置指定一个超时时间，如果被终止的任务没有停止运行，执行器JVM将在该超时后终止自身。缺省值-1禁用此机制并阻止执行程序自毁。此设置的目的是充当安全网，以防止失控的不可取消任务使执行程序无法使用。
  </td>
</tr>
<tr>
  <td><code>spark.stage.maxConsecutiveAttempts</code></td>
  <td>4</td>
  <td>中止阶段之前允许的连续阶段尝试次数。
  </td>
</tr>
</tbody></table>

<h3 id="dynamic-allocation">动态分配</h3>

<table class="table">
<tbody><tr><th>物业名称</th><th>默认</th><th>含义</th></tr>
<tr>
  <td><code>spark.dynamicAllocation.enabled</code></td>
  <td>假</td>
  <td>是否使用动态资源分配，该资源分配将根据工作负载上下扩展在此应用程序中注册的执行程序的数量。有关更多详细信息，请参见<a href="job-scheduling.html#dynamic-resource-allocation">此处</a>的描述。
    <br><br>这需要<code>spark.shuffle.service.enabled</code>要设置。以下配置也相关： <code>spark.dynamicAllocation.minExecutors</code> ， <code>spark.dynamicAllocation.maxExecutors</code>和<code>spark.dynamicAllocation.initialExecutors</code><code>spark.dynamicAllocation.executorAllocationRatio</code>
  </td>
</tr>
<tr>
  <td><code>spark.dynamicAllocation.executorIdleTimeout</code></td>
  <td>60年代</td>
  <td>如果启用了动态分配，并且执行器闲置了超过此持续时间，则该执行器将被删除。有关更多详细信息，请参见此<a href="job-scheduling.html#resource-allocation-policy">描述</a> 。
  </td>
</tr>
<tr>
  <td><code>spark.dynamicAllocation.cachedExecutorIdleTimeout</code></td>
  <td>无限</td>
  <td>如果启用了动态分配，并且已缓存数据块的执行器闲置了超过此持续时间，则该执行器将被删除。有关更多详细信息，请参见此<a href="job-scheduling.html#resource-allocation-policy">描述</a> 。
  </td>
</tr>
<tr>
  <td><code>spark.dynamicAllocation.initialExecutors</code></td>
  <td><code>spark.dynamicAllocation.minExecutors</code></td>
  <td>如果启用了动态分配，则要运行的执行程序的初始数量。
    <br><br>如果设置了--num-executors（或spark.executor.instances），并且大于此值，它将用作执行者的初始数量。
  </td>
</tr>
<tr>
  <td><code>spark.dynamicAllocation.maxExecutors</code></td>
  <td>无限</td>
  <td>如果启用了动态分配，则执行者数量的上限。
  </td>
</tr>
<tr>
  <td><code>spark.dynamicAllocation.minExecutors</code></td>
  <td>0</td>
  <td>如果启用了动态分配，则执行者数量的下限。
  </td>
</tr>
<tr>
  <td><code>spark.dynamicAllocation.executorAllocationRatio</code></td>
  <td>1个</td>
  <td>默认情况下，动态分配将要求足够的执行者根据要处理的任务数量最大化并行性。虽然这可以最大程度地减少作业的等待时间，但是对于小型任务，此设置可能会由于执行程序分配开销而浪费大量资源，因为某些执行程序甚至可能无法执行任何工作。此设置允许设置一个比率，该比率将用于减少具有完全并行性的执行程序的数量。默认为1.0以提供最大的并行度。0.5将执行者的目标数量除以2。由dynamicAllocation计算的执行者的目标数量仍可以由<code>spark.dynamicAllocation.minExecutors</code>和<code>spark.dynamicAllocation.maxExecutors</code>设定</td>
</tr>
<tr>
  <td><code>spark.dynamicAllocation.schedulerBacklogTimeout</code></td>
  <td>1秒</td>
  <td>如果启用了动态分配，并且有待解决的任务积压的时间超过了此期限，则将请求新的执行者。有关更多详细信息，请参见此<a href="job-scheduling.html#resource-allocation-policy">描述</a> 。
  </td>
</tr>
<tr>
  <td><code>spark.dynamicAllocation.sustainedSchedulerBacklogTimeout</code></td>
  <td><code>schedulerBacklogTimeout</code></td>
  <td>和...一样<code>spark.dynamicAllocation.schedulerBacklogTimeout</code> ，但仅用于后续的执行者请求。有关更多详细信息，请参见此<a href="job-scheduling.html#resource-allocation-policy">描述</a> 。
  </td>
</tr>
</tbody></table>

<h3 id="security">安全</h3>

<p>请参阅“ <a href="security.html">安全性”</a>页面以获取有关如何保护不同Spark子系统的可用选项。</p>

<h3 id="spark-sql">Spark SQL</h3>

<p>运行<code>SET -v</code>命令将显示SQL配置的完整列表。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// spark is an existing SparkSession</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SET -v&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">(</span><span class="n">numRows</span> <span class="k">=</span> <span class="mi">200</span><span class="o">,</span> <span class="n">truncate</span> <span class="k">=</span> <span class="kc">false</span><span class="o">)</span></code></pre></figure>

  </div>

<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// spark is an existing SparkSession</span>
<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SET -v&quot;</span><span class="o">).</span><span class="na">show</span><span class="o">(</span><span class="mi">200</span><span class="o">,</span> <span class="kc">false</span><span class="o">);</span></code></pre></figure>

  </div>

<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># spark is an existing SparkSession</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SET -v&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></figure>

  </div>

<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>sparkR.session<span class="p">()</span>
properties <span class="o">&lt;-</span> sql<span class="p">(</span><span class="s">&quot;SET -v&quot;</span><span class="p">)</span>
showDF<span class="p">(</span>properties<span class="p">,</span> numRows <span class="o">=</span> <span class="m">200</span><span class="p">,</span> truncate <span class="o">=</span> <span class="kc">FALSE</span><span class="p">)</span></code></pre></figure>

  </div>
</div>

<h3 id="spark-streaming">火花流</h3>

<table class="table">
<tbody><tr><th>物业名称</th><th>默认</th><th>含义</th></tr>
<tr>
  <td><code>spark.streaming.backpressure.enabled</code></td>
  <td>假</td>
  <td>启用或禁用Spark Streaming的内部背压机制（自1.5开始）。这使Spark Streaming能够基于当前的批处理调度延迟和处理时间来控制接收速率，以便系统仅接收与系统可处理的速度一样的速度。在内部，这可以动态设置接收器的最大接收速率。此速率是值的上限<code>spark.streaming.receiver.maxRate</code>和<code>spark.streaming.kafka.maxRatePerPartition</code>如果已设置（请参见下文）。
  </td>
</tr>
<tr>
  <td><code>spark.streaming.backpressure.initialRate</code></td>
  <td>没有设置</td>
  <td>这是启用背压机制时每个接收器将接收第一批数据的初始最大接收速率。
  </td>
</tr>
<tr>
  <td><code>spark.streaming.blockInterval</code></td>
  <td>200毫秒</td>
  <td>Spark Streaming接收器接收到的数据在存储到Spark中之前被分割成数据块的时间间隔。最低建议-50毫秒。有关更多详细信息，请参见Spark Streaming编程指南中的<a href="streaming-programming-guide.html#level-of-parallelism-in-data-receiving">性能调整</a>部分。
  </td>
</tr>
<tr>
  <td><code>spark.streaming.receiver.maxRate</code></td>
  <td>没有设置</td>
  <td>每个接收器接收数据的最大速率（每秒的记录数）。实际上，每个流每秒最多消耗此数量的记录。将此配置设置为0或负数将不限制速率。有关模式的详细信息，请参见Spark Streaming编程指南中的<a href="streaming-programming-guide.html#deploying-applications">部署指南</a> 。
  </td>
</tr>
<tr>
  <td><code>spark.streaming.receiver.writeAheadLog.enable</code></td>
  <td>假</td>
  <td>为接收器启用预写日志。通过接收器接收的所有输入数据将保存到预写日志中，以便在驱动程序发生故障后将其恢复。有关更多详细信息，请参见Spark Streaming编程指南中的<a href="streaming-programming-guide.html#deploying-applications">部署指南</a> 。
  </td>
</tr>
<tr>
  <td><code>spark.streaming.unpersist</code></td>
  <td>真正</td>
  <td>强制由Spark Streaming生成并保留的RDD自动从Spark的内存中消失。Spark Streaming接收到的原始输入数据也将自动清除。将其设置为false将允许原始数据和持久的RDD在流应用程序外部访问，因为它们不会自动清除。但这是以Spark中更高的内存使用为代价的。
  </td>
</tr>
<tr>
  <td><code>spark.streaming.stopGracefullyOnShutdown</code></td>
  <td>假</td>
  <td>如果<code>true</code> ，Spark会关闭<code>StreamingContext</code>从容关闭JVM，而不是立即关闭。
  </td>
</tr>
<tr>
  <td><code>spark.streaming.kafka.maxRatePerPartition</code></td>
  <td>没有设置</td>
  <td>使用新的Kafka直接流API时，将从每个Kafka分区读取数据的最大速率（每秒的记录数）。有关更多详细信息，请参见《 <a href="streaming-kafka-integration.html">Kafka集成指南</a> 》。
  </td>
</tr>
<tr>
    <td><code>spark.streaming.kafka.minRatePerPartition</code></td>
    <td>1个</td>
    <td>使用新的Kafka直接流API时，将从每个Kafka分区读取数据的最小速率（每秒的记录数）。</td>
</tr>
<tr>
  <td><code>spark.streaming.kafka.maxRetries</code></td>
  <td>1个</td>
  <td>驱动程序将执行的最大连续重试次数，以便在每个分区的引导线上找到最新的偏移量（默认值1表示驱动程序将最多进行2次尝试）。仅适用于新的Kafka直接流API。</td>
</tr>
<tr>
  <td><code>spark.streaming.ui.retainedBatches</code></td>
  <td>1000</td>
  <td>垃圾收集之前，Spark Streaming UI和状态API会记住多少批次。
  </td>
</tr>
<tr>
  <td><code>spark.streaming.driver.writeAheadLog.closeFileAfterWrite</code></td>
  <td>假</td>
  <td>在驱动程序上写入预写日志记录后是否关闭文件。当您要对驱动程序上的元数据WAL使用S3（或任何不支持刷新的文件系统）时，请将其设置为“ true”。
  </td>
</tr>
<tr>
  <td><code>spark.streaming.receiver.writeAheadLog.closeFileAfterWrite</code></td>
  <td>假</td>
  <td>在接收方上写一个预写日志记录后是否关闭文件。当您要对接收器上的数据WAL使用S3（或任何不支持刷新的文件系统）时，请将其设置为“ true”。
  </td>
</tr>
</tbody></table>

<h3 id="sparkr">星火</h3>

<table class="table">
<tbody><tr><th>物业名称</th><th>默认</th><th>含义</th></tr>
<tr>
  <td><code>spark.r.numRBackendThreads</code></td>
  <td>2</td>
  <td>RBackend用于处理来自SparkR程序包的RPC调用的线程数。
  </td>
</tr>
<tr>
  <td><code>spark.r.command</code></td>
  <td>脚本</td>
  <td>在驱动程序和工作程序的群集模式下执行R脚本可执行。
  </td>
</tr>
<tr>
  <td><code>spark.r.driver.command</code></td>
  <td>spark.r.command</td>
  <td>可在客户端模式下为驱动程序执行R脚本而执行。在群集模式下被忽略。
  </td>
</tr>
<tr>
  <td><code>spark.r.shell.command</code></td>
  <td>[R</td>
  <td>在驱动程序的客户端模式下执行sparkR shell的可执行文件。在群集模式下被忽略。与环境变量相同<code>SPARKR_DRIVER_R</code> ，但优先于此。
    <code>spark.r.shell.command</code>用于sparkR外壳，而<code>spark.r.driver.command</code>用于运行R脚本。
  </td>
</tr>
<tr>
  <td><code>spark.r.backendConnectionTimeout</code></td>
  <td>6000</td>
  <td>R进程在与RBackend的连接上设置的连接超时（以秒为单位）。
  </td>
</tr>
<tr>
  <td><code>spark.r.heartBeatInterval</code></td>
  <td>100</td>
  <td>从SparkR后端发送到R进程的心跳的时间间隔，以防止连接超时。
  </td>
</tr>

</tbody></table>

<h3 id="graphx">GraphX</h3>

<table class="table">
<tbody><tr><th>物业名称</th><th>默认</th><th>含义</th></tr>
<tr>
  <td><code>spark.graphx.pregel.checkpointInterval</code></td>
  <td>-1</td>
  <td>Pregel中图形和消息的检查点间隔。它可以避免由于多次迭代后由于谱系链较长而引起的stackOverflowError。默认情况下，检查点处于禁用状态。
  </td>
</tr>
</tbody></table>

<h3 id="deploy">部署</h3>

<table class="table">
  <tbody><tr><th>物业名称</th><th>默认</th><th>含义</th></tr>
  <tr>
    <td><code>spark.deploy.recoveryMode</code></td>
    <td>没有</td>
    <td>恢复模式设置，用于在失败并重新启动时以集群模式恢复提交的Spark作业。这仅适用于与Standalone或Mesos一起运行的群集模式。</td>
  </tr>
  <tr>
    <td><code>spark.deploy.zookeeper.url</code></td>
    <td>没有</td>
    <td>当spark.deploy.recoveryMode设置为ZOOKEEPER时，此配置用于设置要连接的Zookeeper URL。</td>
  </tr>
  <tr>
    <td><code>spark.deploy.zookeeper.dir</code></td>
    <td>没有</td>
    <td>当spark.deploy.recoveryMode设置为ZOOKEEPER时，此配置用于将zookeeper目录设置为存储恢复状态。</td>
  </tr>
</tbody></table>

<h3 id="cluster-managers">集群管理者</h3>

<p>Spark中的每个集群管理器都有其他配置选项。可以在页面上找到每种模式的配置：</p>

<h4 id="yarn"><a href="running-on-yarn.html#configuration">纱</a></h4>

<h4 id="mesos"><a href="running-on-mesos.html#configuration">梅索斯</a></h4>

<h4 id="kubernetes"><a href="running-on-kubernetes.html#configuration">Kubernetes</a></h4>

<h4 id="standalone-mode"><a href="spark-standalone.html#cluster-launch-scripts">独立模式</a></h4>

<h1 id="environment-variables">环境变量</h1>

<p>可以通过环境变量配置某些Spark设置，这些环境变量是从<code>conf/spark-env.sh</code>安装Spark的目录中的脚本（或<code>conf/spark-env.cmd</code>在Windows上）。在独立模式和Mesos模式下，此文件可以提供计算机的特定信息，例如主机名。运行本地Spark应用程序或提交脚本时也会提供该资源。</p>

<p>注意<code>conf/spark-env.sh</code>安装Spark时默认情况下不存在。但是，您可以复制<code>conf/spark-env.sh.template</code>创建它。确保使副本可执行。</p>

<p>可以在以下变量中设置<code>spark-env.sh</code> ：</p>

<table class="table">
  <tbody><tr><th style="width:21%">环境变量</th><th>含义</th></tr>
  <tr>
    <td><code>JAVA_HOME</code></td>
    <td>Java的安装位置（如果不是默认位置） <code>PATH</code> ）。</td>
  </tr>
  <tr>
    <td><code>PYSPARK_PYTHON</code></td>
    <td>在驱动程序和工作程序中都用于PySpark的Python二进制可执行文件（默认为<code>python2.7</code>如果可用，否则<code>python</code> ）。属性<code>spark.pyspark.python</code>如果设置优先</td>
  </tr>
  <tr>
    <td><code>PYSPARK_DRIVER_PYTHON</code></td>
    <td>仅在驱动程序中用于PySpark的Python二进制可执行文件（默认为<code>PYSPARK_PYTHON</code> ）。属性<code>spark.pyspark.driver.python</code>如果设置优先</td>
  </tr>
  <tr>
    <td><code>SPARKR_DRIVER_R</code></td>
    <td>用于SparkR Shell的R二进制可执行文件（默认为<code>R</code> ）。属性<code>spark.r.shell.command</code>如果设置优先</td>
  </tr>
  <tr>
    <td><code>SPARK_LOCAL_IP</code></td>
    <td>绑定机器的IP地址。</td>
  </tr>
  <tr>
    <td><code>SPARK_PUBLIC_DNS</code></td>
    <td>您的Spark程序的主机名将播发到其他计算机。</td>
  </tr>
</tbody></table>

<p>除上述内容外，还有用于设置Spark <a href="spark-standalone.html#cluster-launch-scripts">独立群集脚本的选项</a> ，例如，每台计算机上使用的内核数和最大内存。</p>

<p>以来<code>spark-env.sh</code>是一个shell脚本，其中一些可以通过编程设置-例如，您可以计算<code>SPARK_LOCAL_IP</code>通过查找特定网络接口的IP。</p>

<p>注意：在YARN上运行Spark时<code>cluster</code>模式下，需要使用<code>spark.yarn.appMasterEnv.[EnvironmentVariableName]</code>您的财产<code>conf/spark-defaults.conf</code>文件。在其中设置的环境变量<code>spark-env.sh</code>不会反映在YARN Application Master流程中<code>cluster</code>模式。有关更多信息，请参见<a href="running-on-yarn.html#spark-properties">YARN相关的Spark属性</a> 。</p>

<h1 id="configuring-logging">配置日志</h1>

<p>Spark使用<a href="http://logging.apache.org/log4j/">log4j</a>进行日志记录。您可以通过添加一个<code>log4j.properties</code>文件在<code>conf</code>目录。一种开始的方法是复制现有的<code>log4j.properties.template</code>位于那里。</p>

<h1 id="overriding-configuration-directory">覆盖配置目录</h1>

<p>要指定默认的“ SPARK_HOME / conf”以外的其他配置目录，可以设置SPARK_CONF_DIR。 Spark将使用此目录中的配置文件（spark-defaults.conf，spark-env.sh，log4j.properties等）。</p>

<h1 id="inheriting-hadoop-cluster-configuration">继承Hadoop集群配置</h1>

<p>如果计划使用Spark从HDFS读取和写入，则Spark的类路径中应包含两个Hadoop配置文件：</p>

<ul>
  <li><code>hdfs-site.xml</code> ，它为HDFS客户端提供默认行为。</li>
  <li><code>core-site.xml</code> ，它设置默认文件系统名称。</li>
</ul>

<p>这些配置文件的位置在Hadoop版本之间有所不同，但是内部存在一个公共位置。 <code>/etc/hadoop/conf</code> 。一些工具可以即时创建配置，但是提供了一种下载其副本的机制。</p>

<p>要使这些文件对Spark可见，请设置<code>HADOOP_CONF_DIR</code>在<code>$SPARK_HOME/conf/spark-env.sh</code>到包含配置文件的位置。</p>

<h1 id="custom-hadoophive-configuration">自定义Hadoop / Hive配置</h1>

<p>如果您的Spark应用程序正在与Hadoop和/或Hive交互，则Spark的类路径中可能存在Hadoop / Hive配置文件。</p>

<p>多个正在运行的应用程序可能需要不同的Hadoop / Hive客户端配置。您可以复制和修改<code>hdfs-site.xml</code> ， <code>core-site.xml</code> ， <code>yarn-site.xml</code> ， <code>hive-site.xml</code>在每个应用程序的Spark的类路径中。在YARN上运行的Spark群集中，这些配置文件是在群集范围内设置的，应用程序无法安全地对其进行更改。</p>

<p>更好的选择是使用以下形式的spark hadoop属性： <code>spark.hadoop.*</code> 。它们可以被视为与可以在<code>$SPARK_HOME/conf/spark-defaults.conf</code></p>

<p>在某些情况下，您可能要避免对某些配置进行硬编码<code>SparkConf</code> 。例如，Spark允许您简单地创建一个空的conf并设置spark / spark hadoop属性。</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">().</span><span class="n">set</span><span class="o">(</span><span class="s">&quot;spark.hadoop.abc.def&quot;</span><span class="o">,</span><span class="s">&quot;xyz&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">sc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">)</span></code></pre></figure>

<p>另外，您可以在运行时修改或添加配置：</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>./bin/spark-submit <span class="se">\ </span>
  --name <span class="s2">&quot;My app&quot;</span> <span class="se">\ </span>
  --master local<span class="o">[</span><span class="m">4</span><span class="o">]</span> <span class="se">\ </span> 
  --conf spark.eventLog.enabled<span class="o">=</span><span class="nb">false</span> <span class="se">\ </span>
  --conf <span class="s2">&quot;spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps&quot;</span> <span class="se">\ </span>
  --conf spark.hadoop.abc.def<span class="o">=</span>xyz <span class="se">\ </span>
  myApp.jar</code></pre></figure>



                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-1.12.4.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    

</body></html>