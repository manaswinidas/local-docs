<html class="no-js" ><head></head><body >﻿<!--<![endif]-->
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>结构化流编程指南-Spark 2.4.4文档</title>
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        
        <!-- Google analytics script -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-32518208-2']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
        </script>
        

    
    
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html"><img src="img/spark-logo-hd.png" style="height:50px"></a> <span class="version">2.4.4</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">总览</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">编程指南<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">快速开始</a></li>
                                <li><a href="rdd-programming-guide.html">RDD，累加器，广播变量</a></li>
                                <li><a href="sql-programming-guide.html">SQL，数据框和数据集</a></li>
                                <li><a href="structured-streaming-programming-guide.html">结构化流</a></li>
                                <li><a href="streaming-programming-guide.html">火花流（DStreams）</a></li>
                                <li><a href="ml-guide.html">MLlib（机器学习）</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX（图形处理）</a></li>
                                <li><a href="sparkr.html">SparkR（Spark上的R）</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API文件<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">斯卡拉</a></li>
                                <li><a href="api/java/index.html">爪哇</a></li>
                                <li><a href="api/python/index.html">蟒蛇</a></li>
                                <li><a href="api/R/index.html">[R</a></li>
                                <li><a href="api/sql/index.html">SQL，内置函数</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">部署中<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">总览</a></li>
                                <li><a href="submitting-applications.html">提交申请</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark独立</a></li>
                                <li><a href="running-on-mesos.html">梅索斯</a></li>
                                <li><a href="running-on-yarn.html">纱</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">更多<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">组态</a></li>
                                <li><a href="monitoring.html">监控方式</a></li>
                                <li><a href="tuning.html">调音指南</a></li>
                                <li><a href="job-scheduling.html">作业调度</a></li>
                                <li><a href="security.html">安全</a></li>
                                <li><a href="hardware-provisioning.html">硬件配置</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">建筑火花</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">为Spark贡献</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">第三方项目</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v2.4.4</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                <div class="content" id="content">
                    
                        <h1 class="title">结构化流编程指南</h1>
                    

                    <ul id="markdown-toc">
  <li><a href="#overview" id="markdown-toc-overview">总览</a></li>
  <li><a href="#quick-example" id="markdown-toc-quick-example">快速范例</a></li>
  <li><a href="#programming-model" id="markdown-toc-programming-model">程式设计模型</a>    <ul>
      <li><a href="#basic-concepts" id="markdown-toc-basic-concepts">基本概念</a></li>
      <li><a href="#handling-event-time-and-late-data" id="markdown-toc-handling-event-time-and-late-data">处理事件时间和延迟数据</a></li>
      <li><a href="#fault-tolerance-semantics" id="markdown-toc-fault-tolerance-semantics">容错语义</a></li>
    </ul>
  </li>
  <li><a href="#api-using-datasets-and-dataframes" id="markdown-toc-api-using-datasets-and-dataframes">使用数据集和数据帧的API</a>    <ul>
      <li><a href="#creating-streaming-dataframes-and-streaming-datasets" id="markdown-toc-creating-streaming-dataframes-and-streaming-datasets">创建流数据框架和流数据集</a>        <ul>
          <li><a href="#input-sources" id="markdown-toc-input-sources">输入源</a></li>
          <li><a href="#schema-inference-and-partition-of-streaming-dataframesdatasets" id="markdown-toc-schema-inference-and-partition-of-streaming-dataframesdatasets">流数据帧/数据集的模式推断和分区</a></li>
        </ul>
      </li>
      <li><a href="#operations-on-streaming-dataframesdatasets" id="markdown-toc-operations-on-streaming-dataframesdatasets">流式数据帧/数据集的操作</a>        <ul>
          <li><a href="#basic-operations---selection-projection-aggregation" id="markdown-toc-basic-operations---selection-projection-aggregation">基本操作-选择，投影，汇总</a></li>
          <li><a href="#window-operations-on-event-time" id="markdown-toc-window-operations-on-event-time">事件时间窗口操作</a>            <ul>
              <li><a href="#handling-late-data-and-watermarking" id="markdown-toc-handling-late-data-and-watermarking">处理后期数据和加水印</a></li>
            </ul>
          </li>
          <li><a href="#join-operations" id="markdown-toc-join-operations">加盟运营</a>            <ul>
              <li><a href="#stream-static-joins" id="markdown-toc-stream-static-joins">流静态联接</a></li>
              <li><a href="#stream-stream-joins" id="markdown-toc-stream-stream-joins">流流连接</a>                <ul>
                  <li><a href="#inner-joins-with-optional-watermarking" id="markdown-toc-inner-joins-with-optional-watermarking">内部联接，带有可选水印</a></li>
                  <li><a href="#outer-joins-with-watermarking" id="markdown-toc-outer-joins-with-watermarking">外加水印</a></li>
                  <li><a href="#support-matrix-for-joins-in-streaming-queries" id="markdown-toc-support-matrix-for-joins-in-streaming-queries">流查询中的联接支持矩阵</a></li>
                </ul>
              </li>
            </ul>
          </li>
          <li><a href="#streaming-deduplication" id="markdown-toc-streaming-deduplication">流重复数据删除</a></li>
          <li><a href="#policy-for-handling-multiple-watermarks" id="markdown-toc-policy-for-handling-multiple-watermarks">处理多个水印的政策</a></li>
          <li><a href="#arbitrary-stateful-operations" id="markdown-toc-arbitrary-stateful-operations">任意状态作业</a></li>
          <li><a href="#unsupported-operations" id="markdown-toc-unsupported-operations">不支持的操作</a></li>
        </ul>
      </li>
      <li><a href="#starting-streaming-queries" id="markdown-toc-starting-streaming-queries">开始流查询</a>        <ul>
          <li><a href="#output-modes" id="markdown-toc-output-modes">输出方式</a></li>
          <li><a href="#output-sinks" id="markdown-toc-output-sinks">输出接收器</a>            <ul>
              <li><a href="#using-foreach-and-foreachbatch" id="markdown-toc-using-foreach-and-foreachbatch">使用Foreach和ForeachBatch</a>                <ul>
                  <li><a href="#foreachbatch" id="markdown-toc-foreachbatch">Foreach批次</a></li>
                  <li><a href="#foreach" id="markdown-toc-foreach">Foreach</a></li>
                </ul>
              </li>
            </ul>
          </li>
          <li><a href="#triggers" id="markdown-toc-triggers">扳机</a></li>
        </ul>
      </li>
      <li><a href="#managing-streaming-queries" id="markdown-toc-managing-streaming-queries">管理流查询</a></li>
      <li><a href="#monitoring-streaming-queries" id="markdown-toc-monitoring-streaming-queries">监控流查询</a>        <ul>
          <li><a href="#reading-metrics-interactively" id="markdown-toc-reading-metrics-interactively">交互阅读指标</a></li>
          <li><a href="#reporting-metrics-programmatically-using-asynchronous-apis" id="markdown-toc-reporting-metrics-programmatically-using-asynchronous-apis">使用异步API以编程方式报告指标</a></li>
          <li><a href="#reporting-metrics-using-dropwizard" id="markdown-toc-reporting-metrics-using-dropwizard">使用Dropwizard报告指标</a></li>
        </ul>
      </li>
      <li><a href="#recovering-from-failures-with-checkpointing" id="markdown-toc-recovering-from-failures-with-checkpointing">通过检查点从故障中恢复</a></li>
      <li><a href="#recovery-semantics-after-changes-in-a-streaming-query" id="markdown-toc-recovery-semantics-after-changes-in-a-streaming-query">流查询中的更改后的恢复语义</a></li>
    </ul>
  </li>
  <li><a href="#continuous-processing" id="markdown-toc-continuous-processing">连续加工</a></li>
  <li><a href="#additional-information" id="markdown-toc-additional-information">附加信息</a></li>
</ul>

<h1 id="overview">总览</h1>
<p>结构化流是基于Spark SQL引擎构建的可伸缩且容错的流处理引擎。您可以像对静态数据进行批处理计算一样来表示流计算。当流数据继续到达时，Spark SQL引擎将负责递增地，连续地运行它并更新最终结果。您可以在Scala，Java，Python或R中使用<a href="sql-programming-guide.html">Dataset / DataFrame API</a>来表示流聚合，事件时间窗口，流到批处理联接等。计算在同一优化的Spark SQL引擎上执行。最后，系统通过检查点和预写日志来确保端到端的一次容错保证。简而言之， <em>结构化流提供了快速，可伸缩，容错，端到端的精确一次流处理，而用户无需推理流。</em></p>

<p>在内部，默认情况下，结构化流查询是使用<em>微批处理</em>引擎<em>处理的</em> ，该引擎将数据流作为一系列小批处理作业进行处理，从而实现了低至100毫秒的端到端延迟以及一次精确的容错保证。但是，从Spark 2.3开始，我们引入了一种称为“ <strong>连续处理”</strong>的新低延迟处理模式，该模式可以实现一次最少保证的低至1毫秒的端到端延迟。在不更改查询中的Dataset / DataFrame操作的情况下，您将能够根据应用程序需求选择模式。</p>

<p>在本指南中，我们将带您逐步了解编程模型和API。我们将主要使用默认的微批处理模型来解释这些概念，然后<a href="#continuous-processing">再</a>讨论连续处理模型。首先，让我们从结构化流查询的简单示例开始-流字数。</p>

<h1 id="quick-example">快速范例</h1>
<p>假设您要维护从侦听TCP套接字的数据服务器接收到的文本数据的运行字数。让我们看看如何使用结构化流来表达这一点。您可以在<a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala">Scala</a> / <a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java">Java</a> / <a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/python/sql/streaming/structured_network_wordcount.py">Python</a> / <a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/r/streaming/structured_network_wordcount.R">R中</a>看到完整的代码。如果<a href="https://spark.apache.org/downloads.html">下载了Spark</a> ，则可以直接<a href="index.html#running-the-examples-and-shell">运行该示例</a> 。无论如何，让我们逐步介绍示例并了解其工作原理。首先，我们必须导入必要的类并创建一个本地SparkSession，这是与Spark相关的所有功能的起点。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">org.apache.spark.sql.functions._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span>

<span class="k">val</span> <span class="n">spark</span> <span class="k">=</span> <span class="nc">SparkSession</span>
  <span class="o">.</span><span class="n">builder</span>
  <span class="o">.</span><span class="n">appName</span><span class="o">(</span><span class="s">&quot;StructuredNetworkWordCount&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">getOrCreate</span><span class="o">()</span>
  
<span class="k">import</span> <span class="nn">spark.implicits._</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.api.java.function.FlatMapFunction</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.streaming.StreamingQuery</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">java.util.Arrays</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.Iterator</span><span class="o">;</span>

<span class="n">SparkSession</span> <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span>
  <span class="o">.</span><span class="na">builder</span><span class="o">()</span>
  <span class="o">.</span><span class="na">appName</span><span class="o">(</span><span class="s">&quot;JavaStructuredNetworkWordCount&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">getOrCreate</span><span class="o">();</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">explode</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">split</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span> \
    <span class="o">.</span><span class="n">builder</span> \
    <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;StructuredNetworkWordCount&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>sparkR.session<span class="p">(</span>appName <span class="o">=</span> <span class="s">&quot;StructuredNetworkWordCount&quot;</span><span class="p">)</span></code></pre></figure>

  </div>
</div>

<p>接下来，让我们创建一个流数据框架，该数据框架表示从在localhost：9999上侦听的服务器接收的文本数据，并对数据框架进行转换以计算字数。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// Create DataFrame representing the stream of input lines from connection to localhost:9999</span>
<span class="k">val</span> <span class="n">lines</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;socket&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;host&quot;</span><span class="o">,</span> <span class="s">&quot;localhost&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;port&quot;</span><span class="o">,</span> <span class="mi">9999</span><span class="o">)</span>
  <span class="o">.</span><span class="n">load</span><span class="o">()</span>

<span class="c1">// Split the lines into words</span>
<span class="k">val</span> <span class="n">words</span> <span class="k">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">as</span><span class="o">[</span><span class="kt">String</span><span class="o">].</span><span class="n">flatMap</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">))</span>

<span class="c1">// Generate running word count</span>
<span class="k">val</span> <span class="n">wordCounts</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&quot;value&quot;</span><span class="o">).</span><span class="n">count</span><span class="o">()</span></code></pre></figure>

    <p>这个<code>lines</code> DataFrame表示包含流文本数据的无界表。该表包含一列名为“值”的字符串，流文本数据中的每一行都成为表中的一行。请注意，由于我们正在设置转换，并且尚未开始转换，因此当前未接收到任何数据。接下来，我们使用以下命令将DataFrame转换为String的数据集<code>.as[String]</code> ，以便我们可以应用<code>flatMap</code>将每一行拆分为多个单词的操作。结果<code>words</code>数据集包含所有单词。最后，我们定义了<code>wordCounts</code>通过按数据集中的唯一值分组并对其计数来对DataFrame进行计数。请注意，这是一个流数据帧，它表示流的运行字数。</p>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// Create DataFrame representing the stream of input lines from connection to localhost:9999</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">lines</span> <span class="o">=</span> <span class="n">spark</span>
  <span class="o">.</span><span class="na">readStream</span><span class="o">()</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;socket&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;host&quot;</span><span class="o">,</span> <span class="s">&quot;localhost&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;port&quot;</span><span class="o">,</span> <span class="mi">9999</span><span class="o">)</span>
  <span class="o">.</span><span class="na">load</span><span class="o">();</span>

<span class="c1">// Split the lines into words</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">words</span> <span class="o">=</span> <span class="n">lines</span>
  <span class="o">.</span><span class="na">as</span><span class="o">(</span><span class="n">Encoders</span><span class="o">.</span><span class="na">STRING</span><span class="o">())</span>
  <span class="o">.</span><span class="na">flatMap</span><span class="o">((</span><span class="n">FlatMapFunction</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;)</span> <span class="n">x</span> <span class="o">-&gt;</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">)).</span><span class="na">iterator</span><span class="o">(),</span> <span class="n">Encoders</span><span class="o">.</span><span class="na">STRING</span><span class="o">());</span>

<span class="c1">// Generate running word count</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">wordCounts</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="s">&quot;value&quot;</span><span class="o">).</span><span class="na">count</span><span class="o">();</span></code></pre></figure>

    <p>这个<code>lines</code> DataFrame表示包含流文本数据的无界表。该表包含一列名为“值”的字符串，流文本数据中的每一行都成为表中的一行。请注意，由于我们正在设置转换，并且尚未开始转换，因此当前未接收到任何数据。接下来，我们使用以下命令将DataFrame转换为String的数据集<code>.as(Encoders.STRING())</code> ，以便我们可以应用<code>flatMap</code>将每一行拆分为多个单词的操作。结果<code>words</code>数据集包含所有单词。最后，我们定义了<code>wordCounts</code>通过按数据集中的唯一值分组并对其计数来对DataFrame进行计数。请注意，这是一个流数据帧，它表示流的运行字数。</p>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># Create DataFrame representing the stream of input lines from connection to localhost:9999</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">spark</span> \
    <span class="o">.</span><span class="n">readStream</span> \
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;socket&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;host&quot;</span><span class="p">,</span> <span class="s2">&quot;localhost&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;port&quot;</span><span class="p">,</span> <span class="mi">9999</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="c1"># Split the lines into words</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
   <span class="n">explode</span><span class="p">(</span>
       <span class="n">split</span><span class="p">(</span><span class="n">lines</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
   <span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;word&quot;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Generate running word count</span>
<span class="n">wordCounts</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;word&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span></code></pre></figure>

    <p>这个<code>lines</code> DataFrame表示包含流文本数据的无界表。该表包含一列名为“值”的字符串，流文本数据中的每一行都成为表中的一行。请注意，由于我们正在设置转换，并且尚未开始转换，因此当前未接收到任何数据。接下来，我们使用了两个内置的SQL函数-split和explode，将每一行拆分为多行，每行各有一个单词。另外，我们使用功能<code>alias</code>将新列命名为“ word”。最后，我们定义了<code>wordCounts</code>通过按数据集中的唯一值分组并对其计数来对DataFrame进行计数。请注意，这是一个流数据帧，它表示流的运行字数。</p>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="c1"># Create DataFrame representing the stream of input lines from connection to localhost:9999</span>
lines <span class="o">&lt;-</span> read.stream<span class="p">(</span><span class="s">&quot;socket&quot;</span><span class="p">,</span> host <span class="o">=</span> <span class="s">&quot;localhost&quot;</span><span class="p">,</span> port <span class="o">=</span> <span class="m">9999</span><span class="p">)</span>

<span class="c1"># Split the lines into words</span>
words <span class="o">&lt;-</span> selectExpr<span class="p">(</span>lines<span class="p">,</span> <span class="s">&quot;explode(split(value, &#39; &#39;)) as word&quot;</span><span class="p">)</span>

<span class="c1"># Generate running word count</span>
wordCounts <span class="o">&lt;-</span> count<span class="p">(</span>group_by<span class="p">(</span>words<span class="p">,</span> <span class="s">&quot;word&quot;</span><span class="p">))</span></code></pre></figure>

    <p>这个<code>lines</code> SparkDataFrame表示包含流文本数据的无界表。该表包含一列名为“值”的字符串，流文本数据中的每一行都成为表中的一行。请注意，由于我们正在设置转换，并且尚未开始转换，因此当前未接收到任何数据。接下来，我们有一个带有两个SQL函数的SQL表达式-split和explode，将每一行拆分为多行，每行各有一个单词。另外，我们将新列命名为“ word”。最后，我们定义了<code>wordCounts</code>通过按SparkDataFrame中的唯一值进行分组并对其计数来计算SparkDataFrame。请注意，这是一个流SparkDataFrame，它表示流的运行字数。</p>

  </div>
</div>

<p>现在，我们对流数据进行了查询。剩下的就是实际开始接收数据并计算计数了。为此，我们将其设置为打印完整的计数集（由<code>outputMode("complete")</code> ）更新到控制台。然后使用开始流计算<code>start()</code> 。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// Start running the query that prints the running counts to the console</span>
<span class="k">val</span> <span class="n">query</span> <span class="k">=</span> <span class="n">wordCounts</span><span class="o">.</span><span class="n">writeStream</span>
  <span class="o">.</span><span class="n">outputMode</span><span class="o">(</span><span class="s">&quot;complete&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">start</span><span class="o">()</span>

<span class="n">query</span><span class="o">.</span><span class="n">awaitTermination</span><span class="o">()</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// Start running the query that prints the running counts to the console</span>
<span class="n">StreamingQuery</span> <span class="n">query</span> <span class="o">=</span> <span class="n">wordCounts</span><span class="o">.</span><span class="na">writeStream</span><span class="o">()</span>
  <span class="o">.</span><span class="na">outputMode</span><span class="o">(</span><span class="s">&quot;complete&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">start</span><span class="o">();</span>

<span class="n">query</span><span class="o">.</span><span class="na">awaitTermination</span><span class="o">();</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span> <span class="c1"># Start running the query that prints the running counts to the console</span>
<span class="n">query</span> <span class="o">=</span> <span class="n">wordCounts</span> \
    <span class="o">.</span><span class="n">writeStream</span> \
    <span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s2">&quot;complete&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;console&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="n">query</span><span class="o">.</span><span class="n">awaitTermination</span><span class="p">()</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="c1"># Start running the query that prints the running counts to the console</span>
query <span class="o">&lt;-</span> write.stream<span class="p">(</span>wordCounts<span class="p">,</span> <span class="s">&quot;console&quot;</span><span class="p">,</span> outputMode <span class="o">=</span> <span class="s">&quot;complete&quot;</span><span class="p">)</span>

awaitTermination<span class="p">(</span>query<span class="p">)</span></code></pre></figure>

  </div>
</div>

<p>执行此代码后，流计算将在后台开始。的<code>query</code>对象是该活动流查询的句柄，因此我们决定使用来等待查询终止<code>awaitTermination()</code>以防止在查询处于活动状态时退出该过程。</p>

<p>要实际执行此示例代码，可以在自己的<a href="quick-start.html#self-contained-applications">Spark应用程序中</a>编译代码，也可以在下载Spark之后直接<a href="index.html#running-the-examples-and-shell">运行示例</a> 。我们正在展示后者。您首先需要通过使用以下命令将Netcat（在大多数类Unix系统中找到的一个小实用程序）作为数据服务器运行</p>

<pre><code>$ nc -lk 9999
</code></pre>

<p>然后，在另一个终端中，您可以通过使用</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/run-example org.apache.spark.examples.sql.streaming.StructuredNetworkWordCount localhost <span class="m">9999</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/run-example org.apache.spark.examples.sql.streaming.JavaStructuredNetworkWordCount localhost <span class="m">9999</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/spark-submit examples/src/main/python/sql/streaming/structured_network_wordcount.py localhost <span class="m">9999</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ ./bin/spark-submit examples/src/main/r/streaming/structured_network_wordcount.R localhost <span class="m">9999</span></code></pre></figure>

  </div>
</div>

<p>然后，将对运行netcat服务器的终端中键入的任何行进行计数并每秒打印一次。它将类似于以下内容。</p>

<table width="100%">
    <tbody><tr><td>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="c1"># TERMINAL 1:</span>
<span class="c1"># Running Netcat</span>

$ nc -lk <span class="m">9999</span>
apache spark
apache hadoop



















...</code></pre></figure>

    </td>
    <td width="2%"></td>
    <td>
<div class="codetabs">

<div data-lang="scala">

        <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="c1"># TERMINAL 2: RUNNING StructuredNetworkWordCount</span>

$ ./bin/run-example org.apache.spark.examples.sql.streaming.StructuredNetworkWordCount localhost <span class="m">9999</span>

-------------------------------------------
Batch: <span class="m">0</span>
-------------------------------------------
+------+-----+
<span class="p">|</span> value<span class="p">|</span>count<span class="p">|</span>
+------+-----+
<span class="p">|</span>apache<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
<span class="p">|</span> spark<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
+------+-----+

-------------------------------------------
Batch: <span class="m">1</span>
-------------------------------------------
+------+-----+
<span class="p">|</span> value<span class="p">|</span>count<span class="p">|</span>
+------+-----+
<span class="p">|</span>apache<span class="p">|</span>    <span class="m">2</span><span class="p">|</span>
<span class="p">|</span> spark<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
<span class="p">|</span>hadoop<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
+------+-----+
...</code></pre></figure>

      </div>

<div data-lang="java">

        <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="c1"># TERMINAL 2: RUNNING JavaStructuredNetworkWordCount</span>

$ ./bin/run-example org.apache.spark.examples.sql.streaming.JavaStructuredNetworkWordCount localhost <span class="m">9999</span>

-------------------------------------------
Batch: <span class="m">0</span>
-------------------------------------------
+------+-----+
<span class="p">|</span> value<span class="p">|</span>count<span class="p">|</span>
+------+-----+
<span class="p">|</span>apache<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
<span class="p">|</span> spark<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
+------+-----+

-------------------------------------------
Batch: <span class="m">1</span>
-------------------------------------------
+------+-----+
<span class="p">|</span> value<span class="p">|</span>count<span class="p">|</span>
+------+-----+
<span class="p">|</span>apache<span class="p">|</span>    <span class="m">2</span><span class="p">|</span>
<span class="p">|</span> spark<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
<span class="p">|</span>hadoop<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
+------+-----+
...</code></pre></figure>

      </div>
<div data-lang="python">

        <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="c1"># TERMINAL 2: RUNNING structured_network_wordcount.py</span>

$ ./bin/spark-submit examples/src/main/python/sql/streaming/structured_network_wordcount.py localhost <span class="m">9999</span>

-------------------------------------------
Batch: <span class="m">0</span>
-------------------------------------------
+------+-----+
<span class="p">|</span> value<span class="p">|</span>count<span class="p">|</span>
+------+-----+
<span class="p">|</span>apache<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
<span class="p">|</span> spark<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
+------+-----+

-------------------------------------------
Batch: <span class="m">1</span>
-------------------------------------------
+------+-----+
<span class="p">|</span> value<span class="p">|</span>count<span class="p">|</span>
+------+-----+
<span class="p">|</span>apache<span class="p">|</span>    <span class="m">2</span><span class="p">|</span>
<span class="p">|</span> spark<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
<span class="p">|</span>hadoop<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
+------+-----+
...</code></pre></figure>

      </div>
<div data-lang="r">

        <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span><span class="c1"># TERMINAL 2: RUNNING structured_network_wordcount.R</span>

$ ./bin/spark-submit examples/src/main/r/streaming/structured_network_wordcount.R localhost <span class="m">9999</span>

-------------------------------------------
Batch: <span class="m">0</span>
-------------------------------------------
+------+-----+
<span class="p">|</span> value<span class="p">|</span>count<span class="p">|</span>
+------+-----+
<span class="p">|</span>apache<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
<span class="p">|</span> spark<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
+------+-----+

-------------------------------------------
Batch: <span class="m">1</span>
-------------------------------------------
+------+-----+
<span class="p">|</span> value<span class="p">|</span>count<span class="p">|</span>
+------+-----+
<span class="p">|</span>apache<span class="p">|</span>    <span class="m">2</span><span class="p">|</span>
<span class="p">|</span> spark<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
<span class="p">|</span>hadoop<span class="p">|</span>    <span class="m">1</span><span class="p">|</span>
+------+-----+
...</code></pre></figure>

      </div>
</div>
    </td>
</tr></tbody></table>

<h1 id="programming-model">程式设计模型</h1>

<p>结构化流传输中的关键思想是将实时数据流视为被连续添加的表。这导致了一个新的流处理模型，该模型与批处理模型非常相似。您将像在静态表上一样将流计算表示为类似于批处理的标准查询，Spark在<em>无界</em>输入表上将其作为<em>增量</em>查询运行。让我们更详细地了解此模型。</p>

<h2 id="basic-concepts">基本概念</h2>
<p>将输入数据流视为“输入表”。流上到达的每个数据项都像是将新行附加到输入表中。</p>

<p><img src="img/structured-streaming-stream-as-a-table.png" alt="作为表格流" title="作为表格流"></p>

<p>对输入的查询将生成“结果表”。在每个触发间隔（例如，每1秒钟），新行将附加到输入表中，并最终更新结果表。无论何时更新结果表，我们都希望将更改后的结果行写入外部接收器。</p>

<p><img src="img/structured-streaming-model.png" alt="模型"></p>

<p>“输出”定义为写到外部存储器的内容。可以在不同的模式下定义输出：</p>

<ul>
  <li>
    <p><em>完整模式</em> -整个更新的结果表将被写入外部存储器。由存储连接器决定如何处理整个表的写入。</p>
  </li>
  <li>
    <p><em>追加模式</em> -仅将自上次触发以来追加在结果表中的新行写入外部存储器。这仅适用于结果表中现有行预计不会更改的查询。</p>
  </li>
  <li>
    <p><em>更新模式</em> -仅自上次触发以来在结果表中已更新的行将被写入外部存储（自Spark 2.1.1起可用）。请注意，这与完成模式的不同之处在于此模式仅输出自上次触发以来已更改的行。如果查询不包含聚合，它将等同于追加模式。</p>
  </li>
</ul>

<p>请注意，每种模式都适用于某些类型的查询。<a href="#output-modes">稍后将</a>对此进行详细讨论。</p>

<p>为了说明此模型的用法，让我们在上面的<a href="#quick-example">快速示例的</a>上下文中了解该模型。首先<code>lines</code> DataFrame是输入表，最后是<code>wordCounts</code> DataFrame是结果表。注意流查询<code>lines</code>生成DataFrame <code>wordCounts</code> <em>与</em>静态DataFrame完全相同。但是，启动此查询后，Spark将不断检查套接字连接中是否有新数据。如果有新数据，Spark将运行一个“增量”查询，该查询将先前的运行计数与新数据结合起来以计算更新的计数，如下所示。</p>

<p><img src="img/structured-streaming-example-model.png" alt="模型"></p>

<p><strong>请注意，结构化流不会实现整个表</strong> 。它从流数据源读取最新的可用数据，对其进行增量处理以更新结果，然后丢弃该源数据。它仅保留更新结果所需的最小中间<em>状态</em>数据（例如，前面示例中的中间计数）。</p>

<p>此模型与许多其他流处理引擎明显不同。许多流系统要求用户自己维护运行中的聚合，因此必须考虑容错和数据一致性（至少一次，最多一次或恰好一次）。在此模型中，Spark负责在有新数据时更新结果表，从而使用户免于推理。作为示例，让我们看看该模型如何处理基于事件时间的处理和延迟到达的数据。</p>

<h2 id="handling-event-time-and-late-data">处理事件时间和延迟数据</h2>
<p>事件时间是嵌入数据本身的时间。对于许多应用程序，您可能需要在此事件时间进行操作。例如，如果要获取每分钟由IoT设备生成的事件数，则可能要使用生成数据的时间（即数据中的事件时间），而不是Spark收到的时间。他们。此事件时间在此模型中非常自然地表示–设备中的每个事件都是表中的一行，而事件时间是该行中的列值。这允许基于窗口的聚合（例如，每分钟的事件数）只是事件时间列上的一种特殊类型的分组和聚合-每个时间窗口是一个组，每行可以属于多个窗口/组。因此，可以在静态数据集（例如，从收集的设备事件日志中）以及数据流中一致地定义此类基于事件-时间-窗口的聚合查询，从而使用户的生活变得更加轻松。</p>

<p>此外，此模型自然会根据事件时间处理比预期晚到达的数据。由于Spark正在更新结果表，因此它具有完全控制权，可以在有较晚数据时更新旧聚合，并可以清除旧聚合以限制中间状态数据的大小。从Spark 2.1开始，我们支持水印功能，该功能允许用户指定最新数据的阈值，并允许引擎相应地清除旧状态。这些将在后面的“ <a href="#window-operations-on-event-time">窗口操作”</a>部分中详细介绍。</p>

<h2 id="fault-tolerance-semantics">容错语义</h2>
<p>提供端到端的一次语义是结构化流设计背后的主要目标之一。为此，我们设计了结构化流源，接收器和执行引擎，以可靠地跟踪处理的确切进度，以便它可以通过重新启动和/或重新处理来处理任何类型的故障。假定每个流源都有偏移量（类似于Kafka偏移量或Kinesis序列号），以跟踪流中的读取位置。引擎使用检查点和预写日志来记录每个触发器中正在处理的数据的偏移范围。流接收器被设计为是幂等的，用于处理后处理。结合使用可重播的源和幂等的接收器，结构化流可以确保在发生任何故障时<strong>端到端的一次精确语义</strong> 。</p>

<h1 id="api-using-datasets-and-dataframes">使用数据集和数据帧的API</h1>
<p>从Spark 2.0开始，DataFrame和Dataset可以表示静态的有界数据以及流式无界数据。与静态数据集/数据帧类似，您可以使用公共入口点<code>SparkSession</code> （ <a href="api/scala/index.html#org.apache.spark.sql.SparkSession">Scala</a> / <a href="api/java/org/apache/spark/sql/SparkSession.html">Java</a> / <a href="api/python/pyspark.sql.html#pyspark.sql.SparkSession">Python</a> / <a href="api/R/sparkR.session.html">R</a> docs）从流源创建流DataFrame /数据集，并对它们应用与静态DataFrame /数据集相同的操作。如果您不熟悉Datasets / DataFrames，强烈建议您使用<a href="sql-programming-guide.html">DataFrame / Dataset编程指南</a>来熟悉它们。</p>

<h2 id="creating-streaming-dataframes-and-streaming-datasets">创建流数据框架和流数据集</h2>
<p>流数据帧可以通过以下方式创建<code>DataStreamReader</code>传回的介面（ <a href="api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader">Scala</a> / <a href="api/java/org/apache/spark/sql/streaming/DataStreamReader.html">Java</a> / <a href="api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamReader">Python</a> docs） <code>SparkSession.readStream()</code> 。在<a href="api/R/read.stream.html">R中</a> ， <code>read.stream()</code>方法。与用于创建静态DataFrame的读取接口类似，您可以指定源的详细信息-数据格式，架构，选项等。</p>

<h4 id="input-sources">输入源</h4>
<p>有一些内置源。</p>

<ul>
  <li>
    <p><strong>文件源</strong> -读取写入目录的文件作为数据流。支持的文件格式为text，csv，json，orc，parquet。有关最新列表以及每种文件格式的受支持选项，请参见DataStreamReader界面的文档。请注意，文件必须原子地放置在给定目录中，在大多数文件系统中，这可以通过文件移动操作来实现。</p>
  </li>
  <li>
    <p><strong>Kafka源</strong> -从Kafka读取数据。它与0.10.0或更高版本的Kafka代理兼容。有关更多详细信息，请参见《 <a href="structured-streaming-kafka-integration.html">Kafka集成指南》</a> 。</p>
  </li>
  <li>
    <p><strong>套接字源（用于测试）</strong> -从套接字连接读取UTF8文本数据。监听服务器套接字位于驱动程序处。请注意，这仅应用于测试，因为这不能提供端到端的容错保证。</p>
  </li>
  <li>
    <p><strong>速率源（用于测试）</strong> -以每秒指定的行数生成数据，每个输出行包含一个<code>timestamp</code>和<code>value</code> 。哪里<code>timestamp</code>是一个<code>Timestamp</code>包含消息发送时间的类型，以及<code>value</code>是<code>Long</code>包含消息计数的类型，从0开始作为第一行。此源旨在进行测试和基准测试。</p>
  </li>
</ul>

<p>一些源不是容错的，因为它们不能保证故障后可以使用检查点偏移来重放数据。参见前面有关<a href="#fault-tolerance-semantics">容错语义的部分</a> 。以下是Spark中所有来源的详细信息。</p>

<table class="table">
  <tbody><tr>
    <th>资源</th>
    <th>选件</th>
    <th>容错的</th>
    <th>笔记</th>
  </tr>
  <tr>
    <td><b>档案来源</b></td>
    <td>
        <code>path</code> ：输入目录的路径，并且对所有文件格式通用。
        <br>
        <code>maxFilesPerTrigger</code> ：每个触发器中要考虑的新文件的最大数量（默认值：无最大值）<br>
        <code>latestFirst</code> ：是否首先处理最新的新文件，这在大量积压文件时很有用（默认值：false）<br>
        <code>fileNameOnly</code> ：是否仅根据文件名而不是完整路径来检查新文件（默认值：false）。设置为“ true”时，以下文件将被视为同一文件，因为它们的文件名“ dataset.txt”是相同的：<br>“ file：///dataset.txt”<br>“ s3：//a/dataset.txt”<br>“ s3n：//a/b/dataset.txt”<br>“ s3a：//a/b/c/dataset.txt”<br>
        <br><br>有关特定于文件格式的选项，请参见中的相关方法。 <code>DataStreamReader</code> （ <a href="api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader">Scala</a> / <a href="api/java/org/apache/spark/sql/streaming/DataStreamReader.html">Java</a> / <a href="api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamReader">Python</a> / <a href="api/R/read.stream.html">R</a> ）。例如“镶木地板”格式选项，请参见<code>DataStreamReader.parquet()</code> 。
        <br><br>此外，有些会话配置会影响某些文件格式。有关更多详细信息，请参见《 <a href="sql-programming-guide.html">SQL编程指南》</a> 。例如，对于“实木复合地板”，请参阅<a href="sql-data-sources-parquet.html#configuration">实木</a>复合<a href="sql-data-sources-parquet.html#configuration">地板配置</a>部分。
        </td>
    <td>是</td>
    <td>支持全局路径，但不支持多个逗号分隔的路径/全局路径。</td>
  </tr>
  <tr>
    <td><b>套接字源</b></td>
    <td>
        <code>host</code> ：要连接的主机，必须指定<br>
        <code>port</code> ：要连接的端口，必须指定</td>
    <td>没有</td>
    <td></td>
  </tr>
  <tr>
    <td><b>费率来源</b></td>
    <td>
        <code>rowsPerSecond</code> （例如100，默认值：1）：每秒应生成多少行。<br><br>
        <code>rampUpTime</code> （例如5s，默认值：0s）：在发电速度变慢之前要加速多长时间<code>rowsPerSecond</code> 。使用比秒更精细的粒度将被截断为整数秒。<br><br>
        <code>numPartitions</code> （例如10，默认值：Spark的默认并行性）：生成的行的分区号。<br><br>消息来源将尽力达到<code>rowsPerSecond</code> ，但查询可能受资源限制，并且<code>numPartitions</code>可以进行调整以帮助达到所需的速度。
    </td>
    <td>是</td>
    <td></td>
  </tr>

  <tr>
    <td><b>卡夫卡源</b></td>
    <td>请参阅《 <a href="structured-streaming-kafka-integration.html">Kafka集成指南》</a> 。
    </td>
    <td>是</td>
    <td></td>
  </tr>
  <tr>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
</tbody></table>

<p>这里有些例子。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">spark</span><span class="k">:</span> <span class="kt">SparkSession</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1">// Read text from socket</span>
<span class="k">val</span> <span class="n">socketDF</span> <span class="k">=</span> <span class="n">spark</span>
  <span class="o">.</span><span class="n">readStream</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;socket&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;host&quot;</span><span class="o">,</span> <span class="s">&quot;localhost&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;port&quot;</span><span class="o">,</span> <span class="mi">9999</span><span class="o">)</span>
  <span class="o">.</span><span class="n">load</span><span class="o">()</span>

<span class="n">socketDF</span><span class="o">.</span><span class="n">isStreaming</span>    <span class="c1">// Returns True for DataFrames that have streaming sources</span>

<span class="n">socketDF</span><span class="o">.</span><span class="n">printSchema</span>

<span class="c1">// Read all the csv files written atomically in a directory</span>
<span class="k">val</span> <span class="n">userSchema</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StructType</span><span class="o">().</span><span class="n">add</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">,</span> <span class="s">&quot;string&quot;</span><span class="o">).</span><span class="n">add</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">,</span> <span class="s">&quot;integer&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">csvDF</span> <span class="k">=</span> <span class="n">spark</span>
  <span class="o">.</span><span class="n">readStream</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;sep&quot;</span><span class="o">,</span> <span class="s">&quot;;&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">schema</span><span class="o">(</span><span class="n">userSchema</span><span class="o">)</span>      <span class="c1">// Specify schema of the csv files</span>
  <span class="o">.</span><span class="n">csv</span><span class="o">(</span><span class="s">&quot;/path/to/directory&quot;</span><span class="o">)</span>    <span class="c1">// Equivalent to format(&quot;csv&quot;).load(&quot;/path/to/directory&quot;)</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">SparkSession</span> <span class="n">spark</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1">// Read text from socket</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">socketDF</span> <span class="o">=</span> <span class="n">spark</span>
  <span class="o">.</span><span class="na">readStream</span><span class="o">()</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;socket&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;host&quot;</span><span class="o">,</span> <span class="s">&quot;localhost&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;port&quot;</span><span class="o">,</span> <span class="mi">9999</span><span class="o">)</span>
  <span class="o">.</span><span class="na">load</span><span class="o">();</span>

<span class="n">socketDF</span><span class="o">.</span><span class="na">isStreaming</span><span class="o">();</span>    <span class="c1">// Returns True for DataFrames that have streaming sources</span>

<span class="n">socketDF</span><span class="o">.</span><span class="na">printSchema</span><span class="o">();</span>

<span class="c1">// Read all the csv files written atomically in a directory</span>
<span class="n">StructType</span> <span class="n">userSchema</span> <span class="o">=</span> <span class="k">new</span> <span class="n">StructType</span><span class="o">().</span><span class="na">add</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">,</span> <span class="s">&quot;string&quot;</span><span class="o">).</span><span class="na">add</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">,</span> <span class="s">&quot;integer&quot;</span><span class="o">);</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">csvDF</span> <span class="o">=</span> <span class="n">spark</span>
  <span class="o">.</span><span class="na">readStream</span><span class="o">()</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;sep&quot;</span><span class="o">,</span> <span class="s">&quot;;&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">schema</span><span class="o">(</span><span class="n">userSchema</span><span class="o">)</span>      <span class="c1">// Specify schema of the csv files</span>
  <span class="o">.</span><span class="na">csv</span><span class="o">(</span><span class="s">&quot;/path/to/directory&quot;</span><span class="o">);</span>    <span class="c1">// Equivalent to format(&quot;csv&quot;).load(&quot;/path/to/directory&quot;)</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span> <span class="o">...</span>

<span class="c1"># Read text from socket</span>
<span class="n">socketDF</span> <span class="o">=</span> <span class="n">spark</span> \
    <span class="o">.</span><span class="n">readStream</span> \
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;socket&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;host&quot;</span><span class="p">,</span> <span class="s2">&quot;localhost&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;port&quot;</span><span class="p">,</span> <span class="mi">9999</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="n">socketDF</span><span class="o">.</span><span class="n">isStreaming</span><span class="p">()</span>    <span class="c1"># Returns True for DataFrames that have streaming sources</span>

<span class="n">socketDF</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>

<span class="c1"># Read all the csv files written atomically in a directory</span>
<span class="n">userSchema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;string&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="s2">&quot;integer&quot;</span><span class="p">)</span>
<span class="n">csvDF</span> <span class="o">=</span> <span class="n">spark</span> \
    <span class="o">.</span><span class="n">readStream</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;sep&quot;</span><span class="p">,</span> <span class="s2">&quot;;&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="n">userSchema</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s2">&quot;/path/to/directory&quot;</span><span class="p">)</span>  <span class="c1"># Equivalent to format(&quot;csv&quot;).load(&quot;/path/to/directory&quot;)</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>sparkR.session<span class="p">(</span><span class="kc">...</span><span class="p">)</span>

<span class="c1"># Read text from socket</span>
socketDF <span class="o">&lt;-</span> read.stream<span class="p">(</span><span class="s">&quot;socket&quot;</span><span class="p">,</span> host <span class="o">=</span> hostname<span class="p">,</span> port <span class="o">=</span> port<span class="p">)</span>

isStreaming<span class="p">(</span>socketDF<span class="p">)</span>    <span class="c1"># Returns TRUE for SparkDataFrames that have streaming sources</span>

printSchema<span class="p">(</span>socketDF<span class="p">)</span>

<span class="c1"># Read all the csv files written atomically in a directory</span>
schema <span class="o">&lt;-</span> structType<span class="p">(</span>structField<span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="s">&quot;string&quot;</span><span class="p">),</span>
                     structField<span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="s">&quot;integer&quot;</span><span class="p">))</span>
csvDF <span class="o">&lt;-</span> read.stream<span class="p">(</span><span class="s">&quot;csv&quot;</span><span class="p">,</span> path <span class="o">=</span> <span class="s">&quot;/path/to/directory&quot;</span><span class="p">,</span> schema <span class="o">=</span> schema<span class="p">,</span> sep <span class="o">=</span> <span class="s">&quot;;&quot;</span><span class="p">)</span></code></pre></figure>

  </div>
</div>

<p>这些示例生成未类型化的流式DataFrame，这意味着在编译时不检查DataFrame的架构，仅在提交查询时在运行时检查它。一些操作如<code>map</code> ， <code>flatMap</code>等需要在编译时知道的类型。为此，您可以使用与静态DataFrame相同的方法将这些未类型化的流数据框架转换为已类型化的流数据集。有关更多详细信息，请参见《 <a href="sql-programming-guide.html">SQL编程指南》</a> 。此外，本文档后面将讨论有关受支持的流媒体源的更多详细信息。</p>

<h3 id="schema-inference-and-partition-of-streaming-dataframesdatasets">流数据帧/数据集的模式推断和分区</h3>

<p>默认情况下，从基于文件的源进行结构化流式传输需要您指定架构，而不是依靠Spark自动进行推断。此限制确保即使在发生故障的情况下，也将一致的架构用于流查询。对于临时用例，您可以通过设置来重新启用模式推断<code>spark.sql.streaming.schemaInference</code>至<code>true</code> 。</p>

<p>当命名子目录时，确实会发生分区发现<code>/key=value/</code>存在并且列表将自动递归到这些目录中。如果这些列出现在用户提供的架构中，Spark将根据读取文件的路径来填充它们。启动查询时，必须存在组成分区方案的目录，并且这些目录必须保持静态。例如，可以添加<code>/data/year=2016/</code>什么时候<code>/data/year=2015/</code>存在，但是更改分区列是无效的（即通过创建目录<code>/data/date=2016-04-17/</code> ）。</p>

<h2 id="operations-on-streaming-dataframesdatasets">流式数据帧/数据集的操作</h2>
<p>您可以对流式数据帧/数据集应用各种操作-包括无类型，类似SQL的操作（例如<code>select</code> ， <code>where</code> ， <code>groupBy</code> ），以键入类似RDD的操作（例如<code>map</code> ， <code>filter</code> ， <code>flatMap</code> ）。有关更多详细信息，请参见<a href="sql-programming-guide.html">SQL编程指南</a> 。让我们看一些可以使用的示例操作。</p>

<h3 id="basic-operations---selection-projection-aggregation">基本操作-选择，投影，汇总</h3>
<p>流上支持DataFrame / Dataset上的大多数常见操作。本节<a href="#unsupported-operations">稍后</a>将<a href="#unsupported-operations">讨论</a>不支持的一些操作。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">case</span> <span class="k">class</span> <span class="nc">DeviceData</span><span class="o">(</span><span class="n">device</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">deviceType</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">signal</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">time</span><span class="k">:</span> <span class="kt">DateTime</span><span class="o">)</span>

<span class="k">val</span> <span class="n">df</span><span class="k">:</span> <span class="kt">DataFrame</span> <span class="o">=</span> <span class="o">...</span> <span class="c1">// streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: string }</span>
<span class="k">val</span> <span class="n">ds</span><span class="k">:</span> <span class="kt">Dataset</span><span class="o">[</span><span class="kt">DeviceData</span><span class="o">]</span> <span class="k">=</span> <span class="n">df</span><span class="o">.</span><span class="n">as</span><span class="o">[</span><span class="kt">DeviceData</span><span class="o">]</span>    <span class="c1">// streaming Dataset with IOT device data</span>

<span class="c1">// Select the devices which have signal more than 10</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="s">&quot;device&quot;</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&quot;signal &gt; 10&quot;</span><span class="o">)</span>      <span class="c1">// using untyped APIs   </span>
<span class="n">ds</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">signal</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">device</span><span class="o">)</span>         <span class="c1">// using typed APIs</span>

<span class="c1">// Running count of the number of updates for each device type</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&quot;deviceType&quot;</span><span class="o">).</span><span class="n">count</span><span class="o">()</span>                          <span class="c1">// using untyped API</span>

<span class="c1">// Running average signal for each device type</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.expressions.scalalang.typed</span>
<span class="n">ds</span><span class="o">.</span><span class="n">groupByKey</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">deviceType</span><span class="o">).</span><span class="n">agg</span><span class="o">(</span><span class="n">typed</span><span class="o">.</span><span class="n">avg</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">signal</span><span class="o">))</span>    <span class="c1">// using typed API</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.api.java.function.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.*</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.expressions.javalang.typed</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.catalyst.encoders.ExpressionEncoder</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">DeviceData</span> <span class="o">{</span>
  <span class="kd">private</span> <span class="n">String</span> <span class="n">device</span><span class="o">;</span>
  <span class="kd">private</span> <span class="n">String</span> <span class="n">deviceType</span><span class="o">;</span>
  <span class="kd">private</span> <span class="n">Double</span> <span class="n">signal</span><span class="o">;</span>
  <span class="kd">private</span> <span class="n">java</span><span class="o">.</span><span class="na">sql</span><span class="o">.</span><span class="na">Date</span> <span class="n">time</span><span class="o">;</span>
  <span class="o">...</span>
  <span class="c1">// Getter and setter methods for each field</span>
<span class="o">}</span>

<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">df</span> <span class="o">=</span> <span class="o">...;</span>    <span class="c1">// streaming DataFrame with IOT device data with schema { device: string, type: string, signal: double, time: DateType }</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">DeviceData</span><span class="o">&gt;</span> <span class="n">ds</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="na">as</span><span class="o">(</span><span class="n">ExpressionEncoder</span><span class="o">.</span><span class="na">javaBean</span><span class="o">(</span><span class="n">DeviceData</span><span class="o">.</span><span class="na">class</span><span class="o">));</span> <span class="c1">// streaming Dataset with IOT device data</span>

<span class="c1">// Select the devices which have signal more than 10</span>
<span class="n">df</span><span class="o">.</span><span class="na">select</span><span class="o">(</span><span class="s">&quot;device&quot;</span><span class="o">).</span><span class="na">where</span><span class="o">(</span><span class="s">&quot;signal &gt; 10&quot;</span><span class="o">);</span> <span class="c1">// using untyped APIs</span>
<span class="n">ds</span><span class="o">.</span><span class="na">filter</span><span class="o">((</span><span class="n">FilterFunction</span><span class="o">&lt;</span><span class="n">DeviceData</span><span class="o">&gt;)</span> <span class="n">value</span> <span class="o">-&gt;</span> <span class="n">value</span><span class="o">.</span><span class="na">getSignal</span><span class="o">()</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="o">)</span>
  <span class="o">.</span><span class="na">map</span><span class="o">((</span><span class="n">MapFunction</span><span class="o">&lt;</span><span class="n">DeviceData</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;)</span> <span class="n">value</span> <span class="o">-&gt;</span> <span class="n">value</span><span class="o">.</span><span class="na">getDevice</span><span class="o">(),</span> <span class="n">Encoders</span><span class="o">.</span><span class="na">STRING</span><span class="o">());</span>

<span class="c1">// Running count of the number of updates for each device type</span>
<span class="n">df</span><span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="s">&quot;deviceType&quot;</span><span class="o">).</span><span class="na">count</span><span class="o">();</span> <span class="c1">// using untyped API</span>

<span class="c1">// Running average signal for each device type</span>
<span class="n">ds</span><span class="o">.</span><span class="na">groupByKey</span><span class="o">((</span><span class="n">MapFunction</span><span class="o">&lt;</span><span class="n">DeviceData</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;)</span> <span class="n">value</span> <span class="o">-&gt;</span> <span class="n">value</span><span class="o">.</span><span class="na">getDeviceType</span><span class="o">(),</span> <span class="n">Encoders</span><span class="o">.</span><span class="na">STRING</span><span class="o">())</span>
  <span class="o">.</span><span class="na">agg</span><span class="o">(</span><span class="n">typed</span><span class="o">.</span><span class="na">avg</span><span class="o">((</span><span class="n">MapFunction</span><span class="o">&lt;</span><span class="n">DeviceData</span><span class="o">,</span> <span class="n">Double</span><span class="o">&gt;)</span> <span class="n">value</span> <span class="o">-&gt;</span> <span class="n">value</span><span class="o">.</span><span class="na">getSignal</span><span class="o">()));</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">df</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: DateType }</span>

<span class="c1"># Select the devices which have signal more than 10</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;device&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s2">&quot;signal &gt; 10&quot;</span><span class="p">)</span>

<span class="c1"># Running count of the number of updates for each device type</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;deviceType&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>df <span class="o">&lt;-</span> <span class="kc">...</span>  <span class="c1"># streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: DateType }</span>

<span class="c1"># Select the devices which have signal more than 10</span>
select<span class="p">(</span>where<span class="p">(</span>df<span class="p">,</span> <span class="s">&quot;signal &gt; 10&quot;</span><span class="p">),</span> <span class="s">&quot;device&quot;</span><span class="p">)</span>

<span class="c1"># Running count of the number of updates for each device type</span>
count<span class="p">(</span>groupBy<span class="p">(</span>df<span class="p">,</span> <span class="s">&quot;deviceType&quot;</span><span class="p">))</span></code></pre></figure>

  </div>
</div>

<p>您还可以将流式DataFrame / Dataset注册为临时视图，然后在其上应用SQL命令。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;updates&quot;</span><span class="o">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;select count(*) from updates&quot;</span><span class="o">)</span>  <span class="c1">// returns another streaming DF</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">df</span><span class="o">.</span><span class="na">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;updates&quot;</span><span class="o">);</span>
<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;select count(*) from updates&quot;</span><span class="o">);</span>  <span class="c1">// returns another streaming DF</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;updates&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;select count(*) from updates&quot;</span><span class="p">)</span>  <span class="c1"># returns another streaming DF</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>createOrReplaceTempView<span class="p">(</span>df<span class="p">,</span> <span class="s">&quot;updates&quot;</span><span class="p">)</span>
sql<span class="p">(</span><span class="s">&quot;select count(*) from updates&quot;</span><span class="p">)</span></code></pre></figure>

  </div>
</div>

<p>注意，您可以通过使用来确定DataFrame / Dataset是否具有流数据<code>df.isStreaming</code> 。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">df</span><span class="o">.</span><span class="n">isStreaming</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">df</span><span class="o">.</span><span class="na">isStreaming</span><span class="o">()</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">df</span><span class="o">.</span><span class="n">isStreaming</span><span class="p">()</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>isStreaming<span class="p">(</span>df<span class="p">)</span></code></pre></figure>

  </div>
</div>

<h3 id="window-operations-on-event-time">事件时间窗口操作</h3>
<p>滑动事件时间窗口上的聚合对于结构化流而言非常简单，并且与分组聚合非常相似。在分组聚合中，在用户指定的分组列中为每个唯一值维护聚合值（例如，计数）。在基于窗口的聚合的情况下，行事件时间所属的每个窗口都会维护聚合值。让我们通过插图来了解这一点。</p>

<p>想象一下我们的<a href="#quick-example">快速示例</a>已被修改，并且流现在包含行以及生成行的时间。而不是运行字数统计，我们希望在10分钟的窗口内对字数进行计数，每5分钟更新一次。也就是说，在10分钟窗口12:00-12：10、12：05-12：15、12：10-12:20等之间接收到的单词中的单词计数。请注意，12：00-12:10表示数据12:00之后但12:10之前到达。现在，考虑在12:07收到的单词。此字应增加对应于两个窗口12:00-12:10和12:05-12:15的计数。因此，计数将通过分组键（即单词）和窗口（可以从事件时间计算）来索引。</p>

<p>结果表如下所示。</p>

<p><img src="img/structured-streaming-window.png" alt="窗口操作"></p>

<p>由于此窗口类似于分组，因此在代码中，您可以使用<code>groupBy()</code>和<code>window()</code>表示窗口聚合的操作。您可以在<a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala">Scala</a> / <a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java">Java</a> / <a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py">Python中</a>看到以下示例的完整代码。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">spark.implicits._</span>

<span class="k">val</span> <span class="n">words</span> <span class="k">=</span> <span class="o">...</span> <span class="c1">// streaming DataFrame of schema { timestamp: Timestamp, word: String }</span>

<span class="c1">// Group the data by window and word and compute the count of each group</span>
<span class="k">val</span> <span class="n">windowedCounts</span> <span class="k">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span>
  <span class="n">window</span><span class="o">(</span><span class="n">$</span><span class="s">&quot;timestamp&quot;</span><span class="o">,</span> <span class="s">&quot;10 minutes&quot;</span><span class="o">,</span> <span class="s">&quot;5 minutes&quot;</span><span class="o">),</span>
  <span class="n">$</span><span class="s">&quot;word&quot;</span>
<span class="o">).</span><span class="n">count</span><span class="o">()</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">words</span> <span class="o">=</span> <span class="o">...</span> <span class="c1">// streaming DataFrame of schema { timestamp: Timestamp, word: String }</span>

<span class="c1">// Group the data by window and word and compute the count of each group</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">windowedCounts</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="na">groupBy</span><span class="o">(</span>
  <span class="n">functions</span><span class="o">.</span><span class="na">window</span><span class="o">(</span><span class="n">words</span><span class="o">.</span><span class="na">col</span><span class="o">(</span><span class="s">&quot;timestamp&quot;</span><span class="o">),</span> <span class="s">&quot;10 minutes&quot;</span><span class="o">,</span> <span class="s">&quot;5 minutes&quot;</span><span class="o">),</span>
  <span class="n">words</span><span class="o">.</span><span class="na">col</span><span class="o">(</span><span class="s">&quot;word&quot;</span><span class="o">)</span>
<span class="o">).</span><span class="na">count</span><span class="o">();</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">words</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># streaming DataFrame of schema { timestamp: Timestamp, word: String }</span>

<span class="c1"># Group the data by window and word and compute the count of each group</span>
<span class="n">windowedCounts</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span>
    <span class="n">window</span><span class="p">(</span><span class="n">words</span><span class="o">.</span><span class="n">timestamp</span><span class="p">,</span> <span class="s2">&quot;10 minutes&quot;</span><span class="p">,</span> <span class="s2">&quot;5 minutes&quot;</span><span class="p">),</span>
    <span class="n">words</span><span class="o">.</span><span class="n">word</span>
<span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>words <span class="o">&lt;-</span> <span class="kc">...</span>  <span class="c1"># streaming DataFrame of schema { timestamp: Timestamp, word: String }</span>

<span class="c1"># Group the data by window and word and compute the count of each group</span>
windowedCounts <span class="o">&lt;-</span> count<span class="p">(</span>
                    groupBy<span class="p">(</span>
                      words<span class="p">,</span>
                      window<span class="p">(</span>words<span class="o">$</span>timestamp<span class="p">,</span> <span class="s">&quot;10 minutes&quot;</span><span class="p">,</span> <span class="s">&quot;5 minutes&quot;</span><span class="p">),</span>
                      words<span class="o">$</span>word<span class="p">))</span></code></pre></figure>

  </div>
</div>

<h4 id="handling-late-data-and-watermarking">处理后期数据和加水印</h4>
<p>现在考虑如果事件之一迟到了应用程序会发生什么。例如，例如，应用程序可以在12:11接收在12:04（即事件时间）生成的单词。应用程序应使用时间12:04而不是12:11来更新窗口的旧计数<code>12:00 - 12:10</code> 。这在基于窗口的分组中很自然地发生-结构化流可以长时间保持部分聚合的中间状态，以便后期数据可以正确更新旧窗口的聚合，如下所示。</p>

<p><img src="img/structured-streaming-late-data.png" alt="处理后期数据"></p>

<p>但是，要连续几天运行此查询，系统必须限制其累积的中间内存状态量。这意味着系统需要知道何时可以从内存中状态删除旧聚合，因为应用程序将不再接收该聚合的最新数据。为此，我们在Spark 2.1中引入了<strong>水印功能</strong> ，该功能使引擎自动跟踪数据中的当前事件时间，并尝试相应地清除旧状态。您可以通过指定事件时间列和有关事件时间期望数据延迟的阈值来定义查询的水印。对于特定的窗口，该窗口在时间结束时<code>T</code> ，引擎将维持状态并允许以后的数据更新状态，直到<code>(max event time seen by the engine - late threshold > T)</code> 。换句话说，阈值内的延迟数据将被汇总，但阈值后的数据将开始被丢弃（有关确切保证，请参阅本节<a href="#semantic-guarantees-of-aggregation-with-watermarking">后面</a>的内容）。让我们通过一个例子来理解这一点。我们可以使用以下示例在上一个示例中轻松定义水印<code>withWatermark()</code>如下所示。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">spark.implicits._</span>

<span class="k">val</span> <span class="n">words</span> <span class="k">=</span> <span class="o">...</span> <span class="c1">// streaming DataFrame of schema { timestamp: Timestamp, word: String }</span>

<span class="c1">// Group the data by window and word and compute the count of each group</span>
<span class="k">val</span> <span class="n">windowedCounts</span> <span class="k">=</span> <span class="n">words</span>
    <span class="o">.</span><span class="n">withWatermark</span><span class="o">(</span><span class="s">&quot;timestamp&quot;</span><span class="o">,</span> <span class="s">&quot;10 minutes&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span>
        <span class="n">window</span><span class="o">(</span><span class="n">$</span><span class="s">&quot;timestamp&quot;</span><span class="o">,</span> <span class="s">&quot;10 minutes&quot;</span><span class="o">,</span> <span class="s">&quot;5 minutes&quot;</span><span class="o">),</span>
        <span class="n">$</span><span class="s">&quot;word&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">count</span><span class="o">()</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">words</span> <span class="o">=</span> <span class="o">...</span> <span class="c1">// streaming DataFrame of schema { timestamp: Timestamp, word: String }</span>

<span class="c1">// Group the data by window and word and compute the count of each group</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">windowedCounts</span> <span class="o">=</span> <span class="n">words</span>
    <span class="o">.</span><span class="na">withWatermark</span><span class="o">(</span><span class="s">&quot;timestamp&quot;</span><span class="o">,</span> <span class="s">&quot;10 minutes&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="na">groupBy</span><span class="o">(</span>
        <span class="n">functions</span><span class="o">.</span><span class="na">window</span><span class="o">(</span><span class="n">words</span><span class="o">.</span><span class="na">col</span><span class="o">(</span><span class="s">&quot;timestamp&quot;</span><span class="o">),</span> <span class="s">&quot;10 minutes&quot;</span><span class="o">,</span> <span class="s">&quot;5 minutes&quot;</span><span class="o">),</span>
        <span class="n">words</span><span class="o">.</span><span class="na">col</span><span class="o">(</span><span class="s">&quot;word&quot;</span><span class="o">))</span>
    <span class="o">.</span><span class="na">count</span><span class="o">();</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">words</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># streaming DataFrame of schema { timestamp: Timestamp, word: String }</span>

<span class="c1"># Group the data by window and word and compute the count of each group</span>
<span class="n">windowedCounts</span> <span class="o">=</span> <span class="n">words</span> \
    <span class="o">.</span><span class="n">withWatermark</span><span class="p">(</span><span class="s2">&quot;timestamp&quot;</span><span class="p">,</span> <span class="s2">&quot;10 minutes&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">groupBy</span><span class="p">(</span>
        <span class="n">window</span><span class="p">(</span><span class="n">words</span><span class="o">.</span><span class="n">timestamp</span><span class="p">,</span> <span class="s2">&quot;10 minutes&quot;</span><span class="p">,</span> <span class="s2">&quot;5 minutes&quot;</span><span class="p">),</span>
        <span class="n">words</span><span class="o">.</span><span class="n">word</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">count</span><span class="p">()</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>words <span class="o">&lt;-</span> <span class="kc">...</span>  <span class="c1"># streaming DataFrame of schema { timestamp: Timestamp, word: String }</span>

<span class="c1"># Group the data by window and word and compute the count of each group</span>

words <span class="o">&lt;-</span> withWatermark<span class="p">(</span>words<span class="p">,</span> <span class="s">&quot;timestamp&quot;</span><span class="p">,</span> <span class="s">&quot;10 minutes&quot;</span><span class="p">)</span>
windowedCounts <span class="o">&lt;-</span> count<span class="p">(</span>
                    groupBy<span class="p">(</span>
                      words<span class="p">,</span>
                      window<span class="p">(</span>words<span class="o">$</span>timestamp<span class="p">,</span> <span class="s">&quot;10 minutes&quot;</span><span class="p">,</span> <span class="s">&quot;5 minutes&quot;</span><span class="p">),</span>
                      words<span class="o">$</span>word<span class="p">))</span></code></pre></figure>

  </div>
</div>

<p>在此示例中，我们将在“时间戳”列的值上定义查询的水印，还将“ 10分钟”定义为允许数据晚到的阈值。如果此查询在“更新输出”模式下运行（稍后在“ <a href="#output-modes">输出模式”</a>部分中讨论），则引擎将在“结果表”中保持窗口的更新计数，直到该窗口早于水印为止，该水印滞后于“列”中的当前事件时间。时间戳”的时间缩短10分钟。这是一个例子。</p>

<p><img src="img/structured-streaming-watermark-update-mode.png" alt="更新模式中的水印"></p>

<p>如图所示，引擎跟踪的最大事件时间是<em>蓝色虚线</em> ，并且水印设置为<code>(max event time - '10 mins')</code>在每个触发器的开头都是红线。例如，当引擎观察到数据时<code>(12:14, dog)</code> ，它将下一个触发器的水印设置为<code>12:04</code> 。此水印可让引擎再保持10分钟的中间状态，以便对较晚的数据进行计数。例如数据<code>(12:09, cat)</code>出故障了，很晚，落在窗户上<code>12:00 - 12:10</code>和<code>12:05 - 12:15</code> 。因为，它仍然领先于水印<code>12:04</code>在触发器中，引擎仍将中间计数保持为状态，并正确更新相关窗口的计数。但是，当水印更新为<code>12:11</code> ，窗口的中间状态<code>(12:00 - 12:10)</code>已清除，并且所有后续数据（例如<code>(12:04, donkey)</code> ）被认为“为时已晚”，因此被忽略。请注意，在每次触发之后，更新的计数（即紫色行）都会写入更新，作为更新输出指示的触发输出。</p>

<p>某些接收器（例如文件）可能不支持更新模式所需的细粒度更新。为了与他们一起工作，我们还支持追加模式，其中仅将<em>最终计数</em>写入接收器。如下所示。</p>

<p>注意使用<code>withWatermark</code>在非流数据集上，此操作为空操作。由于水印不应以任何方式影响任何批量查询，因此我们将直接忽略它。</p>

<p><img src="img/structured-streaming-watermark-append-mode.png" alt="追加模式中的水印"></p>

<p>与之前的更新模式类似，引擎为每个窗口维护中间计数。但是，部分计数不会更新到结果表，也不会写入接收器。引擎等待“ 10分钟”来计算延迟日期，然后丢弃窗口的中间状态<水印，并将最终计数附加到结果表/接收器。例如，窗口的最终计数<code>12:00 - 12:10</code>仅在将水印更新为时才将其附加到结果表<code>12:11</code> 。</p>

<h5 class="no_toc" id="conditions-for-watermarking-to-clean-aggregation-state">加水印以清除聚合状态的条件</h5>

<p>重要的是要注意，必须满足以下条件才能使用水印清除聚合查询中的状态<em>（从Spark 2.1.1开始，将来可能会更改）</em> 。</p>

<ul>
  <li>
    <p><strong>输出模式必须为“追加”或“更新”。</strong> 完整模式要求保留所有聚合数据，因此不能使用水印删除中间状态。有关每种输出模式的语义的详细说明，请参见“ <a href="#output-modes">输出模式”</a>部分。</p>
  </li>
  <li>
    <p>聚合中必须包含事件时间列或<code>window</code>在事件时间列上。</p>
  </li>
  <li>
    <p><code>withWatermark</code>必须在与汇总中使用的时间戳列相同的列上调用。例如， <code>df.withWatermark("time", "1 min").groupBy("time2").count()</code>在附加输出模式下无效，因为水印是在与聚合列不同的列上定义的。</p>
  </li>
  <li>
    <p><code>withWatermark</code>必须在使用水印详细信息的聚合之前调用。例如， <code>df.groupBy("time").count().withWatermark("time", "1 min")</code>在追加输出模式下无效。</p>
  </li>
</ul>

<h5 class="no_toc" id="semantic-guarantees-of-aggregation-with-watermarking">带水印聚合的语义保证</h5>

<ul>
  <li>
    <p>水印延迟（设置为<code>withWatermark</code> ）（“ 2小时”）可确保引擎永远不会丢弃任何少于2小时延迟的数据。换句话说，任何聚合的数据（在事件时间方面）比在此之前处理的最新数据晚2小时以内的，都可以保证得到汇总。</p>
  </li>
  <li>
    <p>但是，保证仅在一个方向上严格。延迟超过2小时的数据不能保证被删除；它可能会或可能不会聚合。数据延迟更多，引擎处理数据的可能性越小。</p>
  </li>
</ul>

<h3 id="join-operations">加盟运营</h3>
<p>结构化流支持将流数据集/数据帧与静态数据集/数据帧以及另一个流数据集/数据帧连接在一起。流连接的结果是增量生成的，类似于上一节中的流聚合的结果。在本节中，我们将探讨在上述情况下支持哪些类型的联接（即内部，外部等）。请注意，在所有受支持的联接类型中，使用流数据集/ DataFrame进行联接的结果将与使用包含流中相同数据的静态数据集/ DataFrame进行联接的结果完全相同。</p>

<h4 id="stream-static-joins">流静态联接</h4>

<p>自从Spark 2.0引入以来，结构化流已支持流和静态DataFrame / Dataset之间的联接（内部联接和某种类型的外部联接）。这是一个简单的例子。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">staticDf</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">streamingDf</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span> <span class="o">...</span>

<span class="n">streamingDf</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">staticDf</span><span class="o">,</span> <span class="s">&quot;type&quot;</span><span class="o">)</span>          <span class="c1">// inner equi-join with a static DF</span>
<span class="n">streamingDf</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">staticDf</span><span class="o">,</span> <span class="s">&quot;type&quot;</span><span class="o">,</span> <span class="s">&quot;right_join&quot;</span><span class="o">)</span>  <span class="c1">// right outer join with a static DF  </span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">staticDf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">read</span><span class="o">().</span> <span class="o">...;</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">streamingDf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">readStream</span><span class="o">().</span> <span class="o">...;</span>
<span class="n">streamingDf</span><span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">staticDf</span><span class="o">,</span> <span class="s">&quot;type&quot;</span><span class="o">);</span>         <span class="c1">// inner equi-join with a static DF</span>
<span class="n">streamingDf</span><span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">staticDf</span><span class="o">,</span> <span class="s">&quot;type&quot;</span><span class="o">,</span> <span class="s">&quot;right_join&quot;</span><span class="o">);</span>  <span class="c1">// right outer join with a static DF</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">staticDf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span> <span class="o">...</span>
<span class="n">streamingDf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span> <span class="o">...</span>
<span class="n">streamingDf</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">staticDf</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">)</span>  <span class="c1"># inner equi-join with a static DF</span>
<span class="n">streamingDf</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">staticDf</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">,</span> <span class="s2">&quot;right_join&quot;</span><span class="p">)</span>  <span class="c1"># right outer join with a static DF</span></code></pre></figure>

  </div>

<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>staticDf <span class="o">&lt;-</span> read.df<span class="p">(</span><span class="kc">...</span><span class="p">)</span>
streamingDf <span class="o">&lt;-</span> read.stream<span class="p">(</span><span class="kc">...</span><span class="p">)</span>
joined <span class="o">&lt;-</span> <span class="kp">merge</span><span class="p">(</span>streamingDf<span class="p">,</span> staticDf<span class="p">,</span> sort <span class="o">=</span> <span class="kc">FALSE</span><span class="p">)</span>  <span class="c1"># inner equi-join with a static DF</span>
joined <span class="o">&lt;-</span> join<span class="p">(</span>
            staticDf<span class="p">,</span>
            streamingDf<span class="p">,</span> 
            streamingDf<span class="o">$</span>value <span class="o">==</span> staticDf<span class="o">$</span>value<span class="p">,</span>
            <span class="s">&quot;right_outer&quot;</span><span class="p">)</span>  <span class="c1"># right outer join with a static DF</span></code></pre></figure>

  </div>
</div>

<p>请注意，流静态联接不是有状态的，因此不需要状态管理。但是，尚不支持某些类型的流静态外部联接。这些在<a href="#support-matrix-for-joins-in-streaming-queries">本“连接”部分</a>的<a href="#support-matrix-for-joins-in-streaming-queries">末尾</a>列出。</p>

<h4 id="stream-stream-joins">流流连接</h4>
<p>在Spark 2.3中，我们添加了对流流连接的支持，即，您可以连接两个流数据集/ DataFrame。在两个数据流之间生成联接结果的挑战在于，在任何时间点，联接两侧的数据集视图都不完整，这使得在输入之间查找匹配项变得更加困难。从一个输入流接收到的任何行都可以与另一输入流中将来接收到的任何行匹配。因此，对于两个输入流，我们将过去的输入作为流状态进行缓冲，以便我们可以将每个将来的输入与过去的输入进行匹配，并相应地生成合并的结果。此外，类似于流聚合，我们会自动处理较晚的乱序数据，并可以使用水印限制状态。让我们讨论支持的流连接的不同类型以及如何使用它们。</p>

<h5 id="inner-joins-with-optional-watermarking">内部联接，带有可选水印</h5>
<p>支持任何类型的列上的内部连接以及任何类型的连接条件。但是，随着流的运行，流状态的大小将无限期增长，因为必须保存<em>所有</em>过去的输入，因为任何新输入都可以与过去的任何输入匹配。为了避免无界状态，您必须定义其他联接条件，以使无限期的旧输入不能与将来的输入匹配，因此可以从状态中清除它们。换句话说，您将必须在连接中执行以下其他步骤。</p>

<ol>
  <li>
    <p>在两个输入上定义水印延迟，以便引擎知道输入的延迟（类似于流聚合）</p>
  </li>
  <li>
    <p>定义两个输入之间的事件时间约束，以便引擎可以确定何时不需要与另一个输入匹配的一个输入的旧行（即，不满足时间约束）。可以通过两种方式之一来定义此约束。</p>

    <ol>
      <li>
        <p>时间范围加入条件（例如<code>...JOIN ON leftTime BETWEEN rightTime AND rightTime + INTERVAL 1 HOUR</code> ），</p>
      </li>
      <li>
        <p>加入活动时间窗口（例如<code>...JOIN ON leftTimeWindow = rightTimeWindow</code> ）。</p>
      </li>
    </ol>
  </li>
</ol>

<p>让我们通过一个例子来理解这一点。</p>

<p>假设我们想将广告印象流（在显示广告时）与另一用户点击广告流结合起来，以在印象带来可货币化点击时进行关联。要在此流-流连接中允许状态清除，您将必须指定水印延迟和时间限制，如下所示。</p>

<ol>
  <li>
    <p>水印延迟：说，在事件时间内，印象和相应的点击可能延迟/乱序最多分别为2和3个小时。</p>
  </li>
  <li>
    <p>事件时间范围条件：假设在相应的展示之后0秒到1小时的时间范围内可以发生点击。</p>
  </li>
</ol>

<p>代码看起来像这样。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">org.apache.spark.sql.functions.expr</span>

<span class="k">val</span> <span class="n">impressions</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span> <span class="o">...</span>
<span class="k">val</span> <span class="n">clicks</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span> <span class="o">...</span>

<span class="c1">// Apply watermarks on event-time columns</span>
<span class="k">val</span> <span class="n">impressionsWithWatermark</span> <span class="k">=</span> <span class="n">impressions</span><span class="o">.</span><span class="n">withWatermark</span><span class="o">(</span><span class="s">&quot;impressionTime&quot;</span><span class="o">,</span> <span class="s">&quot;2 hours&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">clicksWithWatermark</span> <span class="k">=</span> <span class="n">clicks</span><span class="o">.</span><span class="n">withWatermark</span><span class="o">(</span><span class="s">&quot;clickTime&quot;</span><span class="o">,</span> <span class="s">&quot;3 hours&quot;</span><span class="o">)</span>

<span class="c1">// Join with event-time constraints</span>
<span class="n">impressionsWithWatermark</span><span class="o">.</span><span class="n">join</span><span class="o">(</span>
  <span class="n">clicksWithWatermark</span><span class="o">,</span>
  <span class="n">expr</span><span class="o">(</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">    clickAdId = impressionAdId AND</span>
<span class="s">    clickTime &gt;= impressionTime AND</span>
<span class="s">    clickTime &lt;= impressionTime + interval 1 hour</span>
<span class="s">    &quot;&quot;&quot;</span><span class="o">)</span>
<span class="o">)</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kn">import static</span> <span class="nn">org.apache.spark.sql.functions.expr</span>

<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">impressions</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">readStream</span><span class="o">().</span> <span class="o">...</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">clicks</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">readStream</span><span class="o">().</span> <span class="o">...</span>

<span class="c1">// Apply watermarks on event-time columns</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">impressionsWithWatermark</span> <span class="o">=</span> <span class="n">impressions</span><span class="o">.</span><span class="na">withWatermark</span><span class="o">(</span><span class="s">&quot;impressionTime&quot;</span><span class="o">,</span> <span class="s">&quot;2 hours&quot;</span><span class="o">);</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">clicksWithWatermark</span> <span class="o">=</span> <span class="n">clicks</span><span class="o">.</span><span class="na">withWatermark</span><span class="o">(</span><span class="s">&quot;clickTime&quot;</span><span class="o">,</span> <span class="s">&quot;3 hours&quot;</span><span class="o">);</span>

<span class="c1">// Join with event-time constraints</span>
<span class="n">impressionsWithWatermark</span><span class="o">.</span><span class="na">join</span><span class="o">(</span>
  <span class="n">clicksWithWatermark</span><span class="o">,</span>
  <span class="n">expr</span><span class="o">(</span>
    <span class="s">&quot;clickAdId = impressionAdId AND &quot;</span> <span class="o">+</span>
    <span class="s">&quot;clickTime &gt;= impressionTime AND &quot;</span> <span class="o">+</span>
    <span class="s">&quot;clickTime &lt;= impressionTime + interval 1 hour &quot;</span><span class="o">)</span>
<span class="o">);</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">expr</span>

<span class="n">impressions</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span> <span class="o">...</span>
<span class="n">clicks</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span> <span class="o">...</span>

<span class="c1"># Apply watermarks on event-time columns</span>
<span class="n">impressionsWithWatermark</span> <span class="o">=</span> <span class="n">impressions</span><span class="o">.</span><span class="n">withWatermark</span><span class="p">(</span><span class="s2">&quot;impressionTime&quot;</span><span class="p">,</span> <span class="s2">&quot;2 hours&quot;</span><span class="p">)</span>
<span class="n">clicksWithWatermark</span> <span class="o">=</span> <span class="n">clicks</span><span class="o">.</span><span class="n">withWatermark</span><span class="p">(</span><span class="s2">&quot;clickTime&quot;</span><span class="p">,</span> <span class="s2">&quot;3 hours&quot;</span><span class="p">)</span>

<span class="c1"># Join with event-time constraints</span>
<span class="n">impressionsWithWatermark</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
  <span class="n">clicksWithWatermark</span><span class="p">,</span>
  <span class="n">expr</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    clickAdId = impressionAdId AND</span>
<span class="s2">    clickTime &gt;= impressionTime AND</span>
<span class="s2">    clickTime &lt;= impressionTime + interval 1 hour</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="p">)</span>
<span class="p">)</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>impressions <span class="o">&lt;-</span> read.stream<span class="p">(</span><span class="kc">...</span><span class="p">)</span>
clicks <span class="o">&lt;-</span> read.stream<span class="p">(</span><span class="kc">...</span><span class="p">)</span>

<span class="c1"># Apply watermarks on event-time columns</span>
impressionsWithWatermark <span class="o">&lt;-</span> withWatermark<span class="p">(</span>impressions<span class="p">,</span> <span class="s">&quot;impressionTime&quot;</span><span class="p">,</span> <span class="s">&quot;2 hours&quot;</span><span class="p">)</span>
clicksWithWatermark <span class="o">&lt;-</span> withWatermark<span class="p">(</span>clicks<span class="p">,</span> <span class="s">&quot;clickTime&quot;</span><span class="p">,</span> <span class="s">&quot;3 hours&quot;</span><span class="p">)</span>

<span class="c1"># Join with event-time constraints</span>
joined <span class="o">&lt;-</span> join<span class="p">(</span>
  impressionsWithWatermark<span class="p">,</span>
  clicksWithWatermark<span class="p">,</span>
  expr<span class="p">(</span>
    <span class="kp">paste</span><span class="p">(</span>
      <span class="s">&quot;clickAdId = impressionAdId AND&quot;</span><span class="p">,</span>
      <span class="s">&quot;clickTime &gt;= impressionTime AND&quot;</span><span class="p">,</span>
      <span class="s">&quot;clickTime &lt;= impressionTime + interval 1 hour&quot;</span>
<span class="p">)))</span></code></pre></figure>

  </div>
</div>

<h6 class="no_toc" id="semantic-guarantees-of-stream-stream-inner-joins-with-watermarking">流水内部连接的语义保证与水印</h6>
<p>这类似于<a href="#semantic-guarantees-of-aggregation-with-watermarking">在聚合上加水印所提供</a>的<a href="#semantic-guarantees-of-aggregation-with-watermarking">保证</a> 。水印延迟为“ 2小时”可确保引擎永远不会丢弃任何少于2小时的数据。但是延迟超过2小时的数据可能会或可能不会得到处理。</p>

<h5 id="outer-joins-with-watermarking">外加水印</h5>
<p>对于内部联接，水印+事件时间约束是可选的，对于左右外部联接，必须指定它们。这是因为为了在外部联接中生成NULL结果，引擎必须知道将来什么时候输入行不匹配任何内容。因此，必须指定水印+事件时间约束以生成正确的结果。因此，使用外部联接的查询看起来与前面的广告获利示例非常相似，不同之处在于，将有一个附加参数将其指定为外部联接。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">impressionsWithWatermark</span><span class="o">.</span><span class="n">join</span><span class="o">(</span>
  <span class="n">clicksWithWatermark</span><span class="o">,</span>
  <span class="n">expr</span><span class="o">(</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">    clickAdId = impressionAdId AND</span>
<span class="s">    clickTime &gt;= impressionTime AND</span>
<span class="s">    clickTime &lt;= impressionTime + interval 1 hour</span>
<span class="s">    &quot;&quot;&quot;</span><span class="o">),</span>
  <span class="n">joinType</span> <span class="k">=</span> <span class="s">&quot;leftOuter&quot;</span>      <span class="c1">// can be &quot;inner&quot;, &quot;leftOuter&quot;, &quot;rightOuter&quot;</span>
 <span class="o">)</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">impressionsWithWatermark</span><span class="o">.</span><span class="na">join</span><span class="o">(</span>
  <span class="n">clicksWithWatermark</span><span class="o">,</span>
  <span class="n">expr</span><span class="o">(</span>
    <span class="s">&quot;clickAdId = impressionAdId AND &quot;</span> <span class="o">+</span>
    <span class="s">&quot;clickTime &gt;= impressionTime AND &quot;</span> <span class="o">+</span>
    <span class="s">&quot;clickTime &lt;= impressionTime + interval 1 hour &quot;</span><span class="o">),</span>
  <span class="s">&quot;leftOuter&quot;</span>                 <span class="c1">// can be &quot;inner&quot;, &quot;leftOuter&quot;, &quot;rightOuter&quot;</span>
<span class="o">);</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">impressionsWithWatermark</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
  <span class="n">clicksWithWatermark</span><span class="p">,</span>
  <span class="n">expr</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    clickAdId = impressionAdId AND</span>
<span class="s2">    clickTime &gt;= impressionTime AND</span>
<span class="s2">    clickTime &lt;= impressionTime + interval 1 hour</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="p">),</span>
  <span class="s2">&quot;leftOuter&quot;</span>                 <span class="c1"># can be &quot;inner&quot;, &quot;leftOuter&quot;, &quot;rightOuter&quot;</span>
<span class="p">)</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>joined <span class="o">&lt;-</span> join<span class="p">(</span>
  impressionsWithWatermark<span class="p">,</span>
  clicksWithWatermark<span class="p">,</span>
  expr<span class="p">(</span>
    <span class="kp">paste</span><span class="p">(</span>
      <span class="s">&quot;clickAdId = impressionAdId AND&quot;</span><span class="p">,</span>
      <span class="s">&quot;clickTime &gt;= impressionTime AND&quot;</span><span class="p">,</span>
      <span class="s">&quot;clickTime &lt;= impressionTime + interval 1 hour&quot;</span><span class="p">),</span>
  <span class="s">&quot;left_outer&quot;</span>                 <span class="c1"># can be &quot;inner&quot;, &quot;left_outer&quot;, &quot;right_outer&quot;</span>
<span class="p">))</span></code></pre></figure>

  </div>
</div>

<h6 class="no_toc" id="semantic-guarantees-of-stream-stream-outer-joins-with-watermarking">带有流水印的流外连接的语义保证</h6>
<p>关于水印延迟以及数据是否会丢失，外连接与<a href="#semantic-guarantees-of-stream-stream-inner-joins-with-watermarking">内</a>连接具有相同的保证。</p>

<h6 class="no_toc" id="caveats">注意事项</h6>
<p>关于如何生成外部结果，有一些重要的特性需要注意。</p>

<ul>
  <li>
    <p><em>外部NULL结果的生成延迟取决于指定的水印延迟和时间范围条件。</em> 这是因为引擎必须等待很长时间才能确保没有匹配项，并且将来将不再有匹配项。</p>
  </li>
  <li>
    <p>在微批处理引擎的当前实现中，水印在微批处理的末尾前进，而下一个微批处理使用更新的水印来清理状态并输出外部结果。由于我们仅在有新数据要处理时才触发微批处理，因此如果流中没有接收到新数据，则外部结果的生成可能会延迟。
<em>简而言之，如果连接的两个输入流中的任何一个在一段时间内未接收到数据，则外部（两种情况，左或右）输出可能会延迟。</em></p>
  </li>
</ul>

<h5 id="support-matrix-for-joins-in-streaming-queries">流查询中的联接支持矩阵</h5>

<table class="table">
  <tbody><tr>
    <th>左输入</th>
    <th>正确输入</th>
    <th>联接类型</th>
    <th></th>
  </tr>
  <tr>
      <td style="vertical-align:middle">静态的</td>
      <td style="vertical-align:middle">静态的</td>
      <td style="vertical-align:middle">所有类型</td>
      <td style="vertical-align:middle">受支持，因为即使它可以出现在流查询中，它也不在流数据上</td>
  </tr>
  <tr>
    <td rowspan="4" style="vertical-align:middle">流</td>
    <td rowspan="4" style="vertical-align:middle">静态的</td>
    <td style="vertical-align:middle">内</td>
    <td style="vertical-align:middle">受支持，没有状态</td>
  </tr>
  <tr>
    <td style="vertical-align:middle">左外</td>
    <td style="vertical-align:middle">受支持，没有状态</td>
  </tr>
  <tr>
    <td style="vertical-align:middle">右外</td>
    <td style="vertical-align:middle">不支持</td>
  </tr>
  <tr>
    <td style="vertical-align:middle">外满</td>
    <td style="vertical-align:middle">不支持</td>
  </tr>
  <tr>
    <td rowspan="4" style="vertical-align:middle">静态的</td>
    <td rowspan="4" style="vertical-align:middle">流</td>
    <td style="vertical-align:middle">内</td>
    <td style="vertical-align:middle">受支持，没有状态</td>
  </tr>
  <tr>
    <td style="vertical-align:middle">左外</td>
    <td style="vertical-align:middle">不支持</td>
  </tr>
  <tr>
    <td style="vertical-align:middle">右外</td>
    <td style="vertical-align:middle">受支持，没有状态</td>
  </tr>
  <tr>
    <td style="vertical-align:middle">外满</td>
    <td style="vertical-align:middle">不支持</td>
  </tr>
  <tr>
    <td rowspan="4" style="vertical-align:middle">流</td>
    <td rowspan="4" style="vertical-align:middle">流</td>
    <td style="vertical-align:middle">内</td>
    <td style="vertical-align:middle">支持，可选择在两侧指定水印+时间限制以进行状态清除</td>
  </tr>
  <tr>
    <td style="vertical-align:middle">左外</td>
    <td style="vertical-align:middle">有条件地受支持，必须在右侧+时间限制上指定水印以获得正确的结果，可以选择在左侧指定所有状态清除的水印</td>
  </tr>
  <tr>
    <td style="vertical-align:middle">右外</td>
    <td style="vertical-align:middle">有条件地受支持，必须在左侧+时间限制上指定水印以获得正确的结果，可以选择在右侧指定所有状态清除的水印</td>
  </tr>
  <tr>
    <td style="vertical-align:middle">外满</td>
    <td style="vertical-align:middle">不支持</td>
  </tr>
 <tr>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
</tbody></table>

<p>有关支持的联接的其他详细信息：</p>

<ul>
  <li>
    <p>联接可以级联，即可以<code>df1.join(df2, ...).join(df3, ...).join(df4, ....)</code> 。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，仅当查询处于Append输出模式时才能使用联接。尚不支持其他输出模式。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，您不能在加入之前使用其他非类图操作。以下是一些无法使用的示例。</p>

    <ul>
      <li>
        <p>加入前无法使用流式聚合。</p>
      </li>
      <li>
        <p>加入前，无法在更新模式下使用mapGroupsWithState和flatMapGroupsWithState。</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="streaming-deduplication">流重复数据删除</h3>
<p>您可以使用事件中的唯一标识符对数据流中的记录进行重复数据删除。这与使用唯一标识符列的静态重复数据删除完全相同。该查询将存储以前记录中的必要数据量，以便可以过滤重复记录。与聚合类似，您可以在带有或不带有水印的情况下使用重复数据删除。</p>

<ul>
  <li>
    <p><em>带水印</em> -如果在重复记录到达的最晚时间上有上限，则可以在事件时间列上定义水印，然后使用guid和事件时间列进行重复数据删除。查询将使用水印从过去的记录中删除旧的状态数据，而这些数据将不再重复。这限制了查询必须维护的状态量。</p>
  </li>
  <li>
    <p><em>没有水印</em> -由于没有重复记录到达的时间限制，因此查询将所有过去记录的数据存储为状态。</p>
  </li>
</ul>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">streamingDf</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span> <span class="o">...</span>  <span class="c1">// columns: guid, eventTime, ...</span>

<span class="c1">// Without watermark using guid column</span>
<span class="n">streamingDf</span><span class="o">.</span><span class="n">dropDuplicates</span><span class="o">(</span><span class="s">&quot;guid&quot;</span><span class="o">)</span>

<span class="c1">// With watermark using guid and eventTime columns</span>
<span class="n">streamingDf</span>
  <span class="o">.</span><span class="n">withWatermark</span><span class="o">(</span><span class="s">&quot;eventTime&quot;</span><span class="o">,</span> <span class="s">&quot;10 seconds&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">dropDuplicates</span><span class="o">(</span><span class="s">&quot;guid&quot;</span><span class="o">,</span> <span class="s">&quot;eventTime&quot;</span><span class="o">)</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">streamingDf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="na">readStream</span><span class="o">().</span> <span class="o">...;</span>  <span class="c1">// columns: guid, eventTime, ...</span>

<span class="c1">// Without watermark using guid column</span>
<span class="n">streamingDf</span><span class="o">.</span><span class="na">dropDuplicates</span><span class="o">(</span><span class="s">&quot;guid&quot;</span><span class="o">);</span>

<span class="c1">// With watermark using guid and eventTime columns</span>
<span class="n">streamingDf</span>
  <span class="o">.</span><span class="na">withWatermark</span><span class="o">(</span><span class="s">&quot;eventTime&quot;</span><span class="o">,</span> <span class="s">&quot;10 seconds&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">dropDuplicates</span><span class="o">(</span><span class="s">&quot;guid&quot;</span><span class="o">,</span> <span class="s">&quot;eventTime&quot;</span><span class="o">);</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">streamingDf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span> <span class="o">...</span>

<span class="c1"># Without watermark using guid column</span>
<span class="n">streamingDf</span><span class="o">.</span><span class="n">dropDuplicates</span><span class="p">(</span><span class="s2">&quot;guid&quot;</span><span class="p">)</span>

<span class="c1"># With watermark using guid and eventTime columns</span>
<span class="n">streamingDf</span> \
  <span class="o">.</span><span class="n">withWatermark</span><span class="p">(</span><span class="s2">&quot;eventTime&quot;</span><span class="p">,</span> <span class="s2">&quot;10 seconds&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">dropDuplicates</span><span class="p">(</span><span class="s2">&quot;guid&quot;</span><span class="p">,</span> <span class="s2">&quot;eventTime&quot;</span><span class="p">)</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>streamingDf <span class="o">&lt;-</span> read.stream<span class="p">(</span><span class="kc">...</span><span class="p">)</span>

<span class="c1"># Without watermark using guid column</span>
streamingDf <span class="o">&lt;-</span> dropDuplicates<span class="p">(</span>streamingDf<span class="p">,</span> <span class="s">&quot;guid&quot;</span><span class="p">)</span>

<span class="c1"># With watermark using guid and eventTime columns</span>
streamingDf <span class="o">&lt;-</span> withWatermark<span class="p">(</span>streamingDf<span class="p">,</span> <span class="s">&quot;eventTime&quot;</span><span class="p">,</span> <span class="s">&quot;10 seconds&quot;</span><span class="p">)</span>
streamingDf <span class="o">&lt;-</span> dropDuplicates<span class="p">(</span>streamingDf<span class="p">,</span> <span class="s">&quot;guid&quot;</span><span class="p">,</span> <span class="s">&quot;eventTime&quot;</span><span class="p">)</span></code></pre></figure>

  </div>
</div>

<h3 id="policy-for-handling-multiple-watermarks">处理多个水印的政策</h3>
<p>流查询可以具有多个输入流，这些输入流可以合并或连接在一起。每个输入流都可以具有不同的阈值阈值，以进行有状态操作。您可以使用以下命令指定这些阈值<code>withWatermarks("eventTime", delay)</code>在每个输入流上。例如，考虑一个查询，其中包含<code>inputStream1</code>和<code>inputStream2</code> 。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">inputStream1</span><span class="o">.</span><span class="n">withWatermark</span><span class="o">(</span><span class="s">&quot;eventTime1&quot;</span><span class="o">,</span> <span class="s">&quot;1 hour&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">join</span><span class="o">(</span>
    <span class="n">inputStream2</span><span class="o">.</span><span class="n">withWatermark</span><span class="o">(</span><span class="s">&quot;eventTime2&quot;</span><span class="o">,</span> <span class="s">&quot;2 hours&quot;</span><span class="o">),</span>
    <span class="n">joinCondition</span><span class="o">)</span></code></pre></figure>

  </div>
</div>

<p>执行查询时，结构化流分别跟踪在每个输入流中看到的最大事件时间，根据相应的延迟计算水印，并选择单个全局水印用于有状态操作。默认情况下，选择最小值作为全局水印，因为如果其中一个流落后于其他流（例如，其中一个流由于上游故障而停止接收数据），它将确保没有数据被丢弃为时已晚。换句话说，全局水印将安全地以最慢的流的速度移动，并且查询输出将因此而延迟。</p>

<p>但是，在某些情况下，即使这意味着从最慢的流中删除数据，您可能也想获得更快的结果。从Spark 2.4开始，您可以通过设置SQL配置来设置多重水印策略以选择最大值作为全局水印<code>spark.sql.streaming.multipleWatermarkPolicy</code>至<code>max</code> （默认为<code>min</code> ）。这使全局水印以最快的速度移动。但是，副作用是，速度较慢的流中的数据将被大量丢弃。因此，请谨慎使用此配置。</p>

<h3 id="arbitrary-stateful-operations">任意状态作业</h3>
<p>与聚合相比，许多用例需要更高级的状态操作。例如，在许多用例中，您必须跟踪事件数据流中的会话。为了进行这种会话化，您将必须将任意类型的数据保存为状态，并在每个触发器中使用数据流事件对状态执行任意操作。从Spark 2.2开始，可以使用以下操作完成此操作<code>mapGroupsWithState</code>以及更强大的操作<code>flatMapGroupsWithState</code> 。两种操作都允许您将用户定义的代码应用于分组的数据集以更新用户定义的状态。有关更多具体细节，请查看API文档（ <a href="api/scala/index.html#org.apache.spark.sql.streaming.GroupState">Scala</a> / <a href="api/java/org/apache/spark/sql/streaming/GroupState.html">Java</a> ）和示例（ <a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala">Scala</a> / <a href="https://github.com/apache/spark/blob/v2.4.4/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java">Java</a> ）。</p>

<h3 id="unsupported-operations">不支持的操作</h3>
<p>流数据帧/数据集不支持一些数据帧/数据集操作。其中一些如下。</p>

<ul>
  <li>
    <p>流数据集尚不支持多个流聚合（即，流DF上的聚合链）。</p>
  </li>
  <li>
    <p>流数据集不支持限制和前N行。</p>
  </li>
  <li>
    <p>不支持对流数据集进行不同的操作。</p>
  </li>
  <li>
    <p>仅在聚合之后且在“完整输出模式”下，流数据集才支持排序操作。</p>
  </li>
  <li>
    <p>不支持流数据集上很少类型的外部联接。有关更多详细信息，请参见“ <a href="#support-matrix-for-joins-in-streaming-queries">联接操作”部分中</a>的<a href="#support-matrix-for-joins-in-streaming-queries">支持矩阵</a> 。</p>
  </li>
</ul>

<p>此外，有些数据集方法不适用于流式数据集。它们是将立即运行查询并返回结果的操作，这对于流数据集没有意义。相反，可以通过显式启动流查询来完成这些功能（有关此内容，请参见下一节）。</p>

<ul>
  <li>
    <p><code>count()</code> -无法从流数据集返回单个计数。相反，使用<code>ds.groupBy().count()</code>它返回包含运行计数的流数据集。</p>
  </li>
  <li>
    <p><code>foreach()</code> -改为使用<code>ds.writeStream.foreach(...)</code> （请参阅下一部分）。</p>
  </li>
  <li>
    <p><code>show()</code> -而是使用控制台接收器（请参阅下一节）。</p>
  </li>
</ul>

<p>如果您尝试这些操作中的任何一个，将会看到一个<code>AnalysisException</code>例如“流数据帧/数据集不支持XYZ操作”。尽管将来的Spark版本可能会支持其中一些功能，但从根本上讲，还有一些功能很难有效地在流数据上实现。例如，不支持对输入流进行排序，因为它需要跟踪流中接收到的所有数据。因此，从根本上讲，这很难有效执行。</p>

<h2 id="starting-streaming-queries">开始流查询</h2>
<p>一旦定义了最终结果DataFrame / Dataset，剩下的一切就是您开始进行流计算。为此，您必须使用<code>DataStreamWriter</code> （ <a href="api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter">Scala</a> / <a href="api/java/org/apache/spark/sql/streaming/DataStreamWriter.html">Java</a> / <a href="api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamWriter">Python</a> docs）通过以下方式返回<code>Dataset.writeStream()</code> 。您必须在此界面中指定以下一项或多项。</p>

<ul>
  <li>
    <p><em>输出接收器的详细信息：</em>数据格式，位置等。</p>
  </li>
  <li>
    <p><em>输出模式：</em>指定要写入输出接收器的内容。</p>
  </li>
  <li>
    <p><em>查询名称：（</em>可选）指定查询的唯一名称以进行标识。</p>
  </li>
  <li>
    <p><em>触发间隔：（</em>可选）指定触发间隔。如果未指定，则先前的处理完成后，系统将检查新数据的可用性。如果由于先前的处理尚未完成而错过了触发时间，那么系统将立即触发处理。</p>
  </li>
  <li>
    <p><em>检查点位置：</em>对于某些可以确保端到端容错的输出接收器，请指定系统将在其中写入所有检查点信息的位置。这应该是与HDFS兼容的容错文件系统中的目录。下一节将详细讨论检查点的语义。</p>
  </li>
</ul>

<h4 id="output-modes">输出方式</h4>
<p>有几种类型的输出模式。</p>

<ul>
  <li>
    <p><strong>追加模式（默认）</strong> -这是默认模式，在该模式下，仅将自上次触发以来添加到结果表中的新行输出到接收器。只有那些添加到“结果表”中的行永不改变的查询才支持此功能。因此，此模式保证每行仅输出一次（假设容错接收器）。例如，仅查询<code>select</code> ， <code>where</code> ， <code>map</code> ， <code>flatMap</code> ， <code>filter</code> ， <code>join</code>等等将支持追加模式。</p>
  </li>
  <li>
    <p><strong>完成模式</strong> -每次触发后，整个结果表将输出到接收器。聚合查询支持此功能。</p>
  </li>
  <li>
    <p><strong>更新模式</strong> -（ <em>自Spark 2.1.1起可用</em> ）仅从上次触发以来已更新的结果表中的行将输出到接收器。在将来的版本中将添加更多信息。</p>
  </li>
</ul>

<p>不同类型的流查询支持不同的输出模式。这是兼容性矩阵。</p>

<table class="table">
  <tbody><tr>
    <th>查询类型</th>
    <th></th>
    <th>支持的输出模式</th>
    <th>笔记</th>        
  </tr>
  <tr>
    <td rowspan="2" style="vertical-align:middle">聚合查询</td>
    <td style="vertical-align:middle">带水印的事件时间聚合</td>
    <td style="vertical-align:middle">追加，更新，完成</td>
    <td>追加模式使用水印删除旧的聚合状态。但是窗口聚合的输出会延迟“ withWatermark（）”中指定的延迟阈值，这是由模式语义所决定的，行最终确定后（即，越过水印之后）只能添加到结果表一次。有关更多详细信息，请参见“ <a href="#handling-late-data-and-watermarking">后期数据”</a>部分。
        <br><br>更新模式使用水印删除旧的聚合状态。
        <br><br>完全模式不会删除旧的聚合状态，因为根据定义，此模式会将所有数据保留在结果表中。
    </td>    
  </tr>
  <tr>
    <td style="vertical-align:middle">其他汇总</td>
    <td style="vertical-align:middle">完成，更新</td>
    <td>由于未定义水印（仅在其他类别中定义），因此不会删除旧的聚合状态。
        <br><br>不支持追加模式，因为聚合可以更新，从而违反了该模式的语义。
    </td>  
  </tr>
  <tr>
    <td colspan="2" style="vertical-align:middle">查询与<code>mapGroupsWithState</code></td>
    <td style="vertical-align:middle">更新资料</td>
    <td style="vertical-align:middle"></td>
  </tr>
  <tr>
    <td rowspan="2" style="vertical-align:middle">查询与<code>flatMapGroupsWithState</code></td>
    <td style="vertical-align:middle">追加操作模式</td>
    <td style="vertical-align:middle">附加</td>
    <td style="vertical-align:middle">允许聚合<code>flatMapGroupsWithState</code> 。
    </td>
  </tr>
  <tr>
    <td style="vertical-align:middle">更新操作模式</td>
    <td style="vertical-align:middle">更新资料</td>
    <td style="vertical-align:middle">之后不允许进行汇总<code>flatMapGroupsWithState</code> 。
    </td>
  </tr>
  <tr>
      <td colspan="2" style="vertical-align:middle">查询与<code>joins</code></td>
      <td style="vertical-align:middle">附加</td>
      <td style="vertical-align:middle">尚不支持更新和完成模式。有关<a href="#support-matrix-for-joins-in-streaming-queries">支持</a>哪些类型的联接的更多详细信息，请参见“ <a href="#support-matrix-for-joins-in-streaming-queries">联接操作”部分</a>中的支持列表。
      </td>
    </tr>
  <tr>
    <td colspan="2" style="vertical-align:middle">其他查询</td>
    <td style="vertical-align:middle">追加，更新</td>
    <td style="vertical-align:middle">不支持使用完整模式，因为将所有未聚合的数据保留在结果表中是不可行的。
    </td>
  </tr>
  <tr>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
</tbody></table>

<h4 id="output-sinks">输出接收器</h4>
<p>有几种类型的内置输出接收器。</p>

<ul>
  <li><strong>文件接收器</strong> -将输出存储到目录。</li>
</ul>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">writeStream</span>
    <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;parquet&quot;</span><span class="o">)</span>        <span class="c1">// can be &quot;orc&quot;, &quot;json&quot;, &quot;csv&quot;, etc.</span>
    <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;path&quot;</span><span class="o">,</span> <span class="s">&quot;path/to/destination/dir&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">start</span><span class="o">()</span></code></pre></figure>

<ul>
  <li><strong>Kafka接收器</strong> -将输出存储到Kafka中的一个或多个主题。</li>
</ul>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">writeStream</span>
    <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;kafka&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;kafka.bootstrap.servers&quot;</span><span class="o">,</span> <span class="s">&quot;host1:port1,host2:port2&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;topic&quot;</span><span class="o">,</span> <span class="s">&quot;updates&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">start</span><span class="o">()</span></code></pre></figure>

<ul>
  <li><strong>Foreach接收器</strong> -对输出中的记录运行任意计算。有关更多详细信息，请参见本节后面的内容。</li>
</ul>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">writeStream</span>
    <span class="o">.</span><span class="n">foreach</span><span class="o">(...)</span>
    <span class="o">.</span><span class="n">start</span><span class="o">()</span></code></pre></figure>

<ul>
  <li><strong>控制台接收器（用于调试）</strong> -每次有触发器时，将输出打印到控制台/ stdout。支持追加和完整输出模式。由于每次触发后都会收集整个输出并将其存储在驱动程序的内存中，因此应在数据量较小时用于调试目的。</li>
</ul>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">writeStream</span>
    <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">start</span><span class="o">()</span></code></pre></figure>

<ul>
  <li><strong>内存接收器（用于调试）</strong> -输出作为内存表存储在内存中。支持追加和完整输出模式。由于整个输出已收集并存储在驱动程序的内存中，因此应在数据量较小时用于调试目的。因此，请谨慎使用。</li>
</ul>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">writeStream</span>
    <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;memory&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">queryName</span><span class="o">(</span><span class="s">&quot;tableName&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">start</span><span class="o">()</span></code></pre></figure>

<p>一些接收器不是容错的，因为它们不能保证输出的持久性，并且仅用于调试目的。参见前面有关<a href="#fault-tolerance-semantics">容错语义的部分</a> 。以下是Spark中所有接收器的详细信息。</p>

<table class="table">
  <tbody><tr>
    <th>下沉</th>
    <th>支持的输出模式</th>
    <th>选件</th>
    <th>容错的</th>
    <th>笔记</th>
  </tr>
  <tr>
    <td><b>文件接收器</b></td>
    <td>附加</td>
    <td>
        <code>path</code> ：必须指定输出目录的路径。
        <br><br>有关特定于文件格式的选项，请参见DataFrameWriter（ <a href="api/scala/index.html#org.apache.spark.sql.DataFrameWriter">Scala</a> / <a href="api/java/org/apache/spark/sql/DataFrameWriter.html">Java</a> / <a href="api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter">Python</a> / <a href="api/R/write.stream.html">R</a> ）中的相关方法。例如“镶木地板”格式选项，请参见<code>DataFrameWriter.parquet()</code>
    </td>
    <td>是（一次）</td>
    <td>支持对分区表的写入。按时间分区可能很有用。</td>
  </tr>
  <tr>
    <td><b>卡夫卡水槽</b></td>
    <td>追加，更新，完成</td>
    <td>请参阅《 <a href="structured-streaming-kafka-integration.html">Kafka集成指南》</a></td>
    <td>是的（至少一次）</td>
    <td><a href="structured-streaming-kafka-integration.html">Kafka集成指南</a>中的更多详细信息</td>
  </tr>
  <tr>
    <td><b>Foreach水槽</b></td>
    <td>追加，更新，完成</td>
    <td>没有</td>
    <td>是的（至少一次）</td>
    <td><a href="#using-foreach-and-foreachbatch">下一节中的</a>更多详细信息</td>
  </tr>
  <tr>
      <td><b>Foreach批次水槽</b></td>
      <td>追加，更新，完成</td>
      <td>没有</td>
      <td>取决于实施</td>
      <td><a href="#using-foreach-and-foreachbatch">下一节中的</a>更多详细信息</td>
    </tr>
    
  <tr>
    <td><b>控制台水槽</b></td>
    <td>追加，更新，完成</td>
    <td>
        <code>numRows</code> ：每个触发器要打印的行数（默认值：20）<br>
        <code>truncate</code> ：是否在太长的时间内截断输出（默认值：true）</td>
    <td>没有</td>
    <td></td>
  </tr>
  <tr>
    <td><b>内存槽</b></td>
    <td>追加完成</td>
    <td>没有</td>
    <td>否。但是在完成模式下，重新启动的查询将重新创建完整表。</td>
    <td>表名称是查询名称。</td>
  </tr>
  <tr>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
</tbody></table>

<p>请注意，您必须致电<code>start()</code>真正开始执行查询。这将返回一个StreamingQuery对象，该对象是连续运行的执行的句柄。您可以使用此对象来管理查询，我们将在下一部分中讨论。现在，让我们通过一些示例来了解所有这一切。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// ========== DF with no aggregations ==========</span>
<span class="k">val</span> <span class="n">noAggDF</span> <span class="k">=</span> <span class="n">deviceDataDf</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="s">&quot;device&quot;</span><span class="o">).</span><span class="n">where</span><span class="o">(</span><span class="s">&quot;signal &gt; 10&quot;</span><span class="o">)</span>   

<span class="c1">// Print new data to console</span>
<span class="n">noAggDF</span>
  <span class="o">.</span><span class="n">writeStream</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">start</span><span class="o">()</span>

<span class="c1">// Write new data to Parquet files</span>
<span class="n">noAggDF</span>
  <span class="o">.</span><span class="n">writeStream</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;parquet&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;checkpointLocation&quot;</span><span class="o">,</span> <span class="s">&quot;path/to/checkpoint/dir&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;path&quot;</span><span class="o">,</span> <span class="s">&quot;path/to/destination/dir&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">start</span><span class="o">()</span>

<span class="c1">// ========== DF with aggregation ==========</span>
<span class="k">val</span> <span class="n">aggDF</span> <span class="k">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&quot;device&quot;</span><span class="o">).</span><span class="n">count</span><span class="o">()</span>

<span class="c1">// Print updated aggregations to console</span>
<span class="n">aggDF</span>
  <span class="o">.</span><span class="n">writeStream</span>
  <span class="o">.</span><span class="n">outputMode</span><span class="o">(</span><span class="s">&quot;complete&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">start</span><span class="o">()</span>

<span class="c1">// Have all the aggregates in an in-memory table</span>
<span class="n">aggDF</span>
  <span class="o">.</span><span class="n">writeStream</span>
  <span class="o">.</span><span class="n">queryName</span><span class="o">(</span><span class="s">&quot;aggregates&quot;</span><span class="o">)</span>    <span class="c1">// this query name will be the table name</span>
  <span class="o">.</span><span class="n">outputMode</span><span class="o">(</span><span class="s">&quot;complete&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;memory&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">start</span><span class="o">()</span>

<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;select * from aggregates&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">()</span>   <span class="c1">// interactively query in-memory table</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// ========== DF with no aggregations ==========</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">noAggDF</span> <span class="o">=</span> <span class="n">deviceDataDf</span><span class="o">.</span><span class="na">select</span><span class="o">(</span><span class="s">&quot;device&quot;</span><span class="o">).</span><span class="na">where</span><span class="o">(</span><span class="s">&quot;signal &gt; 10&quot;</span><span class="o">);</span>

<span class="c1">// Print new data to console</span>
<span class="n">noAggDF</span>
  <span class="o">.</span><span class="na">writeStream</span><span class="o">()</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">start</span><span class="o">();</span>

<span class="c1">// Write new data to Parquet files</span>
<span class="n">noAggDF</span>
  <span class="o">.</span><span class="na">writeStream</span><span class="o">()</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;parquet&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;checkpointLocation&quot;</span><span class="o">,</span> <span class="s">&quot;path/to/checkpoint/dir&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;path&quot;</span><span class="o">,</span> <span class="s">&quot;path/to/destination/dir&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">start</span><span class="o">();</span>

<span class="c1">// ========== DF with aggregation ==========</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Row</span><span class="o">&gt;</span> <span class="n">aggDF</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="s">&quot;device&quot;</span><span class="o">).</span><span class="na">count</span><span class="o">();</span>

<span class="c1">// Print updated aggregations to console</span>
<span class="n">aggDF</span>
  <span class="o">.</span><span class="na">writeStream</span><span class="o">()</span>
  <span class="o">.</span><span class="na">outputMode</span><span class="o">(</span><span class="s">&quot;complete&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">start</span><span class="o">();</span>

<span class="c1">// Have all the aggregates in an in-memory table</span>
<span class="n">aggDF</span>
  <span class="o">.</span><span class="na">writeStream</span><span class="o">()</span>
  <span class="o">.</span><span class="na">queryName</span><span class="o">(</span><span class="s">&quot;aggregates&quot;</span><span class="o">)</span>    <span class="c1">// this query name will be the table name</span>
  <span class="o">.</span><span class="na">outputMode</span><span class="o">(</span><span class="s">&quot;complete&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;memory&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">start</span><span class="o">();</span>

<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;select * from aggregates&quot;</span><span class="o">).</span><span class="na">show</span><span class="o">();</span>   <span class="c1">// interactively query in-memory table</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># ========== DF with no aggregations ==========</span>
<span class="n">noAggDF</span> <span class="o">=</span> <span class="n">deviceDataDf</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;device&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s2">&quot;signal &gt; 10&quot;</span><span class="p">)</span>   

<span class="c1"># Print new data to console</span>
<span class="n">noAggDF</span> \
    <span class="o">.</span><span class="n">writeStream</span> \
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;console&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="c1"># Write new data to Parquet files</span>
<span class="n">noAggDF</span> \
    <span class="o">.</span><span class="n">writeStream</span> \
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;parquet&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;checkpointLocation&quot;</span><span class="p">,</span> <span class="s2">&quot;path/to/checkpoint/dir&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;path&quot;</span><span class="p">,</span> <span class="s2">&quot;path/to/destination/dir&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="c1"># ========== DF with aggregation ==========</span>
<span class="n">aggDF</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;device&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

<span class="c1"># Print updated aggregations to console</span>
<span class="n">aggDF</span> \
    <span class="o">.</span><span class="n">writeStream</span> \
    <span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s2">&quot;complete&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;console&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="c1"># Have all the aggregates in an in-memory table. The query name will be the table name</span>
<span class="n">aggDF</span> \
    <span class="o">.</span><span class="n">writeStream</span> \
    <span class="o">.</span><span class="n">queryName</span><span class="p">(</span><span class="s2">&quot;aggregates&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s2">&quot;complete&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;memory&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;select * from aggregates&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>   <span class="c1"># interactively query in-memory table</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="c1"># ========== DF with no aggregations ==========</span>
noAggDF <span class="o">&lt;-</span> select<span class="p">(</span>where<span class="p">(</span>deviceDataDf<span class="p">,</span> <span class="s">&quot;signal &gt; 10&quot;</span><span class="p">),</span> <span class="s">&quot;device&quot;</span><span class="p">)</span>

<span class="c1"># Print new data to console</span>
write.stream<span class="p">(</span>noAggDF<span class="p">,</span> <span class="s">&quot;console&quot;</span><span class="p">)</span>

<span class="c1"># Write new data to Parquet files</span>
write.stream<span class="p">(</span>noAggDF<span class="p">,</span>
             <span class="s">&quot;parquet&quot;</span><span class="p">,</span>
             path <span class="o">=</span> <span class="s">&quot;path/to/destination/dir&quot;</span><span class="p">,</span>
             checkpointLocation <span class="o">=</span> <span class="s">&quot;path/to/checkpoint/dir&quot;</span><span class="p">)</span>

<span class="c1"># ========== DF with aggregation ==========</span>
aggDF <span class="o">&lt;-</span> count<span class="p">(</span>groupBy<span class="p">(</span>df<span class="p">,</span> <span class="s">&quot;device&quot;</span><span class="p">))</span>

<span class="c1"># Print updated aggregations to console</span>
write.stream<span class="p">(</span>aggDF<span class="p">,</span> <span class="s">&quot;console&quot;</span><span class="p">,</span> outputMode <span class="o">=</span> <span class="s">&quot;complete&quot;</span><span class="p">)</span>

<span class="c1"># Have all the aggregates in an in memory table. The query name will be the table name</span>
write.stream<span class="p">(</span>aggDF<span class="p">,</span> <span class="s">&quot;memory&quot;</span><span class="p">,</span> queryName <span class="o">=</span> <span class="s">&quot;aggregates&quot;</span><span class="p">,</span> outputMode <span class="o">=</span> <span class="s">&quot;complete&quot;</span><span class="p">)</span>

<span class="c1"># Interactively query in-memory table</span>
<span class="kp">head</span><span class="p">(</span>sql<span class="p">(</span><span class="s">&quot;select * from aggregates&quot;</span><span class="p">))</span></code></pre></figure>

  </div>
</div>

<h5 id="using-foreach-and-foreachbatch">使用Foreach和ForeachBatch</h5>
<p>的<code>foreach</code>和<code>foreachBatch</code>操作允许您在流查询的输出上应用任意操作并编写逻辑。他们的用例略有不同-而<code>foreach</code>允许在每一行上自定义写逻辑， <code>foreachBatch</code>允许对每个微批处理的输出进行任意操作和自定义逻辑。让我们更详细地了解它们的用法。</p>

<h6 id="foreachbatch">Foreach批次</h6>
<p><code>foreachBatch(...)</code>允许您指定在流查询的每个微批处理的输出数据上执行的函数。从Spark 2.4开始，Scala，Java和Python支持此功能。它具有两个参数：具有微批处理的输出数据的数据帧或数据集和微批处理的唯一ID。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">streamingDF</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">foreachBatch</span> <span class="o">{</span> <span class="o">(</span><span class="n">batchDF</span><span class="k">:</span> <span class="kt">DataFrame</span><span class="o">,</span> <span class="n">batchId</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span> <span class="k">=&gt;</span>
  <span class="c1">// Transform and write batchDF </span>
<span class="o">}.</span><span class="n">start</span><span class="o">()</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">streamingDatasetOfString</span><span class="o">.</span><span class="na">writeStream</span><span class="o">().</span><span class="na">foreachBatch</span><span class="o">(</span>
  <span class="k">new</span> <span class="n">VoidFunction2</span><span class="o">&lt;</span><span class="n">Dataset</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;,</span> <span class="n">Long</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">call</span><span class="o">(</span><span class="n">Dataset</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">dataset</span><span class="o">,</span> <span class="n">Long</span> <span class="n">batchId</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// Transform and write batchDF</span>
    <span class="o">}</span>    
  <span class="o">}</span>
<span class="o">).</span><span class="na">start</span><span class="o">();</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">def</span> <span class="nf">foreach_batch_function</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">epoch_id</span><span class="p">):</span>
    <span class="c1"># Transform and write batchDF</span>
    <span class="k">pass</span>
  
<span class="n">streamingDF</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">foreachBatch</span><span class="p">(</span><span class="n">foreach_batch_function</span><span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>   </code></pre></figure>

  </div>
<div data-lang="r">
    <p>尚不支持R。</p>
  </div>
</div>

<p>用<code>foreachBatch</code> ，您可以执行以下操作。</p>

<ul>
  <li><strong>重用现有的批处理数据源</strong> -对于许多存储系统，可能还没有流接收器可用，但是可能已经存在用于批处理查询的数据写入器。使用<code>foreachBatch</code> ，您可以在每个微批处理的输出上使用批处理数据写入器。</li>
  <li><strong>写入多个位置</strong> -如果要将流查询的输出写入多个位置，则只需将输出DataFrame / Dataset多次写入即可。但是，每次写入尝试都可能导致重新计算输出数据（包括可能重新读取输入数据）。为了避免重新计算，您应该缓存输出DataFrame / Dataset，将其写入多个位置，然后取消缓存。这是一个轮廓。</li>
</ul>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">streamingDF</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">foreachBatch</span> <span class="o">{</span> <span class="o">(</span><span class="n">batchDF</span><span class="k">:</span> <span class="kt">DataFrame</span><span class="o">,</span> <span class="n">batchId</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span> <span class="k">=&gt;</span>
  <span class="n">batchDF</span><span class="o">.</span><span class="n">persist</span><span class="o">()</span>
  <span class="n">batchDF</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="o">(...).</span><span class="n">save</span><span class="o">(...)</span>  <span class="c1">// location 1</span>
  <span class="n">batchDF</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="o">(...).</span><span class="n">save</span><span class="o">(...)</span>  <span class="c1">// location 2</span>
  <span class="n">batchDF</span><span class="o">.</span><span class="n">unpersist</span><span class="o">()</span>
<span class="o">}</span></code></pre></figure>

  </div>
</div>

<ul>
  <li><strong>应用其他DataFrame操作</strong> -流式DataFrame中不支持许多DataFrame和Dataset操作，因为在这种情况下，Spark不支持生成增量计划。使用<code>foreachBatch</code> ，您可以在每个微批量输出中应用其中一些操作。但是，您将不得不自己考虑执行该操作的端到端语义。</li>
</ul>

<p><strong>注意：</strong></p>
<ul>
  <li>默认， <code>foreachBatch</code>仅提供至少一次写入保证。但是，您可以使用提供给该函数的batchId作为对输出进行重复数据删除并获得一次准确保证的方式。</li>
  <li><code>foreachBatch</code>不适用于连续处理模式，因为它基本上依赖于流查询的微批执行。如果以连续模式写入数据，请使用<code>foreach</code>代替。</li>
</ul>

<h6 id="foreach">Foreach</h6>
<p>如果<code>foreachBatch</code>不是一个选项（例如，相应的批处理数据编写器不存在，或处于连续处理模式），则可以使用来表达自定义编写器逻辑<code>foreach</code> 。具体来说，您可以通过将数据分为三种方法来表示数据写入逻辑： <code>open</code> ， <code>process</code>和<code>close</code> 。从Spark 2.4开始， <code>foreach</code>可在Scala，Java和Python中使用。</p>

<div class="codetabs">
<div data-lang="scala">

    <p>在Scala中，您必须扩展类<code>ForeachWriter</code> （ <a href="api/scala/index.html#org.apache.spark.sql.ForeachWriter">docs</a> ）。</p>

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">streamingDatasetOfString</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span>
  <span class="k">new</span> <span class="nc">ForeachWriter</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="o">{</span>

    <span class="k">def</span> <span class="n">open</span><span class="o">(</span><span class="n">partitionId</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">version</span><span class="k">:</span> <span class="kt">Long</span><span class="o">)</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="o">{</span>
      <span class="c1">// Open connection</span>
    <span class="o">}</span>

    <span class="k">def</span> <span class="n">process</span><span class="o">(</span><span class="n">record</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="c1">// Write string to connection</span>
    <span class="o">}</span>

    <span class="k">def</span> <span class="n">close</span><span class="o">(</span><span class="n">errorOrNull</span><span class="k">:</span> <span class="kt">Throwable</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
      <span class="c1">// Close the connection</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">).</span><span class="n">start</span><span class="o">()</span></code></pre></figure>

  </div>
<div data-lang="java">

    <p>在Java中，您必须扩展类<code>ForeachWriter</code> （ <a href="api/java/org/apache/spark/sql/ForeachWriter.html">docs</a> ）。</p>

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">streamingDatasetOfString</span><span class="o">.</span><span class="na">writeStream</span><span class="o">().</span><span class="na">foreach</span><span class="o">(</span>
  <span class="k">new</span> <span class="n">ForeachWriter</span><span class="o">[</span><span class="n">String</span><span class="o">]</span> <span class="o">{</span>

    <span class="nd">@Override</span> <span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">open</span><span class="o">(</span><span class="kt">long</span> <span class="n">partitionId</span><span class="o">,</span> <span class="kt">long</span> <span class="n">version</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// Open connection</span>
    <span class="o">}</span>

    <span class="nd">@Override</span> <span class="kd">public</span> <span class="kt">void</span> <span class="nf">process</span><span class="o">(</span><span class="n">String</span> <span class="n">record</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// Write string to connection</span>
    <span class="o">}</span>

    <span class="nd">@Override</span> <span class="kd">public</span> <span class="kt">void</span> <span class="nf">close</span><span class="o">(</span><span class="n">Throwable</span> <span class="n">errorOrNull</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// Close the connection</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">).</span><span class="na">start</span><span class="o">();</span></code></pre></figure>

  </div>
<div data-lang="python">

    <p>在Python中，您可以通过两种方式调用foreach：在函数或对象中。该函数提供了一种表达处理逻辑的简单方法，但是当故障导致某些输入数据的重新处理时，不允许您对生成的数据进行重复数据删除。对于这种情况，您必须在一个对象中指定处理逻辑。</p>

    <ul>
      <li>首先，该函数将一行作为输入。</li>
    </ul>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">def</span> <span class="nf">process_row</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
    <span class="c1"># Write row to storage</span>
    <span class="k">pass</span>

<span class="n">query</span> <span class="o">=</span> <span class="n">streamingDF</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">process_row</span><span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>  </code></pre></figure>

    <ul>
      <li>其次，对象具有处理方法和可选的打开和关闭方法：</li>
    </ul>

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="k">class</span> <span class="nc">ForeachWriter</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">open</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partition_id</span><span class="p">,</span> <span class="n">epoch_id</span><span class="p">):</span>
        <span class="c1"># Open connection. This method is optional in Python.</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
        <span class="c1"># Write row to connection. This method is NOT optional in Python.</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">close</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">):</span>
        <span class="c1"># Close the connection. This method in optional in Python.</span>
        <span class="k">pass</span>
      
<span class="n">query</span> <span class="o">=</span> <span class="n">streamingDF</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">ForeachWriter</span><span class="p">())</span><span class="o">.</span><span class="n">start</span><span class="p">()</span></code></pre></figure>

  </div>
<div data-lang="r">
    <p>尚不支持R。</p>
  </div>
</div>

<p><strong>执行语义</strong>启动流查询时，Spark通过以下方式调用函数或对象的方法：</p>

<ul>
  <li>
    <p>该对象的单个副本负责查询中单个任务生成的所有数据。换句话说，一个实例负责处理以分布式方式生成的数据的一个分区。</p>
  </li>
  <li>
    <p>该对象必须是可序列化的，因为每个任务都将获得所提供对象的新的序列化反序列化副本。因此，强烈建议在调用open（）方法之后执行任何用于写入数据的初始化操作（例如，打开连接或启动事务），这表明任务已准备好生成数据。</p>
  </li>
  <li>
    <p>这些方法的生命周期如下：</p>

    <ul>
      <li>
        <p>对于每个具有partition_id的分区：</p>

        <ul>
          <li>
            <p>对于每个具有epoch_id的流数据的每个批次/时代：</p>

            <ul>
              <li>
                <p>调用open（partitionId，epochId）方法。</p>
              </li>
              <li>
                <p>如果open（…）返回true，则对于分区和批处理/纪元中的每一行，将调用process（row）方法。</p>
              </li>
              <li>
                <p>调用方法close（error）时，处理行时看到错误（如果有）。</p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>如果存在open（）方法并成功返回（与返回值无关），则调用close（）方法（如果存在），除非JVM或Python进程在中间崩溃。</p>
  </li>
  <li>
    <p><strong>注意：</strong> Spark无法保证（partitionId，epochId）的输出相同，因此无法使用（partitionId，epochId）实现重复数据删除。例如，出于某些原因，source提供了不同数量的分区，Spark优化更改了分区数量，等等。有关更多详细信息，请参见<a href="https://issues.apache.org/jira/browse/SPARK-28650">SPARK-28650</a> 。如果您需要在输出中进行重复数据删除，请尝试一下<code>foreachBatch</code>代替。</p>
  </li>
</ul>

<h4 id="triggers">扳机</h4>
<p>流查询的触发器设置定义了流数据处理的时间，无论该查询是作为具有固定批处理间隔的微批查询执行还是作为连续处理查询执行。以下是受支持的各种触发器。</p>

<table class="table">
  <tbody><tr>
    <th>触发类型</th>
    <th>描述</th>
  </tr>
  <tr>
    <td><i>未指定（默认）</i></td>
    <td>如果未显式指定触发器设置，则默认情况下，查询将以微批处理模式执行，在该模式下，前一个微批处理完成后，将立即生成微批处理。
    </td>
  </tr>
  <tr>
    <td><b>固定间隔微批</b></td>
    <td>查询将以微批次模式执行，在该模式下，微批次将以用户指定的时间间隔启动。
        <ul>
          <li>如果前一个微批处理在该间隔内完成，则引擎将等待该间隔结束，然后再开始下一个微批处理。</li>

          <li>如果前一个微批处理花费的时间比间隔要长（例如，缺少间隔边界），则下一个微批处理将在前一个微批处理完成后立即开始（即，它不会等待下一个间隔边界） ）。</li>

          <li>如果没有新数据可用，则不会启动微批量。</li>
        </ul>
    </td>
  </tr>
  <tr>
    <td><b>一次性微批量</b></td>
    <td>该查询将仅执行一个微批处理来处理所有可用数据，然后自行停止。在您要定期启动群集，处理自上一周期以来可用的所有内容然后关闭群集的情况下，这很有用。在某些情况下，这可能会节省大量成本。
    </td>
  </tr>
  <tr>
    <td><b>以固定的检查点间隔连续</b><br><i>（实验性）</i></td>
    <td>查询将以新的低延迟，连续处理模式执行。在下面的“ <a href="#continuous-processing">连续处理”部分</a>中了解有关此内容的更多信息。
    </td>
  </tr>
</tbody></table>

<p>这是一些代码示例。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">org.apache.spark.sql.streaming.Trigger</span>

<span class="c1">// Default trigger (runs micro-batch as soon as it can)</span>
<span class="n">df</span><span class="o">.</span><span class="n">writeStream</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">start</span><span class="o">()</span>

<span class="c1">// ProcessingTime trigger with two-seconds micro-batch interval</span>
<span class="n">df</span><span class="o">.</span><span class="n">writeStream</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">trigger</span><span class="o">(</span><span class="nc">Trigger</span><span class="o">.</span><span class="nc">ProcessingTime</span><span class="o">(</span><span class="s">&quot;2 seconds&quot;</span><span class="o">))</span>
  <span class="o">.</span><span class="n">start</span><span class="o">()</span>

<span class="c1">// One-time trigger</span>
<span class="n">df</span><span class="o">.</span><span class="n">writeStream</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">trigger</span><span class="o">(</span><span class="nc">Trigger</span><span class="o">.</span><span class="nc">Once</span><span class="o">())</span>
  <span class="o">.</span><span class="n">start</span><span class="o">()</span>

<span class="c1">// Continuous trigger with one-second checkpointing interval</span>
<span class="n">df</span><span class="o">.</span><span class="n">writeStream</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">trigger</span><span class="o">(</span><span class="nc">Trigger</span><span class="o">.</span><span class="nc">Continuous</span><span class="o">(</span><span class="s">&quot;1 second&quot;</span><span class="o">))</span>
  <span class="o">.</span><span class="n">start</span><span class="o">()</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.sql.streaming.Trigger</span>

<span class="c1">// Default trigger (runs micro-batch as soon as it can)</span>
<span class="n">df</span><span class="o">.</span><span class="na">writeStream</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">start</span><span class="o">();</span>

<span class="c1">// ProcessingTime trigger with two-seconds micro-batch interval</span>
<span class="n">df</span><span class="o">.</span><span class="na">writeStream</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">trigger</span><span class="o">(</span><span class="n">Trigger</span><span class="o">.</span><span class="na">ProcessingTime</span><span class="o">(</span><span class="s">&quot;2 seconds&quot;</span><span class="o">))</span>
  <span class="o">.</span><span class="na">start</span><span class="o">();</span>

<span class="c1">// One-time trigger</span>
<span class="n">df</span><span class="o">.</span><span class="na">writeStream</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">trigger</span><span class="o">(</span><span class="n">Trigger</span><span class="o">.</span><span class="na">Once</span><span class="o">())</span>
  <span class="o">.</span><span class="na">start</span><span class="o">();</span>

<span class="c1">// Continuous trigger with one-second checkpointing interval</span>
<span class="n">df</span><span class="o">.</span><span class="na">writeStream</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">trigger</span><span class="o">(</span><span class="n">Trigger</span><span class="o">.</span><span class="na">Continuous</span><span class="o">(</span><span class="s">&quot;1 second&quot;</span><span class="o">))</span>
  <span class="o">.</span><span class="na">start</span><span class="o">();</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="c1"># Default trigger (runs micro-batch as soon as it can)</span>
<span class="n">df</span><span class="o">.</span><span class="n">writeStream</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;console&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="c1"># ProcessingTime trigger with two-seconds micro-batch interval</span>
<span class="n">df</span><span class="o">.</span><span class="n">writeStream</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;console&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">processingTime</span><span class="o">=</span><span class="s1">&#39;2 seconds&#39;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="c1"># One-time trigger</span>
<span class="n">df</span><span class="o">.</span><span class="n">writeStream</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;console&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">once</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="c1"># Continuous trigger with one-second checkpointing interval</span>
<span class="n">df</span><span class="o">.</span><span class="n">writeStream</span>
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;console&quot;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">continuous</span><span class="o">=</span><span class="s1">&#39;1 second&#39;</span><span class="p">)</span>
  <span class="o">.</span><span class="n">start</span><span class="p">()</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span><span class="c1"># Default trigger (runs micro-batch as soon as it can)</span>
write.stream<span class="p">(</span>df<span class="p">,</span> <span class="s">&quot;console&quot;</span><span class="p">)</span>

<span class="c1"># ProcessingTime trigger with two-seconds micro-batch interval</span>
write.stream<span class="p">(</span>df<span class="p">,</span> <span class="s">&quot;console&quot;</span><span class="p">,</span> trigger.processingTime <span class="o">=</span> <span class="s">&quot;2 seconds&quot;</span><span class="p">)</span>

<span class="c1"># One-time trigger</span>
write.stream<span class="p">(</span>df<span class="p">,</span> <span class="s">&quot;console&quot;</span><span class="p">,</span> trigger.once <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>

<span class="c1"># Continuous trigger is not yet supported</span></code></pre></figure>

  </div>
</div>

<h2 id="managing-streaming-queries">管理流查询</h2>
<p>的<code>StreamingQuery</code>启动查询时创建的对象可用于监视和管理查询。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">query</span> <span class="k">=</span> <span class="n">df</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">).</span><span class="n">start</span><span class="o">()</span>   <span class="c1">// get the query object</span>

<span class="n">query</span><span class="o">.</span><span class="n">id</span>          <span class="c1">// get the unique identifier of the running query that persists across restarts from checkpoint data</span>

<span class="n">query</span><span class="o">.</span><span class="n">runId</span>       <span class="c1">// get the unique id of this run of the query, which will be generated at every start/restart</span>

<span class="n">query</span><span class="o">.</span><span class="n">name</span>        <span class="c1">// get the name of the auto-generated or user-specified name</span>

<span class="n">query</span><span class="o">.</span><span class="n">explain</span><span class="o">()</span>   <span class="c1">// print detailed explanations of the query</span>

<span class="n">query</span><span class="o">.</span><span class="n">stop</span><span class="o">()</span>      <span class="c1">// stop the query</span>

<span class="n">query</span><span class="o">.</span><span class="n">awaitTermination</span><span class="o">()</span>   <span class="c1">// block until query is terminated, with stop() or with error</span>

<span class="n">query</span><span class="o">.</span><span class="n">exception</span>       <span class="c1">// the exception if the query has been terminated with error</span>

<span class="n">query</span><span class="o">.</span><span class="n">recentProgress</span>  <span class="c1">// an array of the most recent progress updates for this query</span>

<span class="n">query</span><span class="o">.</span><span class="n">lastProgress</span>    <span class="c1">// the most recent progress update of this streaming query</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">StreamingQuery</span> <span class="n">query</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="na">writeStream</span><span class="o">().</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;console&quot;</span><span class="o">).</span><span class="na">start</span><span class="o">();</span>   <span class="c1">// get the query object</span>

<span class="n">query</span><span class="o">.</span><span class="na">id</span><span class="o">();</span>          <span class="c1">// get the unique identifier of the running query that persists across restarts from checkpoint data</span>

<span class="n">query</span><span class="o">.</span><span class="na">runId</span><span class="o">();</span>       <span class="c1">// get the unique id of this run of the query, which will be generated at every start/restart</span>

<span class="n">query</span><span class="o">.</span><span class="na">name</span><span class="o">();</span>        <span class="c1">// get the name of the auto-generated or user-specified name</span>

<span class="n">query</span><span class="o">.</span><span class="na">explain</span><span class="o">();</span>   <span class="c1">// print detailed explanations of the query</span>

<span class="n">query</span><span class="o">.</span><span class="na">stop</span><span class="o">();</span>      <span class="c1">// stop the query</span>

<span class="n">query</span><span class="o">.</span><span class="na">awaitTermination</span><span class="o">();</span>   <span class="c1">// block until query is terminated, with stop() or with error</span>

<span class="n">query</span><span class="o">.</span><span class="na">exception</span><span class="o">();</span>       <span class="c1">// the exception if the query has been terminated with error</span>

<span class="n">query</span><span class="o">.</span><span class="na">recentProgress</span><span class="o">();</span>  <span class="c1">// an array of the most recent progress updates for this query</span>

<span class="n">query</span><span class="o">.</span><span class="na">lastProgress</span><span class="o">();</span>    <span class="c1">// the most recent progress update of this streaming query</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">query</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;console&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>   <span class="c1"># get the query object</span>

<span class="n">query</span><span class="o">.</span><span class="n">id</span><span class="p">()</span>          <span class="c1"># get the unique identifier of the running query that persists across restarts from checkpoint data</span>

<span class="n">query</span><span class="o">.</span><span class="n">runId</span><span class="p">()</span>       <span class="c1"># get the unique id of this run of the query, which will be generated at every start/restart</span>

<span class="n">query</span><span class="o">.</span><span class="n">name</span><span class="p">()</span>        <span class="c1"># get the name of the auto-generated or user-specified name</span>

<span class="n">query</span><span class="o">.</span><span class="n">explain</span><span class="p">()</span>   <span class="c1"># print detailed explanations of the query</span>

<span class="n">query</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>      <span class="c1"># stop the query</span>

<span class="n">query</span><span class="o">.</span><span class="n">awaitTermination</span><span class="p">()</span>   <span class="c1"># block until query is terminated, with stop() or with error</span>

<span class="n">query</span><span class="o">.</span><span class="n">exception</span><span class="p">()</span>       <span class="c1"># the exception if the query has been terminated with error</span>

<span class="n">query</span><span class="o">.</span><span class="n">recentProgress</span><span class="p">()</span>  <span class="c1"># an array of the most recent progress updates for this query</span>

<span class="n">query</span><span class="o">.</span><span class="n">lastProgress</span><span class="p">()</span>    <span class="c1"># the most recent progress update of this streaming query</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>query <span class="o">&lt;-</span> write.stream<span class="p">(</span>df<span class="p">,</span> <span class="s">&quot;console&quot;</span><span class="p">)</span>  <span class="c1"># get the query object</span>

queryName<span class="p">(</span>query<span class="p">)</span>          <span class="c1"># get the name of the auto-generated or user-specified name</span>

explain<span class="p">(</span>query<span class="p">)</span>            <span class="c1"># print detailed explanations of the query</span>

stopQuery<span class="p">(</span>query<span class="p">)</span>          <span class="c1"># stop the query</span>

awaitTermination<span class="p">(</span>query<span class="p">)</span>   <span class="c1"># block until query is terminated, with stop() or with error</span>

lastProgress<span class="p">(</span>query<span class="p">)</span>       <span class="c1"># the most recent progress update of this streaming query</span></code></pre></figure>

  </div>
</div>

<p>您可以在单个SparkSession中启动任意数量的查询。它们将同时运行以共享群集资源。您可以使用<code>sparkSession.streams()</code>得到<code>StreamingQueryManager</code> （ <a href="api/scala/index.html#org.apache.spark.sql.streaming.StreamingQueryManager">Scala</a> / <a href="api/java/org/apache/spark/sql/streaming/StreamingQueryManager.html">Java</a> / <a href="api/python/pyspark.sql.html#pyspark.sql.streaming.StreamingQueryManager">Python</a>文档），可用于管理当前活动的查询。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">spark</span><span class="k">:</span> <span class="kt">SparkSession</span> <span class="o">=</span> <span class="o">...</span>

<span class="n">spark</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">active</span>    <span class="c1">// get the list of currently active streaming queries</span>

<span class="n">spark</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="n">id</span><span class="o">)</span>   <span class="c1">// get a query object by its unique id</span>

<span class="n">spark</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">awaitAnyTermination</span><span class="o">()</span>   <span class="c1">// block until any one of them terminates</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">SparkSession</span> <span class="n">spark</span> <span class="o">=</span> <span class="o">...</span>

<span class="n">spark</span><span class="o">.</span><span class="na">streams</span><span class="o">().</span><span class="na">active</span><span class="o">();</span>    <span class="c1">// get the list of currently active streaming queries</span>

<span class="n">spark</span><span class="o">.</span><span class="na">streams</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="n">id</span><span class="o">);</span>   <span class="c1">// get a query object by its unique id</span>

<span class="n">spark</span><span class="o">.</span><span class="na">streams</span><span class="o">().</span><span class="na">awaitAnyTermination</span><span class="o">();</span>   <span class="c1">// block until any one of them terminates</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">spark</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># spark session</span>

<span class="n">spark</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">active</span>  <span class="c1"># get the list of currently active streaming queries</span>

<span class="n">spark</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">id</span><span class="p">)</span>  <span class="c1"># get a query object by its unique id</span>

<span class="n">spark</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">awaitAnyTermination</span><span class="p">()</span>  <span class="c1"># block until any one of them terminates</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>Not available in R.</code></pre></figure>

  </div>
</div>

<h2 id="monitoring-streaming-queries">监控流查询</h2>
<p>有多种监视活动流查询的方法。您可以使用Spark的Dropwizard Metrics支持将指标推送到外部系统，也可以通过编程方式访问它们。</p>

<h3 id="reading-metrics-interactively">交互阅读指标</h3>

<p>您可以使用以下方法直接获取活动查询的当前状态和指标<code>streamingQuery.lastProgress()</code>和<code>streamingQuery.status()</code> 。
<code>lastProgress()</code>返回一个<code>StreamingQueryProgress</code> <a href="api/scala/index.html#org.apache.spark.sql.streaming.StreamingQueryProgress">Scala</a>和<a href="api/java/org/apache/spark/sql/streaming/StreamingQueryProgress.html">Java中的</a>对象，以及Python中具有相同字段的字典。它具有有关流的最后一个触发中所取得的进度的所有信息-处理了哪些数据，处理率，等待时间等。还有<code>streamingQuery.recentProgress</code>它返回最近的一些进展的数组。</p>

<p>此外， <code>streamingQuery.status()</code>返回一个<code>StreamingQueryStatus</code> <a href="api/scala/index.html#org.apache.spark.sql.streaming.StreamingQueryStatus">Scala</a>和<a href="api/java/org/apache/spark/sql/streaming/StreamingQueryStatus.html">Java中的</a>对象，以及Python中具有相同字段的字典。它提供有关查询立即执行的操作的信息-触发器处于活动状态，正在处理数据等。</p>

<p>这里有一些例子。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">query</span><span class="k">:</span> <span class="kt">StreamingQuery</span> <span class="o">=</span> <span class="o">...</span>

<span class="n">println</span><span class="o">(</span><span class="n">query</span><span class="o">.</span><span class="n">lastProgress</span><span class="o">)</span>

<span class="cm">/* Will print something like the following.</span>

<span class="cm">{</span>
<span class="cm">  &quot;id&quot; : &quot;ce011fdc-8762-4dcb-84eb-a77333e28109&quot;,</span>
<span class="cm">  &quot;runId&quot; : &quot;88e2ff94-ede0-45a8-b687-6316fbef529a&quot;,</span>
<span class="cm">  &quot;name&quot; : &quot;MyQuery&quot;,</span>
<span class="cm">  &quot;timestamp&quot; : &quot;2016-12-14T18:45:24.873Z&quot;,</span>
<span class="cm">  &quot;numInputRows&quot; : 10,</span>
<span class="cm">  &quot;inputRowsPerSecond&quot; : 120.0,</span>
<span class="cm">  &quot;processedRowsPerSecond&quot; : 200.0,</span>
<span class="cm">  &quot;durationMs&quot; : {</span>
<span class="cm">    &quot;triggerExecution&quot; : 3,</span>
<span class="cm">    &quot;getOffset&quot; : 2</span>
<span class="cm">  },</span>
<span class="cm">  &quot;eventTime&quot; : {</span>
<span class="cm">    &quot;watermark&quot; : &quot;2016-12-14T18:45:24.873Z&quot;</span>
<span class="cm">  },</span>
<span class="cm">  &quot;stateOperators&quot; : [ ],</span>
<span class="cm">  &quot;sources&quot; : [ {</span>
<span class="cm">    &quot;description&quot; : &quot;KafkaSource[Subscribe[topic-0]]&quot;,</span>
<span class="cm">    &quot;startOffset&quot; : {</span>
<span class="cm">      &quot;topic-0&quot; : {</span>
<span class="cm">        &quot;2&quot; : 0,</span>
<span class="cm">        &quot;4&quot; : 1,</span>
<span class="cm">        &quot;1&quot; : 1,</span>
<span class="cm">        &quot;3&quot; : 1,</span>
<span class="cm">        &quot;0&quot; : 1</span>
<span class="cm">      }</span>
<span class="cm">    },</span>
<span class="cm">    &quot;endOffset&quot; : {</span>
<span class="cm">      &quot;topic-0&quot; : {</span>
<span class="cm">        &quot;2&quot; : 0,</span>
<span class="cm">        &quot;4&quot; : 115,</span>
<span class="cm">        &quot;1&quot; : 134,</span>
<span class="cm">        &quot;3&quot; : 21,</span>
<span class="cm">        &quot;0&quot; : 534</span>
<span class="cm">      }</span>
<span class="cm">    },</span>
<span class="cm">    &quot;numInputRows&quot; : 10,</span>
<span class="cm">    &quot;inputRowsPerSecond&quot; : 120.0,</span>
<span class="cm">    &quot;processedRowsPerSecond&quot; : 200.0</span>
<span class="cm">  } ],</span>
<span class="cm">  &quot;sink&quot; : {</span>
<span class="cm">    &quot;description&quot; : &quot;MemorySink&quot;</span>
<span class="cm">  }</span>
<span class="cm">}</span>
<span class="cm">*/</span>


<span class="n">println</span><span class="o">(</span><span class="n">query</span><span class="o">.</span><span class="n">status</span><span class="o">)</span>

<span class="cm">/*  Will print something like the following.</span>
<span class="cm">{</span>
<span class="cm">  &quot;message&quot; : &quot;Waiting for data to arrive&quot;,</span>
<span class="cm">  &quot;isDataAvailable&quot; : false,</span>
<span class="cm">  &quot;isTriggerActive&quot; : false</span>
<span class="cm">}</span>
<span class="cm">*/</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">StreamingQuery</span> <span class="n">query</span> <span class="o">=</span> <span class="o">...</span>

<span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="n">query</span><span class="o">.</span><span class="na">lastProgress</span><span class="o">());</span>
<span class="cm">/* Will print something like the following.</span>

<span class="cm">{</span>
<span class="cm">  &quot;id&quot; : &quot;ce011fdc-8762-4dcb-84eb-a77333e28109&quot;,</span>
<span class="cm">  &quot;runId&quot; : &quot;88e2ff94-ede0-45a8-b687-6316fbef529a&quot;,</span>
<span class="cm">  &quot;name&quot; : &quot;MyQuery&quot;,</span>
<span class="cm">  &quot;timestamp&quot; : &quot;2016-12-14T18:45:24.873Z&quot;,</span>
<span class="cm">  &quot;numInputRows&quot; : 10,</span>
<span class="cm">  &quot;inputRowsPerSecond&quot; : 120.0,</span>
<span class="cm">  &quot;processedRowsPerSecond&quot; : 200.0,</span>
<span class="cm">  &quot;durationMs&quot; : {</span>
<span class="cm">    &quot;triggerExecution&quot; : 3,</span>
<span class="cm">    &quot;getOffset&quot; : 2</span>
<span class="cm">  },</span>
<span class="cm">  &quot;eventTime&quot; : {</span>
<span class="cm">    &quot;watermark&quot; : &quot;2016-12-14T18:45:24.873Z&quot;</span>
<span class="cm">  },</span>
<span class="cm">  &quot;stateOperators&quot; : [ ],</span>
<span class="cm">  &quot;sources&quot; : [ {</span>
<span class="cm">    &quot;description&quot; : &quot;KafkaSource[Subscribe[topic-0]]&quot;,</span>
<span class="cm">    &quot;startOffset&quot; : {</span>
<span class="cm">      &quot;topic-0&quot; : {</span>
<span class="cm">        &quot;2&quot; : 0,</span>
<span class="cm">        &quot;4&quot; : 1,</span>
<span class="cm">        &quot;1&quot; : 1,</span>
<span class="cm">        &quot;3&quot; : 1,</span>
<span class="cm">        &quot;0&quot; : 1</span>
<span class="cm">      }</span>
<span class="cm">    },</span>
<span class="cm">    &quot;endOffset&quot; : {</span>
<span class="cm">      &quot;topic-0&quot; : {</span>
<span class="cm">        &quot;2&quot; : 0,</span>
<span class="cm">        &quot;4&quot; : 115,</span>
<span class="cm">        &quot;1&quot; : 134,</span>
<span class="cm">        &quot;3&quot; : 21,</span>
<span class="cm">        &quot;0&quot; : 534</span>
<span class="cm">      }</span>
<span class="cm">    },</span>
<span class="cm">    &quot;numInputRows&quot; : 10,</span>
<span class="cm">    &quot;inputRowsPerSecond&quot; : 120.0,</span>
<span class="cm">    &quot;processedRowsPerSecond&quot; : 200.0</span>
<span class="cm">  } ],</span>
<span class="cm">  &quot;sink&quot; : {</span>
<span class="cm">    &quot;description&quot; : &quot;MemorySink&quot;</span>
<span class="cm">  }</span>
<span class="cm">}</span>
<span class="cm">*/</span>


<span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="n">query</span><span class="o">.</span><span class="na">status</span><span class="o">());</span>
<span class="cm">/*  Will print something like the following.</span>
<span class="cm">{</span>
<span class="cm">  &quot;message&quot; : &quot;Waiting for data to arrive&quot;,</span>
<span class="cm">  &quot;isDataAvailable&quot; : false,</span>
<span class="cm">  &quot;isTriggerActive&quot; : false</span>
<span class="cm">}</span>
<span class="cm">*/</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">query</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># a StreamingQuery</span>
<span class="k">print</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">lastProgress</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">Will print something like the following.</span>

<span class="sd">{u&#39;stateOperators&#39;: [], u&#39;eventTime&#39;: {u&#39;watermark&#39;: u&#39;2016-12-14T18:45:24.873Z&#39;}, u&#39;name&#39;: u&#39;MyQuery&#39;, u&#39;timestamp&#39;: u&#39;2016-12-14T18:45:24.873Z&#39;, u&#39;processedRowsPerSecond&#39;: 200.0, u&#39;inputRowsPerSecond&#39;: 120.0, u&#39;numInputRows&#39;: 10, u&#39;sources&#39;: [{u&#39;description&#39;: u&#39;KafkaSource[Subscribe[topic-0]]&#39;, u&#39;endOffset&#39;: {u&#39;topic-0&#39;: {u&#39;1&#39;: 134, u&#39;0&#39;: 534, u&#39;3&#39;: 21, u&#39;2&#39;: 0, u&#39;4&#39;: 115}}, u&#39;processedRowsPerSecond&#39;: 200.0, u&#39;inputRowsPerSecond&#39;: 120.0, u&#39;numInputRows&#39;: 10, u&#39;startOffset&#39;: {u&#39;topic-0&#39;: {u&#39;1&#39;: 1, u&#39;0&#39;: 1, u&#39;3&#39;: 1, u&#39;2&#39;: 0, u&#39;4&#39;: 1}}}], u&#39;durationMs&#39;: {u&#39;getOffset&#39;: 2, u&#39;triggerExecution&#39;: 3}, u&#39;runId&#39;: u&#39;88e2ff94-ede0-45a8-b687-6316fbef529a&#39;, u&#39;id&#39;: u&#39;ce011fdc-8762-4dcb-84eb-a77333e28109&#39;, u&#39;sink&#39;: {u&#39;description&#39;: u&#39;MemorySink&#39;}}</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="k">print</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">status</span><span class="p">)</span>
<span class="sd">&#39;&#39;&#39; </span>
<span class="sd">Will print something like the following.</span>

<span class="sd">{u&#39;message&#39;: u&#39;Waiting for data to arrive&#39;, u&#39;isTriggerActive&#39;: False, u&#39;isDataAvailable&#39;: False}</span>
<span class="sd">&#39;&#39;&#39;</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>query <span class="o">&lt;-</span> <span class="kc">...</span>  <span class="c1"># a StreamingQuery</span>
lastProgress<span class="p">(</span>query<span class="p">)</span>

<span class="s">&#39;&#39;&#39;</span>
<span class="s">Will print something like the following.</span>

<span class="s">{</span>
<span class="s">  &quot;id&quot; : &quot;8c57e1ec-94b5-4c99-b100-f694162df0b9&quot;,</span>
<span class="s">  &quot;runId&quot; : &quot;ae505c5a-a64e-4896-8c28-c7cbaf926f16&quot;,</span>
<span class="s">  &quot;name&quot; : null,</span>
<span class="s">  &quot;timestamp&quot; : &quot;2017-04-26T08:27:28.835Z&quot;,</span>
<span class="s">  &quot;numInputRows&quot; : 0,</span>
<span class="s">  &quot;inputRowsPerSecond&quot; : 0.0,</span>
<span class="s">  &quot;processedRowsPerSecond&quot; : 0.0,</span>
<span class="s">  &quot;durationMs&quot; : {</span>
<span class="s">    &quot;getOffset&quot; : 0,</span>
<span class="s">    &quot;triggerExecution&quot; : 1</span>
<span class="s">  },</span>
<span class="s">  &quot;stateOperators&quot; : [ {</span>
<span class="s">    &quot;numRowsTotal&quot; : 4,</span>
<span class="s">    &quot;numRowsUpdated&quot; : 0</span>
<span class="s">  } ],</span>
<span class="s">  &quot;sources&quot; : [ {</span>
<span class="s">    &quot;description&quot; : &quot;TextSocketSource[host: localhost, port: 9999]&quot;,</span>
<span class="s">    &quot;startOffset&quot; : 1,</span>
<span class="s">    &quot;endOffset&quot; : 1,</span>
<span class="s">    &quot;numInputRows&quot; : 0,</span>
<span class="s">    &quot;inputRowsPerSecond&quot; : 0.0,</span>
<span class="s">    &quot;processedRowsPerSecond&quot; : 0.0</span>
<span class="s">  } ],</span>
<span class="s">  &quot;sink&quot; : {</span>
<span class="s">    &quot;description&quot; : &quot;org.apache.spark.sql.execution.streaming.ConsoleSink@76b37531&quot;</span>
<span class="s">  }</span>
<span class="s">}</span>
<span class="s">&#39;&#39;&#39;</span>

status<span class="p">(</span>query<span class="p">)</span>
<span class="s">&#39;&#39;&#39;</span>
<span class="s">Will print something like the following.</span>

<span class="s">{</span>
<span class="s">  &quot;message&quot; : &quot;Waiting for data to arrive&quot;,</span>
<span class="s">  &quot;isDataAvailable&quot; : false,</span>
<span class="s">  &quot;isTriggerActive&quot; : false</span>
<span class="s">}</span>
<span class="s">&#39;&#39;&#39;</span></code></pre></figure>

  </div>
</div>

<h3 id="reporting-metrics-programmatically-using-asynchronous-apis">使用异步API以编程方式报告指标</h3>

<p>您还可以异步监视与<code>SparkSession</code>通过附加一个<code>StreamingQueryListener</code> （ <a href="api/scala/index.html#org.apache.spark.sql.streaming.StreamingQueryListener">Scala</a> / <a href="api/java/org/apache/spark/sql/streaming/StreamingQueryListener.html">Java</a>文档）。一旦您附加了自定义<code>StreamingQueryListener</code>与对象<code>sparkSession.streams.attachListener()</code> ，当查询开始和停止以及活动查询中取得进展时，您将获得回调。这是一个例子</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">val</span> <span class="n">spark</span><span class="k">:</span> <span class="kt">SparkSession</span> <span class="o">=</span> <span class="o">...</span>

<span class="n">spark</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">addListener</span><span class="o">(</span><span class="k">new</span> <span class="nc">StreamingQueryListener</span><span class="o">()</span> <span class="o">{</span>
    <span class="k">override</span> <span class="k">def</span> <span class="n">onQueryStarted</span><span class="o">(</span><span class="n">queryStarted</span><span class="k">:</span> <span class="kt">QueryStartedEvent</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
        <span class="n">println</span><span class="o">(</span><span class="s">&quot;Query started: &quot;</span> <span class="o">+</span> <span class="n">queryStarted</span><span class="o">.</span><span class="n">id</span><span class="o">)</span>
    <span class="o">}</span>
    <span class="k">override</span> <span class="k">def</span> <span class="n">onQueryTerminated</span><span class="o">(</span><span class="n">queryTerminated</span><span class="k">:</span> <span class="kt">QueryTerminatedEvent</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
        <span class="n">println</span><span class="o">(</span><span class="s">&quot;Query terminated: &quot;</span> <span class="o">+</span> <span class="n">queryTerminated</span><span class="o">.</span><span class="n">id</span><span class="o">)</span>
    <span class="o">}</span>
    <span class="k">override</span> <span class="k">def</span> <span class="n">onQueryProgress</span><span class="o">(</span><span class="n">queryProgress</span><span class="k">:</span> <span class="kt">QueryProgressEvent</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
        <span class="n">println</span><span class="o">(</span><span class="s">&quot;Query made progress: &quot;</span> <span class="o">+</span> <span class="n">queryProgress</span><span class="o">.</span><span class="n">progress</span><span class="o">)</span>
    <span class="o">}</span>
<span class="o">})</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">SparkSession</span> <span class="n">spark</span> <span class="o">=</span> <span class="o">...</span>

<span class="n">spark</span><span class="o">.</span><span class="na">streams</span><span class="o">().</span><span class="na">addListener</span><span class="o">(</span><span class="k">new</span> <span class="n">StreamingQueryListener</span><span class="o">()</span> <span class="o">{</span>
    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">onQueryStarted</span><span class="o">(</span><span class="n">QueryStartedEvent</span> <span class="n">queryStarted</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">&quot;Query started: &quot;</span> <span class="o">+</span> <span class="n">queryStarted</span><span class="o">.</span><span class="na">id</span><span class="o">());</span>
    <span class="o">}</span>
    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">onQueryTerminated</span><span class="o">(</span><span class="n">QueryTerminatedEvent</span> <span class="n">queryTerminated</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">&quot;Query terminated: &quot;</span> <span class="o">+</span> <span class="n">queryTerminated</span><span class="o">.</span><span class="na">id</span><span class="o">());</span>
    <span class="o">}</span>
    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">onQueryProgress</span><span class="o">(</span><span class="n">QueryProgressEvent</span> <span class="n">queryProgress</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">&quot;Query made progress: &quot;</span> <span class="o">+</span> <span class="n">queryProgress</span><span class="o">.</span><span class="na">progress</span><span class="o">());</span>
    <span class="o">}</span>
<span class="o">});</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>Not available in Python.</code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>Not available in R.</code></pre></figure>

  </div>
</div>

<h3 id="reporting-metrics-using-dropwizard">使用Dropwizard报告指标</h3>
<p>Spark使用<a href="monitoring.html#metrics">Dropwizard库</a>支持报告指标。要同时报告结构化流查询的指标，您必须显式启用配置<code>spark.sql.streaming.metricsEnabled</code>在SparkSession中。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="o">(</span><span class="s">&quot;spark.sql.streaming.metricsEnabled&quot;</span><span class="o">,</span> <span class="s">&quot;true&quot;</span><span class="o">)</span>
<span class="c1">// or</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SET spark.sql.streaming.metricsEnabled=true&quot;</span><span class="o">)</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">spark</span><span class="o">.</span><span class="na">conf</span><span class="o">().</span><span class="na">set</span><span class="o">(</span><span class="s">&quot;spark.sql.streaming.metricsEnabled&quot;</span><span class="o">,</span> <span class="s">&quot;true&quot;</span><span class="o">);</span>
<span class="c1">// or</span>
<span class="n">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">&quot;SET spark.sql.streaming.metricsEnabled=true&quot;</span><span class="o">);</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.streaming.metricsEnabled&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span>
<span class="c1"># or</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SET spark.sql.streaming.metricsEnabled=true&quot;</span><span class="p">)</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>sql<span class="p">(</span><span class="s">&quot;SET spark.sql.streaming.metricsEnabled=true&quot;</span><span class="p">)</span></code></pre></figure>

  </div>
</div>

<p>启用此配置后，在SparkSession中启动的所有查询都将通过Dropwizard向所有已配置的<a href="monitoring.html#metrics">接收器</a> （例如Ganglia，Graphite，JMX等）报告指标。</p>

<h2 id="recovering-from-failures-with-checkpointing">通过检查点从故障中恢复</h2>
<p>如果发生故障或有意关闭，您可以恢复先前的进度和先前查询的状态，并在中断的地方继续进行。这是通过使用检查点日志和预写日志来完成的。您可以配置一个带有检查点位置的查询，该查询会将所有进度信息（即在每个触发器中处理的偏移量范围）和正在运行的聚合（例如<a href="#quick-example">快速示例</a>中的字数）保存到检查点位置。此检查点位置必须是与HDFS兼容的文件系统中的路径，并且可以在<a href="#starting-streaming-queries">启动查询</a>时在DataStreamWriter中设置为选项。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">aggDF</span>
  <span class="o">.</span><span class="n">writeStream</span>
  <span class="o">.</span><span class="n">outputMode</span><span class="o">(</span><span class="s">&quot;complete&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;checkpointLocation&quot;</span><span class="o">,</span> <span class="s">&quot;path/to/HDFS/dir&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;memory&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">start</span><span class="o">()</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">aggDF</span>
  <span class="o">.</span><span class="na">writeStream</span><span class="o">()</span>
  <span class="o">.</span><span class="na">outputMode</span><span class="o">(</span><span class="s">&quot;complete&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;checkpointLocation&quot;</span><span class="o">,</span> <span class="s">&quot;path/to/HDFS/dir&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;memory&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">start</span><span class="o">();</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">aggDF</span> \
    <span class="o">.</span><span class="n">writeStream</span> \
    <span class="o">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s2">&quot;complete&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;checkpointLocation&quot;</span><span class="p">,</span> <span class="s2">&quot;path/to/HDFS/dir&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;memory&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">start</span><span class="p">()</span></code></pre></figure>

  </div>
<div data-lang="r">

    <figure class="highlight"><pre><code class="language-r" data-lang="r"><span></span>write.stream<span class="p">(</span>aggDF<span class="p">,</span> <span class="s">&quot;memory&quot;</span><span class="p">,</span> outputMode <span class="o">=</span> <span class="s">&quot;complete&quot;</span><span class="p">,</span> checkpointLocation <span class="o">=</span> <span class="s">&quot;path/to/HDFS/dir&quot;</span><span class="p">)</span></code></pre></figure>

  </div>
</div>

<h2 id="recovery-semantics-after-changes-in-a-streaming-query">流查询中的更改后的恢复语义</h2>
<p>从同一检查点位置重新启动之间允许在流查询中进行哪些更改是有限制的。这是一些不允许的更改，或者更改的效果不清楚。对于所有人：</p>

<ul>
  <li>
    <p>“ <em>允许</em> ”一词意味着您可以进行指定的更改，但是其效果的语义是否定义明确取决于查询和更改。</p>
  </li>
  <li>
    <p>术语“ <em>不允许”</em>表示您不应该进行指定的更改，因为重新启动的查询很可能因不可预测的错误而失败。 <code>sdf</code>表示使用sparkSession.readStream生成的流式DataFrame / Dataset。</p>
  </li>
</ul>

<p><strong>变更类型</strong></p>

<ul>
  <li>
    <p><em>输入源的数量或类型（即不同源）的更改</em> ：不允许这样做。</p>
  </li>
  <li>
    <p><em>输入源参数的更改</em> ：是否允许<em>输入</em>以及更改的语义是否定义良好取决于输入源和查询。这里有一些例子。</p>

    <ul>
      <li>
        <p>允许增加/删除/修改速率限制： <code>spark.readStream.format("kafka").option("subscribe", "topic")</code>至<code>spark.readStream.format("kafka").option("subscribe", "topic").option("maxOffsetsPerTrigger", ...)</code></p>
      </li>
      <li>
        <p>通常不允许更改订阅的主题/文件，因为结果是不可预测的： <code>spark.readStream.format("kafka").option("subscribe", "topic")</code>至<code>spark.readStream.format("kafka").option("subscribe", "newTopic")</code></p>
      </li>
    </ul>
  </li>
  <li>
    <p><em>输出接收器类型的</em>更改：允许在几个特定的接收器组合之间进行更改。这需要根据具体情况进行验证。这里有一些例子。</p>

    <ul>
      <li>
        <p>允许文件接收器到Kafka接收器。Kafka将仅看到新数据。</p>
      </li>
      <li>
        <p>卡夫卡接收器到文件接收器是不允许的。</p>
      </li>
      <li>
        <p>Kafka接收器更改为foreach，反之亦然。</p>
      </li>
    </ul>
  </li>
  <li>
    <p><em>输出接收器的参数更改</em> ：是否允许<em>输出</em>以及更改的语义是否定义良好，取决于接收器和查询。这里有一些例子。</p>

    <ul>
      <li>
        <p>不允许更改文件接收器的输出目录： <code>sdf.writeStream.format("parquet").option("path", "/somePath")</code>至<code>sdf.writeStream.format("parquet").option("path", "/anotherPath")</code></p>
      </li>
      <li>
        <p>允许更改输出主题： <code>sdf.writeStream.format("kafka").option("topic", "someTopic")</code>至<code>sdf.writeStream.format("kafka").option("topic", "anotherTopic")</code></p>
      </li>
      <li>
        <p>更改为用户定义的foreach接收器（即<code>ForeachWriter</code>代码），但更改的语义取决于代码。</p>
      </li>
    </ul>
  </li>
  <li>
    <p>*更改投影/过滤器/类似地图的操作**：允许某些情况。例如：</p>

    <ul>
      <li>
        <p>允许添加/删除过滤器： <code>sdf.selectExpr("a")</code>至<code>sdf.where(...).selectExpr("a").filter(...)</code> 。</p>
      </li>
      <li>
        <p>允许更改具有相同输出模式的投影： <code>sdf.selectExpr("stringColumn AS json").writeStream</code>至<code>sdf.selectExpr("anotherStringColumn AS json").writeStream</code></p>
      </li>
      <li>
        <p>有条件地允许更改具有不同输出模式的投影： <code>sdf.selectExpr("a").writeStream</code>至<code>sdf.selectExpr("b").writeStream</code>仅当输出接收器允许架构更改为<code>"a"</code>至<code>"b"</code> 。</p>
      </li>
    </ul>
  </li>
  <li>
    <p><em>有状态操作的变化</em> ：流查询中的某些操作需要维护状态数据，以便连续更新结果。结构化流自动将状态数据检查点指向容错存储（例如，HDFS，AWS S3，Azure Blob存储），并在重启后将其还原。但是，这假设状态数据的架构在重新启动后保持不变。这意味着<em>在重新启动之间不允许对流查询的有状态操作进行任何更改（即添加，删除或架构修改）</em> 。以下是有状态操作的列表，其状态在重启之间不应该更改，以确保状态恢复：</p>

    <ul>
      <li>
        <p><em>流式聚合</em> ：例如， <code>sdf.groupBy("a").agg(...)</code> 。不允许对分组键或聚合的数量或类型进行任何更改。</p>
      </li>
      <li>
        <p><em>流重复数据删除</em> ：例如， <code>sdf.dropDuplicates("a")</code> 。不允许对分组键或聚合的数量或类型进行任何更改。</p>
      </li>
      <li>
        <p><em>流流连接</em> ：例如， <code>sdf1.join(sdf2, ...)</code> （即两个输入都是通过<code>sparkSession.readStream</code> ）。不允许更改架构或等效联接列。不允许更改连接类型（外部或内部）。连接条件的其他变化是不确定的。</p>
      </li>
      <li>
        <p><em>任意状态操作</em> ：例如， <code>sdf.groupByKey(...).mapGroupsWithState(...)</code>要么<code>sdf.groupByKey(...).flatMapGroupsWithState(...)</code> 。不允许对用户定义状态的架构和超时类型进行任何更改。允许在用户定义的状态映射功能内进行任何更改，但是更改的语义效果取决于用户定义的逻辑。如果您确实想支持状态模式更改，则可以使用支持模式迁移的编码/解码方案将复杂的状态数据结构显式编码/解码为字节。例如，如果将状态另存为Avro编码的字节，则可以自由地在两次查询重新启动之间更改Avro状态模式，因为二进制状态将始终成功恢复。</p>
      </li>
    </ul>
  </li>
</ul>

<h1 id="continuous-processing">连续加工</h1>
<h2 class="no_toc" id="experimental">[实验]</h2>

<p><strong>连续处理</strong>是Spark 2.3中引入的一种新的实验性流执行模式，可实现低（〜1 ms）的端到端延迟，并保证至少一次容错。将其与默认的<em>微批量处理</em>引擎进行比较，该引擎可以实现一次精确的保证，但最多只能实现约100ms的延迟。对于某些类型的查询（如下所述），您可以选择在不修改应用程序逻辑（即不更改DataFrame / Dataset操作）的情况下以哪种模式执行查询。</p>

<p>要以连续处理模式运行受支持的查询，您需要做的就是指定一个以所需检查点间隔为参数的<strong>连续触发器</strong> 。例如，</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="k">import</span> <span class="nn">org.apache.spark.sql.streaming.Trigger</span>

<span class="n">spark</span>
  <span class="o">.</span><span class="n">readStream</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;kafka&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;kafka.bootstrap.servers&quot;</span><span class="o">,</span> <span class="s">&quot;host1:port1,host2:port2&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;subscribe&quot;</span><span class="o">,</span> <span class="s">&quot;topic1&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">load</span><span class="o">()</span>
  <span class="o">.</span><span class="n">selectExpr</span><span class="o">(</span><span class="s">&quot;CAST(key AS STRING)&quot;</span><span class="o">,</span> <span class="s">&quot;CAST(value AS STRING)&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">writeStream</span>
  <span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;kafka&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;kafka.bootstrap.servers&quot;</span><span class="o">,</span> <span class="s">&quot;host1:port1,host2:port2&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;topic&quot;</span><span class="o">,</span> <span class="s">&quot;topic1&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="n">trigger</span><span class="o">(</span><span class="nc">Trigger</span><span class="o">.</span><span class="nc">Continuous</span><span class="o">(</span><span class="s">&quot;1 second&quot;</span><span class="o">))</span>  <span class="c1">// only change in query</span>
  <span class="o">.</span><span class="n">start</span><span class="o">()</span></code></pre></figure>

  </div>
<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="kn">import</span> <span class="nn">org.apache.spark.sql.streaming.Trigger</span><span class="o">;</span>

<span class="n">spark</span>
  <span class="o">.</span><span class="na">readStream</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;kafka&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;kafka.bootstrap.servers&quot;</span><span class="o">,</span> <span class="s">&quot;host1:port1,host2:port2&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;subscribe&quot;</span><span class="o">,</span> <span class="s">&quot;topic1&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">load</span><span class="o">()</span>
  <span class="o">.</span><span class="na">selectExpr</span><span class="o">(</span><span class="s">&quot;CAST(key AS STRING)&quot;</span><span class="o">,</span> <span class="s">&quot;CAST(value AS STRING)&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">writeStream</span>
  <span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;kafka&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;kafka.bootstrap.servers&quot;</span><span class="o">,</span> <span class="s">&quot;host1:port1,host2:port2&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">option</span><span class="o">(</span><span class="s">&quot;topic&quot;</span><span class="o">,</span> <span class="s">&quot;topic1&quot;</span><span class="o">)</span>
  <span class="o">.</span><span class="na">trigger</span><span class="o">(</span><span class="n">Trigger</span><span class="o">.</span><span class="na">Continuous</span><span class="o">(</span><span class="s">&quot;1 second&quot;</span><span class="o">))</span>  <span class="c1">// only change in query</span>
  <span class="o">.</span><span class="na">start</span><span class="o">();</span></code></pre></figure>

  </div>
<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="n">spark</span> \
  <span class="o">.</span><span class="n">readStream</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;kafka&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;kafka.bootstrap.servers&quot;</span><span class="p">,</span> <span class="s2">&quot;host1:port1,host2:port2&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;subscribe&quot;</span><span class="p">,</span> <span class="s2">&quot;topic1&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">load</span><span class="p">()</span> \
  <span class="o">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s2">&quot;CAST(key AS STRING)&quot;</span><span class="p">,</span> <span class="s2">&quot;CAST(value AS STRING)&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">writeStream</span> \
  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;kafka&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;kafka.bootstrap.servers&quot;</span><span class="p">,</span> <span class="s2">&quot;host1:port1,host2:port2&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;topic&quot;</span><span class="p">,</span> <span class="s2">&quot;topic1&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">trigger</span><span class="p">(</span><span class="n">continuous</span><span class="o">=</span><span class="s2">&quot;1 second&quot;</span><span class="p">)</span> \     <span class="c1"># only change in query</span>
  <span class="o">.</span><span class="n">start</span><span class="p">()</span></code></pre></figure>

  </div>
</div>

<p>1秒的检查点间隔意味着连续处理引擎将每秒记录一次查询的进度。生成的检查点的格式与微批处理引擎兼容，因此可以使用任何触发器重新启动任何查询。例如，以微批量模式开始的受支持查询可以以连续模式重新启动，反之亦然。请注意，无论何时切换到连续模式，都将获得至少一次的容错保证。</p>

<h2 class="no_toc" id="supported-queries">支持的查询</h2>

<p>从Spark 2.4开始，连续处理模式仅支持以下类型的查询。</p>

<ul>
  <li><em>操作</em> ：在连续模式下仅支持类似地图的Dataset / DataFrame操作，即仅投影（ <code>select</code> ， <code>map</code> ， <code>flatMap</code> ， <code>mapPartitions</code>等）和选择（ <code>where</code> ， <code>filter</code>等）。
    <ul>
      <li>支持所有SQL函数，但聚合函数除外（因为尚未支持聚合）， <code>current_timestamp()</code>和<code>current_date()</code> （使用时间进行确定性计算具有挑战性）。</li>
    </ul>
  </li>
  <li><em>资料来源</em> ：<ul>
      <li>Kafka来源：支持所有选项。</li>
      <li>评分来源：适合测试。仅连续模式支持的选项是<code>numPartitions</code>和<code>rowsPerSecond</code> 。</li>
    </ul>
  </li>
  <li><em>水槽</em> ：<ul>
      <li>Kafka sink：支持所有选项。</li>
      <li>内存接收器：适用于调试。</li>
      <li>控制台接收器：适用于调试。支持所有选项。请注意，控制台将打印您在连续触发器中指定的每个检查点间隔。</li>
    </ul>
  </li>
</ul>

<p>有关它们的更多详细信息，请参见<a href="#input-sources">输入源</a>和<a href="#output-sinks">输出接收器</a>部分。尽管控制台接收器非常适合测试，但是可以使用Kafka作为源和接收器来最好地观察端到端低延迟处理，因为这允许引擎处理数据并使结果可用于以下位置的输出主题中：输入主题中可用的输入数据的毫秒数。</p>

<h2 class="no_toc" id="caveats-1">注意事项</h2>

<ul>
  <li>连续处理引擎启动了多个长期运行的任务，这些任务不断地从源中读取数据，对其进行处理并连续地写入接收器。查询所需的任务数取决于查询可以并行从源读取多少个分区。因此，在开始连续处理查询之前，必须确保集群中有足够的内核来并行执行所有任务。例如，如果您正在从具有10个分区的Kafka主题中读取信息，则集群必须至少具有10个核心，查询才能取得进展。</li>
  <li>停止连续的处理流可能会产生虚假的任务终止警告。这些可以安全地忽略。</li>
  <li>当前没有自动重试失败的任务。任何失败都将导致查询被停止，并且需要从检查点手动重新启动。</li>
</ul>

<h1 id="additional-information">附加信息</h1>

<p><strong>进一步阅读</strong></p>

<ul>
  <li>查看并运行<a href="https://github.com/apache/spark/tree/v2.4.4/examples/src/main/scala/org/apache/spark/examples/sql/streaming">Scala</a> / <a href="https://github.com/apache/spark/tree/v2.4.4/examples/src/main/java/org/apache/spark/examples/sql/streaming">Java</a> / <a href="https://github.com/apache/spark/tree/v2.4.4/examples/src/main/python/sql/streaming">Python</a> / <a href="https://github.com/apache/spark/tree/v2.4.4/examples/src/main/r/streaming">R</a>示例。
    <ul>
      <li><a href="index.html#running-the-examples-and-shell">说明</a>如何运行星火例子</li>
    </ul>
  </li>
  <li>在《 <a href="structured-streaming-kafka-integration.html">结构化流式Kafka集成指南》中</a>阅读有关与<a href="structured-streaming-kafka-integration.html">Kafka集成的信息。</a></li>
  <li>在《 <a href="sql-programming-guide.html">Spark SQL编程指南》中</a>阅读有关使用DataFrames / Datasets的更多详细信息</li>
  <li>第三方博客文章<ul>
      <li><a href="https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html">Apache Spark 2.1中具有结构化流的实时流ETL（Databricks博客）</a></li>
      <li><a href="https://databricks.com/blog/2017/04/04/real-time-end-to-end-integration-with-apache-kafka-in-apache-sparks-structured-streaming.html">在Apache Spark的结构化流中与Apache Kafka进行实时端到端集成（Databricks博客）</a></li>
      <li><a href="https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html">Apache Spark的结构化流中的事件时间聚合和水印（Databricks博客）</a></li>
    </ul>
  </li>
</ul>

<p><strong>会谈</strong></p>

<ul>
  <li>欧洲火花峰会2017<ul>
      <li>Apache Spark中具有结构化流的简单，可扩展，容错流处理- <a href="https://databricks.com/session/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-in-apache-spark">第1部分幻灯片/视频</a> ， <a href="https://databricks.com/session/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-in-apache-spark-continues">第2部分幻灯片/视频</a></li>
      <li>深入研究结构化流中的状态流处理- <a href="https://databricks.com/session/deep-dive-into-stateful-stream-processing-in-structured-streaming">幻灯片/视频</a></li>
    </ul>
  </li>
  <li>2016年火花峰会<ul>
      <li>深入了解结构化流- <a href="https://spark-summit.org/2016/events/a-deep-dive-into-structured-streaming/">幻灯片/视频</a></li>
    </ul>
  </li>
</ul>



                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-1.12.4.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    

</body></html>