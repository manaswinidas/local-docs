<html class="no-js" ><!--<![endif]--><head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Building Spark-Spark 2.4.4文档</title>
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        
        <!-- Google analytics script -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-32518208-2']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
        </script>
        

    </head>
    <body >
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html"><img src="img/spark-logo-hd.png" style="height:50px"></a> <span class="version">2.4.4</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">总览</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">编程指南<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">快速开始</a></li>
                                <li><a href="rdd-programming-guide.html">RDD，累加器，广播变量</a></li>
                                <li><a href="sql-programming-guide.html">SQL，数据框和数据集</a></li>
                                <li><a href="structured-streaming-programming-guide.html">结构化流</a></li>
                                <li><a href="streaming-programming-guide.html">火花流（DStreams）</a></li>
                                <li><a href="ml-guide.html">MLlib（机器学习）</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX（图形处理）</a></li>
                                <li><a href="sparkr.html">SparkR（Spark上的R）</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API文件<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">斯卡拉</a></li>
                                <li><a href="api/java/index.html">爪哇</a></li>
                                <li><a href="api/python/index.html">蟒蛇</a></li>
                                <li><a href="api/R/index.html">[R</a></li>
                                <li><a href="api/sql/index.html">SQL，内置函数</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">部署中<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">总览</a></li>
                                <li><a href="submitting-applications.html">提交申请</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark独立</a></li>
                                <li><a href="running-on-mesos.html">梅索斯</a></li>
                                <li><a href="running-on-yarn.html">纱</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">更多<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">组态</a></li>
                                <li><a href="monitoring.html">监控方式</a></li>
                                <li><a href="tuning.html">调音指南</a></li>
                                <li><a href="job-scheduling.html">作业调度</a></li>
                                <li><a href="security.html">安全</a></li>
                                <li><a href="hardware-provisioning.html">硬件配置</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">建筑火花</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">为Spark贡献</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">第三方项目</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v2.4.4</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                <div class="content" id="content">
                    
                        <h1 class="title">建筑火花</h1>
                    

                    <ul id="markdown-toc">
  <li><a href="#building-apache-spark" id="markdown-toc-building-apache-spark">构建Apache Spark</a>    <ul>
      <li><a href="#apache-maven" id="markdown-toc-apache-maven">阿帕奇Maven</a>        <ul>
          <li><a href="#setting-up-mavens-memory-usage" id="markdown-toc-setting-up-mavens-memory-usage">设置Maven的内存使用情况</a></li>
          <li><a href="#buildmvn" id="markdown-toc-buildmvn">建立/ MVN</a></li>
        </ul>
      </li>
      <li><a href="#building-a-runnable-distribution" id="markdown-toc-building-a-runnable-distribution">建立可运行的发行版</a></li>
      <li><a href="#specifying-the-hadoop-version-and-enabling-yarn" id="markdown-toc-specifying-the-hadoop-version-and-enabling-yarn">指定Hadoop版本并启用YARN</a></li>
      <li><a href="#building-with-hive-and-jdbc-support" id="markdown-toc-building-with-hive-and-jdbc-support">使用Hive和JDBC支持进行构建</a></li>
      <li><a href="#packaging-without-hadoop-dependencies-for-yarn" id="markdown-toc-packaging-without-hadoop-dependencies-for-yarn">对于YARN，没有Hadoop依赖项的打包</a></li>
      <li><a href="#building-with-mesos-support" id="markdown-toc-building-with-mesos-support">在Mesos支持下进行构建</a></li>
      <li><a href="#building-with-kubernetes-support" id="markdown-toc-building-with-kubernetes-support">在Kubernetes支持下进行构建</a></li>
      <li><a href="#building-with-kafka-08-support" id="markdown-toc-building-with-kafka-08-support">使用Kafka 0.8支持进行构建</a></li>
      <li><a href="#building-with-flume-support" id="markdown-toc-building-with-flume-support">在Flume支持下进行构建</a></li>
      <li><a href="#building-submodules-individually" id="markdown-toc-building-submodules-individually">分别构建子模块</a></li>
      <li><a href="#continuous-compilation" id="markdown-toc-continuous-compilation">连续编译</a></li>
      <li><a href="#building-with-sbt" id="markdown-toc-building-with-sbt">使用SBT构建</a></li>
      <li><a href="#speeding-up-compilation" id="markdown-toc-speeding-up-compilation">加快编译速度</a></li>
      <li><a href="#encrypted-filesystems" id="markdown-toc-encrypted-filesystems">加密文件系统</a></li>
      <li><a href="#intellij-idea-or-eclipse" id="markdown-toc-intellij-idea-or-eclipse">IntelliJ IDEA或Eclipse</a></li>
    </ul>
  </li>
  <li><a href="#running-tests" id="markdown-toc-running-tests">运行测试</a>    <ul>
      <li><a href="#testing-with-sbt" id="markdown-toc-testing-with-sbt">使用SBT测试</a></li>
      <li><a href="#running-individual-tests" id="markdown-toc-running-individual-tests">运行个别测试</a></li>
      <li><a href="#pyspark-pip-installable" id="markdown-toc-pyspark-pip-installable">PySpark pip可安装</a></li>
      <li><a href="#pyspark-tests-with-maven-or-sbt" id="markdown-toc-pyspark-tests-with-maven-or-sbt">使用Maven或SBT进行PySpark测试</a></li>
      <li><a href="#running-r-tests" id="markdown-toc-running-r-tests">运行R测试</a></li>
      <li><a href="#running-docker-based-integration-test-suites" id="markdown-toc-running-docker-based-integration-test-suites">运行基于Docker的集成测试套件</a></li>
      <li><a href="#change-scala-version" id="markdown-toc-change-scala-version">更改Scala版本</a></li>
    </ul>
  </li>
</ul>

<h1 id="building-apache-spark">构建Apache Spark</h1>

<h2 id="apache-maven">阿帕奇Maven</h2>

<p>基于Maven的构建是Apache Spark的参考构建。使用Maven构建Spark需要Maven 3.5.4和Java 8。请注意，自Spark 2.2.0起已删除了对Java 7的支持。</p>

<h3 id="setting-up-mavens-memory-usage">设置Maven的内存使用情况</h3>

<p>您需要通过设置将Maven配置为使用比平常更多的内存<code>MAVEN_OPTS</code> ：</p>

<pre><code>export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=512m"
</code></pre>

<p>（ <code>ReservedCodeCacheSize</code>设置是可选的，但建议使用。）如果您不将这些参数添加到<code>MAVEN_OPTS</code> ，您可能会看到类似以下的错误和警告：</p>

<pre><code>[INFO] Compiling 203 Scala sources and 9 Java sources to /Users/me/Development/spark/core/target/scala-2.12/classes...
[ERROR] Java heap space -&gt; [Help 1]
</code></pre>

<p>您可以通过设置<code>MAVEN_OPTS</code>如前所述。</p>

<p><strong>注意：</strong></p>

<ul>
  <li>如果使用<code>build/mvn</code>没有<code>MAVEN_OPTS</code>设置后，脚本会自动将上述选项添加到<code>MAVEN_OPTS</code>环境变量。</li>
  <li>的<code>test</code> Spark构建阶段会自动将这些选项添加到<code>MAVEN_OPTS</code> ，即使不使用<code>build/mvn</code> 。</li>
</ul>

<h3 id="buildmvn">建立/ MVN</h3>

<p>现在，Spark随附了一个独立的Maven安装包，可简化位于以下位置的源代码的Spark构建和部署。 <code>build/</code>目录。该脚本将在本地自动下载并设置所有必要的构建要求（ <a href="https://maven.apache.org/">Maven</a> ， <a href="http://www.scala-lang.org/">Scala</a>和<a href="https://github.com/typesafehub/zinc">Zinc</a> ）。 <code>build/</code>目录本身。它荣誉任何<code>mvn</code>但是，二进制文件（如果已存在）将拉低其自己的Scala和Zinc副本，以确保满足正确的版本要求。 <code>build/mvn</code>执行是传递给<code>mvn</code>调用可轻松从以前的构建方法过渡。例如，可以构建一个Spark版本，如下所示：</p>

<pre><code>./build/mvn -DskipTests clean package
</code></pre>

<p>其他构建示例可以在下面找到。</p>

<h2 id="building-a-runnable-distribution">建立可运行的发行版</h2>

<p>要创建一个类似于<a href="https://spark.apache.org/downloads.html">Spark Downloads</a>页面所分发的<a href="https://spark.apache.org/downloads.html">Spark发行版</a> ，并且其布局可以运行，请使用<code>./dev/make-distribution.sh</code>在项目根目录中。可以使用Maven配置文件设置等进行配置，例如直接进行Maven构建。例：</p>

<pre><code>./dev/make-distribution.sh --name custom-spark --pip --r --tgz -Psparkr -Phadoop-2.7 -Phive -Phive-thriftserver -Pmesos -Pyarn -Pkubernetes
</code></pre>

<p>这将构建Spark发行版以及Python pip和R包。有关用法的更多信息，请运行<code>./dev/make-distribution.sh --help</code></p>

<h2 id="specifying-the-hadoop-version-and-enabling-yarn">指定Hadoop版本并启用YARN</h2>

<p>您可以通过以下命令指定要编译的Hadoop的确切版本<code>hadoop.version</code>属性。如果未设置，Spark将针对Hadoop 2.6构建。默认为X。</p>

<p>您可以启用<code>yarn</code>配置文件并选择设置<code>yarn.version</code>属性，如果不同于<code>hadoop.version</code> 。</p>

<p>例子：</p>

<pre><code># Apache Hadoop 2.6.X
./build/mvn -Pyarn -DskipTests clean package

# Apache Hadoop 2.7.X and later
./build/mvn -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.3 -DskipTests clean package
</code></pre>

<h2 id="building-with-hive-and-jdbc-support">使用Hive和JDBC支持进行构建</h2>

<p>要为Spark SQL及其JDBC服务器和CLI启用Hive集成，请添加<code>-Phive</code>和<code>Phive-thriftserver</code>配置文件到您现有的构建选项。默认情况下，Spark将使用Hive 1.2.1绑定进行构建。</p>

<pre><code># With Hive 1.2.1 support
./build/mvn -Pyarn -Phive -Phive-thriftserver -DskipTests clean package
</code></pre>

<h2 id="packaging-without-hadoop-dependencies-for-yarn">对于YARN，没有Hadoop依赖项的打包</h2>

<p>组装目录由<code>mvn package</code>默认情况下，将包含Spark的所有依赖项，包括Hadoop及其某些生态系统项目。在YARN部署上，这会导致它们的多个版本出现在执行程序类路径上：Spark程序集中打包的版本以及每个节点上随附的版本<code>yarn.application.classpath</code> 。的<code>hadoop-provided</code>概要文件将在不包括Hadoop生态系统项目（例如ZooKeeper和Hadoop本身）的情况下构建程序集。</p>

<h2 id="building-with-mesos-support">在Mesos支持下进行构建</h2>

<pre><code>./build/mvn -Pmesos -DskipTests clean package
</code></pre>

<h2 id="building-with-kubernetes-support">在Kubernetes支持下进行构建</h2>

<pre><code>./build/mvn -Pkubernetes -DskipTests clean package
</code></pre>

<h2 id="building-with-kafka-08-support">使用Kafka 0.8支持进行构建</h2>

<p>必须使用以下命令明确启用Kafka 0.8支持<code>kafka-0-8</code>轮廓。注意：自Spark 2.3.0起已弃用Kafka 0.8支持。</p>

<pre><code>./build/mvn -Pkafka-0-8 -DskipTests clean package
</code></pre>

<p>Kafka 0.10支持仍会自动建立。</p>

<h2 id="building-with-flume-support">在Flume支持下进行构建</h2>

<p>必须通过显式启用Apache Flume支持<code>flume</code>轮廓。注意：从Spark 2.3.0开始不建议使用Flume。</p>

<pre><code>./build/mvn -Pflume -DskipTests clean package
</code></pre>

<h2 id="building-submodules-individually">分别构建子模块</h2>

<p>可以使用来构建Spark子模块<code>mvn -pl</code>选项。</p>

<p>例如，您可以使用以下命令构建Spark Streaming模块：</p>

<pre><code>./build/mvn -pl :spark-streaming_2.11 clean install
</code></pre>

<p>哪里<code>spark-streaming_2.11</code>是个<code>artifactId</code>如定义<code>streaming/pom.xml</code>文件。</p>

<h2 id="continuous-compilation">连续编译</h2>

<p>我们使用scala-maven-plugin支持增量和连续编译。例如</p>

<pre><code>./build/mvn scala:cc
</code></pre>

<p>应该运行连续编译（即等待更改）。但是，这尚未经过广泛测试。需要注意的几个问题：</p>

<ul>
  <li>
    <p>它只扫描路径<code>src/main</code>和<code>src/test</code> （请参阅<a href="http://davidb.github.io/scala-maven-plugin/example_cc.html">docs</a> ），因此它只能在具有该结构的某些子模块中运行。</p>
  </li>
  <li>
    <p>您通常需要运行<code>mvn install</code>从项目根目录进行编译以在特定子模块中工作；这是因为依赖于其他子模块的子模块通过<code>spark-parent</code>模块）。</p>
  </li>
</ul>

<p>因此，运行连续编译的完整流程<code>core</code>子模块可能更像：</p>

<pre><code>$ ./build/mvn install
$ cd core
$ ../build/mvn scala:cc
</code></pre>

<h2 id="building-with-sbt">使用SBT构建</h2>

<p>Maven是推荐用于打包Spark的官方构建工具，并且是<em>参考</em>的<em>构建</em> 。但是SBT支持日常开发，因为它可以提供更快的迭代编译。更高级的开发人员可能希望使用SBT。</p>

<p>SBT构建源自Maven POM文件，因此可以设置相同的Maven配置文件和变量来控制SBT构建。例如：</p>

<pre><code>./build/sbt package
</code></pre>

<p>为了避免每次需要重新编译时启动sbt的开销，可以通过运行以下命令以交互方式启动sbt <code>build/sbt</code> ，然后在命令提示符处运行所有构建命令。</p>

<h2 id="speeding-up-compilation">加快编译速度</h2>

<p>经常编译Spark的开发人员可能希望加快编译速度。例如，通过使用Zinc（对于使用Maven进行开发的开发人员）或避免重新编译程序集JAR（对于使用SBT进行开发的开发人员）。有关如何执行此操作的更多信息，请参阅“ <a href="https://spark.apache.org/developer-tools.html#reducing-build-times">有用的开发人员工具”页面</a> 。</p>

<h2 id="encrypted-filesystems">加密文件系统</h2>

<p>在加密文件系统上构建时（例如，如果您的主目录已加密），则Spark构建可能会因“文件名太长”错误而失败。解决方法是，将以下内容添加到<code>scala-maven-plugin</code>在项目中<code>pom.xml</code> ：</p>

<pre><code>&lt;arg&gt;-Xmax-classfile-name&lt;/arg&gt;
&lt;arg&gt;128&lt;/arg&gt;
</code></pre>

<p>和在<code>project/SparkBuild.scala</code>加：</p>

<pre><code>scalacOptions in Compile ++= Seq("-Xmax-classfile-name", "128"),
</code></pre>

<p>到<code>sharedSettings</code>瓦尔如果不确定在何处添加这些行，也请参阅<a href="https://github.com/apache/spark/pull/2883/files">此PR</a> 。</p>

<h2 id="intellij-idea-or-eclipse">IntelliJ IDEA或Eclipse</h2>

<p>有关设置IntelliJ IDEA或Eclipse进行Spark开发以及进行故障排除的帮助，请参阅“ <a href="https://spark.apache.org/developer-tools.html">有用的开发人员工具”页面</a> 。</p>

<h1 id="running-tests">运行测试</h1>

<p>默认情况下，通过<a href="http://www.scalatest.org/user_guide/using_the_scalatest_maven_plugin">ScalaTest Maven插件</a>运行测试。请注意，测试不应以root或admin用户身份运行。</p>

<p>以下是运行测试的命令示例：</p>

<pre><code>./build/mvn test
</code></pre>

<h2 id="testing-with-sbt">使用SBT测试</h2>

<p>以下是运行测试的命令示例：</p>

<pre><code>./build/sbt test
</code></pre>

<h2 id="running-individual-tests">运行个别测试</h2>

<p>有关如何运行各个测试的信息，请参阅“ <a href="https://spark.apache.org/developer-tools.html#running-individual-tests">有用的开发人员工具”页面</a> 。</p>

<h2 id="pyspark-pip-installable">PySpark pip可安装</h2>

<p>如果要构建用于Python环境的Spark并希望通过pip安装它，则首先需要如上所述构建Spark JAR。然后，您可以构建一个适用于setup.py和pip可安装软件包的sdist软件包。</p>

<pre><code>cd python; python setup.py sdist
</code></pre>

<p><strong>注意：</strong>由于包装要求，您不能直接从Python目录中进行pip安装，而是必须如上所述首先构建sdist软件包。</p>

<p>另外，您也可以使用–pip选项运行make-distribution。</p>

<h2 id="pyspark-tests-with-maven-or-sbt">使用Maven或SBT进行PySpark测试</h2>

<p>如果您要构建PySpark并希望运行PySpark测试，则需要在Hive支持下构建Spark。</p>

<pre><code>./build/mvn -DskipTests clean package -Phive
./python/run-tests
</code></pre>

<p>如果您要使用SBT构建PySpark，并希望运行PySpark测试，则需要在Hive支持下构建Spark，还需要构建测试组件：</p>

<pre><code>./build/sbt -Phive clean package
./build/sbt test:compile
./python/run-tests
</code></pre>

<p>运行测试脚本也可以限于特定的Python版本或特定的模块</p>

<pre><code>./python/run-tests --python-executables=python --modules=pyspark-sql
</code></pre>

<h2 id="running-r-tests">运行R测试</h2>

<p>要运行SparkR测试，您需要首先安装<a href="https://cran.r-project.org/package=knitr">knitr</a> ， <a href="https://cran.r-project.org/package=rmarkdown">rmarkdown</a> ， <a href="https://cran.r-project.org/package=testthat">testthat</a> ， <a href="https://cran.r-project.org/package=e1071">e1071</a>和<a href="https://cran.r-project.org/package=survival">生存</a>软件包：</p>

<pre><code>Rscript -e "install.packages(c('knitr', 'rmarkdown', 'devtools', 'e1071', 'survival'), repos='https://cloud.r-project.org/')"
Rscript -e "devtools::install_version('testthat', version = '1.0.2', repos='https://cloud.r-project.org/')"
</code></pre>

<p>您可以使用以下命令仅运行SparkR测试：</p>

<pre><code>./R/run-tests.sh
</code></pre>

<h2 id="running-docker-based-integration-test-suites">运行基于Docker的集成测试套件</h2>

<p>为了运行Docker集成测试，您必须安装<code>docker</code>盒子上的引擎。可以在<a href="https://docs.docker.com/engine/installation/">Docker站点上</a>找到安装说明。安装完成后， <code>docker</code>如果尚未运行，则需要启动该服务。在Linux上，这可以通过<code>sudo service docker start</code> 。</p>

<pre><code>./build/mvn install -DskipTests
./build/mvn test -Pdocker-integration-tests -pl :spark-docker-integration-tests_2.11
</code></pre>

<p>要么</p>

<pre><code>./build/sbt docker-integration-tests/test
</code></pre>

<h2 id="change-scala-version">更改Scala版本</h2>

<p>要使用另一个受支持的Scala版本构建Spark，请使用（例如2.12）更改主要的Scala版本：</p>

<pre><code>./dev/change-scala-version.sh 2.12
</code></pre>

<p>对于Maven，请启用配置文件（例如2.12）：</p>

<pre><code>./build/mvn -Pscala-2.12 compile
</code></pre>

<p>对于SBT，使用（例如2.12.6）指定完整的scala版本：</p>

<pre><code>./build/sbt -Dscala.version=2.12.6
</code></pre>

<p>否则，sbt-pom-reader插件将使用<code>scala.version</code>在spark-parent pom中指定。</p>


                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-1.12.4.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    

</body></html>