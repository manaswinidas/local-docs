<html class="no-js" ><!--<![endif]--><head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>PySpark使用Apache Arrow的熊猫使用指南-Spark 2.4.4文档</title>
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        
        <!-- Google analytics script -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-32518208-2']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
        </script>
        

    </head>
    <body >
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html"><img src="img/spark-logo-hd.png" style="height:50px"></a> <span class="version">2.4.4</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">总览</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">编程指南<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">快速开始</a></li>
                                <li><a href="rdd-programming-guide.html">RDD，累加器，广播变量</a></li>
                                <li><a href="sql-programming-guide.html">SQL，数据框和数据集</a></li>
                                <li><a href="structured-streaming-programming-guide.html">结构化流</a></li>
                                <li><a href="streaming-programming-guide.html">火花流（DStreams）</a></li>
                                <li><a href="ml-guide.html">MLlib（机器学习）</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX（图形处理）</a></li>
                                <li><a href="sparkr.html">SparkR（Spark上的R）</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API文件<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">斯卡拉</a></li>
                                <li><a href="api/java/index.html">爪哇</a></li>
                                <li><a href="api/python/index.html">蟒蛇</a></li>
                                <li><a href="api/R/index.html">[R</a></li>
                                <li><a href="api/sql/index.html">SQL，内置函数</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">部署中<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">总览</a></li>
                                <li><a href="submitting-applications.html">提交申请</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark独立</a></li>
                                <li><a href="running-on-mesos.html">梅索斯</a></li>
                                <li><a href="running-on-yarn.html">纱</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">更多<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">组态</a></li>
                                <li><a href="monitoring.html">监控方式</a></li>
                                <li><a href="tuning.html">调音指南</a></li>
                                <li><a href="job-scheduling.html">作业调度</a></li>
                                <li><a href="security.html">安全</a></li>
                                <li><a href="hardware-provisioning.html">硬件配置</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">建筑火花</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">为Spark贡献</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">第三方项目</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v2.4.4</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                
                    <div class="left-menu-wrapper">
    <div class="left-menu">
        <h3><a href="sql-programming-guide.html">Spark SQL指南</a></h3>
        
<ul>

    <li>
        <a href="sql-getting-started.html">入门</a>
    </li>
    
    

    <li>
        <a href="sql-data-sources.html">数据源</a>
    </li>
    
    

    <li>
        <a href="sql-performance-tuning.html">性能调优</a>
    </li>
    
    

    <li>
        <a href="sql-distributed-sql-engine.html">分布式SQL引擎</a>
    </li>
    
    

    <li>
        <a href="sql-pyspark-pandas-with-arrow.html">
            
                <b>PySpark使用Apache Arrow的熊猫使用指南</b>
            
        </a>
    </li>
    
    
        
<ul>

    <li>
        <a href="sql-pyspark-pandas-with-arrow.html#apache-arrow-in-spark">Spark中的Apache Arrow</a>
    </li>
    
    

    <li>
        <a href="sql-pyspark-pandas-with-arrow.html#enabling-for-conversion-tofrom-pandas">启用与熊猫之间的转换</a>
    </li>
    
    

    <li>
        <a href="sql-pyspark-pandas-with-arrow.html#pandas-udfs-aka-vectorized-udfs">熊猫UDF（又名矢量化UDF）</a>
    </li>
    
    

    <li>
        <a href="sql-pyspark-pandas-with-arrow.html#usage-notes">使用说明</a>
    </li>
    
    

</ul>

    

    <li>
        <a href="sql-migration-guide.html">迁移指南</a>
    </li>
    
    

    <li>
        <a href="sql-reference.html">参考</a>
    </li>
    
    

</ul>

    </div>
</div>
                
                <input id="nav-trigger" class="nav-trigger" type="checkbox" checked>
                <label for="nav-trigger"></label>
                <div class="content-with-sidebar" id="content">
                    
                        <h1 class="title">PySpark使用Apache Arrow的熊猫使用指南</h1>
                    

                    <ul id="markdown-toc">
  <li><a href="#apache-arrow-in-spark" id="markdown-toc-apache-arrow-in-spark">Spark中的Apache Arrow</a>    <ul>
      <li><a href="#ensure-pyarrow-installed" id="markdown-toc-ensure-pyarrow-installed">确保已安装PyArrow</a></li>
    </ul>
  </li>
  <li><a href="#enabling-for-conversion-tofrom-pandas" id="markdown-toc-enabling-for-conversion-tofrom-pandas">启用与熊猫之间的转换</a></li>
  <li><a href="#pandas-udfs-aka-vectorized-udfs" id="markdown-toc-pandas-udfs-aka-vectorized-udfs">熊猫UDF（又名矢量化UDF）</a>    <ul>
      <li><a href="#scalar" id="markdown-toc-scalar">标量</a></li>
      <li><a href="#grouped-map" id="markdown-toc-grouped-map">分组地图</a></li>
      <li><a href="#grouped-aggregate" id="markdown-toc-grouped-aggregate">分组汇总</a></li>
    </ul>
  </li>
  <li><a href="#usage-notes" id="markdown-toc-usage-notes">使用说明</a>    <ul>
      <li><a href="#supported-sql-types" id="markdown-toc-supported-sql-types">支持的SQL类型</a></li>
      <li><a href="#setting-arrow-batch-size" id="markdown-toc-setting-arrow-batch-size">设置箭头批处理大小</a></li>
      <li><a href="#timestamp-with-time-zone-semantics" id="markdown-toc-timestamp-with-time-zone-semantics">带时区语义的时间戳</a></li>
    </ul>
  </li>
</ul>

<h2 id="apache-arrow-in-spark">Spark中的Apache Arrow</h2>

<p>Apache Arrow是一种内存中的列式数据格式，在Spark中使用它来有效地在JVM和Python进程之间传输数据。目前，这对于使用Pandas / NumPy数据的Python用户最为有利。它的使用不是自动的，可能需要对配置或代码进行一些小的更改才能充分利用并确保兼容性。本指南将对如何在Spark中使用Arrow进行高层描述，并突出显示使用启用了Arrow的数据时的所有差异。</p>

<h3 id="ensure-pyarrow-installed">确保已安装PyArrow</h3>

<p>如果您使用pip安装PySpark，则可以使用以下命令将PyArrow作为SQL模块的额外依赖项引入<code>pip install pyspark[sql]</code> 。否则，您必须确保PyArrow已安装并在所有群集节点上可用。当前支持的版本是0.8.0。您可以从conda-forge频道使用pip或conda进行安装。有关详细信息，请参见PyArrow <a href="https://arrow.apache.org/docs/python/install.html">安装</a> 。</p>

<h2 id="enabling-for-conversion-tofrom-pandas">启用与熊猫之间的转换</h2>

<p>使用调用将Spark DataFrame转换为Pandas DataFrame时，可以使用Arrow作为优化。 <code>toPandas()</code>并且在使用Pandas DataFrame创建Spark DataFrame时<code>createDataFrame(pandas_df)</code> 。要在执行这些调用时使用Arrow，用户需要首先设置Spark配置<code>spark.sql.execution.arrow.enabled</code>至<code>true</code> 。默认情况下禁用此功能。</p>

<p>此外，通过<code>spark.sql.execution.arrow.enabled</code>如果在Spark内部实际计算之前发生错误，则可能会自动回退到非箭头优化实现。这可以通过控制<code>spark.sql.execution.arrow.fallback.enabled</code> 。</p>

<div class="codetabs">
<div data-lang="python">
    <div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="c1"># Enable Arrow-based columnar data transfers</span>
<span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.execution.arrow.enabled&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span>

<span class="c1"># Generate a Pandas DataFrame</span>
<span class="n">pdf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Create a Spark DataFrame from a Pandas DataFrame using Arrow</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">pdf</span><span class="p">)</span>

<span class="c1"># Convert the Spark DataFrame back to a Pandas DataFrame using Arrow</span>
<span class="n">result_pdf</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;*&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>
</pre></div>
    <div><small>在Spark存储库中的“ examples / src / main / python / sql / arrow.py”中找到完整的示例代码。</small></div>
  </div>
</div>

<p>将上述优化与Arrow一起使用将产生与未启用Arrow时相同的结果。请注意，即使使用Arrow， <code>toPandas()</code>导致将DataFrame中的所有记录收集到驱动程序中，并且应该对数据的一小部分进行处理。当前尚不支持所有Spark数据类型，如果列的类型不受支持，则会引发错误，请参阅<a href="#supported-sql-types">支持的SQL类型</a> 。如果在期间发生错误<code>createDataFrame()</code> ，Spark将回退以创建不带箭头的DataFrame。</p>

<h2 id="pandas-udfs-aka-vectorized-udfs">熊猫UDF（又名矢量化UDF）</h2>

<p>熊猫UDF是用户定义的函数，由Spark使用Arrow来传输数据，并通过Pandas处理数据来执行。使用关键字定义了Pandas UDF <code>pandas_udf</code>作为装饰器或包装功能，不需要其他配置。当前，有两种类型的熊猫UDF：标量和分组地图。</p>

<h3 id="scalar">标量</h3>

<p>标量熊猫UDF用于向量化标量操作。它们可以与以下功能一起使用<code>select</code>和<code>withColumn</code> 。Python函数应采用<code>pandas.Series</code>作为输入并返回一个<code>pandas.Series</code>相同的长度。在内部，Spark将通过将列拆分为批处理并为每个批处理调用函数作为数据的子集来执行Pandas UDF，然后将结果串联在一起。</p>

<p>以下示例显示了如何创建一个计算两列乘积的标量Pandas UDF。</p>

<div class="codetabs">
<div data-lang="python">
    <div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span><span class="p">,</span> <span class="n">pandas_udf</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">LongType</span>

<span class="c1"># Declare the function and create the UDF</span>
<span class="k">def</span> <span class="nf">multiply_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>

<span class="n">multiply</span> <span class="o">=</span> <span class="n">pandas_udf</span><span class="p">(</span><span class="n">multiply_func</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">LongType</span><span class="p">())</span>

<span class="c1"># The function for a pandas_udf should be able to execute with local Pandas data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">multiply_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
<span class="c1"># 0    1</span>
<span class="c1"># 1    4</span>
<span class="c1"># 2    9</span>
<span class="c1"># dtype: int64</span>

<span class="c1"># Create a Spark DataFrame, &#39;spark&#39; is an existing SparkSession</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">]))</span>

<span class="c1"># Execute function as a Spark vectorized UDF</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">multiply</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">),</span> <span class="n">col</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +-------------------+</span>
<span class="c1"># |multiply_func(x, x)|</span>
<span class="c1"># +-------------------+</span>
<span class="c1"># |                  1|</span>
<span class="c1"># |                  4|</span>
<span class="c1"># |                  9|</span>
<span class="c1"># +-------------------+</span>
</pre></div>
    <div><small>在Spark存储库中的“ examples / src / main / python / sql / arrow.py”中找到完整的示例代码。</small></div>
  </div>
</div>

<h3 id="grouped-map">分组地图</h3>
<p>分组地图熊猫UDF用于<code>groupBy().apply()</code>它实现了“拆分应用合并”模式。拆分应用合并包括三个步骤：</p>
<ul>
  <li>通过使用将数据分成组<code>DataFrame.groupBy</code> 。</li>
  <li>在每个组上应用功能。函数的输入和输出都是<code>pandas.DataFrame</code> 。输入数据包含每个组的所有行和列。</li>
  <li>将结果合并成新的<code>DataFrame</code> 。</li>
</ul>

<p>使用<code>groupBy().apply()</code> ，用户需要定义以下内容：</p>
<ul>
  <li>一个Python函数，用于定义每个组的计算。</li>
  <li>一种<code>StructType</code>对象或定义输出模式的字符串<code>DataFrame</code> 。</li>
</ul>

<p>返回的列标签<code>pandas.DataFrame</code>如果指定为字符串，则必须与定义的输出模式中的字段名称匹配，如果不是字符串，则必须按位置匹配字段数据类型，例如整数索引。见<a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame">熊猫。DataFrame</a>关于在构造一个列时如何标记列<code>pandas.DataFrame</code> 。</p>

<p>请注意，在应用该功能之前，组的所有数据将被加载到内存中。这可能导致内存不足异常，尤其是在组大小偏斜的情况下。<a href="#setting-arrow-batch-size">maxRecordsPerBatch</a>的配置不适用于组，并且由用户决定是否将分组的数据放入可用内存中。</p>

<p>以下示例显示了如何使用<code>groupby().apply()</code>从组中的每个值中减去平均值。</p>

<div class="codetabs">
<div data-lang="python">
    <div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
    <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)],</span>
    <span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">))</span>

<span class="nd">@pandas_udf</span><span class="p">(</span><span class="s2">&quot;id long, v double&quot;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">GROUPED_MAP</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">subtract_mean</span><span class="p">(</span><span class="n">pdf</span><span class="p">):</span>
    <span class="c1"># pdf is a pandas.DataFrame</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">v</span>
    <span class="k">return</span> <span class="n">pdf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">v</span> <span class="o">-</span> <span class="n">v</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">subtract_mean</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +---+----+</span>
<span class="c1"># | id|   v|</span>
<span class="c1"># +---+----+</span>
<span class="c1"># |  1|-0.5|</span>
<span class="c1"># |  1| 0.5|</span>
<span class="c1"># |  2|-3.0|</span>
<span class="c1"># |  2|-1.0|</span>
<span class="c1"># |  2| 4.0|</span>
<span class="c1"># +---+----+</span>
</pre></div>
    <div><small>在Spark存储库中的“ examples / src / main / python / sql / arrow.py”中找到完整的示例代码。</small></div>
  </div>
</div>

<p>有关详细用法，请参阅<a href="api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf"><code>pyspark.sql.functions.pandas_udf</code></a>和<a href="api/python/pyspark.sql.html#pyspark.sql.GroupedData.apply"><code>pyspark.sql.GroupedData.apply</code></a> 。</p>

<h3 id="grouped-aggregate">分组汇总</h3>

<p>分组聚合熊猫UDF与Spark聚合函数相似。分组聚合熊猫UDF与<code>groupBy().agg()</code>和<a href="api/python/pyspark.sql.html#pyspark.sql.Window"><code>pyspark.sql.Window</code></a> 。它定义了一个或多个的聚合<code>pandas.Series</code>到标量值，其中每个<code>pandas.Series</code>表示组或窗口中的一列。</p>

<p>请注意，这种类型的UDF不支持部分聚合，并且组或窗口的所有数据都将加载到内存中。另外，分组聚合熊猫UDF当前仅支持无边界窗口。</p>

<p>以下示例说明如何使用此类UDF通过groupBy和window操作计算均值：</p>

<div class="codetabs">
<div data-lang="python">
    <div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Window</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
    <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)],</span>
    <span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">))</span>

<span class="nd">@pandas_udf</span><span class="p">(</span><span class="s2">&quot;double&quot;</span><span class="p">,</span> <span class="n">PandasUDFType</span><span class="o">.</span><span class="n">GROUPED_AGG</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">mean_udf</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">mean_udf</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;v&#39;</span><span class="p">]))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +---+-----------+</span>
<span class="c1"># | id|mean_udf(v)|</span>
<span class="c1"># +---+-----------+</span>
<span class="c1"># |  1|        1.5|</span>
<span class="c1"># |  2|        6.0|</span>
<span class="c1"># +---+-----------+</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">Window</span> \
    <span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s1">&#39;id&#39;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">rowsBetween</span><span class="p">(</span><span class="n">Window</span><span class="o">.</span><span class="n">unboundedPreceding</span><span class="p">,</span> <span class="n">Window</span><span class="o">.</span><span class="n">unboundedFollowing</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;mean_v&#39;</span><span class="p">,</span> <span class="n">mean_udf</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;v&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">w</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># +---+----+------+</span>
<span class="c1"># | id|   v|mean_v|</span>
<span class="c1"># +---+----+------+</span>
<span class="c1"># |  1| 1.0|   1.5|</span>
<span class="c1"># |  1| 2.0|   1.5|</span>
<span class="c1"># |  2| 3.0|   6.0|</span>
<span class="c1"># |  2| 5.0|   6.0|</span>
<span class="c1"># |  2|10.0|   6.0|</span>
<span class="c1"># +---+----+------+</span>
</pre></div>
    <div><small>在Spark存储库中的“ examples / src / main / python / sql / arrow.py”中找到完整的示例代码。</small></div>
  </div>
</div>

<p>有关详细用法，请参阅<a href="api/python/pyspark.sql.html#pyspark.sql.functions.pandas_udf"><code>pyspark.sql.functions.pandas_udf</code></a></p>

<h2 id="usage-notes">使用说明</h2>

<h3 id="supported-sql-types">支持的SQL类型</h3>

<p>当前，基于箭头的转换支持所有Spark SQL数据类型，除了<code>MapType</code> ， <code>ArrayType</code>的<code>TimestampType</code>和嵌套<code>StructType</code> 。 <code>BinaryType</code>仅当安装的PyArrow等于或大于0.10.0时才支持。</p>

<h3 id="setting-arrow-batch-size">设置箭头批处理大小</h3>

<p>Spark中的数据分区将转换为Arrow记录批，这可能会暂时导致JVM中的内存使用率很高。为了避免可能的内存不足异常，可以通过将conf“ spark.sql.execution.arrow.maxRecordsPerBatch”设置为整数来调整Arrow记录批的大小，该整数将确定每个批处理的最大行数。默认值为每批10,000条记录。如果列数很大，则应相应地调整该值。使用此限制，每个数据分区将被分为1个或多个记录批次以进行处理。</p>

<h3 id="timestamp-with-time-zone-semantics">带时区语义的时间戳</h3>

<p>Spark在内部将时间戳存储为UTC值，并且在没有指定时区的情况下引入的时间戳数据将作为本地时间转换为具有微秒分辨率的UTC。当时间戳数据导出或在Spark中显示时，会话时区用于本地化时间戳值。会话时区使用配置“ spark.sql.session.timeZone”设置，如果未设置，则默认为JVM系统本地时区。熊猫使用<code>datetime64</code>具有纳秒分辨率的类型<code>datetime64[ns]</code> ，每个列均带有可选时区。</p>

<p>当时间戳数据从Spark传输到Pandas时，它将转换为纳秒，每列将转换为Spark会话时区，然后本地化到该时区，该时区将删除时区并将值显示为本地时间。呼叫时会发生这种情况<code>toPandas()</code>要么<code>pandas_udf</code>与时间戳列。</p>

<p>当时间戳数据从Pandas传输到Spark时，它将转换为UTC微秒。呼叫时会发生这种情况<code>createDataFrame</code>使用Pandas DataFrame或从<code>pandas_udf</code> 。这些转换是自动完成的，以确保Spark拥有预期格式的数据，因此无需您自己进行任何转换。任何毫微秒的值都会被截断。</p>

<p>请注意，标准UDF（非Pandas）会将时间戳数据加载为Python日期时间对象，这与Pandas时间戳不同。在使用以下格式的时间戳时，建议使用熊猫时间序列功能<code>pandas_udf</code>以获得最佳性能，请参阅<a href="https://pandas.pydata.org/pandas-docs/stable/timeseries.html">此处</a>以了解详细信息。</p>


                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-1.12.4.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    

</body></html>