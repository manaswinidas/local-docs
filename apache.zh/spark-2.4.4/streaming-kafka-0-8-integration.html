<html class="no-js" ><!--<![endif]--><head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Spark Streaming + Kafka集成指南（Kafka代理版本0.8.2.1或更高版本）-Spark 2.4.4文档</title>
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        
        <!-- Google analytics script -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-32518208-2']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
        </script>
        

    </head>
    <body >
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html"><img src="img/spark-logo-hd.png" style="height:50px"></a> <span class="version">2.4.4</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">总览</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">编程指南<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">快速开始</a></li>
                                <li><a href="rdd-programming-guide.html">RDD，累加器，广播变量</a></li>
                                <li><a href="sql-programming-guide.html">SQL，数据框和数据集</a></li>
                                <li><a href="structured-streaming-programming-guide.html">结构化流</a></li>
                                <li><a href="streaming-programming-guide.html">火花流（DStreams）</a></li>
                                <li><a href="ml-guide.html">MLlib（机器学习）</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX（图形处理）</a></li>
                                <li><a href="sparkr.html">SparkR（Spark上的R）</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API文件<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">斯卡拉</a></li>
                                <li><a href="api/java/index.html">爪哇</a></li>
                                <li><a href="api/python/index.html">蟒蛇</a></li>
                                <li><a href="api/R/index.html">[R</a></li>
                                <li><a href="api/sql/index.html">SQL，内置函数</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">部署中<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">总览</a></li>
                                <li><a href="submitting-applications.html">提交申请</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark独立</a></li>
                                <li><a href="running-on-mesos.html">梅索斯</a></li>
                                <li><a href="running-on-yarn.html">纱</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">更多<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">组态</a></li>
                                <li><a href="monitoring.html">监控方式</a></li>
                                <li><a href="tuning.html">调音指南</a></li>
                                <li><a href="job-scheduling.html">作业调度</a></li>
                                <li><a href="security.html">安全</a></li>
                                <li><a href="hardware-provisioning.html">硬件配置</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">建筑火花</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">为Spark贡献</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">第三方项目</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v2.4.4</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                <div class="content" id="content">
                    
                        <h1 class="title">Spark Streaming + Kafka集成指南（Kafka代理版本0.8.2.1或更高版本）</h1>
                    

                    <p><strong>注意：自Spark 2.3.0起已弃用Kafka 0.8支持。</strong></p>

<p>在这里，我们解释了如何配置Spark Streaming以从Kafka接收数据。有两种解决方法-使用Receivers和Kafka的高级API的旧方法，以及不使用Receivers的新方法（Spark 1.3中引入）。它们具有不同的编程模型，性能特征和语义保证，因此请继续阅读以获取更多详细信息。从当前版本的Spark开始，这两种方法都被认为是稳定的API。</p>

<h2 id="approach-1-receiver-based-approach">方法1：基于接收者的方法</h2>
<p>这种方法使用接收器来接收数据。接收器是使用Kafka高级消费者API实现的。与所有接收器一样，通过接收器从Kafka接收的数据存储在Spark执行程序中，然后由Spark Streaming启动的作业将处理数据。</p>

<p>但是，在默认配置下，这种方法可能会在发生故障时丢失数据（请参阅<a href="streaming-programming-guide.html#receiver-reliability">接收器可靠性）</a> 。为了确保零数据丢失，您还必须在Spark Streaming（Spark 1.2中引入）中另外启用预写日志。这会将所有接收到的Kafka数据同步保存到分布式文件系统（例如HDFS）上的预写日志中，以便可以在发生故障时恢复所有数据。有关预写日志的更多详细信息，请参见流编程指南中的“ <a href="streaming-programming-guide.html#deploying-applications">部署”部分</a> 。</p>

<p>接下来，我们讨论如何在流应用程序中使用这种方法。</p>

<ol>
  <li>
    <p><strong>链接：</strong>对于使用SBT / Maven项目定义的Scala / Java应用程序，将您的流式应用程序与以下工件<a href="streaming-programming-guide.html#linking">链接</a> （有关更多信息，请参见主编程指南中的“ <a href="streaming-programming-guide.html#linking">链接”部分</a> ）。</p>

    <pre><code> groupId = org.apache.spark
 artifactId = spark-streaming-kafka-0-8_2.12
 version = 2.4.4
</code></pre>

    <p>对于Python应用程序，在部署应用程序时必须添加上述库及其依赖项。请参阅下面的“ <em>部署”</em>小节。</p>
  </li>
  <li>
    <p><strong>编程：</strong>在流应用程序代码中，导入<code>KafkaUtils</code>并创建一个输入DStream，如下所示。</p>

    <div class="codetabs">
 <div data-lang="scala">
        <pre><code> import org.apache.spark.streaming.kafka._

 val kafkaStream = KafkaUtils.createStream(streamingContext,
     [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume])
</code></pre>

        <p>您还可以使用的变体指定键和值类以及它们对应的解码器类<code>createStream</code> 。请参阅<a href="api/scala/index.html#org.apache.spark.streaming.kafka.KafkaUtils$">API文档</a> 。</p>
      </div>
 <div data-lang="java">
        <pre><code> import org.apache.spark.streaming.kafka.*;

 JavaPairReceiverInputDStream&lt;String, String&gt; kafkaStream =
     KafkaUtils.createStream(streamingContext,
     [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume]);
</code></pre>

        <p>您还可以使用的变体指定键和值类以及它们对应的解码器类<code>createStream</code> 。请参阅<a href="api/java/index.html?org/apache/spark/streaming/kafka/KafkaUtils.html">API文档</a> 。</p>

      </div>
 <div data-lang="python">
        <pre><code> from pyspark.streaming.kafka import KafkaUtils

 kafkaStream = KafkaUtils.createStream(streamingContext, \
     [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume])
</code></pre>

        <p>默认情况下，Python API会将Kafka数据解码为UTF8编码的字符串。您可以指定自定义解码功能，以将Kafka记录中的字节数组解码为任意数据类型。请参阅<a href="api/python/pyspark.streaming.html#pyspark.streaming.kafka.KafkaUtils">API文档</a> 。</p>
      </div>
 </div>

    <p><strong>要记住的要点：</strong></p>

    <ul>
      <li>
        <p>Kafka中的主题分区与Spark Streaming中生成的RDD的分区不相关。因此，增加了主题特定分区的数量<code>KafkaUtils.createStream()</code>仅增加在单个接收器中使用哪些主题消耗的线程数量。它不会在处理数据时增加Spark的并行性。有关更多信息，请参考主文档。</p>
      </li>
      <li>
        <p>可以使用不同的组和主题创建多个Kafka输入DStream，以使用多个接收器并行接收数据。</p>
      </li>
      <li>
        <p>如果已使用复制文件系统（例如HDFS）启用了预写日志，则已在日志中复制了接收到的数据。因此，存储级别为输入流的存储级别<code>StorageLevel.MEMORY_AND_DISK_SER</code> （即使用<code>KafkaUtils.createStream(..., StorageLevel.MEMORY_AND_DISK_SER)</code> ）。</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>部署：</strong>与任何Spark应用程序一样， <code>spark-submit</code>用于启动您的应用程序。但是，Scala / Java应用程序和Python应用程序的详细信息略有不同。</p>

    <p>对于Scala和Java应用程序，如果您使用SBT或Maven进行项目管理，则打包<code>spark-streaming-kafka-0-8_2.12</code>及其依赖关系到应用程序JAR中。确保<code>spark-core_2.12</code>和<code>spark-streaming_2.12</code>被标记为<code>provided</code>依赖关系，如Spark安装中已经存在的依赖关系。然后使用<code>spark-submit</code>以启动您的应用程序（请参阅主要编程指南中的“ <a href="streaming-programming-guide.html#deploying-applications">部署”部分</a> ）。</p>

    <p>对于缺少SBT / Maven项目管理的Python应用程序， <code>spark-streaming-kafka-0-8_2.12</code>它的依赖关系可以直接添加到<code>spark-submit</code>使用<code>--packages</code> （请参阅<a href="submitting-applications.html">申请提交指南</a> ）。那是，</p>

    <pre><code> ./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.12:2.4.4 ...
</code></pre>

    <p>或者，您也可以下载Maven工件的JAR <code>spark-streaming-kafka-0-8-assembly</code>从<a href="https://search.maven.org/#search|ga|1|a%3A"spark-streaming-kafka-0-8-assembly_2.12" AND v%3A"2.4.4"">Maven存储库</a>并将其添加到<code>spark-submit</code>与<code>--jars</code> 。</p>
  </li>
</ol>

<h2 id="approach-2-direct-approach-no-receivers">方法2：直接方法（无接收者）</h2>
<p>Spark 1.3中引入了这种新的无接收器“直接”方法，以确保更强的端到端保证。该方法不是使用接收器来接收数据，而是定期向Kafka查询每个主题+分区中的最新偏移量，并相应地定义要在每个批次中处理的偏移量范围。启动用于处理数据的作业时，Kafka的简单使用者API用于读取Kafka定义的偏移范围（类似于从文件系统读取文件）。请注意，Scala和Java API在Spark 1.3中引入了此功能，而Python API在Spark 1.4中引入了此功能。</p>

<p>与基于接收器的方法（即方法1）相比，此方法具有以下优点。</p>

<ul>
  <li>
    <p><em>简化的并行性：</em>无需创建多个输入Kafka流并将它们合并。用<code>directStream</code> ，Spark Streaming将创建与要使用的Kafka分区一样多的RDD分区，所有这些分区都将从Kafka并行读取数据。因此，Kafka和RDD分区之间存在一对一的映射，这更易于理解和调整。</p>
  </li>
  <li>
    <p><em>效率：</em>为了实现零数据丢失，第一种方法要求将数据存储在预写日志中，该日志进一步复制了数据。这实际上是低效的，因为数据被有效地复制了两次-一次是通过Kafka复制，另一次是通过“预写日志”复制。第二种方法消除了该问题，因为没有接收器，因此不需要预写日志。只要您有足够的Kafka保留时间，就可以从Kafka中恢复邮件。</p>
  </li>
  <li>
    <p><em>一次精确的语义：</em>第一种方法使用Kafka的高级API将消耗的偏移量存储在Zookeeper中。传统上，这是从Kafka消费数据的方式。尽管这种方法（与预写日志结合使用）可以确保零数据丢失（即至少一次语义），但在某些故障下，某些记录可能会被消耗两次，这是很小的机会。发生这种情况是由于Spark Streaming可靠接收的数据与Zookeeper跟踪的偏移量之间存在不一致。因此，在第二种方法中，我们使用不使用Zookeeper的简单Kafka API。Spark Streaming在其检查点内跟踪偏移。这消除了Spark Streaming和Zookeeper / Kafka之间的不一致，因此即使失败，Spark Streaming也会有效地一次接收每条记录。为了获得结果输出的精确语义，将数据保存到外部数据存储的输出操作必须是幂等的，或者是保存结果和偏移量的原子事务（请参见主程序中<a href="streaming-programming-guide.html#semantics-of-output-operations">的输出操作语义）</a>有关更多信息的指南）。</p>
  </li>
</ul>

<p>请注意，这种方法的一个缺点是它不会更新Zookeeper中的偏移量，因此基于Zookeeper的Kafka监视工具将不会显示进度。但是，您可以在每个批次中访问通过此方法处理的偏移量，并自己更新Zookeeper（请参见下文）。</p>

<p>接下来，我们讨论如何在流应用程序中使用这种方法。</p>

<ol>
  <li>
    <p><strong>链接：</strong>仅Scala / Java应用程序支持此方法。用以下工件链接您的SBT / Maven项目（有关更多信息，请参见主编程指南中的<a href="streaming-programming-guide.html#linking">链接部分</a> ）。</p>

    <pre><code> groupId = org.apache.spark
 artifactId = spark-streaming-kafka-0-8_2.12
 version = 2.4.4
</code></pre>
  </li>
  <li>
    <p><strong>编程：</strong>在流应用程序代码中，导入<code>KafkaUtils</code>并创建一个输入DStream，如下所示。</p>

    <div class="codetabs">
 <div data-lang="scala">
        <pre><code> import org.apache.spark.streaming.kafka._

 val directKafkaStream = KafkaUtils.createDirectStream[
     [key class], [value class], [key decoder class], [value decoder class] ](
     streamingContext, [map of Kafka parameters], [set of topics to consume])
</code></pre>

        <p>您也可以通过<code>messageHandler</code>至<code>createDirectStream</code>访问<code>MessageAndMetadata</code>包含有关当前消息的元数据，并将其转换为任何所需的类型。请参阅<a href="api/scala/index.html#org.apache.spark.streaming.kafka.KafkaUtils$">API文档</a> 。</p>
      </div>
 <div data-lang="java">
        <pre><code> import org.apache.spark.streaming.kafka.*;

 JavaPairInputDStream&lt;String, String&gt; directKafkaStream =
     KafkaUtils.createDirectStream(streamingContext,
         [key class], [value class], [key decoder class], [value decoder class],
         [map of Kafka parameters], [set of topics to consume]);
</code></pre>

        <p>您也可以通过<code>messageHandler</code>至<code>createDirectStream</code>访问<code>MessageAndMetadata</code>包含有关当前消息的元数据，并将其转换为任何所需的类型。请参阅<a href="api/java/index.html?org/apache/spark/streaming/kafka/KafkaUtils.html">API文档</a> 。</p>

      </div>
 <div data-lang="python">
        <pre><code> from pyspark.streaming.kafka import KafkaUtils
 directKafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {"metadata.broker.list": brokers})
</code></pre>

        <p>您也可以通过<code>messageHandler</code>至<code>createDirectStream</code>访问<code>KafkaMessageAndMetadata</code>包含有关当前消息的元数据，并将其转换为任何所需的类型。默认情况下，Python API会将Kafka数据解码为UTF8编码的字符串。您可以指定自定义解码功能，以将Kafka记录中的字节数组解码为任意数据类型。请参阅<a href="api/python/pyspark.streaming.html#pyspark.streaming.kafka.KafkaUtils">API文档</a> 。</p>
      </div>
 </div>

    <p>在Kafka参数中，您必须指定<code>metadata.broker.list</code>要么<code>bootstrap.servers</code> 。默认情况下，它将从每个Kafka分区的最新偏移量开始消耗。如果设置配置<code>auto.offset.reset</code>在Kafka参数中<code>smallest</code> ，那么它将从最小的偏移量开始消耗。</p>

    <p>您还可以使用<code>KafkaUtils.createDirectStream</code> 。此外，如果要访问每个批次中消耗的Kafka偏移量，可以执行以下操作。</p>

    <div class="codetabs">
 <div data-lang="scala">
        <pre><code> // Hold a reference to the current offset ranges, so it can be used downstream
 var offsetRanges = Array.empty[OffsetRange]

 directKafkaStream.transform { rdd =&gt;
   offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
   rdd
 }.map {
           ...
 }.foreachRDD { rdd =&gt;
   for (o &lt;- offsetRanges) {
     println(s"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}")
   }
   ...
 }
</code></pre>
      </div>
 <div data-lang="java">
        <pre><code> // Hold a reference to the current offset ranges, so it can be used downstream
 AtomicReference&lt;OffsetRange[]&gt; offsetRanges = new AtomicReference&lt;&gt;();

 directKafkaStream.transformToPair(rdd -&gt; {    OffsetRange[] offsets = ((HasOffsetRanges) rdd.rdd()).offsetRanges();    offsetRanges.set(offsets);    return rdd;
 }).map(
   ...
 ).foreachRDD(rdd -&gt; {    for (OffsetRange o : offsetRanges.get()) {
 System.out.println(
   o.topic() + " " + o.partition() + " " + o.fromOffset() + " " + o.untilOffset()
 );    }    ...
 });
</code></pre>
      </div>
 <div data-lang="python">
        <pre><code> offsetRanges = []

 def storeOffsetRanges(rdd):
     global offsetRanges
     offsetRanges = rdd.offsetRanges()
     return rdd

 def printOffsetRanges(rdd):
     for o in offsetRanges:
         print "%s %s %s %s" % (o.topic, o.partition, o.fromOffset, o.untilOffset)

 directKafkaStream \
     .transform(storeOffsetRanges) \
     .foreachRDD(printOffsetRanges)
</code></pre>
      </div>
 </div>

    <p>如果您希望基于Zookeeper的Kafka监视工具显示流应用程序的进度，则可以使用此方法自己更新Zookeeper。</p>

    <p>请注意，只有在directKafkaStream上调用的第一个方法中完成了对HasOffsetRanges的类型转换，此转换才会成功，而不是在方法链中。您可以在第一个方法调用中使用transform（）而不是foreachRDD（）来访问偏移量，然后再调用其他Spark方法。但是，请注意，在任何随机播放或重新分区的方法（例如reduceByKey（）或window（））之后，RDD分区和Kafka分区之间的一对一映射不会保留。</p>

    <p>要注意的另一件事是，由于此方法不使用接收器，因此与接收器相关的标准（即表单的<a href="configuration.html">配置</a> <code>spark.streaming.receiver.*</code> ）将不适用于通过此方法创建的输入DStreams（尽管将适用于其他输入DStreams）。而是使用<a href="configuration.html">配置</a> <code>spark.streaming.kafka.*</code> 。一个重要的是<code>spark.streaming.kafka.maxRatePerPartition</code>这是此直接API读取每个Kafka分区的最大速率（以每秒消息数为单位）。</p>
  </li>
  <li>
    <p><strong>部署：</strong>与第一种方法相同。</p>
  </li>
</ol>


                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-1.12.4.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    

</body></html>