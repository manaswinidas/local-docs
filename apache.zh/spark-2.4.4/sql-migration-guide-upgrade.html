<html class="no-js" ><!--<![endif]--><head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Spark SQL升级指南-Spark 2.4.4文档</title>
        

        

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="css/main.css">

        <script src="js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">

        
        <!-- Google analytics script -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-32518208-2']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();
        </script>
        

    </head>
    <body >
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="https://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->

        <div class="navbar navbar-fixed-top" id="topbar">
            <div class="navbar-inner">
                <div class="container">
                    <div class="brand"><a href="index.html"><img src="img/spark-logo-hd.png" style="height:50px"></a> <span class="version">2.4.4</span>
                    </div>
                    <ul class="nav">
                        <!--TODO(andyk): Add class="active" attribute to li some how.-->
                        <li><a href="index.html">总览</a></li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">编程指南<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="quick-start.html">快速开始</a></li>
                                <li><a href="rdd-programming-guide.html">RDD，累加器，广播变量</a></li>
                                <li><a href="sql-programming-guide.html">SQL，数据框和数据集</a></li>
                                <li><a href="structured-streaming-programming-guide.html">结构化流</a></li>
                                <li><a href="streaming-programming-guide.html">火花流（DStreams）</a></li>
                                <li><a href="ml-guide.html">MLlib（机器学习）</a></li>
                                <li><a href="graphx-programming-guide.html">GraphX（图形处理）</a></li>
                                <li><a href="sparkr.html">SparkR（Spark上的R）</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API文件<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="api/scala/index.html#org.apache.spark.package">斯卡拉</a></li>
                                <li><a href="api/java/index.html">爪哇</a></li>
                                <li><a href="api/python/index.html">蟒蛇</a></li>
                                <li><a href="api/R/index.html">[R</a></li>
                                <li><a href="api/sql/index.html">SQL，内置函数</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">部署中<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="cluster-overview.html">总览</a></li>
                                <li><a href="submitting-applications.html">提交申请</a></li>
                                <li class="divider"></li>
                                <li><a href="spark-standalone.html">Spark独立</a></li>
                                <li><a href="running-on-mesos.html">梅索斯</a></li>
                                <li><a href="running-on-yarn.html">纱</a></li>
                                <li><a href="running-on-kubernetes.html">Kubernetes</a></li>
                            </ul>
                        </li>

                        <li class="dropdown">
                            <a href="api.html" class="dropdown-toggle" data-toggle="dropdown">更多<b class="caret"></b></a>
                            <ul class="dropdown-menu">
                                <li><a href="configuration.html">组态</a></li>
                                <li><a href="monitoring.html">监控方式</a></li>
                                <li><a href="tuning.html">调音指南</a></li>
                                <li><a href="job-scheduling.html">作业调度</a></li>
                                <li><a href="security.html">安全</a></li>
                                <li><a href="hardware-provisioning.html">硬件配置</a></li>
                                <li class="divider"></li>
                                <li><a href="building-spark.html">建筑火花</a></li>
                                <li><a href="https://spark.apache.org/contributing.html">为Spark贡献</a></li>
                                <li><a href="https://spark.apache.org/third-party-projects.html">第三方项目</a></li>
                            </ul>
                        </li>
                    </ul>
                    <!--<p class="navbar-text pull-right"><span class="version-text">v2.4.4</span></p>-->
                </div>
            </div>
        </div>

        <div class="container-wrapper">

            
                
                    <div class="left-menu-wrapper">
    <div class="left-menu">
        <h3><a href="sql-programming-guide.html">Spark SQL指南</a></h3>
        
<ul>

    <li>
        <a href="sql-getting-started.html">入门</a>
    </li>
    
    

    <li>
        <a href="sql-data-sources.html">数据源</a>
    </li>
    
    

    <li>
        <a href="sql-performance-tuning.html">性能调优</a>
    </li>
    
    

    <li>
        <a href="sql-distributed-sql-engine.html">分布式SQL引擎</a>
    </li>
    
    

    <li>
        <a href="sql-pyspark-pandas-with-arrow.html">PySpark使用Apache Arrow的熊猫使用指南</a>
    </li>
    
    

    <li>
        <a href="sql-migration-guide.html">迁移指南</a>
    </li>
    
    
        
<ul>

    <li>
        <a href="sql-migration-guide-upgrade.html">
            
                <b>Spark SQL升级指南</b>
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-migration-guide-hive-compatibility.html">与Apache Hive的兼容性</a>
    </li>
    
    

</ul>

    

    <li>
        <a href="sql-reference.html">参考</a>
    </li>
    
    

</ul>

    </div>
</div>
                
                <input id="nav-trigger" class="nav-trigger" type="checkbox" checked>
                <label for="nav-trigger"></label>
                <div class="content-with-sidebar" id="content">
                    
                        <h1 class="title">Spark SQL升级指南</h1>
                    

                    <ul id="markdown-toc">
  <li><a href="#upgrading-from-spark-sql-24-to-241" id="markdown-toc-upgrading-from-spark-sql-24-to-241">从Spark SQL 2.4升级到2.4.1</a></li>
  <li><a href="#upgrading-from-spark-sql-23-to-24" id="markdown-toc-upgrading-from-spark-sql-23-to-24">从Spark SQL 2.3升级到2.4</a></li>
  <li><a href="#upgrading-from-spark-sql-230-to-231-and-above" id="markdown-toc-upgrading-from-spark-sql-230-to-231-and-above">从Spark SQL 2.3.0升级到2.3.1及更高版本</a></li>
  <li><a href="#upgrading-from-spark-sql-22-to-23" id="markdown-toc-upgrading-from-spark-sql-22-to-23">从Spark SQL 2.2升级到2.3</a></li>
  <li><a href="#upgrading-from-spark-sql-21-to-22" id="markdown-toc-upgrading-from-spark-sql-21-to-22">从Spark SQL 2.1升级到2.2</a></li>
  <li><a href="#upgrading-from-spark-sql-20-to-21" id="markdown-toc-upgrading-from-spark-sql-20-to-21">从Spark SQL 2.0升级到2.1</a></li>
  <li><a href="#upgrading-from-spark-sql-16-to-20" id="markdown-toc-upgrading-from-spark-sql-16-to-20">从Spark SQL 1.6升级到2.0</a></li>
  <li><a href="#upgrading-from-spark-sql-15-to-16" id="markdown-toc-upgrading-from-spark-sql-15-to-16">从Spark SQL 1.5升级到1.6</a></li>
  <li><a href="#upgrading-from-spark-sql-14-to-15" id="markdown-toc-upgrading-from-spark-sql-14-to-15">从Spark SQL 1.4升级到1.5</a></li>
  <li><a href="#upgrading-from-spark-sql-13-to-14" id="markdown-toc-upgrading-from-spark-sql-13-to-14">从Spark SQL 1.3升级到1.4</a>    <ul>
      <li><a href="#dataframe-data-readerwriter-interface" id="markdown-toc-dataframe-data-readerwriter-interface">DataFrame数据读取器/写入器接口</a></li>
      <li><a href="#dataframegroupby-retains-grouping-columns" id="markdown-toc-dataframegroupby-retains-grouping-columns">DataFrame.groupBy保留分组列</a></li>
      <li><a href="#behavior-change-on-dataframewithcolumn" id="markdown-toc-behavior-change-on-dataframewithcolumn">在DataFrame.withColumn上的行为更改</a></li>
    </ul>
  </li>
  <li><a href="#upgrading-from-spark-sql-10-12-to-13" id="markdown-toc-upgrading-from-spark-sql-10-12-to-13">从Spark SQL 1.0-1.2升级到1.3</a>    <ul>
      <li><a href="#rename-of-schemardd-to-dataframe" id="markdown-toc-rename-of-schemardd-to-dataframe">将SchemaRDD重命名为DataFrame</a></li>
      <li><a href="#unification-of-the-java-and-scala-apis" id="markdown-toc-unification-of-the-java-and-scala-apis">Java和Scala API的统一</a></li>
      <li><a href="#isolation-of-implicit-conversions-and-removal-of-dsl-package-scala-only" id="markdown-toc-isolation-of-implicit-conversions-and-removal-of-dsl-package-scala-only">隐式转换的隔离和dsl软件包的删除（仅Scala）</a></li>
      <li><a href="#removal-of-the-type-aliases-in-orgapachesparksql-for-datatype-scala-only" id="markdown-toc-removal-of-the-type-aliases-in-orgapachesparksql-for-datatype-scala-only">删除org.apache.spark.sql中用于DataType的类型别名（仅限Scala）</a></li>
      <li><a href="#udf-registration-moved-to-sqlcontextudf-java--scala" id="markdown-toc-udf-registration-moved-to-sqlcontextudf-java--scala">UDF注册已移至<code>sqlContext.udf</code> （Java和Scala）</a></li>
      <li><a href="#python-datatypes-no-longer-singletons" id="markdown-toc-python-datatypes-no-longer-singletons">Python数据类型不再单例</a></li>
    </ul>
  </li>
</ul>

<h2 id="upgrading-from-spark-sql-24-to-241">从Spark SQL 2.4升级到2.4.1</h2>

<ul>
  <li>的价值<code>spark.executor.heartbeatInterval</code>在未指定单位（例如“ 30”而不是“ 30s”）时，在Spark 2.4.0中代码的不同部分不一致地解释为秒和毫秒。现在，无单位值始终被解释为毫秒。设置为“ 30”的值的应用程序现在需要以“ 30s”为单位指定一个值，以避免被解释为毫秒。否则，产生的极短间隔可能会导致应用程序失败。</li>
</ul>

<h2 id="upgrading-from-spark-sql-23-to-24">从Spark SQL 2.3升级到2.4</h2>

<ul>
  <li>在Spark 2.3及更早版本中，array_contains函数的第二个参数隐式提升为第一个数组类型参数的元素类型。这种类型的促销可能会造成损失，并可能导致<code>array_contains</code>函数返回错误的结果。通过使用更安全的类型升级机制，已在2.4中解决了此问题。这可能会导致某些行为变化，如下表所示。</li>
</ul>
<table class="table">
        <tbody><tr>
          <th>
            <b>询问</b>
          </th>
          <th>
            <b>结果Spark 2.3或更低版本</b>
          </th>
          <th>
            <b>结果星火2.4</b>
          </th>
          <th>
            <b>备注</b>
          </th>
        </tr>
        <tr>
          <th>
            <b>选择<br>array_contains（array（1），1.34D）;</b>
          </th>
          <th>
            <b>真正</b>
          </th>
          <th>
            <b>假</b>
          </th>
          <th>
            <b>在Spark 2.4中，left和right参数分别提升为array（double）和double类型。</b>
          </th>
        </tr>
        <tr>
          <th>
            <b>选择<br>array_contains（array（1），'1'）;</b>
          </th>
          <th>
            <b>真正</b>
          </th>
          <th>
            <b>由于无法以无损方式将整数类型提升为字符串类型，因此引发AnalysisException。</b>
          </th>
          <th>
            <b>用户可以使用显式强制转换</b>
          </th>
        </tr>
        <tr>
          <th>
            <b>选择<br>array_contains（array（1），'anystring'）;</b>
          </th>
          <th>
            <b>空值</b>
          </th>
          <th>
            <b>由于无法以无损方式将整数类型提升为字符串类型，因此引发AnalysisException。</b>
          </th>
          <th>
            <b>用户可以使用显式强制转换</b>
          </th>
        </tr>
  </tbody></table>

<ul>
  <li>
    <p>从Spark 2.4开始，当子查询之前IN运算符前面有一个struct字段时，内部查询也必须包含一个struct字段。相反，在以前的版本中，将结构的字段与内部查询的输出进行了比较。例如。如果<code>a</code>是一个<code>struct(a string, b int)</code> ，在Spark 2.4中<code>a in (select (1 as a, 'a' as b) from range(1))</code>是有效的查询，而<code>a in (select 1, 'a' from range(1))</code>不是。在以前的版本中则相反。</p>
  </li>
  <li>
    <p>在2.2.1+和2.3版本中，如果<code>spark.sql.caseSensitive</code>设置为true，则<code>CURRENT_DATE</code>和<code>CURRENT_TIMESTAMP</code>函数不正确地区分大小写，并且会解析为列（除非以小写形式键入）。在Spark 2.4中，此问题已得到修复，功能不再区分大小写。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，Spark将按照SQL标准遵循优先级规则来评估查询中引用的设置操作。如果括号中未指定顺序，则设置操作从左到右执行，除了所有INTERSECT操作在所有UNION，EXCEPT或MINUS操作之前执行。在所有新增的设置操作中赋予相同优先级的旧行为保留在新添加的配置下<code>spark.sql.legacy.setopsPrecedence.enabled</code>默认值为<code>false</code> 。当此属性设置为<code>true</code> ，由于没有使用括号强制执行任何显式排序，因此spark将在查询中从左到右评估集合运算符。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，当值是1970年1月1日时，Spark将表描述列的Last Access值显示为UNKNOWN。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，默认情况下，Spark会最大化使用矢量化ORC阅读器处理ORC文件。要做到这一点， <code>spark.sql.orc.impl</code>和<code>spark.sql.orc.filterPushdown</code>将其默认值更改为<code>native</code>和<code>true</code>分别。一些旧的Apache Hive版本无法读取由本机ORC编写器创建的ORC文件。使用<code>spark.sql.orc.impl=hive</code>创建与Hive 2.1.1和更早版本共享的文件。</p>
  </li>
  <li>
    <p>在PySpark中，启用箭头优化后，以前<code>toPandas</code>当无法使用Arrow优化时失败，而<code>createDataFrame</code>来自Pandas DataFrame的支持允许回退到非优化。现在，两者<code>toPandas</code>和<code>createDataFrame</code>来自Pandas DataFrame的默认情况下允许后备，可以通过以下方式将其关闭<code>spark.sql.execution.arrow.fallback.enabled</code> 。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，将空数据帧写入目录将启动至少一个写入任务，即使该数据帧实际上没有分区。这引入了一个小的行为更改，对于自描述文件格式（例如Parquet和Orc），Spark在写入0分区数据帧时在目标目录中创建仅元数据文件，因此，如果用户以后读取该目录，则模式推断仍然可以工作。关于写入空数据框，新行为更合理，更一致。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，UDF参数中的表达式ID不会出现在列名中。例如，Spark 2.4中的列名称不是<code>UDF:f(col0 AS colA#28)</code>但<code>UDF:f(col0 AS `colA`)</code> 。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，不允许使用任何文件格式（镶木地板，orc，json，文本，csv等）编写具有空或嵌套空模式的数据框。尝试使用空模式写入数据帧时引发异常。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，在将双方都提升为TIMESTAMP之后，Spark会将DATE类型与TIMESTAMP类型进行比较。设置<code>false</code>至<code>spark.sql.legacy.compareDateTimestampInTimestamp</code>恢复以前的行为。此选项将在Spark 3.0中删除。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，不允许创建具有非空位置的托管表。尝试创建具有非空位置的托管表时将引发异常。设置<code>true</code>至<code>spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation</code>恢复以前的行为。此选项将在Spark 3.0中删除。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，不允许将托管表重命名为现有位置。尝试将托管表重命名到现有位置时将引发异常。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，类型强制规则可以自动将可变参数SQL函数的参数类型（例如IN / COALESCE）提升为最广泛的通用类型，而不管输入参数的排序如何。在以前的Spark版本中，升级可能会按某些特定顺序失败（例如，TimestampType，IntegerType和StringType）并引发异常。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，除了传统的缓存失效机制之外，Spark还启用了非级联SQL缓存失效。非级联高速缓存失效机制使用户可以删除高速缓存，而不会影响其从属高速缓存。此新的缓存失效机制用于要删除的缓存数据仍然有效的场景，例如，在数据集上调用unpersist（）或删除临时视图。这使用户可以释放内存并同时保持所需的缓存有效。</p>
  </li>
  <li>
    <p>在2.3及更早版本中，Spark默认会转换Parquet Hive表，但会忽略表属性，例如<code>TBLPROPERTIES (parquet.compression 'NONE')</code> 。对于ORC Hive表属性，例如<code>TBLPROPERTIES (orc.compress 'NONE')</code>的情况下<code>spark.sql.hive.convertMetastoreOrc=true</code>也一样从Spark 2.4开始，Spark在转换Parquet / ORC Hive表时会遵循Parquet / ORC特定的表属性。举个例子， <code>CREATE TABLE t(id int) STORED AS PARQUET TBLPROPERTIES (parquet.compression 'NONE')</code>在Spark 2.3中插入时会生成Snappy实木复合地板文件，而在Spark 2.4中，结果将是未压缩的实木复合地板文件。</p>
  </li>
  <li>
    <p>从Spark 2.0开始，Spark默认会转换Parquet Hive表以获得更好的性能。从Spark 2.4开始，Spark也会默认转换ORC Hive表。这意味着默认情况下，Spark使用其自己的ORC支持而不是Hive SerDe。举个例子， <code>CREATE TABLE t(id int) STORED AS ORC</code>将在Spark 2.3中使用Hive SerDe进行处理，在Spark 2.4中将其转换为Spark的ORC数据源表，并应用ORC向量化。设置<code>false</code>至<code>spark.sql.hive.convertMetastoreOrc</code>恢复以前的行为。</p>
  </li>
  <li>
    <p>在2.3版及更早版本中，如果行中的至少一个列值格式错误，则认为CSV行格式错误。CSV解析器在DROPMALFORMED模式下丢弃了这样的行，或者在FAILFAST模式下输出了错误。从Spark 2.4开始，仅当CSV行包含从CSV数据源请求的格式错误的列值时，才将其视为格式错误的其他值。例如，CSV文件包含“ id，name”标头和一行“ 1234”。在Spark 2.4中，对id列的选择由具有一个列值1234的行组成，但在Spark 2.3及更早版本中，在DROPMALFORMED模式下为空。要恢复以前的行为，请设置<code>spark.sql.csv.parser.columnPruning.enabled</code>至<code>false</code> 。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，默认情况下将并行进行计算统计信息的文件列表。可以通过设置禁用<code>spark.sql.statistics.parallelFileListingInStatsComputation.enabled</code>至<code>False</code> 。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，在统计信息计算过程中计算表大小时，元数据文件（例如Parquet摘要文件）和临时文件不计为数据文件。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，将空字符串保存为带引号的空字符串<code>""</code> 。在版本2.3和更早版本中，空字符串等于<code>null</code>值，并且不反映保存的CSV文件中的任何字符。例如， <code>"a", null, "", 1</code>被写成<code>a,,,1</code> 。从Spark 2.4开始，同一行另存为<code>a,,"",1</code> 。要恢复以前的行为，请设置CSV选项<code>emptyValue</code>清空（不加引号）字符串。</p>
  </li>
  <li>
    <p>从Spark 2.4开始，LOAD DATA命令支持通配符<code>?</code>和<code>*</code> ，分别与任何一个字符和零个或多个字符匹配。例： <code>LOAD DATA INPATH '/tmp/folder*/'</code>要么<code>LOAD DATA INPATH '/tmp/part-?'</code> 。特殊字符，例如<code>space</code>现在也可以在路径中工作。例： <code>LOAD DATA INPATH '/tmp/folder name/'</code> 。</p>
  </li>
  <li>
    <p>在Spark 2.3及更早版本中，没有GROUP BY的HAVING被视为WHERE。这意味着， <code>SELECT 1 FROM range(10) HAVING true</code>被执行为<code>SELECT 1 FROM range(10) WHERE true</code>并返回10行。这违反了SQL标准，并已在Spark 2.4中修复。从Spark 2.4开始，没有GROUP BY的HAVING被视为全局集合，这意味着<code>SELECT 1 FROM range(10) HAVING true</code>将仅返回一行。要恢复以前的行为，请设置<code>spark.sql.legacy.parser.havingWithoutGroupByAsWhere</code>至<code>true</code> 。</p>
  </li>
  <li>
    <p>在2.3版及更早版本中，从Parquet数据源表中读取数据时，无论Hive Metastore模式和Parquet模式中的列名使用不同的字母大小写，Spark始终返回null <code>spark.sql.caseSensitive</code>被设定为<code>true</code>要么<code>false</code> 。从2.4开始<code>spark.sql.caseSensitive</code>被设定为<code>false</code> ，Spark在Hive Metastore模式和Parquet模式之间执行不区分大小写的列名解析，因此即使列名大小写不同，Spark也会返回相应的列值。如果存在歧义，即匹配多个Parquet列，则会引发异常。此更改也适用于Parquet Hive表，当<code>spark.sql.hive.convertMetastoreParquet</code>被设定为<code>true</code> 。</p>
  </li>
</ul>

<h2 id="upgrading-from-spark-sql-230-to-231-and-above">从Spark SQL 2.3.0升级到2.3.1及更高版本</h2>

<ul>
  <li>从2.3.1版开始，箭头功能包括<code>pandas_udf</code>和<code>toPandas()</code> / <code>createDataFrame()</code>与<code>spark.sql.execution.arrow.enabled</code>调成<code>True</code> ，已标记为实验性。这些仍在发展中，目前不建议在生产中使用。</li>
</ul>

<h2 id="upgrading-from-spark-sql-22-to-23">从Spark SQL 2.2升级到2.3</h2>

<ul>
  <li>
    <p>从Spark 2.3开始，当引用的列仅包含内部损坏的记录列（命名为）时，不允许从原始JSON / CSV文件进行查询<code>_corrupt_record</code>默认）。例如， <code>spark.read.schema(schema).json(file).filter($"_corrupt_record".isNotNull).count()</code>和<code>spark.read.schema(schema).json(file).select("_corrupt_record").show()</code> 。相反，您可以缓存或保存已解析的结果，然后发送相同的查询。例如， <code>val df = spark.read.schema(schema).json(file).cache()</code>然后<code>df.filter($"_corrupt_record".isNotNull).count()</code> 。</p>
  </li>
  <li>
    <p>的<code>percentile_approx</code>函数先前接受的数字类型输入和输出双精度类型结果。现在，它支持日期类型，时间戳类型和数字类型作为输入类型。结果类型也更改为与输入类型相同，这对于百分位数而言更为合理。</p>
  </li>
  <li>
    <p>从Spark 2.3开始，如果可能，在第一个非确定性谓词之后的Join / Filter的确定性谓词也会被下推/通过子运算符。在以前的Spark版本中，这些过滤器不适合进行谓词下推。</p>
  </li>
  <li>分区列推断先前为不同的推断类型发现了不正确的通用类型，例如，先前它以double type作为double type和date type的通用类型而结束。现在，它会为此类冲突找到正确的通用类型。解决冲突的方法如下表所示：<table class="table">
  <tbody><tr>
    <th>
      <b>输入A \输入B</b>
    </th>
    <th>
      <b>空类型</b>
    </th>
    <th>
      <b>整数类型</b>
    </th>
    <th>
      <b>长型</b>
    </th>
    <th>
      <b>DecimalType（38,0）*</b>
    </th>
    <th>
      <b>双重类型</b>
    </th>
    <th>
      <b>日期类型</b>
    </th>
    <th>
      <b>时间戳类型</b>
    </th>
    <th>
      <b>StringType</b>
    </th>
  </tr>
  <tr>
    <td>
      <b>空类型</b>
    </td>
    <td>空类型</td>
    <td>整数类型</td>
    <td>长型</td>
    <td>DecimalType（38,0）</td>
    <td>双重类型</td>
    <td>日期类型</td>
    <td>时间戳类型</td>
    <td>StringType</td>
  </tr>
  <tr>
    <td>
      <b>整数类型</b>
    </td>
    <td>整数类型</td>
    <td>整数类型</td>
    <td>长型</td>
    <td>DecimalType（38,0）</td>
    <td>双重类型</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
  </tr>
  <tr>
    <td>
      <b>长型</b>
    </td>
    <td>长型</td>
    <td>长型</td>
    <td>长型</td>
    <td>DecimalType（38,0）</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
  </tr>
  <tr>
    <td>
      <b>DecimalType（38,0）*</b>
    </td>
    <td>DecimalType（38,0）</td>
    <td>DecimalType（38,0）</td>
    <td>DecimalType（38,0）</td>
    <td>DecimalType（38,0）</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
  </tr>
  <tr>
    <td>
      <b>双重类型</b>
    </td>
    <td>双重类型</td>
    <td>双重类型</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>双重类型</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
  </tr>
  <tr>
    <td>
      <b>日期类型</b>
    </td>
    <td>日期类型</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>日期类型</td>
    <td>时间戳类型</td>
    <td>StringType</td>
  </tr>
  <tr>
    <td>
      <b>时间戳类型</b>
    </td>
    <td>时间戳类型</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>时间戳类型</td>
    <td>时间戳类型</td>
    <td>StringType</td>
  </tr>
  <tr>
    <td>
      <b>StringType</b>
    </td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
    <td>StringType</td>
  </tr>
</tbody></table>

    <p>请注意，对于<b>DecimalType（38,0）*</b> ，上面的表有意不涵盖小数位数和精度的所有其他组合，因为当前我们仅推断像<code>BigInteger</code> / <code>BigInt</code> 。例如，将1.1推断为double类型。</p>
  </li>
  <li>
    <p>在PySpark中，如果要使用与Pandas相关的功能，例如Pandas 0.19.2或更高版本， <code>toPandas</code> ， <code>createDataFrame</code>来自Pandas DataFrame等</p>
  </li>
  <li>
    <p>在PySpark中，已将Pandas相关功能的时间戳值行为更改为尊重会话时区。如果要使用旧的行为，则需要设置配置<code>spark.sql.execution.pandas.respectSessionTimeZone</code>至<code>False</code> 。有关详细信息，请参见<a href="https://issues.apache.org/jira/browse/SPARK-22395">SPARK-22395</a> 。</p>
  </li>
  <li>
    <p>在PySpark中， <code>na.fill()</code>要么<code>fillna</code>还接受布尔值，并用布尔值替换空值。在以前的Spark版本中，PySpark只会忽略它并返回原始的Dataset / DataFrame。</p>
  </li>
  <li>
    <p>从Spark 2.3开始，当适用广播哈希联接或广播嵌套循环联接时，我们更喜欢广播在广播提示中显式指定的表。有关详细信息，请参阅“ <a href="sql-performance-tuning.html#broadcast-hint-for-sql-queries">广播提示</a>和<a href="https://issues.apache.org/jira/browse/SPARK-22489">SPARK-22489”部分</a> 。</p>
  </li>
  <li>
    <p>从Spark 2.3开始，当所有输入均为二进制时， <code>functions.concat()</code>返回二进制输出。否则，它将作为字符串返回。在Spark 2.3之前，无论输入类型如何，它始终以字符串形式返回。要保留旧的行为，请设置<code>spark.sql.function.concatBinaryAsString</code>至<code>true</code> 。</p>
  </li>
  <li>
    <p>从Spark 2.3开始，当所有输入均为二进制时，SQL <code>elt()</code>返回二进制输出。否则，它将作为字符串返回。在Spark 2.3之前，无论输入类型如何，它始终以字符串形式返回。要保留旧的行为，请设置<code>spark.sql.function.eltOutputAsString</code>至<code>true</code> 。</p>
  </li>
  <li>
    <p>从Spark 2.3开始，默认情况下，如果不可能使用精确的表示形式，则小数点之间的算术运算将返回舍入值（而不是返回NULL）。这符合SQL ANSI 2011规范以及Hive 2.2（HIVE-15331）中引入的Hive新行为。这涉及以下变化</p>

    <ul>
      <li>
        <p>确定算术运算结果类型的规则已更新。特别是，如果所需的精度/小数位数超出了可用值的范围，则将小数位数减小到6，以防止小数的整数部分被截断。所有算术运算均受更改影响，即。加法（ <code>+</code> ），减法（ <code>-</code> ），乘法（ <code>*</code> ），部门（ <code>/</code> ），余数（ <code>%</code> ）和肯定模块（ <code>pmod</code> ）。</p>
      </li>
      <li>
        <p>SQL操作中使用的文字值将以它们所需的精确度和小数位数转换为DECIMAL。</p>
      </li>
      <li>
        <p>配置<code>spark.sql.decimalOperations.allowPrecisionLoss</code>已经介绍了。默认为<code>true</code> ，表示此处描述的新行为；如果设置为<code>false</code> ，Spark使用以前的规则，即。它不会调整所需的小数位数来表示值，并且如果不可能精确表示该值，则返回NULL。</p>
      </li>
    </ul>
  </li>
  <li>
    <p>在PySpark中， <code>df.replace</code>不允许省略<code>value</code>什么时候<code>to_replace</code>不是字典。先前， <code>value</code>在其他情况下可以省略，并具有<code>None</code>默认情况下，这违反直觉且容易出错。</p>
  </li>
  <li>
    <p>未混淆的子查询的语义尚未通过混乱的行为很好地定义。从Spark 2.3开始，我们使这种令人困惑的情况无效，例如： <code>SELECT v.i from (SELECT i FROM v)</code> ，在这种情况下，Spark会引发分析异常，因为用户不应在子查询中使用限定符。有关更多详细信息，请参见<a href="https://issues.apache.org/jira/browse/SPARK-20690">SPARK-20690</a>和<a href="https://issues.apache.org/jira/browse/SPARK-21335">SPARK-21335</a> 。</p>
  </li>
  <li>创建一个<code>SparkSession</code>与<code>SparkSession.builder.getOrCreate()</code> ，如果存在<code>SparkContext</code> ，构建者正在尝试更新<code>SparkConf</code>现有的<code>SparkContext</code>具有为建造者指定的配置，但是<code>SparkContext</code>被所有人共享<code>SparkSession</code> s，因此我们不应该更新它们。从2.3开始，构建器开始不更新配置。如果要更新它们，则需要先更新它们，然后再创建<code>SparkSession</code> 。</li>
</ul>

<h2 id="upgrading-from-spark-sql-21-to-22">从Spark SQL 2.1升级到2.2</h2>

<ul>
  <li>
    <p>Spark 2.1.1引入了新的配置密钥： <code>spark.sql.hive.caseSensitiveInferenceMode</code> 。它的默认设置为<code>NEVER_INFER</code> ，其行为与2.1.0相同。但是，Spark 2.2.0将此设置的默认值更改为<code>INFER_AND_SAVE</code>恢复与读取其基础文件架构具有大小写混合的列名称的Hive Metastore表的兼容性。随着<code>INFER_AND_SAVE</code>配置值，在首次访问时，Spark将在尚未为其存储已推断模式的任何Hive Metastore表上执行模式推断。请注意，对于具有数千个分区的表，模式推断可能是非常耗时的操作。如果不考虑与大小写混合的列名称兼容，则可以安全地设置<code>spark.sql.hive.caseSensitiveInferenceMode</code>至<code>NEVER_INFER</code>以避免架构推断的初始开销。请注意，使用新的默认设置<code>INFER_AND_SAVE</code>设置，模式推断的结果将另存为元存储密钥，以备将来使用。因此，初始模式推断仅在表的首次访问时发生。</p>
  </li>
  <li>
    <p>从Spark 2.2.1和2.3.0开始，当数据源表具有分区模式和数据模式中都存在的列时，总是在运行时推断模式。推断的架构没有分区列。在读取表时，Spark会考虑这些重叠列的分区值，而不是存储在数据源文件中的值。在2.2.0和2.1.x版本中，推断的架构已分区，但表的数据对用户不可见（即，结果集为空）。</p>
  </li>
  <li>
    <p>从Spark 2.2开始，视图定义的存储方式与以前的版本不同。这可能会导致Spark无法读取以前版本创建的视图。在这种情况下，您需要使用以下命令重新创建视图<code>ALTER VIEW AS</code>要么<code>CREATE OR REPLACE VIEW AS</code>使用更新的Spark版本。</p>
  </li>
</ul>

<h2 id="upgrading-from-spark-sql-20-to-21">从Spark SQL 2.0升级到2.1</h2>

<ul>
  <li>
    <p>现在，数据源表将分区元数据存储在Hive元存储中。这意味着Hive DDL如<code>ALTER TABLE PARTITION ... SET LOCATION</code>现在可用于使用数据源API创建的表。</p>

    <ul>
      <li>
        <p>可以通过以下方式将旧版数据源表迁移为这种格式： <code>MSCK REPAIR TABLE</code>命令。建议迁移旧表以利用Hive DDL支持和改进的计划性能。</p>
      </li>
      <li>
        <p>要确定表是否已迁移，请查找<code>PartitionProvider: Catalog</code>发行时的属性<code>DESCRIBE FORMATTED</code>在桌子上。</p>
      </li>
    </ul>
  </li>
  <li>
    <p>更改为<code>INSERT OVERWRITE TABLE ... PARTITION ...</code>数据源表的行为。</p>

    <ul>
      <li>
        <p>在以前的Spark版本中<code>INSERT OVERWRITE</code>即使给定了分区规范，也会覆盖整个数据源表。现在仅覆盖符合规范的分区。</p>
      </li>
      <li>
        <p>请注意，这仍然与Hive表的行为不同，后者仅覆盖与新插入的数据重叠的分区。</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="upgrading-from-spark-sql-16-to-20">从Spark SQL 1.6升级到2.0</h2>

<ul>
  <li>
    <p><code>SparkSession</code>现在是Spark的新切入点，取代了旧的<code>SQLContext</code>和</p>

    <p><code>HiveContext</code> 。请注意，保留旧的SQLContext和HiveContext是为了向后兼容。一个新的<code>catalog</code>界面可从以下位置访问<code>SparkSession</code> -有关数据库和表访问的现有API，例如<code>listTables</code> ， <code>createExternalTable</code> ， <code>dropTempView</code> ， <code>cacheTable</code>搬到这里了</p>
  </li>
  <li>
    <p>数据集API和DataFrame API是统一的。在斯卡拉， <code>DataFrame</code>成为其的类型别名<code>Dataset[Row]</code> ，而Java API用户必须替换<code>DataFrame</code>与<code>Dataset<Row></code> 。两种类型的转换（例如， <code>map</code> ， <code>filter</code>和<code>groupByKey</code> ）和无类型的转换（例如， <code>select</code>和<code>groupBy</code> ）在数据集类上可用。由于Python和R中的编译时类型安全不是语言功能，因此数据集的概念不适用于这些语言的API。代替， <code>DataFrame</code>仍然是主要的编程抽象，类似于这些语言中的单节点数据帧概念。</p>
  </li>
  <li>
    <p>数据集和DataFrame API <code>unionAll</code>已不推荐使用，并已替换为<code>union</code></p>
  </li>
  <li>
    <p>数据集和DataFrame API <code>explode</code>已被弃用，或者使用<code>functions.explode()</code>与<code>select</code>要么<code>flatMap</code></p>
  </li>
  <li>
    <p>数据集和DataFrame API <code>registerTempTable</code>已不推荐使用，并已替换为<code>createOrReplaceTempView</code></p>
  </li>
  <li>
    <p>更改为<code>CREATE TABLE ... LOCATION</code> Hive表的行为。</p>

    <ul>
      <li>
        <p>从Spark 2.0开始， <code>CREATE TABLE ... LOCATION</code>相当于<code>CREATE EXTERNAL TABLE ... LOCATION</code>为了防止意外丢失现有数据在用户提供的位置。这意味着，在Spark SQL中创建且用户指定位置的Hive表始终是Hive外部表。删除外部表不会删除数据。不允许用户为Hive托管表指定位置。请注意，这与Hive行为不同。</p>
      </li>
      <li>
        <p>结果是， <code>DROP TABLE</code>这些表上的语句不会删除数据。</p>
      </li>
    </ul>
  </li>
  <li>
    <p><code>spark.sql.parquet.cacheMetadata</code>不再使用。有关详细信息，请参见<a href="https://issues.apache.org/jira/browse/SPARK-13664">SPARK-13664</a> 。</p>
  </li>
</ul>

<h2 id="upgrading-from-spark-sql-15-to-16">从Spark SQL 1.5升级到1.6</h2>

<ul>
  <li>默认情况下，从Spark 1.6开始，Thrift服务器以多会话模式运行。这意味着每个JDBC / ODBC连接都拥有自己的SQL配置和临时功能注册表的副本。缓存的表仍然共享。如果您希望在旧的单会话模式下运行Thrift服务器，请设置选项<code>spark.sql.hive.thriftServer.singleSession</code>至<code>true</code> 。您可以将此选项添加到<code>spark-defaults.conf</code> ，或将其传递给<code>start-thriftserver.sh</code>通过<code>--conf</code> ：</li>
</ul>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>   ./sbin/start-thriftserver.sh <span class="se">\</span>
     --conf spark.sql.hive.thriftServer.singleSession<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
     ...
   </code></pre></figure>

<ul>
  <li>
    <p>从1.6.1开始，sparkR中的withColumn方法支持向新列添加或替换具有相同DataFrame名称的现有列。</p>
  </li>
  <li>
    <p>从Spark 1.6开始，LongType转换为TimestampType的时间期望为秒而不是微秒。进行此更改以使Hive 1.2的行为与从数字类型到TimestampType的更一致的类型转换匹配。有关详细信息，请参见<a href="https://issues.apache.org/jira/browse/SPARK-11724">SPARK-11724</a> 。</p>
  </li>
</ul>

<h2 id="upgrading-from-spark-sql-14-to-15">从Spark SQL 1.4升级到1.5</h2>

<ul>
  <li>
    <p>现在默认启用使用手动管理的内存（Tungsten）的优化执行，并生成用于表达式评估的代码。这些功能都可以通过设置禁用<code>spark.sql.tungsten.enabled</code>至<code>false</code> 。</p>
  </li>
  <li>
    <p>默认情况下，不再启用Parquet模式合并。可以通过设置重新启用<code>spark.sql.parquet.mergeSchema</code>至<code>true</code> 。</p>
  </li>
  <li>
    <p>python中将字符串解析为列现在支持使用点（ <code>.</code> ）以限定列或访问嵌套值。例如<code>df['table.column.nestedField']</code> 。但是，这意味着，如果您的列名包含任何点，那么您现在必须使用反引号将它们转义（例如， <code>table.`column.with.dots`.nested</code> ）。</p>
  </li>
  <li>
    <p>默认情况下，内存中列式存储分区修剪功能处于打开状态。可以通过设置禁用<code>spark.sql.inMemoryColumnarStorage.partitionPruning</code>至<code>false</code> 。</p>
  </li>
  <li>
    <p>不再支持无限制的精度十进制列，相反，Spark SQL强制将最大精度设为38。从中推断架构时<code>BigDecimal</code>对象，现在使用的精度为（38，18）。如果在DDL中未指定精度，则保留默认值<code>Decimal(10, 0)</code> 。</p>
  </li>
  <li>
    <p>时间戳现在以1us而不是1ns的精度存储</p>
  </li>
  <li>
    <p>在里面<code>sql</code>方言，浮点数现在解析为十进制。HiveQL解析保持不变。</p>
  </li>
  <li>
    <p>现在，SQL / DataFrame函数的规范名称为小写（例如，sum与SUM）。</p>
  </li>
  <li>
    <p>JSON数据源不会自动加载其他应用程序创建的新文件（即未通过Spark SQL插入到数据集中的文件）。对于JSON持久性表（即表的元数据存储在Hive Metastore中），用户可以使用<code>REFRESH TABLE</code> SQL命令或<code>HiveContext</code>的<code>refreshTable</code>方法，以将那些新文件包括到表中。对于表示JSON数据集的DataFrame，用户需要重新创建DataFrame，新的DataFrame将包含新文件。</p>
  </li>
  <li>
    <p>pySpark中的DataFrame.withColumn方法支持添加新列或替换具有相同名称的现有列。</p>
  </li>
</ul>

<h2 id="upgrading-from-spark-sql-13-to-14">从Spark SQL 1.3升级到1.4</h2>

<h4 id="dataframe-data-readerwriter-interface">DataFrame数据读取器/写入器接口</h4>

<p>根据用户的反馈，我们创建了一个新的，更流畅的API来读取（ <code>SQLContext.read</code> ）并将数据写出（ <code>DataFrame.write</code> ），并弃用了旧版API（例如， <code>SQLContext.parquetFile</code> ， <code>SQLContext.jsonFile</code> ）。</p>

<p>请参阅API文档以获取<code>SQLContext.read</code> （ <a href="api/scala/index.html#org.apache.spark.sql.SQLContext@read:DataFrameReader">Scala</a> ， <a href="api/java/org/apache/spark/sql/SQLContext.html#read()">Java</a> ， <a href="api/python/pyspark.sql.html#pyspark.sql.SQLContext.read">Python</a> ）和<code>DataFrame.write</code> （ <a href="api/scala/index.html#org.apache.spark.sql.DataFrame@write:DataFrameWriter">Scala</a> ， <a href="api/java/org/apache/spark/sql/Dataset.html#write()">Java</a> ， <a href="api/python/pyspark.sql.html#pyspark.sql.DataFrame.write">Python</a> ）更多信息。</p>

<h4 id="dataframegroupby-retains-grouping-columns">DataFrame.groupBy保留分组列</h4>

<p>根据用户反馈，我们更改了<code>DataFrame.groupBy().agg()</code>在结果中保留分组列<code>DataFrame</code> 。要使行为保持在1.3中，请设置<code>spark.sql.retainGroupColumns</code>至<code>false</code> 。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="c1">// In 1.3.x, in order for the grouping column &quot;department&quot; to show up,</span>
<span class="c1">// it must be included explicitly as part of the agg function call.</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&quot;department&quot;</span><span class="o">).</span><span class="n">agg</span><span class="o">(</span><span class="n">$</span><span class="s">&quot;department&quot;</span><span class="o">,</span> <span class="n">max</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">),</span> <span class="n">sum</span><span class="o">(</span><span class="s">&quot;expense&quot;</span><span class="o">))</span>

<span class="c1">// In 1.4+, grouping column &quot;department&quot; is included automatically.</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="s">&quot;department&quot;</span><span class="o">).</span><span class="n">agg</span><span class="o">(</span><span class="n">max</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">),</span> <span class="n">sum</span><span class="o">(</span><span class="s">&quot;expense&quot;</span><span class="o">))</span>

<span class="c1">// Revert to 1.3 behavior (not retaining grouping column) by:</span>
<span class="n">sqlContext</span><span class="o">.</span><span class="n">setConf</span><span class="o">(</span><span class="s">&quot;spark.sql.retainGroupColumns&quot;</span><span class="o">,</span> <span class="s">&quot;false&quot;</span><span class="o">)</span></code></pre></figure>

  </div>

<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="c1">// In 1.3.x, in order for the grouping column &quot;department&quot; to show up,</span>
<span class="c1">// it must be included explicitly as part of the agg function call.</span>
<span class="n">df</span><span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="s">&quot;department&quot;</span><span class="o">).</span><span class="na">agg</span><span class="o">(</span><span class="n">col</span><span class="o">(</span><span class="s">&quot;department&quot;</span><span class="o">),</span> <span class="n">max</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">),</span> <span class="n">sum</span><span class="o">(</span><span class="s">&quot;expense&quot;</span><span class="o">));</span>

<span class="c1">// In 1.4+, grouping column &quot;department&quot; is included automatically.</span>
<span class="n">df</span><span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="s">&quot;department&quot;</span><span class="o">).</span><span class="na">agg</span><span class="o">(</span><span class="n">max</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">),</span> <span class="n">sum</span><span class="o">(</span><span class="s">&quot;expense&quot;</span><span class="o">));</span>

<span class="c1">// Revert to 1.3 behavior (not retaining grouping column) by:</span>
<span class="n">sqlContext</span><span class="o">.</span><span class="na">setConf</span><span class="o">(</span><span class="s">&quot;spark.sql.retainGroupColumns&quot;</span><span class="o">,</span> <span class="s">&quot;false&quot;</span><span class="o">);</span></code></pre></figure>

  </div>

<div data-lang="python">

    <figure class="highlight"><pre><code class="language-python" data-lang="python"><span></span><span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">as</span> <span class="nn">func</span>

<span class="c1"># In 1.3.x, in order for the grouping column &quot;department&quot; to show up,</span>
<span class="c1"># it must be included explicitly as part of the agg function call.</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;department&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;department&quot;</span><span class="p">],</span> <span class="n">func</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">),</span> <span class="n">func</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;expense&quot;</span><span class="p">))</span>

<span class="c1"># In 1.4+, grouping column &quot;department&quot; is included automatically.</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;department&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">),</span> <span class="n">func</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;expense&quot;</span><span class="p">))</span>

<span class="c1"># Revert to 1.3.x behavior (not retaining grouping column) by:</span>
<span class="n">sqlContext</span><span class="o">.</span><span class="n">setConf</span><span class="p">(</span><span class="s2">&quot;spark.sql.retainGroupColumns&quot;</span><span class="p">,</span> <span class="s2">&quot;false&quot;</span><span class="p">)</span></code></pre></figure>

  </div>

</div>

<h4 id="behavior-change-on-dataframewithcolumn">在DataFrame.withColumn上的行为更改</h4>

<p>在1.4之前的版本中，DataFrame.withColumn（）仅支持添加列。该列将始终以其指定名称作为新列添加到结果DataFrame中，即使可能存在相同名称的任何现有列也是如此。从1.4开始，DataFrame.withColumn（）支持添加名称与所有现有列名称不同的列，或替换具有相同名称的现有列。</p>

<p>请注意，此更改仅适用于Scala API，不适用于PySpark和SparkR。</p>

<h2 id="upgrading-from-spark-sql-10-12-to-13">从Spark SQL 1.0-1.2升级到1.3</h2>

<p>在Spark 1.3中，我们从Spark SQL中删除了“ Alpha”标签，并以此清理了可用的API。从Spark 1.3开始，Spark SQL将提供与1.中其他版本的二进制兼容性。X系列。此兼容性保证不包括被明确标记为不稳定的API（即DeveloperAPI或实验性）。</p>

<h4 id="rename-of-schemardd-to-dataframe">将SchemaRDD重命名为DataFrame</h4>

<p>用户升级到Spark SQL 1.3时会注意到的最大变化是<code>SchemaRDD</code>已重命名为<code>DataFrame</code> 。这主要是因为DataFrames不再直接从RDD继承，而是通过其自己的实现提供RDD提供的大多数功能。仍然可以通过调用DataFrame将其转换为RDD。 <code>.rdd</code>方法。</p>

<p>在Scala中，存在来自的类型别名<code>SchemaRDD</code>至<code>DataFrame</code>为某些用例提供源兼容性。仍然建议用户更新其代码以使用<code>DataFrame</code>代替。Java和Python用户将需要更新其代码。</p>

<h4 id="unification-of-the-java-and-scala-apis">Java和Scala API的统一</h4>

<p>在Spark 1.3之前，存在单独的Java兼容类（ <code>JavaSQLContext</code>和<code>JavaSchemaRDD</code> ）的镜像Scala API。在Spark 1.3中，Java API和Scala API已统一。两种语言的用户都应使用<code>SQLContext</code>和<code>DataFrame</code> 。通常，这些类尝试使用两种语言都可以使用的类型（即<code>Array</code>而不是特定语言的集合）。在某些不存在公共类型的情况下（例如，用于传递闭包或Maps），使用函数重载代替。</p>

<p>此外，已删除了Java特定类型的API。Scala和Java的用户都应使用存在于<code>org.apache.spark.sql.types</code>以编程方式描述架构。</p>

<h4 id="isolation-of-implicit-conversions-and-removal-of-dsl-package-scala-only">隐式转换的隔离和dsl软件包的删除（仅Scala）</h4>

<p>Spark 1.3之前的许多代码示例均始于<code>import sqlContext._</code> ，这将sqlContext的所有功能引入了作用域。在Spark 1.3中，我们隔离了用于转换的隐式转换<code>RDD</code>入<code>DataFrame</code>进入对象内部的对象<code>SQLContext</code> 。用户现在应该写<code>import sqlContext.implicits._</code> 。</p>

<p>此外，隐式转换现在仅会增加由以下内容组成的RDD： <code>Product</code> s（即案例类或元组）的方法<code>toDF</code> ，而不是自动应用。</p>

<p>在DSL内部使用功能时（现已替换为<code>DataFrame</code> API）用户用来导入<code>org.apache.spark.sql.catalyst.dsl</code> 。相反，应该使用公共数据框函数API： <code>import org.apache.spark.sql.functions._</code> 。</p>

<h4 id="removal-of-the-type-aliases-in-orgapachesparksql-for-datatype-scala-only">删除org.apache.spark.sql中用于DataType的类型别名（仅限Scala）</h4>

<p>Spark 1.3删除了以下基本sql包中存在的类型别名<code>DataType</code> 。用户应改为将类导入<code>org.apache.spark.sql.types</code></p>

<h4 id="udf-registration-moved-to-sqlcontextudf-java--scala">UDF注册已移至<code>sqlContext.udf</code> （Java和Scala）</h4>

<p>用于注册UDF的功能（已在DataFrame DSL或SQL中使用）已移至的udf对象中。 <code>SQLContext</code> 。</p>

<div class="codetabs">
<div data-lang="scala">

    <figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span></span><span class="n">sqlContext</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">register</span><span class="o">(</span><span class="s">&quot;strLen&quot;</span><span class="o">,</span> <span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="o">.</span><span class="n">length</span><span class="o">())</span></code></pre></figure>

  </div>

<div data-lang="java">

    <figure class="highlight"><pre><code class="language-java" data-lang="java"><span></span><span class="n">sqlContext</span><span class="o">.</span><span class="na">udf</span><span class="o">().</span><span class="na">register</span><span class="o">(</span><span class="s">&quot;strLen&quot;</span><span class="o">,</span> <span class="o">(</span><span class="n">String</span> <span class="n">s</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">s</span><span class="o">.</span><span class="na">length</span><span class="o">(),</span> <span class="n">DataTypes</span><span class="o">.</span><span class="na">IntegerType</span><span class="o">);</span></code></pre></figure>

  </div>

</div>

<p>Python UDF注册保持不变。</p>

<h4 id="python-datatypes-no-longer-singletons">Python数据类型不再单例</h4>

<p>在Python中使用DataType时，您需要构造它们（即<code>StringType()</code> ），而不是引用单例。</p>


                </div>
            
             <!-- /container -->
        </div>

        <script src="js/vendor/jquery-1.12.4.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    

</body></html>